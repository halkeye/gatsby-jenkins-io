{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/event/page/4",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2017-07-27T00:00:00.000Z","id":"92796c2d-9e8b-530e-82dd-441263269c06","slug":"/blog/2017/07/27/standardizing-builds-with-shared-libraries/","strippedHtml":"This is a guest post by Alvin Huang, DevOps Engineer at\nFireEye.\n\nAs a security company, FireEye relentlessly protects our customers from cyber attacks. To act\nquickly on intelligence and expertise learned, the feedback loop from the front lines to features\nand capabilities in software must be small. Jenkins helps us achieve this by allowing us to build,\ntest, and deploy to our hardware and software platforms faster, so we can stop the bad guys\nbefore they reach our customers.\n\nMore capabilities and functionalities in our product offerings means more applications and\nsystems, which means more software builds and jobs in Jenkins. Within the FaaS (FireEye as a\nService) organization, the tens of Jenkins jobs that were manageable manually in the web GUI\nquickly grew to hundreds of jobs that required more automation. Along the way, we outgrew\nour old legacy datacenter and were tasked with migrating 150+ Freestyle jobs on an old 1.x\nJenkins instance to a newer 2.x instance in the new datacenter in 60 days.\n\nCopying Freestyle job XML configuration files to the new server would leave\ntechnical debt.  Using Freestyle job templates would be better but for\ncomplicated jobs that require multiple templates, this would still create large\ndependency chains that would be hard to trace in the log output. Finally,\ndevelopers were not excited about having to replicate global changes, such as\nadd an email recipient when a new member joins the team, across tens of jobs\nmanually or using the\nConfiguration\nSlicer. We needed a way to migrate the jobs in a timely fashion while getting\nrid of as much technical debt as possible.\n\nJenkins Pipeline to the rescue! In 2.0, Jenkins added the capability to create pipelines as first-\nclass entities. At FireEye, we leveraged many of the features available in pipeline to aid in the\nmigration process including the ability to:\n\ncreate Pipeline as Code in a Jenkinsfile stored in SCM\n\ncreate Jenkins projects automatically when new branches or repos get added with a Jenkinsfile\n\ncontinue jobs after the Jenkins controller or build agent crashes\n\nand most importantly, build a Pipeline\nShared Library that keeps projects\nDRY and\nallows new applications to be on boarded into Jenkins within seconds\n\nHowever, Jenkins Pipeline came with a DSL that our users would have to learn to translate their\nFreestyle jobs to pipeline jobs. This would be a significant undertaking across multiple teams\njust to create Jenkins jobs. Instead, the DevOps team identified similarities across all the\nFreestyle jobs that we were migrating, learned the Jenkins DSL to become SMEs for the\norganization, and built a shared library of functions and wrappers that saved each Dev/QA\nengineer hours of time.\n\nBelow is an example function we created to promote builds in Artifactory:\n\nvars/promoteBuild.groovy\n\ndef call(source_repo, target_repo, build_name, build_number) {\n    stage('Promote to Production repo') {\n        milestone label: 'promote to production'\n        input 'Promote this build to Production?'\n\n        node {\n            Artifactory.server(getArtifactoryServerID()).promote([\n                'buildName'   : build_name,\n                'buildNumber' : build_number,\n                'targetRepo'  : target_repo,\n                'sourceRepo'  : source_repo,\n                'copy'        : true,\n            ])\n    }\n}\n\ndef call(source_repo, target_repo) {\n    buildInfo = getBuildInfo()\n\n    call(source_repo, target_repo, buildInfo.name, buildInfo.number)\n}\n\nRather than learning the Jenkins DSL and looking up how the Artifactory Plugin worked in\nPipeline, users could easily call this function and pass it parameters to do the promotion work\nfor them. In the Shared Library, we can also create build wrappers of opinionated workflows,\nthat encompasses multiple functions, based on a set of parameters defined in the Jenkinsfile.\nIn addition to migrating the jobs, we also had to migrate the build agents. No one knew the\nexact list of packages, versions, and build tools installed on each build server, so rebuilding\nthem would be extremely difficult. Rather than copying the VMs or trying to figure out what\npackages were on the build agents, we opted to use Docker to build containers with all\ndependencies needed for an application.\n\nI hope you will join me at my Jenkins World session:\nCodifying the Build and Release Process with a Jenkins\nPipeline Shared Library, as I deep dive into the inner workings of our Shared\nPipeline Library and explore how we integrated Docker into our CI/CD pipeline.\nCome see how we can turn a Jenkinsfile with just a set of parameters like this:\n\nJenkinsfile\n\nstandardBuild {\n    machine          = 'docker'\n    dev_branch       = 'develop'\n    release_branch   = 'master'\n    artifact_apttern = '*.rpm'\n    html_pattern     = [keepAll: true, reportDir: '.', reportFiles: 'output.html', reportName: 'OutputReport']\n    dev_repo         = 'pipeline-examples-dev'\n    prod_repo        = 'pipeline-examples-prod'\n    pr_script        = 'make prs'\n    dev_script       = 'make dev'\n    release_script   = 'make release'\n}\n\nand a Dockerfile like this:\n\nDockerfile\n\nFROM faas/el7-python:base\n\nRUN yum install -y python-virtualenv \\\n        rpm-build && \\\n        yum clean all\n\nInto a full Jenkins Pipeline like this:\n\nAs we look ahead at FireEye, I will explore how the Shared Library sets us up for easier future\nmigrations of other tools such as Puppet, JIRA, and Artifactory, and easier integration with new\ntools like Openshift. I will also cover our strategies for deployments and plans to move to\nDeclarative Pipeline.\n\nAlvin will be\npresenting\nmore on this topic at\nJenkins World in August,\nregister with the code JWFOSS for a 30% discount off your pass.","title":"Codifying the Build and Release Process with a Pipeline Shared Library","tags":["event","JenkinsWorld"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/author/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2017-07-17T00:00:00.000Z","id":"0116e3f0-03af-5f1b-b064-5d44aaf60398","slug":"/blog/2017/07/17/speaker-blog-care/","strippedHtml":"This is a guest post by Mandy Hubbard, Software Engineer/QA Architect at\nCare.com.\n\nImagine this: It’s 4:30pm on a Friday,\nyou have a major release on Monday, and your Jenkins server goes down.\nIt doesn’t matter if it experienced a hardware failure,\nfell victim to a catastrophic\nfat-finger error,\nor just got hit by a meteor - your Jenkins server is toast.\nHow long did it take to perfect your Pipeline,\nall your Continuous Delivery jobs, plugins, and credentials?\nHopefully you at least have a recent backup of your Jenkins home directory,\nbut you’re still going have to work over the weekend with IT to procure a new server,\ninstall it, and do full regression testing to be up and running by Monday morning.\nGo ahead and take a moment, go to your car and just scream.\nIt will help …​ a little.\n\nBut what if you could have a Jenkins environment that is completely disposable,\none that could be easily rebuilt at any time?\nUsing Docker and Joyent’s\nContainerPilot, the team at\nCare.com HomePay\nhas created a production Jenkins environment that is completely software-defined.\nEverything required to set up a new Jenkins environment is stored in source control,\nversioned, and released just like any other software.\nAt Jenkins World, I’ll do a developer deep-dive into this approach during my technical session,\nIndispensable, Disposable Jenkins,\nincluding a demo of bringing up a fully configured Jenkins server in a Docker container.\nFor now, let me give you a basic outline of what we’ve done.\n\nMandy will be\npresenting\nmore on this topic at\nJenkins World in August,\nregister with the code JWFOSS for a 30% discount off your pass.\n\nFirst, we add ContainerPilot to our Jenkins image by including it in the Dockerfile.\n\nDockerfile\n\n## ContainerPilot\n\nENV CONTAINERPILOT_VERSION 2.7.0\nENV CONTAINERPILOT_SHA256 3cf91aabd3d3651613942d65359be9af0f6a25a1df9ec9bd9ea94d980724ee13\nENV CONTAINERPILOT file:///etc/containerpilot/containerpilot.json\n\nRUN curl -Lso /tmp/containerpilot.tar.gz https://github.com/joyent/containerpilot/releases/download/${CONTAINERPILOT_VERSION}/containerpilot-${CONTAINERPILOT_VERSION}.tar.gz && \\\n    echo \"${CONTAINERPILOT_SHA256}  /tmp/containerpilot.tar.gz\" | sha256sum -c && \\\n    tar zxf /tmp/containerpilot.tar.gz -C /bin && \\\nrm /tmp/containerpilot.tar.gz\n\nThen we specify containerpilot as the Docker command in the docker-compose.yml\nand pass the Jenkins startup script as an argument.\nThis allows ContainerPilot to perform our preStart business before starting the Jenkins server.\n\ndocker-compose.yml\n\njenkins:\n    image: devmandy/auto-jenkins:latest\n    restart: always\n    mem_limit: 8g\n    ports:\n      - 80\n      - 22\n    dns:\n      - 8.8.8.8\n      - 127.0.0.1\n    env_file: _env\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    environment:\n      - CONSUL=consul\n    links:\n      - consul:consul\n    ports:\n      - \"8080:80\"\n      - \"2222:22\"\n    command: >\n      containerpilot\n      /usr/local/bin/jenkins.sh\n\nConfiguration data is read from a Docker Compose _env file,\nas specified in the docker-compose.yml file,\nand stored in environment variables inside the container.\nThis is an example of our _env file:\n\n_env\n\nGITHUB_TOKEN=\nGITHUB_USERNAME=DevMandy\nGITHUB_ORGANIZATION=DevMandy\nDOCKERHUB_ORGANIZATION=DevMandy\nDOCKERHUB_USERNAME=DevMandy\nDOCKERHUB_PASSWORD=\nDOCKER_HOST=\nSLACK_TEAM_DOMAIN=DevMandy\nSLACK_CHANNEL=jenkinsbuilds\nSLACK_TOKEN=\nBASIC_AUTH=\nAD_NAME=\nAD_SERVER=\nPRIVATE_KEY=\n\nJenkins stores its credentials and plugin information in various xml files.\nThe preStart script modifies the relevant files,\nsubstituting the environment variables as appropriate,\nusing a set of command line utilities called xmlstarlet.\nHere is an example method from our preStart script that configures Github credentials:\n\ngithub_credentials_setup() {\n    ## Setting Up Github username in credentials.xml file\n    echo\n    echo -e \"Adding Github username to credentials.xml file for SSH key\"\n    xmlstarlet \\\n        ed \\\n        --inplace \\\n        -u '//com.cloudbees.jenkins.plugins.sshcredentials.impl.BasicSSHUserPrivateKey[id=\"github\"]/username' \\\n        -v ${GITHUB_USERNAME} \\\n        ${JENKINS_HOME}/credentials.xml\n\n    echo -e \"Adding Github username to credentials.xml file for Github token\"\n    xmlstarlet \\\n        ed \\\n         --inplace \\\n        -u '//com.cloudbees.plugins.credentials.impl.UsernamePasswordCredentialsImpl[id=\"github_token\"]/username' \\\n        -v ${GITHUB_USERNAME} \\\n        ${JENKINS_HOME}/credentials.xml\n\n    PASSWORD=${GITHUB_TOKEN}\n    echo -e \"Adding Github token to credentials.xml\"\n    xmlstarlet \\\n        ed \\\n        --inplace \\\n        -u '//com.cloudbees.plugins.credentials.impl.UsernamePasswordCredentialsImpl[id=\"github_token\"]/password' \\\n        -v ${PASSWORD} \\\n        ${JENKINS_HOME}/credentials.xml\n}\n\nThis approach can be used to automate all things Jenkins.\nThese are just a few of the things I’ll show you in my Jenkins World session,\nwhich you can build on to automate anything else your Jenkins environment needs.\n\nCreation of credentials sets for interacting with third party services\nlike Github, Docker Hub and Slack\n\nConfiguration of the Active Directory plugin\nand setup of matrix-based security\n\nConfiguration of the Github Organization plugin,\nwhich results in the automatic creation of all Jenkins pipeline jobs\nby scanning the organization for all repositories containing a Jenkinsfile\n\nConfiguration of the\nDocker Pipeline plugin, including creating templates for all custom build agents\n\nConfiguration of the Global Pipeline Libraries plugin\n\nConfiguration of the Slack Notifier plugin\n\nWith software-defined Jenkins, pipeline infrastructure\ngains the same flexibility and resiliency as the rest of the development pipeline.\nIf we decide to change our Jenkins configuration in any way –\nfor example installing a new plugin or upgrading an existing one,\nadding a new global library, or adding new Docker images for build agents –\nwe simply edit our preStart script to include these changes, build a new Docker image,\nand the Jenkins environment is automatically reconfigured when we start a new container.\nBecause the entire configuration specification lives in a Github repository,\nchanges are merged to the \"master\" branch using pull requests,\nand our Jenkins Docker image is tagged using\nsemantic versioning just like any other component.\nJenkins can be both indispensable and completely disposable at the same time.","title":"Indispensable, Disposable Jenkins","tags":["event","JenkinsWorld"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"","id":"hinman","irc":null,"linkedin":null,"name":"Hannah Inman","slug":"/blog/author/hinman","twitter":null}]}},{"node":{"date":"2017-07-13T00:00:00.000Z","id":"00ad337a-5c86-5032-be3e-f6f4934de80d","slug":"/blog/2017/07/13/speaker-blog-rosetta-stone/","strippedHtml":"This is a guest post by Kevin Burnett, DevOps Lead at\nRosetta Stone.\n\nHave you experienced that thing where you make a change in an app, and when you\ngo to check on the results of the build, you find an error that really doesn’t\nseem relevant to your change? And then you notice that your build is the first\nin over a year. And then you realize that you have accidentally become the\nsubject matter expert in this app.\n\nYou have no clue what change caused this failure or when that change occurred.\nDid one Jenkins agent become a\nsnowflake server,\naccruing cruft on the file system that is not cleaned up before each build?\nDid some unpinned external dependency upgrade in a backwards-incompatible fashion?\nDid the credentials the build plan was using to connect to source control get rotated?\nDid a dependent system go offline?\nOr - and I realize that this is unthinkable - did you legitimately break a test?\n\nNot only is this type of archaeological expedition often a bad time for the\nperson who happened to commit to this app (\"No good deed goes unpunished\"), but\nit’s also unnecessary. There’s a simple way to reduce the cognitive load it\ntakes to connect cause and effect: build more frequently.\n\nOne way we achieve this is by writing scripts to maintain our apps. When we\nbuild, the goal is that an equivalent artifact should be produced unless there\nwas a change to the app in source control. As such, we pin all of our\ndependencies to specific versions. But we also don’t want to languish on old\nversions of dependencies, whether internal or external. So we also have an\nauto-maintain script that bumps all of these versions and commits the result.\n\nI’ll give an example. We use docker to build and deploy our apps, and each app\ndepends on a base image that we host in a docker registry. So a Dockerfile in\none of our apps would have a line like this:\n\nFROM our.registry.example.com/rosettastone/sweet-repo:jenkins-awesome-project-sweet-repo-5\n\nWe build our base images in Jenkins and tag them with the Jenkins $BUILD_TAG,\nso this app is using build 5 of the rosettastone/sweet-repo base image.\nLet’s say we updated our sweet-repo base image to use ubuntu 16.04 instead of 14.04\nand this resulted in build 6 of the base image. Our auto-maintain script takes\ncare of upgrading an app that uses this base image to the most recent version.\nThe steps in the auto-maintain script look like this:\n\nFigure out what base image tag you’re using.\n\nFind the newest version of that base image tag by querying the docker registry.\n\nIf necessary, update the FROM line in the app’s Dockerfile to pull in the most recent version.\n\nWe do the same thing with library dependencies.\nIf our Gemfile.lock is referencing an old library, running auto-maintain will update things.\nThe same applies to the Jenkinsfile for each app. If we decide to implement a new policy where we\ndiscard old builds, we update auto-maintain so that it will bring each app into\ncompliance with the policy, by changing, for example, this Jenkinsfile :\n\nJenkinsfile (Before)\n\npipeline {\n  agent { label 'docker' }\n  stages {\n    stage('commit_stage') {\n      steps {\n        sh('./bin/ci')\n      }\n    }\n  }\n}\n\nto this:\n\nJenkinsfile (After)\n\npipeline {\n  agent { label 'docker' }\n  options {\n    buildDiscarder(logRotator(numToKeepStr: '100'))\n  }\n  stages {\n    stage('commit_stage') {\n      steps {\n        sh('./bin/ci')\n      }\n    }\n  }\n}\n\nWe try to account for these sorts of things (everything that we can) in our\nauto-maintain script rather than updating apps manually, since this reduces the\nfriction in keeping apps standardized.\n\nOnce you create an auto-maintain script (start small), you just have to run it.\nWe run ours based on both \"actions\" and \"non-actions.\" When an internal library\nchanges, we kick off app builds, so a library’s Jenkinsfile might look like\nthis:\n\nJenkinsfile\n\npipeline {\n  agent { label 'docker' }\n  stages {\n    stage('commit_stage') {\n      steps {\n        sh('./bin/ci')\n      }\n    }\n    stage('auto_maintain_things_that_might_be_using_me') {\n      steps {\n        build('hot-project/auto-maintain-all-apps/master')\n      }\n    }\n  }\n}\n\nWhen auto-maintain updates something in an app, we have it commit the change\nback to the app, which in turn triggers a build of that app, and—​if all is\nwell—​a production deployment.\n\nThe only missing link then for avoiding one-year build droughts is to get around\nthe problem where auto-maintain isn’t actually updating anything in a certain app.\nIf no dependencies are changing, or if the technology in question is not\nreceiving much attention, auto-maintain might not do anything for an\nextended period of time, even if the script is run on a schedule using\ncron . For those cases, putting\na cron trigger in the Pipeline for each app will ensure that builds still happen periodically:\n\nJenkinsfile\n\npipeline {\n  agent { label 'docker' }\n  triggers {\n    cron('@weekly')\n  }\n  stages {\n    stage('commit_stage') {\n      steps {\n        sh('./bin/ci')\n      }\n    }\n  }\n}\n\nIn most cases, these periodic builds won’t do anything different from the last\nbuild, but when something does break, this strategy will allow you to decide\nwhen you find out about it (by making your cron @weekly, @daily, etc)\ninstead of letting some poor developer find out about it when they do\nsomething silly like commit code to an infrequently-modified app.\n\nKevin will be\npresenting\nmore on this topic at\nJenkins World in August,\nregister with the code JWFOSS for a 30% discount off your pass.","title":"Automated Software Maintenance","tags":["event","jenkinsworld","jenkinsworld2017"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"","id":"hinman","irc":null,"linkedin":null,"name":"Hannah Inman","slug":"/blog/author/hinman","twitter":null}]}},{"node":{"date":"2017-07-07T00:00:00.000Z","id":"7541af12-10e1-5d79-bcc7-41f0ae0d3010","slug":"/blog/2017/07/07/jenkins-conan/","strippedHtml":"This is a guest post by Luis Martínez de Bartolomé,\nConan Co-Founder\n\nC and C++ are present in very important industries today, including Operating Systems, embedded systems, finances, research, automotive, robotics, gaming, and many more. The main reason for this is performance, which is critical to many of these industries, and cannot be compared to any other technology.\nAs a counterpart, the C/C++ ecosystem has a few important challenges to face:\n\nHuge projects - With millions of lines of code, it’s very hard to manage your projects without using modern tools.\n\nApplication Binary Interface (ABI) incompatibility - To guarantee the compatibility of a library with other libraries and your application,  different configurations (such as the operating system, architecture, and compiler) need to be under control.\n\nSlow compilation times - Due to header inclusion and pre-processor bloat, together with the challenges mentioned above, it requires special attention to optimize the process and rebuild only the libraries that need to be rebuilt.\n\nCode linkage and inlining - A static C/C++ library can embed headers from a dependent library. Also, a shared library can embed a static library. In both cases, you need to manage the rebuild of your library when any of its dependencies change.\n\nVaried ecosystem - There are many different compilers and build systems, for different platforms, targets and purposes.\n\nThis post will show how to implement DevOps best practices for C/C++ development, using Jenkins CI, Conan C/C++ package manager, and JFrog Artifactory the universal artifact repository.\n\nConan, The C/C++ Package Manager\n\nConan was born to mitigate these pains.\n\nConan uses python recipes, describing how to build a library by explicitly calling any build system, and also describing the needed information for the consumers (include directories, library names etc.).\nTo manage the different configurations and the ABI compatibility, Conan uses \"settings\" (os, architecture, compiler…). When a setting is changed, Conan generates a different binary version for the same library:\n\nThe built binaries can be uploaded to JFrog Artifactory or Bintray, to be shared with your team or the whole community. The developers in your team won’t need to rebuild the libraries again, Conan will fetch only the needed Binary packages matching the user’s configuration from the configured remotes (distributed model).\nBut there are still some more challenges to solve:\n\nHow to manage the development and release process of your C/C++ projects?\n\nHow to distribute your C/C++ libraries?\n\nHow to test your C/C++ project?\n\nHow to generate multiple packages for different configurations?\n*How to manage the rebuild of the libraries when one of them changes?\n\nConan Ecosystem\n\nThe Conan ecosystem is growing fast, and DevOps with C/C++ is now a reality:\n\nJFrog Artifactory manages the full development and releasing cycles.\n\nJFrog Bintray is the universal distribution hub.\n\nJenkins automates the project testing, generates different binary configurations of your Conan packages, and automates the rebuilt libraries.\n\nJenkins Artifactory plugin\n\nProvides a Conan DSL, a very generic but powerful way to call Conan from a Jenkins Pipeline script.\n\nManages the remote configuration with your Artifactory instance, hiding the authentication details.\n\nCollects from any Conan operation (installing/uploading packages) all the involved artifacts to generate and publish the buildInfo to Artifactory. The buildInfo object is very useful, for example, to promote the created Conan packages to a different repository and to have full traceability of the Jenkins build:\n\nHere’s an example of the Conan DSL with the Artifactory plugin.  First we configure the Artifactory repository, then retrieve the dependencies and finally build it:\n\ndef artifactory_name = \"artifactory\"\ndef artifactory_repo = \"conan-local\"\ndef repo_url = 'https://github.com/memsharded/example-boost-poco.git'\ndef repo_branch = 'master'\n\nnode {\n   def server\n   def client\n   def serverName\n\nstage(\"Get project\"){\n    git branch: repo_branch, url: repo_url\n}\n\nstage(\"Configure Artifactory/Conan\"){\n    server = Artifactory.server artifactory_name\n    client = Artifactory.newConanClient()\n    serverName = client.remote.add server: server, repo: artifactory_repo\n}\n\nstage(\"Get dependencies and publish build info\"){\n    sh \"mkdir -p build\"\n    dir ('build') {\n      def b = client.run(command: \"install ..\")\n      server.publishBuildInfo b\n    }\n}\n\nstage(\"Build/Test project\"){\n        dir ('build') {\n          sh \"cmake ../ && cmake --build .\"\n        }\n    }\n}\n\nYou can see in the above example that the Conan DSL is very explicit. It helps a lot with common operations, but also allows powerful and custom integrations. This is very important for C/C++ projects, because every company has a very specific project structure, custom integrations etc.\n\nComplex Jenkins Pipeline operations: Managed and parallelized libraries building\n\nAs we saw at the beginning of this blog post, it’s crucial to save time when building a C/C++ project. Here are several ways to optimize the process:\n\nOnly re-build the libraries that need to be rebuilt. These are the libraries that  have been affected by a dependant library that has changed.\n\nBuild in parallel, if possible. When there is no relation between two or more libraries in the project graph, you can build them in parallel.\n\nBuild different configurations (os, compiler, etc) in parallel. Use different agents if needed.\n\nLet’s see an example using Jenkins Pipeline feature\n\nThe above graph represents our project P and its dependencies (A-G). We want to distribute the project for two different architectures, x86 and x86_64.\n\nWhat happens if we change library A?\n\nIf we bump the version to A(v1) there is no problem, we can update the B requirement and also bump its version to B(v1) and so on. The complete flow would be as follows:\n\nPush A(v1) version to Git, Jenkins will build the x86 and x86_64 binaries. Jenkins will upload all the packages to Artifactory.\n\nManually change B to v1, now depending on A1, push to Git, Jenkins will build the B(v1) for x86 and x86_64 using the retrieved new A1 from Artifactory.\n\nRepeat the same process for C, D, F, G and finally our project.\n\nBut if we are developing our libraries in a development repository, we probably depend on the latest A version or will override A (v0) packages on every git push, and we want to automatically rebuild the affected libraries in this case B, D, F, G and P.\n\nHow we can do this with Jenkins Pipelines?\n\nFirst we need to know which libraries need to be rebuilt. The \"conan info --build_order\" command identifies the libraries that were changed in our project, and also tells us which can be rebuilt in parallel.\n\nSo, we created two Jenkins pipelines tasks:\n\nThe\"SimpleBuild\" task which builds every single library. Similar to the first example using Conan DSL with the Jenkins Artifactory plugin. It’s a parameterized task that receives the libraries that need to built.\n\nThe\"MultiBuild\" task which coordinates and launches the \" SimpleBuild\" tasks, in parallel when possible.\n\nWe also have a repository with a configuration yml. The Jenkins tasks will use it to know where the recipe of each library is, and the different profiles to be used. In this case they are x86 and x86_64.\n\nleaves:\n  PROJECT:\n    profiles:\n       - ./profiles/osx_64\n       - ./profiles/osx_32\n\nartifactory:\n  name: artifactory\n  repo: conan-local\n\nrepos:\n LIB_A/1.0:\n   url: https://github.com/lasote/skynet_example.git\n   branch: master\n   dir: ./recipes/A\n\nLIB_B/1.0:\n url: https://github.com/lasote/skynet_example.git\n branch: master\n dir: ./recipes/b\n\n…\n\nPROJECT:\n url: https://github.com/lasote/skynet_example.git\n branch: master\n dir: ./recipes/PROJECT\n\nIf we change and push library A to the repository, the \" MultiBuild\" task will be triggered. It will start by checking which libraries need to be rebuilt, using the \"conan info\" command.\nConan will return something like this:\n[B, [D, F], G]\n\nThis means that we need to start building B, then we can build D and F in parallel, and finally build G. Note that library C does not need to be rebuilt, because it’s not affected by a change in library A.\n\nThe \" MultiBuild\" Jenkins pipeline script will create closures with the parallelized calls to the \" SimpleBuild\" task, and finally launch the groups in parallel.\n\n//for each group\n      tasks = [:]\n      // for each dep in group\n         tasks[label] = { -> build(job: \"SimpleBuild\",\n                            parameters: [\n                               string(name: \"build_label\", value: label),\n                               string(name: \"channel\", value: a_build[\"channel\"]),\n                               string(name: \"name_version\", value: a_build[\"name_version\"]),\n                               string(name: \"conf_repo_url\", value: conf_repo_url),\n                               string(name: \"conf_repo_branch\", value: conf_repo_branch),\n                               string(name: \"profile\", value: a_build[\"profile\"])\n                            ]\n                     )\n          }\n     parallel(tasks)\n\nEventually, this is what will happen:\n\nTwo SimpleBuild tasks will be  triggered, both for building library B, one for x86 and another for x86_64 architectures\n\nOnce \"A\" and \"B\" are built, \"F\" and \"D\" will be triggered, 4 workers will run the \"SimpleBuild\" task in parallel, (x86, x86_64)\n\nFinally \"G\" will be built. So 2 workers will run in parallel.\n\nThe Jenkins Stage View for the will looks similar to the figures below:\n\nMultiBuild\n\nSimpleBuild\n\nWe can configure the \" SimpleBuild\" task within different nodes (Windows, OSX, Linux…), and control the number of executors available in our Jenkins configuration.\n\nConclusions\n\nEmbracing DevOps for C/C++ is still marked as a to-do for many companies. It requires a big investment of time but can save huge amounts of time in the development and releasing life cycle for the long run. Moreover it increases the quality and the reliability of the C/C++ products. Very soon, adoption of DevOps for C/C++ companies will be a must!\n\nThe Jenkins example shown above that demonstrating how to control the library building in parallel is just Groovy code and a custom convenient yml file. The great thing about it is not the example or the code itself. The great thing is the possibility of defining your own pipeline scripts to adapt to your specific workflows, thanks to Jenkins Pipeline, Conan and JFrog Artifactory.\n\nMore on this topic will be presented at Jenkins Community Day Paris on\nJuly 11, and Jenkins User Conference Israel on July 13.","title":"Continuous Integration for C/C++ Projects with Jenkins and Conan","tags":["event","jenkins-user-conference","jenkins-community-day-paris"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#281818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg","srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/8d248/alyssat.jpg 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/c004c/alyssat.jpg 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/9e67b/alyssat.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/22924/alyssat.webp 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/89767/alyssat.webp 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/40d97/alyssat.webp 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/5028e/alyssat.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":166}}},"blog":null,"github":"alyssat","html":"<div class=\"paragraph\">\n<p>Member of the <a href=\"/sigs/advocacy-and-outreach/\">Jenkins Advocacy and Outreach SIG</a>.\nAlyssa drives and manages Jenkins participation in community events and conferences like <a href=\"https://fosdem.org/\">FOSDEM</a>, <a href=\"https://www.socallinuxexpo.org/\">SCaLE</a>, <a href=\"https://events.linuxfoundation.org/cdcon/\">cdCON</a>, and <a href=\"https://events19.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2019/\">KubeCon</a>.\nShe is also responsible for Marketing &amp; Community Programs at <a href=\"https://cloudbees.com\">CloudBees, Inc.</a></p>\n</div>","id":"alyssat","irc":null,"linkedin":null,"name":"Alyssa Tong","slug":"/blog/author/alyssat","twitter":null}]}},{"node":{"date":"2017-07-03T00:00:00.000Z","id":"3eff5455-8949-570e-ae59-6f180d034c6f","slug":"/blog/2017/07/03/contributor-summit/","strippedHtml":"As in previous years, there’ll be a contributor summit at Jenkins World 2017 :\n\nLet’s talk about the future of Jenkins and how you can help shape it! The contributor summit is the place where the current and future contributors of the Jenkins project get together. This year, the theme is “working together”. Traditionally, most contributors are plugin maintainers focusing on their own plugins, but it’s important to look at Jenkins as a whole, because that’s how users experience it. There is more to the project beyond writing plugins, and even for plugin developers, there are increasing number of common libraries and modules that plugins should rely on. In short, we should work better together.\n\nA few contributors will prepare some presentations to help clarify what that means, and what all of us can do. And in the afternoon, there will be \"unconference\" sessions to brainstorm and discuss what we can do and how.\n\nWhether you are already a contributor or just thinking about becoming one, please join us for this full day free event.\n\nDetails about this year’s agenda are available on the event’s meetup page.\nAttending is free, and no Jenkins World ticket is needed, but RSVP if you’re going to attend to help us plan.\n\nSee you there!","title":"Jenkins World Contributor Summit","tags":["event","jenkinsworld","jenkinsworld2017"],"authors":[{"avatar":null,"blog":null,"github":"daniel-beck","html":"<div class=\"paragraph\">\n<p>Daniel is a Jenkins core maintainer and, as security officer, leads the <a href=\"/security/#team\">Jenkins security team</a>.\nHe sometimes contributes to developer documentation and project infrastructure.</p>\n</div>","id":"daniel-beck","irc":null,"linkedin":null,"name":"Daniel Beck","slug":"/blog/author/daniel-beck","twitter":null}]}},{"node":{"date":"2017-06-27T00:00:00.000Z","id":"c68cdf2d-4588-5bfa-80b3-2a0c1581eedd","slug":"/blog/2017/06/27/speaker-blog-SAS-jenkins-world/","strippedHtml":"This is a guest post by Brent Laster, Senior Manager, Research and Development at\nSAS.\n\nJenkins Pipeline\nhas fundamentally changed how users can orchestrate their pipelines and workflows.\nEssentially, anything that you can do in a script or program can now be done in a Jenkinsfile or in a pipeline script created within the application.\nBut just because you can do nearly anything directly in those mechanisms doesn’t mean you necessarily should.\n\nIn some cases, it’s better to abstract the functionality out separately from your main Pipeline.\nPreviously, the main way to do this in Jenkins itself was through creating plugins.\nWith Jenkins 2 and the tight incorporation of Pipeline, we now have another approach – shared libraries.\n\nBrent will be\npresenting\nmore of this topic at Jenkins World in\nAugust, register with the code JWFOSS for a 30% discount off your pass.\n\nShared libraries\nprovide solutions for a number of situations that can be challenging or time-consuming to deal with in Pipeline.\nAmong them:\n\nProviding common routines that can be accessed across a number of pipelines or within a designated scope (more on scope later)\n\nAbstracting out complex or restricted code\n\nProviding a means to execute scripted code from calls in declarative pipelines (where scripted code is not normally allowed)\n\nSimplifying calls in a script to custom code that only differ by calling parameters\n\nTo understand how to use shared libraries in Pipeline, we first need to understand how they are constructed.\nA shared library for Jenkins consists of a source code repository with a structure like the one below:\n\nEach of the top-level directories has its own purpose.\n\nThe resources directory can have non-groovy resources that get loaded via the libraryResource step.\nThink of this as a place to store supporting data files such as json files.\n\nThe src directory uses a structure similar to the standard Java src layout.\nThis area is added to the classpath when a Pipeline that includes this shared library is executed.\n\nThe vars directory holds global variables that should be accessible from pipeline scripts.\nA corresponding.txt file can be included that defines documentation for objects here.\nIf found, this will be pulled in as part of the documentation in the Jenkins application.\n\nAlthough you might think that it would always be best to define library functions in the src structure, it actually works better in many cases to define them in the vars area.\nThe notion of a global variable may not correspond very well to a global function, but you can think of it as the function being a global value that can be pulled in and used in your pipeline.\nIn fact, to work in a declarative style pipeline, having your function in the vars area is the only option.\n\nLet’s look at a simple function that we can create for a shared library.\nIn this case, we’ll just wrap picking up the location of the Gradle installation from Jenkins and calling the corresponding executable with whatever tasks are passed in as arguments.\nThe code is below:\n\n/vars/gbuild.groovy\n\ndef call(args) {\n      sh \"${tool 'gradle3'}/bin/gradle ${args}\"\n}\n\nNotice that we are using a structured form here with the def call syntax.\nThis allows us to simply invoke the routine in our pipeline (assuming we have loaded the shared library) based on the name of the file in the vars area.\nFor example, since we named this file gbuild.groovy, then we can invoke it in our pipeline via a step like this:\n\ngbuild 'clean compileJava'\n\nSo, how do we get our shared library loaded to use in our pipeline?\nThe shared library itself is just code in the structure outlined above committed/pushed into a source code repository that Jenkins can access.\nIn our example, we’ll assume we’ve staged, committed, and pushed this code into a local Git repository on the system at /opt/git/shared-library.git.\n\nLike most other things in Jenkins, we need to first tell Jenkins where this shared library can be found and how to reference it \"globally\" so that pipelines can reference it specifically.\n\nFirst, though, we need to decide at what scope you want this shared library to be available.\nThe most common case is making it a \"global shared library\" so that all Pipelines can access it.\nHowever, we also have the option of only making shared libraries available for projects in a particular Jenkins Folder structure,\nor those in a Multibranch Pipeline, or those in a GitHub Organization pipeline project.\n\nTo keep it simple, we’ll just define ours to be globally available to all pipelines.\nDoing this is a two-step process.\nWe first tell Jenkins what we want to call the library and define some default behavior for Jenkins related to the library,\nsuch as whether we wanted it loaded implicitly for all pipelines.\nThis is done in the Global Pipeline Libraries section of the Configure System page.\n\nFor the second part, we need to tell Jenkins where the actual source repository for the shared library is located.\nSCM plugins that have been modified to understand how to work with shared libraries are called \" Modern SCM\".\nThe git plugin in one of these updated plugin, so we just supply the information in the same Configure System page.\n\nAfter configuring Jenkins so that it can find the shared library repository, we can load the shared library into our pipeline using the @Library(' ') annotation.\nSince Annotations\nare designed to annotate something that follows them,\nwe need to either include a specific import statement, or, if we want to include everything, we can use an underscore character as a placeholder.\nSo our basic step to load the library in a pipeline would be:\n\n@Library('Utilities2') _\n\nBased on this step, when Jenkins runs our Pipeline, it will first go out to the repository that holds the shared library and clone down a copy to use.\nThe log output during this part of the pipeline execution would look something like this:\n\nLoading library Utilities2@master\n > git rev-parse --is-inside-work-tree # timeout=10\nSetting origin to /opt/git/shared-libraries\n > git config remote.origin.url /opt/git/shared-libraries # timeout=10\nFetching origin...\nFetching upstream changes from origin\n > git --version # timeout=10\nusing GIT_SSH to set credentials Jenkins2 SSH\n > git fetch --tags --progress origin +refs/heads/*:refs/remotes/origin/*\n > git rev-parse master^{commit} # timeout=10\n > git rev-parse origin/master^{commit} # timeout=10\nCloning the remote Git repository\nCloning repository /opt/git/shared-libraries\n\nThen Pipeline can call our shared library gbuild function and translate it to the desired Gradle build commands.\n\nFirst time build.\nSkipping changelog.\n[Pipeline] }\n[Pipeline] // stage\n[Pipeline] stage\n[Pipeline] { (Compile)\n[Pipeline] tool\n[Pipeline] sh\n[gsummit17_lab2-4T357CUTJORMC2TIF7WW5LMRR37F7PM2QRUHXUNSRTWTTRHB3XGA]\nRunning shell script\n+ /usr/share/gradle/bin/gradle clean compileJava -x test\nStarting a Gradle Daemon (subsequent builds will be faster)\n\nThis is a very basic illustration of how using shared libraries work.\nThere is much more detail and functionality surrounding shared libraries, and extending your pipeline in general, than we can cover here.\n\nBe sure to catch my talk on\nExtending your Pipeline with Shared Libraries, Global Functions and External Code\nat Jenkins World 2017.\nAlso, watch for my new book on\nJenkins 2 Up and Running\nwhich will have a dedicated chapter on this – expected to be available later this year from O’Reilly.","title":"Extending your Pipeline with Shared Libraries, Global Functions and External Code","tags":["event","jenkinsworld","jenkinsworld2017"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"","id":"hinman","irc":null,"linkedin":null,"name":"Hannah Inman","slug":"/blog/author/hinman","twitter":null}]}},{"node":{"date":"2017-06-26T00:00:00.000Z","id":"7b02e5d8-2c15-5100-9be1-7256902c46eb","slug":"/blog/2017/06/26/share-jenkins-world-keynote-stage/","strippedHtml":"Jenkins World is approaching fast,\nand the event staff are all busy preparing.\nI’ve decided to do something different this year as part of my keynote:\nI want to invite a few Jenkins users like you come up on stage with me.\n\nThere have been amazing developments in Jenkins over the past year.\nFor my keynote, I want highlight how the new Jenkins\n(Pipeline as code with the Jenkinsfile, no more creating jobs,\nBlue Ocean)\nis different and better than the old Jenkins (freestyle jobs, chaining jobs together, etc.).\nAll these developments have helped Jenkins users,\nand it would be more meaningful to have fellow users, like you, share their stories\nabout how recent Jenkins improvements like Pipeline and Blue Ocean have positively impacted them.\n\nIf you’re interested sharing your story, please complete\nthis form\nso that I can contact you.\nThis is a great opportunity to let\nthe rest of the world (and your boss!) hear about your accomplishments.\nYou’ll also get into Jenkins World for free and get to join me backstage.\nIf you concerns about traveling to Jenkins World,\nI’m happy to discuss helping with that as well.\n\nI look forward to hearing from you.","title":"Come Share the Jenkins World Keynote Stage with Me!","tags":["event","jenkinsworld","jenkinsworld2017"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/edb43/kohsuke.jpg","srcSet":"/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/f81fe/kohsuke.jpg 32w,\n/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/01b1b/kohsuke.jpg 64w,\n/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/edb43/kohsuke.jpg 128w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/035c3/kohsuke.webp 32w,\n/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/273f8/kohsuke.webp 64w,\n/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/e3840/kohsuke.webp 128w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":148}}},"blog":null,"github":"kohsuke","html":"<div class=\"paragraph\">\n<p>Kohsuke is the creator of Jenkins.</p>\n</div>","id":"kohsuke","irc":null,"linkedin":null,"name":"Kohsuke Kawaguchi","slug":"/blog/author/kohsuke","twitter":"kohsukekawa"}]}},{"node":{"date":"2017-06-14T00:00:00.000Z","id":"d17e39c6-5d60-518a-9ff6-8280ace32930","slug":"/blog/2017/06/14/jenkinsworld-awards-lastcall/","strippedHtml":"This is a guest post by Alyssa Tong, who runs\nthe Jenkins Area Meetup program and is also responsible for\nMarketing & Community Programs at CloudBees, Inc.\n\nWe have received a good number of nominations for the Jenkins World 2017 Community Awards. These nominations are indicative of the excellent work Jenkins members are doing for the betterment of Jenkins.\n\nThe deadline for nomination is this Friday, June 16.\n\nThis will be the first year we are commemorating community members who have\nshown excellence through commitment, creative thinking, and contributions to\ncontinue making Jenkins a great open source automation server. The award\ncategories includes:\n\nMost Valuable Contributor -\nThis award is presented to the Jenkins contributor who has helped move the Jenkins project forward the most through their invaluable feature contributions, bug fixes or plugin development efforts.\n\nJenkins Security MVP -\nThis award is presented to the individual most consistently providing excellent security reports or who helped secure Jenkins by fixing security issues.\n\nMost Valuable Advocate -\nThis award is presented to an individual who has helped advocate for Jenkins through organization of their local Jenkins Area Meetup.\n\nSubmit your story, or nominate someone today! Winners will be announced at Jenkins World 2017 in San Francisco on August 28-31.\n\nWe look forward to hearing about the great Jenkins work you are doing.","title":"Jenkins World 2017 Community Awards - Last Call for Nominations!","tags":["event","jenkinsworld","jenkinsworld2017"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#281818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg","srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/8d248/alyssat.jpg 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/c004c/alyssat.jpg 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/9e67b/alyssat.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/22924/alyssat.webp 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/89767/alyssat.webp 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/40d97/alyssat.webp 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/5028e/alyssat.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":166}}},"blog":null,"github":"alyssat","html":"<div class=\"paragraph\">\n<p>Member of the <a href=\"/sigs/advocacy-and-outreach/\">Jenkins Advocacy and Outreach SIG</a>.\nAlyssa drives and manages Jenkins participation in community events and conferences like <a href=\"https://fosdem.org/\">FOSDEM</a>, <a href=\"https://www.socallinuxexpo.org/\">SCaLE</a>, <a href=\"https://events.linuxfoundation.org/cdcon/\">cdCON</a>, and <a href=\"https://events19.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2019/\">KubeCon</a>.\nShe is also responsible for Marketing &amp; Community Programs at <a href=\"https://cloudbees.com\">CloudBees, Inc.</a></p>\n</div>","id":"alyssat","irc":null,"linkedin":null,"name":"Alyssa Tong","slug":"/blog/author/alyssat","twitter":null}]}}]}},"pageContext":{"tag":"event","limit":8,"skip":24,"numPages":8,"currentPage":4}},
    "staticQueryHashes": ["3649515864"]}