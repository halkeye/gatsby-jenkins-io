{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/pipeline",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2021-10-26T00:00:00.000Z","id":"f6c301ff-6215-52d6-bc98-2b3d767cc4e0","slug":"/blog/2021/10/26/just-enough-pipeline/","strippedHtml":"Jenkins Pipeline (or simply Pipeline with a capital P) is a suite of plugins that supports implementing and integrating continuous delivery pipelines into Jenkins.\nThis allows you to automate the process of getting software from version control through to your users and customers.\n\nPipeline code works beautifully for its intended role of automating build, test, deploy, and administration tasks.\nBut, as it is pressed into more complex roles and unexpected uses, some users have run into snags.\nUsing best practices – and avoiding common mistakes – can help you design a pipeline that is more robust, scalable, and high-performing.\n\nWe see a lot of users making basic mistakes that can sabotage their pipeline.\n(Yes, you can sabotage yourself when you’re creating a pipeline.)\nIn fact, it’s easy to spot someone who is going down this dangerous path – and it’s usually because they don’t understand some key technical concepts about Pipeline.\nThis invariably leads to scalability mistakes that you’ll pay dearly for down the line.\n\nDon’t make this mistake!\n\nPerhaps the biggest misstep people make is deciding that they need to write their entire pipeline in a programming language.\nAfter all, Pipeline is a domain specific language (DSL).\nHowever, that does not mean that it is a general-purpose programming language.\n\nIf you treat the DSL as a general-purpose programming language, you are making a serious architectural blunder by doing the wrong work in the wrong place.\nRemember that the core of Pipeline code runs on the controller.\nSo, you should be mindful that everything you express in the Pipeline domain specific language (DSL) will compete with every other Jenkins job running on the controller.\n\nFor example, it’s easy to include a lot of conditionals, flow control logic, and requests using scripted syntax in the pipeline job.\nExperience tells us this is not a good idea and can result in serious damage to pipeline performance.\nWe’ve actually seen organizations with poorly written Pipeline jobs bring a controller to its knees, while only running a few concurrent builds.\n\nWait a minute, you might ask, “Isn’t handling code what the controller is there for?”\nYes, the controller certainly is there to execute pipelines.\nBut, it’s much better to assign individual steps of the pipeline to command line calls that execute on an agent.\nSo, instead of running a lot of conditionals inside the pipeline DSL, it’s better to put those conditionals inside a shell script or batch file and call that script from the pipeline.\n\nHowever, this raises another question: “What if I don’t have any agents connected to my controller?”\nIf this is the case, then you’ve just made another bad mistake in scaling Jenkins pipelines.\nWhy? Because the first rule of building an effective pipeline is to make sure you use agents.\nIf you’re using a Jenkins controller and haven’t defined any agents, then your first step should be to define at least one agent and use that agent instead of executing on the controller.\n\nFor the sake of maintaining scalability in your pipeline, the general rule is to avoid processing any workload on your controller.\nIf you’re running Jenkins jobs on the controller, you are sacrificing controller performance.\nSo, try to avoid using Jenkins controller capacity for things that should be passed off to an agent.\nThen, as you grow and develop, all of your work should be running agents.\nThis is why we always recommend setting the number of executors on the master to zero.\n\nUse Just Enough Pipeline to Keep Your Pipeline Scalable\n\nAll of this serves to highlight our overarching theme of “using just enough pipeline.”\nSimply put, you want to use enough code to connect the pipeline steps and integrate tools – but no more than that.\nLimit the amount of complex logic embedded in the Pipeline itself (similarly to a shell script), and avoid treating it as a general-purpose programming language.\nThis makes the pipeline easier to maintain, protects against bugs, and reduces the load on controllers.\n\nAnother best practice for keeping your pipeline lean, fast, and scalable is to use declarative syntax instead of scripted syntax for your Pipeline.\nDeclarative naturally leads you away from the kinds of mistakes we’ve just described.\nIt is a simpler expression of code and an easier way to define your job.\nIt’s computed at the startup of the pipeline instead of executing continually during the pipeline.\n\nTherefore, when creating a pipeline, start with declarative, and keep it simple for as long as possible.\nAnytime a script block shows up inside of a declarative pipeline, you should extract that block and put it in a shared library step.\nThat keeps the declarative pipeline clean.\nBy combining declarative with a shared library, that will take care of the vast majority of use cases you’ll encounter.\n\nThat said, it’s not accurate to say that declarative plus a shared library will solve every problem.\nThere are cases where scripted is the right solution.\nHowever, declarative is a great starting point until you discover that you absolutely must use scripted.\n\nJust remember, at the end of the day, you’ll do well to follow the adage: “Use just enough pipeline and no more.”\n\nThanks to our Sponsor\n\nMark and Darin both work for CloudBees.\nCloudBees helps Fortune 1000 enterprises manage and scale Jenkins.\nThanks to CloudBees for sponsoring the creation of this blog post.\n\nMark and Darin joined Hope Lynch and Joost van der Griendt to share additional topics in a CloudBees on-demand recording,\"Optimizing Jenkins for the Enterprise\".\nRegister for the on-demand recording to receive more information on configuration as code, plugin management, and Pipelines.","title":"Use Just Enough Pipeline","tags":["pipeline"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#786888","images":{"fallback":{"src":"/gatsby-jenkins-io/static/b6d1673d3033c967ff61ee8d4c73aefc/60e20/markewaite.jpg","srcSet":"/gatsby-jenkins-io/static/b6d1673d3033c967ff61ee8d4c73aefc/f4523/markewaite.jpg 32w,\n/gatsby-jenkins-io/static/b6d1673d3033c967ff61ee8d4c73aefc/6859a/markewaite.jpg 64w,\n/gatsby-jenkins-io/static/b6d1673d3033c967ff61ee8d4c73aefc/60e20/markewaite.jpg 128w,\n/gatsby-jenkins-io/static/b6d1673d3033c967ff61ee8d4c73aefc/57001/markewaite.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/b6d1673d3033c967ff61ee8d4c73aefc/1fd06/markewaite.webp 32w,\n/gatsby-jenkins-io/static/b6d1673d3033c967ff61ee8d4c73aefc/a7803/markewaite.webp 64w,\n/gatsby-jenkins-io/static/b6d1673d3033c967ff61ee8d4c73aefc/1a87d/markewaite.webp 128w,\n/gatsby-jenkins-io/static/b6d1673d3033c967ff61ee8d4c73aefc/27a57/markewaite.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":140}}},"blog":null,"github":"markewaite","html":"<div class=\"paragraph\">\n<p>Mark is the <a href=\"/project/team-leads/#documentation\">Jenkins Documentation Officer</a>, a long-time Jenkins user and contributor, and maintains the <a href=\"https://plugins.jenkins.io/git\">git plugin</a> and the <a href=\"https://plugins.jenkins.io/git-client\">git client plugin</a>.\nHe is active in <a href=\"/sigs/\">Jenkins special interest groups</a> including the <a href=\"/sigs/docs/\">Docs SIG</a>, <a href=\"/sigs/platform\">Platform SIG</a>, and <a href=\"/sigs/advocacy-and-outreach\">Advocacy SIG</a>.</p>\n</div>","id":"markewaite","irc":"markewaite","linkedin":"markwaite","name":"Mark Waite","slug":"/blog/authors/markewaite","twitter":"MarkEWaite"},{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/d2e4143b4f4937f53714ff368da3e6e0/19e71/darinpope.jpg","srcSet":"/gatsby-jenkins-io/static/d2e4143b4f4937f53714ff368da3e6e0/77b35/darinpope.jpg 32w,\n/gatsby-jenkins-io/static/d2e4143b4f4937f53714ff368da3e6e0/d4a57/darinpope.jpg 64w,\n/gatsby-jenkins-io/static/d2e4143b4f4937f53714ff368da3e6e0/19e71/darinpope.jpg 128w,\n/gatsby-jenkins-io/static/d2e4143b4f4937f53714ff368da3e6e0/68974/darinpope.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/d2e4143b4f4937f53714ff368da3e6e0/ef6ff/darinpope.webp 32w,\n/gatsby-jenkins-io/static/d2e4143b4f4937f53714ff368da3e6e0/8257c/darinpope.webp 64w,\n/gatsby-jenkins-io/static/d2e4143b4f4937f53714ff368da3e6e0/6766a/darinpope.webp 128w,\n/gatsby-jenkins-io/static/d2e4143b4f4937f53714ff368da3e6e0/22bfc/darinpope.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"darinpope","html":"<div class=\"paragraph\">\n<p>Darin is a developer advocate for <a href=\"https://www.cloudbees.com\">CloudBees</a>.\nHe is the co-host of the weekly <a href=\"https://www.devopsparadox.com/\">DevOps Paradox</a> podcast.\nHe also hosts regular livestream sessions on continuous integration and continuous delivery.\nRecordings of his live stream sessions are available on the <a href=\"https://www.youtube.com/c/CloudBeesTV/search?query=Darin%20Pope\">CloudBees TV YouTube channel</a>.</p>\n</div>","id":"darinpope","irc":null,"linkedin":"darinpope","name":"Darin Pope","slug":"/blog/authors/darinpope","twitter":"DarinPope"}]}},{"node":{"date":"2021-04-21T00:00:00.000Z","id":"be129944-4fa0-5e51-a899-a198162b78c4","slug":"/blog/2021/04/21/tekton-plugin/","strippedHtml":"What is Tekton?\n\nTekton is a powerful and flexible open-source framework for creating CI/CD systems, allowing developers to build, test, and deploy across cloud providers and on-premises systems.\n\nWhy use Tekton?\n\nTekton pipelines have a number of benefits:\n\nthey are cloud native and designed from the ground up for kubernetes\n\neach Tekton Pipeline is fully declarative and completely self described; it does not depend on any separate out of band Jenkins controllers, plugins or plugin/controller configurations\n\neach Pipeline Task runs as a stand alone kubernetes Pod which is completely independent of any other pods and pipelines and are fully scheduled by Kubernetes to maximise resilience and optimize resource usage. A bad pipeline cannot take down another one & the kubernetes scheduler manages them all\n\neach step can be any command in any container image with whatever secrets, volume mounts, environment variables and resource limits you need\n\nthere is no need to bundle a JVM or Jenkins Remoting container into the pod so you can keep resources and cost down\n\nWhy use Jenkins and Tekton together?\n\nJenkins is the most popular open source automation server around. Lots of developers use it every day to get things done.\nJenkins can now be used to automate Tekton pipelines too which helps teams digitally transform to more cloud native solutions for their CI and CD.\nIn such a case, you can use Tekton pipeline engine while getting all benefits from Jenkins as an orchestrator, user interface and the reporting engine.\n\nIntroducing the Tekton Plugin for Jenkins\n\nThe Tekton Client plugin for Jenkins lets you easily use Jenkins to automate creating and running Tekton pipelines.\nIt bridges the Kubernetes learning gap and allows invoking Tekton Pipelines and resources through Jenkins.\nThis allows users to not have much of the Kubernetes specific knowledge beforehand and work.\n\nIts a single Jenkins plugin to install - so it’s easy to use.\n\nFor background check out the  blog post Bridging the Gap with Tekton-client-plugin for Jenkins by the founder of the plugin Vibhav Bobade.\n\nRequirements\n\nThe Tekton Client plugin for jenkins assumes you have access to a kubernetes cluster.\n\nThe kubernetes cluster should have Tekton pipelines installed.\n\nIf you have not yet installed Tekton you could use this tekton helm chart\n\nThe Jenkins controller should also have kubernetes RBAC access to be able to create Tekton resources and watch them and their associated pods and pod logs.\n\nIf you are running your Jenkins controller inside Kubernetes then an easy way to setup the RBAC is to install the Jenkins Resource Helm Chart in the same namespace as your Jenkins controller.\n\nAnother option is to use an installation of Jenkins X and let it setup a Jenkins controller via GitOps\n\nSpecifying the Tekton pipelines\n\nYou can configure the Tekton pipeline via:\n\na file path in a git clone block\n\na URL to a tekton YAML file\n\na block of YAML\n\nWe recommend defining Tekton pipelines as YAML files and checking them into a git repository so that you can use GitOps and follow the Pipeline As Code pattern.\n\nThis means that you can version your pipelines in git. It also means you can benefit from the various IDE plugins available for Tekton such as VS Code and IDEA so that you get auto completion, formatting and validation while editing the YAML.\n\nSo you can use the usual Git provider support in Jenkins to clone the git repository that contains then Tekton YAML file then reference the file by name.\n\nReusing Pipelines from the Tekton Catalog\n\nThe Tekton Catalog defines a ton of Tekton Tasks you can reuse in your pipelines\n\nWe have found when it comes to a microsevices style architecture you end up with lots of repositories and pipelines. Then using a Pipeline As Code pattern with GitOps we want to Version Everything but also make it easy for any repository to use any version of any task or pipeline.\n\ne.g. you may have many repositories using the current version of a pipeline but want to try out a new change to the pipeline in just 1 repository to verify it works well; then if it does, incrementally roll that change out to more repositories.\n\nThis can make it hard trying to reuse as much as you can across the different git repositories while also minimising the number of versions and forks of git repositories you have and simplifying the maintenance of all of the pipelines.\n\nWe have found on the Jenkins X project that a nice way to do this via GitOps such that we reference versioned Tekton Tasks and Pipelines in git so that they are easy to reuse or override.\n\nSo we reuse Tasks and Pipelines via the uses: image notation which lets us keep all of our Tekton Tasks and Pipelines in vanilla Tekton YAML; so that the IDE completion and validation works - but we can easily reuse Tasks or steps from libraries while also Versioning Everything\n\nNote that if wish to reuse steps/tasks via the uses: image notation then you must click the Tekton Catalog flag in your Job definition which will then resolve the uses: clause with the actual step/task.\n\nWhat is Jenkins X?\n\nThe Jenkins X project automates your CI/CD on kubernetes to help you accelerate :\n\nAutomated CI/CD pipelines lets you focus on your actually application code while Jenkins X automatically creates battle tested Tekton CI/CD pipelines for your project which are managed via GitOps so that its super easy to keep your pipelines up to date across your repositories or to upgrade or override pipelines or steps for specific repositories.\n\nAutomatic promotion of versioned artifacts via GitOps through your Environments such as Staging, Pre-production and Production whether they are running in the same kubernetes cluster or you are using multiple clusters for your environments\n\nPreview Environments lets you propose code changes via Pull Requests and have a Preview Environment automatically created, running your code in kubernetes to get fast feedback from your team before agreeing to merge changes to the main branch\n\nChatOps comment on Pull Requests to give feedback, approve/hold changes, trigger optional pipelines for additional testing and other ChatOps commands\n\nAll of the above is implemented in reusable Tekton pipelines.\n\nReusing Jenkins X Pipelines\n\nSo how can we reuse automated CI/CD pipelines from Jenkins X project from Jenkins?\n\nMake sure you have the Tekton Client plugin for Jenkins installed in your Jenkins server.\n\nUsing a working template\n\nIf you want to start with a working example then\n\nCreate A Git Repository From This Template\n\nadd a new Frestyle project to your Jenkins server\n\nenable the Git source code management for your new github.com repository\n\nclick Add build Step (near the bottom of the page) and then select Tekton : Create Resource (Raw)\n\nmake sure that FILE is selected for the input and enter the name.lighthouse/jenkins-x/release.yaml for the file name\n\nif you are using a Jenkins X cluster enter jx for the namespace\n\nensure that Enable Tekton Catalog is checked\n\nnow save the pipeline - it should look something like this:\n\nNow if you trigger the pipeline you should see it create a Tekton Pipeline and you should see the output of the tekton pipeline in the Jenkins console. The pipeline is actually running as a completely separate Pod in kubernetes; the Jenkins controller just tails the log into the console.\n\nIn a Jenkins X cluster this pipeline should just work (reusing all the cloud resources and IAM roles setup by the Terraform) but in an arbitrary kubernetes cluster you may get issues around not being able to push images or promote due to lack of GitOps environments being defined which we can help you work through via the Jenkins X slack room\n\nUsing an existing repository\n\nYou can configure a Pull Request or Release pipeline in your project by copying the YAML file for the language pack you wish to use.\n\ne.g. if you are using maven then copy pullrequest.yaml or release.yaml into your projects source code then reference it from your Jenkins Job:\n\nThen follow the above instructions for setting up a Freestyle project for your git repository and referencing the file name for your pipeline.\n\nOverriding steps\n\nBeing able to reuse steps from libraries of pipelines is awesome; but sometimes you need to change things. The assumptions, commands, arguments, environment variables or approaches used for every step in a library may not quite match what you need on a specific application. You may need to run steps before/after steps in the library or you may need to override a specific step to do something different.\n\nYou can easily customize any inherited step in any shared pipeline or add custom steps before/after any step.\n\nThe fact that all the Tekton YAML is fully declarative makes it super easy to modify things via your IDE with validation and smart completion and not have to use a scripting language and understand complex shared pipeline libraries.\n\nThe easiest way to try overriding a step is to install the jx binary to your $PATH then use the jx pipeline override command which will create a new locally overridden step you can then just edit in your IDE.\n\nThen at any time you can view the effective pipeline when you make local changes\n\nComparing the Kubernetes and Tekton plugins\n\nThose of you using Jenkins on a Kubernetes cluster are probably using the kubernetes plugin right now.\n\nHere is an example of how to use a Jenkinsfile with a pod YAML file so that you can run commands in different containers in the pod.\n\nWhat this means is that:\n\na kubernetes pod is created based on the pod YAML file which is scheduled by kubernetes\n\nthe Jenkinsfile runs on the Jenkins controller talking over Jenkins remoting to the pod to tell it to run commands in different containers. The pod includes the jnlp container which does the remoting between the Jenkins controller and the pod\n\nThis has a few issues:\n\neach container in the pod must have a shell so that jnlp can invoke commands. This may mean you have to create your own images\n\nit can be a little slow to start since there is chattiness with the Jenkins controller and the pod - whereas with Tekton pods just start and run locally without any coodination with the Jenkins controller\n\nyou have to maintain 2 files: the Jenkinsfile and the pod.yaml and it’s hard to share/override both of those files across multiple repositories as you need to make changes (e.g. overriding environment variables/images/commands/resource limits on demand on steps).\n\nThough one downside of the tekton approach is that by default there is no automatic synchronisation of state; after a Task in tekton completes there’s no automatic upload of state to the Jenkins controllers disk. You can always add a step in your Task to upload workspace state to the Jenkins controller if that’s what you want.\n\nThough remember that tekton plugin doesn’t take anything away; so you can mix and match the kubernetes and tekton plugins to suit your needs.\n\nConclusion\n\nWe are really excited about the combination of Jenkins, Tekton and Jenkins X letting developers pick the best tool for the job while becoming more cloud native and increasing the automation help reduce the amount of manual work creating and maintaining pipelines while also helping to improve the quality and practices of our CI/CD.\n\nPlease try it out and let us know how you get on!","title":"Easily reuse Tekton and Jenkins X from Jenkins","tags":["jenkins-x","kubernetes","pipeline","tekton","gitops","interoperability"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/gatsby-jenkins-io/static/70241374c0e6a5665fafbc121c330d3c/19e71/jstrachan.jpg","srcSet":"/gatsby-jenkins-io/static/70241374c0e6a5665fafbc121c330d3c/77b35/jstrachan.jpg 32w,\n/gatsby-jenkins-io/static/70241374c0e6a5665fafbc121c330d3c/d4a57/jstrachan.jpg 64w,\n/gatsby-jenkins-io/static/70241374c0e6a5665fafbc121c330d3c/19e71/jstrachan.jpg 128w,\n/gatsby-jenkins-io/static/70241374c0e6a5665fafbc121c330d3c/68974/jstrachan.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/70241374c0e6a5665fafbc121c330d3c/ef6ff/jstrachan.webp 32w,\n/gatsby-jenkins-io/static/70241374c0e6a5665fafbc121c330d3c/8257c/jstrachan.webp 64w,\n/gatsby-jenkins-io/static/70241374c0e6a5665fafbc121c330d3c/6766a/jstrachan.webp 128w,\n/gatsby-jenkins-io/static/70241374c0e6a5665fafbc121c330d3c/22bfc/jstrachan.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"jstrachan","html":"<div class=\"paragraph\">\n<p>James is a long time open source contributor, created the Groovy programming language and Apache Camel integration framework.\nFor the past few years he&#8217;s been working on CI/CD with Kubernetes.</p>\n</div>","id":"jstrachan","irc":null,"linkedin":null,"name":"James Strachan","slug":"/blog/authors/jstrachan","twitter":"jstrachan"}]}},{"node":{"date":"2020-10-21T00:00:00.000Z","id":"7c9b1586-7a87-5fd5-8543-1a27aa94a68f","slug":"/blog/2020/10/21/a-sustainable-pattern-with-shared-library/","strippedHtml":"Table of Contents\n\nContext\nThe Problems\nThe Solution\n\nShared Library\nDuplication\nDocumentation\nScalability\nInstallation Agnostic\nFeature Toggling\n\nThis post will describe how I use a shared library in Jenkins. Typically when using multibranch pipeline.\n\nIf possible (if not forced to) I implement the pipelines without multibranch. I previously wrote about how I do that with my Generic Webhook Trigger Plugin in a previous post. But this will be my second choice, If I am not allowed to remove the Jenkinsfile :s from the repositories entirely.\n\nContext\n\nWithin an organization, you typically have a few different kinds of repositories. Each repository versioning one application. You may use different techniques for different kinds of applications. The Jenkins organization on GitHub is an example with 2300 repositories.\n\nThe Problems\n\nLarge Jenkinsfiles in every repository containing duplicated code. It seems common that the Jenkinsfile :s in every repository contains much more than just the things that are unique for that repository. The shared libraries feature may not be used, or it is used but not with an optimal pattern.\n\nInstallation specific Jenkinsfile:s that only work with one specific Jenkins installation. Sometimes I see multiple Jenkinsfile :s, one for each purpose or Jenkins installation.\n\nNo documentation and/or no natural place to write documentation.\n\nDevelopment is slow. Adding new features to repositories is a time consuming task. I want to be able to push features to 1000+ repositories without having to update their Jenkinsfile :s.\n\nNo flexible way of doing feature toggling. When maintaining a large number of repositories it is sometimes nice to introduce a feature to a subset of those repositories. If that works well, the feature is introduced to all repositories.\n\nThe Solution\n\nMy solution is a pattern that is inspired by how the Jenkins organization on GitHub does it with its buildPlugin(). But it is not exactly the same.\n\nShared Library\n\nHere is how I organize my shared libraries.\n\nJenkinsfile\n\nI put this in the Jenkinsfile :s:\n\nbuildRepo()\n\nDefault Configuration\n\nI provide a default configuration that any repository will get, if no other configuration is given in buildRepo().\n\nI create a vars/getConfig.groovy with:\n\ndef call(givenConfig = [:]) {\n  def defaultConfig = [\n    /**\n      * The Jenkins node, or label, that will be allocated for this build.\n      */\n    \"jenkinsNode\": \"BUILD\",\n    /**\n      * All config specific to NPM repo type.\n      */\n    \"npm\": [\n      /**\n        * Whether or not to run Cypress tests, if there are any.\n        */\n      \"cypress\": true\n    ],\n    \"maven\": [\n      /**\n        * Whether or not to run integration tests, if there are any.\n        */\n      \"integTest\": true\n    ]\n  ]\n  // https://e.printstacktrace.blog/how-to-merge-two-maps-in-groovy/\n  def effectiveConfig merge(defaultConfig, givenConfig)\n  println \"Configuration is documented here: https://whereverYouHos/getConfig.groovy\"\n  println \"Default config: \" + defaultConfig\n  println \"Given config: \" + givenConfig\n  println \"Effective config: \" + effectiveConfig\n  return effectiveConfig\n}\n\nBuild Plan\n\nI construct a build plan as early as possible. Taking decisions on what will be done in this build. So that the rest of the code becomes more streamlined.\n\nI try to rely as much as possible on conventions. I may provide configuration that lets users turn off features, but they are otherwise turned on if they are detected.\n\nI create a vars/getBuildPlan.groovy with:\n\ndef call(effectiveConfig = [:]) {\n  def derivedBuildPlan = [\n    \"repoType\": \"NOT DETECTED\"\n    \"npm\": [],\n    \"maven\": []\n  ]\n\n  node {\n    deleteDir()\n    checkout([$class: 'GitSCM',\n      branches: [[name: '*/branchName']],\n      extensions: [\n          [$class: 'SparseCheckoutPaths',\n            sparseCheckoutPaths:\n            [[$class:'SparseCheckoutPath', path:'package.json,pom.xml']]\n          ]\n      ],\n      userRemoteConfigs: [[credentialsId: 'someID',\n      url: 'git@link.git']]\n    ])\n\n    if (fileExists('package.json')) {\n      def packageJSON = readJSON file: 'package.json'\n      derivedBuildPlan.repoType = \"NPM\"\n      derivedBuildPlan.npm.cypress = effectiveConfig.npm.cypress && packageJSON.devDependencies.cypress\n      derivedBuildPlan.npm.eslint = packageJSON.devDependencies.eslint\n      derivedBuildPlan.npm.tslint = packageJSON.devDependencies.tslint\n    } else if (fileExists('pom.xml')) {\n      derivedBuildPlan.repoType = \"MAVEN\"\n      derivedBuildPlan.maven.integTest = effectiveConfig.maven.integTest && fileExists('src/integtest')\n    } else {\n      throw RuntimeException('Unable to detect repoType')\n    }\n\n    println \"Build plan: \" + derivedBuildPlan\n    deleteDir()\n  }\n  return derivedBuildPlan\n}\n\nPublic API\n\nThis is the public API, this is what I want the users of this library to actually invoke.\n\nI implement a buildRepo() method that will use that default configuration. It can also be called with a subset of the default configuration to tweak it.\n\nI create a vars/buildRepo.groovy with:\n\ndef call(givenConfig = [:]) {\n  def effectiveConfig = getConfig(givenConfig)\n  def buildPlan = getBuildPlan(effectiveConfig)\n\n  if (effectiveConfig.repoType == 'MAVEN')\n    buildRepoMaven(buildPlan);\n  } else if (effectiveConfig.repoType == 'NPM')\n    buildRepoNpm(buildPlan);\n  }\n}\n\nA user can get all the default behavior with:\n\nbuildRepo()\n\nA user can also choose not to run Cypress, even if it exists in the repository:\n\nbuildRepo([\n  \"npm\": [\n    \"cypress\": false\n  ]\n])\n\nSupporting Methods\n\nThis is usually much more complex, but I put some code here just to have a complete implementation.\n\nI create a vars/buildRepoNpm.groovy with:\n\ndef call(buildPlan = [:]) {\n  node(buildPlan.jenkinsNode) {\n    stage(\"Install\") {\n      sh \"npm install\"\n    }\n    stage(\"Build\") {\n      sh \"npm run build\"\n    }\n    if (buildPlan.npm.tslint) {\n      stage(\"TSlint\") {\n        sh \"npm run tslint\"\n      }\n    }\n    if (buildPlan.npm.eslint) {\n      stage(\"ESlint\") {\n        sh \"npm run eslint\"\n      }\n    }\n    if (buildPlan.npm.cypress) {\n      stage(\"Cypress\") {\n        sh \"npm run e2e:cypress\"\n      }\n    }\n  }\n}\n\nI create a vars/buildRepoMaven.groovy with:\n\ndef call(buildPlan = [:]) {\n  node(buildPlan.jenkinsNode) {\n    if (buildPlan.maven.integTest) {\n      stage(\"Verify\") {\n        sh \"mvn verify\"\n      }\n    } else {\n      stage(\"Package\") {\n        sh \"mvn package\"\n      }\n    }\n  }\n}\n\nDuplication\n\nThe Jenkinsfile :s are kept extremely small. It is only when they, for some reason, diverge from the default config that they need to be changed.\n\nDocumentation\n\nThere is one single point where documentation is written, the getConfig.groovy -file. It can be referred to whenever someone asks for documentation.\n\nScalability\n\nThis is a highly scalable pattern. Both with regards to performance and maintainability in code.\n\nIt scales in performance because the Jenkinsfile :s can be used by any Jenkins installation. So that you can scale by adding several completely separate Jenkins installations, not only nodes.\n\nIt scales in code because it adds just a tiny Jenkinsfile to repositories. It relies on conventions instead, like the existence of attributes in package.json and location of integration tests in src/integtest.\n\nInstallation Agnostic\n\nThe Jenkinsfile :s does not point at any implementation of this API. It just invokes it and it is up to the Jenkins installation to implement it, with a shared libraries.\n\nIt can even be used by something that is not Jenkins. Perhaps you decide to do something in a Docker container, you can still parse the Jenkinsfile with Groovy or (with some magic) with any language.\n\nFeature Toggling\n\nThe shared library can do feature toggling by:\n\nLetting some feature be enabled by default for every repository with name starting with x.\n\nOr, adding some default config saying\"feature-x-enabled\": false, while some repos change their Jenkinsfile :s to buildRepo([\"feature-x-enabled\": true]).\n\nWhenever the feature feels stable, it can be enabled for everyone by changing only the shared library.","title":"A sustainable pattern with shared library","tags":["pipeline","scalability","sharedlibrary","infrastructure"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#98a8c8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/b56c7aed67e544184e7807e4f2d189f6/19e71/tomasbjerre.jpg","srcSet":"/gatsby-jenkins-io/static/b56c7aed67e544184e7807e4f2d189f6/77b35/tomasbjerre.jpg 32w,\n/gatsby-jenkins-io/static/b56c7aed67e544184e7807e4f2d189f6/d4a57/tomasbjerre.jpg 64w,\n/gatsby-jenkins-io/static/b56c7aed67e544184e7807e4f2d189f6/19e71/tomasbjerre.jpg 128w,\n/gatsby-jenkins-io/static/b56c7aed67e544184e7807e4f2d189f6/68974/tomasbjerre.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/b56c7aed67e544184e7807e4f2d189f6/ef6ff/tomasbjerre.webp 32w,\n/gatsby-jenkins-io/static/b56c7aed67e544184e7807e4f2d189f6/8257c/tomasbjerre.webp 64w,\n/gatsby-jenkins-io/static/b56c7aed67e544184e7807e4f2d189f6/6766a/tomasbjerre.webp 128w,\n/gatsby-jenkins-io/static/b56c7aed67e544184e7807e4f2d189f6/22bfc/tomasbjerre.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"https://bjurr.com/","github":"tomasbjerre","html":"<div class=\"paragraph\">\n<p>Tomas Bjerre is an experienced fullstack software developer. Been working full time since 2010 after graduating with a masters degree in computer science from Lund University (Faculty of Engineering, LTH). Is currently working full time and maintaining a bunch of Jenkins plugins on his spare time.</p>\n</div>","id":"tomasbjerre","irc":null,"linkedin":"tomasbjerre","name":"Tomas Bjerre","slug":"/blog/authors/tomasbjerre","twitter":null}]}},{"node":{"date":"2020-04-16T00:00:00.000Z","id":"c9bdc8f4-e8bc-5080-883f-d67fbd7f7847","slug":"/blog/2020/04/16/github-app-authentication/","strippedHtml":"This blogpost was updated to reflect the general availability of the feature after the release of GitHub Branch Source 2.7.1 on April 26th, 2020.\n\nI’m excited to announce support for authenticating as a GitHub app in Jenkins.\nThis has been a long awaited feature by many users.\nIt has been released in GitHub Branch Source 2.7.1 which is now available in the Jenkins update centers.\n\nAuthenticating as a GitHub app brings many benefits:\n\nLarger rate limits - The rate limit for a GitHub app scales with your organization size,\nwhereas a user based token has a limit of 5000 regardless of how many repositories you have.\n\nUser-independent authentication - Each GitHub app has its own user-independent authentication. No more need for 'bot' users or figuring out who should be the owner of 2FA or OAuth tokens.\n\nImproved security and tighter permissions - GitHub Apps offer much finer-grained permissions compared to a service user and its personal access tokens. This lets the Jenkins GitHub app require a much smaller set of privileges to run properly.\n\nAccess to GitHub Checks API - GitHub Apps can access the the GitHub Checks API to create check runs and check suites from Jenkins jobs and provide detailed feedback on commits as well as code annotation\n\nGetting started\n\nInstall the GitHub Branch Source plugin,\nmake sure the version is 2.7.1 or above.\n\nConfiguring the GitHub Organization Folder\n\nFollow the GitHub App Authentication setup guide.  These instructions are also linked from the plugin’s README on GitHub.\n\nOnce you’ve finished setting it up, Jenkins will validate your credential and you should see your new rate limit.\nHere’s an example on a large org:\n\nHow do I get an API token in my pipeline?\n\nIn addition to usage of GitHub App authentication for Multi-Branch Pipeline, you can also use app authentication directly in your Pipelines.\nYou can access the Bearer token for the GitHub API by just loading a 'Username/Password' credential as usual,\nthe plugin will handle authenticating with GitHub in the background.\n\nThis could be used to call additional GitHub API endpoints from your pipeline, possibly the\ndeployments api or you may wish to implement your own\nchecks api integration until Jenkins supports this out of the box.\n\nNote: the API token you get will only be valid for one hour, don’t get it at the start of the pipeline and assume it will be valid all the way through\n\nExample: Let’s submit a check run to Jenkins from our Pipeline:\n\npipeline {\n  agent any\n\n  stages{\n    stage('Check run') {\n      steps {\n        withCredentials([usernamePassword(credentialsId: 'githubapp-jenkins',\n                                          usernameVariable: 'GITHUB_APP',\n                                          passwordVariable: 'GITHUB_ACCESS_TOKEN')]) {\n            sh '''\n            curl -H \"Content-Type: application/json\" \\\n                 -H \"Accept: application/vnd.github.antiope-preview+json\" \\\n                 -H \"authorization: Bearer ${GITHUB_ACCESS_TOKEN}\" \\\n                 -d '{ \"name\": \"check_run\", \\\n                       \"head_sha\": \"'${GIT_COMMIT}'\", \\\n                       \"status\": \"in_progress\", \\\n                       \"external_id\": \"42\", \\\n                       \"started_at\": \"2020-03-05T11:14:52Z\", \\\n                       \"output\": { \"title\": \"Check run from Jenkins!\", \\\n                                   \"summary\": \"This is a check run which has been generated from Jenkins as GitHub App\", \\\n                                   \"text\": \"...and that is awesome\"}}' https://api.github.com/repos/ / /check-runs\n            '''\n        }\n      }\n    }\n  }\n}\n\nWhat’s next\n\nGitHub Apps authentication in Jenkins is a huge improvement.  Many teams have already started using it and have helped improve it by giving pre-release feedback. There are more improvements on the way.\n\nThere’s a proposed Google Summer of Code project: GitHub Checks API for Jenkins Plugins.\nIt will look at integrating with the Checks API,\nwith a focus on reporting issues found using the warnings-ng plugin\ndirectly onto the GitHub pull requests, along with test results summary on GitHub.\nHopefully it will make the Pipeline example below much simpler for Jenkins users :)\nIf you want to get involved with this, join the GSoC Gitter channel\nand ask how you can help.","title":"GitHub App authentication support released","tags":["github","github-branch-source","pipeline","announcement"],"authors":[{"avatar":null,"blog":null,"github":"timja","html":"<div class=\"paragraph\">\n<p>Jenkins core maintainer, along with slack, azure-keyvault and configuration-as-code plugins.\nTim started using Jenkins in 2013 and became an active contributor in 2018.\nTim enjoys working on open source software in his “free” time.</p>\n</div>","id":"timja","irc":null,"linkedin":"tim-jacomb-98043174","name":"Tim Jacomb","slug":"/blog/authors/timja","twitter":"Tjaynz"}]}},{"node":{"date":"2020-03-30T00:00:00.000Z","id":"479ec095-b427-5e47-95a5-1bca6fbc97c5","slug":"/blog/2020/03/30/azure-key-vault-cred-provider/","strippedHtml":"Azure Key Vault is a product for securely managing keys, secrets and certificates.\n\nI’m happy to announce two new features in the Azure Key Vault plugin:\n\na credential provider to tightly link Jenkins and Azure Key Vault.\n\nhuge thanks to Jie Shen for contributing this\n\nintegration with the configuration-as-code plugin.\n\nThese changes were released in v1.8 but make sure to run the latest version of the plugin, there has been some fixes since then.\n\nSome advantages of using the credential provider rather than your own scripts:\n\nyour Jenkins jobs consume the credentials with no knowledge of Azure Key Vault, so they stay vendor-independent.\n\nthe provider integrates with the ecosystem of existing Jenkins credential consumers, such as the Slack Notifications plugin.\n\ncredential usage is recorded in the central Jenkins credentials tracking log.\n\nJenkins can use multiple credentials providers concurrently, so you can incrementally migrate credentials to Azure Key Vault while consuming other credentials from your existing providers.\n\nNote: Currently only secret text credentials are supported via the credential provider, you can use the configuration-as-code integration to load the secret from Azure Key Vault into the System Credential Provider to work around this limitation.\n\nGetting started\n\nInstall the Azure Key Vault plugin\n\nThen you will need to configure the plugin.\n\nAzure authentication\n\nThere’s two types of authentication you can use 'Microsoft Azure Service Principal' or 'Managed Identities for Azure Resources'\n\nThe easiest one to set this up quickly with is the 'Microsoft Azure Service Principal',\n\n$ az ad sp create-for-rbac --name http://service-principal-name\nCreating a role assignment under the scope of \"/subscriptions/ff251390-d7c3-4d2f-8352-f9c6f0cc8f3b\"\n  Retrying role assignment creation: 1/36\n  Retrying role assignment creation: 2/36\n{\n  \"appId\": \"021b5050-9177-4268-a300-7880f2beede3\",\n  \"displayName\": \"service-principal-name\",\n  \"name\": \"http://service-principal-name\",\n  \"password\": \"d9d0d1ba-d16f-4e85-9b48-81ea45a46448\",\n  \"tenant\": \"7e593e3e-9a1e-4c3d-a26a-b5f71de28463\"\n}\n\nIf this doesn’t work then take a look at the Microsoft documentation for creating a service principal.\n\nNote: for production 'Managed Identities for Azure Resources' is more secure as there’s no password involved and you don’t need to worry about the service principal’s password or certificate expiring.\n\nVault setup\n\nYou need to create a vault and give your service principal access to it:\n\nRESOURCE_GROUP_NAME=my-resource-group\naz group create --location uksouth --name $RESOURCE_GROUP_NAME\n\nVAULT=my-vault # you will need a unique name for the vault\naz keyvault create --resource-group $RESOURCE_GROUP_NAME --name $VAULT\naz keyvault set-policy --resource-group $RESOURCE_GROUP_NAME --name $VAULT \\\n  --secret-permissions get list --spn http://service-principal-name\n\nJenkins credential\n\nThe next step is to configure the credential in Jenkins:\n\nclick 'Credentials'\n\nclick 'System' (it’ll appear below the Credentials link in the side bar)\n\nclick 'Global credentials (unrestricted)'\n\nclick 'Add Credentials'\n\nselect 'Microsoft Azure Service Principal'\n\nfill out the form from the credential created above, appId is 'Client ID', password is 'Client Secret'\n\nclick 'Verify Service Principal', you should see 'Successfully verified the Microsoft Azure Service Principal'.\n\nclick 'Save'\n\nJenkins Azure Key Vault plugin configuration\n\nYou now have a credential you can use to interact with Azure resources from Jenkins, now you need to configure the plugin:\n\ngo back to the Jenkins home page\n\nclick 'Manage Jenkins'\n\nclick 'Configure System'\n\nsearch for 'Azure Key Vault Plugin'\n\nenter your vault url and select your credential\n\nclick 'Save'\n\nStore a secret in Azure Key Vault\n\nFor the step after this you will need a secret, so let’s create one now:\n\n$ az keyvault secret set --vault-name $YOUR_VAULT --name secret-key --value my-super-secret\n\nCreate a pipeline\n\nInstall the Pipeline plugin if you don’t already have it.\n\nFrom the Jenkins home page, click 'New item', and then:\n\nenter a name, i.e. 'key-vault-test'\n\nclick on 'Pipeline'\n\nadd the following to the pipeline definition:\n\n// Declarative //\npipeline {\n  agent any\n  environment {\n    SECRET_KEY = credentials('secret-key')\n  }\n  stages {\n    stage('Foo') {\n      steps {\n        echo SECRET_KEY\n        echo SECRET_KEY.substring(0, SECRET_KEY.size() - 1) // shows the right secret was loaded, don't do this for real secrets unless you're debugging\n      }\n    }\n  }\n}\n\n// Scripted //\nwithCredentials([string(credentialsId: 'secret-key', variable: 'SECRET_KEY')]) {\n    echo SECRET_KEY\n    echo SECRET_KEY.substring(0, SECRET_KEY.size() - 1) // shows the right secret was loaded, don't do this for real secrets unless you're debugging\n}\n\nYou have now successfully retrieved a credential from Azure Key Vault using native Jenkins credentials integration.\n\nconfiguration-as-code integration\n\nThe Configuration as Code plugin has been designed as an opinionated way to configure Jenkins based on human-readable declarative configuration files. Writing such a file should be easy without being a Jenkins expert.\n\nFor many secrets the credential provider is enough,\nbut when integrating with other plugins you will likely need more than string credentials.\n\nYou can use the configuration-as-code plugin (aka JCasC) to allow integrating with other credential types.\n\nconfigure authentication\n\nAs the JCasC plugin runs during initial startup the Azure Key Vault credential provider needs to be configured before JCasC runs during startup.\n\nThe easiest way to do that is via environment variables set before Jenkins starts up:\n\nexport AZURE_KEYVAULT_URL=https://my.vault.azure.net\nexport AZURE_KEYVAULT_SP_CLIENT_ID=...\nexport AZURE_KEYVAULT_SP_CLIENT_SECRET=...\nexport AZURE_KEYVAULT_SP_SUBSCRIPTION_ID=...\nexport AZURE_KEYVAULT_SP_SUBSCRIPTION_ID=...\n\nSee the azure-keyvault documentation for other authentication options.\n\nYou will now be able to refer to Azure Key Vault secret IDs in your jenkins.yaml file:\n\ncredentials:\n  system:\n    domainCredentials:\n      - credentials:\n        - usernamePassword:\n            description: \"GitHub\"\n            id: \"jenkins-github\"\n            password: \"${jenkins-github-apikey}\"\n            scope: GLOBAL\n            username: \"jenkinsadmin\"\n\nThanks for reading, send feedback on twitter using the tweet button in the top right, any issues or feature requests use GitHub issues.","title":"Introducing the Azure Key Vault Credentials Provider for Jenkins","tags":["jenkins","pipeline","security","azure","credentials","credential-provider","configuration-as-code"],"authors":[{"avatar":null,"blog":null,"github":"timja","html":"<div class=\"paragraph\">\n<p>Jenkins core maintainer, along with slack, azure-keyvault and configuration-as-code plugins.\nTim started using Jenkins in 2013 and became an active contributor in 2018.\nTim enjoys working on open source software in his “free” time.</p>\n</div>","id":"timja","irc":null,"linkedin":"tim-jacomb-98043174","name":"Tim Jacomb","slug":"/blog/authors/timja","twitter":"Tjaynz"}]}},{"node":{"date":"2020-01-08T00:00:00.000Z","id":"9f6eb48f-3ae6-5fa4-8bc2-bee1eeeed2d0","slug":"/blog/2020/01/08/atlassians-new-bitbucket-server-integration-for-jenkins/","strippedHtml":"We know that for many of our customers Jenkins is incredibly important and its integration with Bitbucket Server is a key part of their development workflow.\nUnfortunately, we also know that integrating Bitbucket Server with Jenkins wasn’t always easy – it may have required multiple plugins and considerable time.\nThat’s why earlier this year we set out to change this.\nWe began building our own integration, and we’re proud to announce that v1.0 is out.\n\nThe new Bitbucket Server integration for Jenkins plugin, which is built and supported by Atlassian, is the easiest way to link Jenkins with Bitbucket Server.\nIt streamlines the entire set-up process, from creating a webhook to trigger builds in Jenkins, to posting build statuses back to Bitbucket Server.\nIt also supports smart mirroring and lets Jenkins clone from mirrors to free up valuable resources on your primary server.\n\nOur plugin is available to install through Jenkins now.\nWatch this video to find out how, or read the BitBucket Server solution page to learn more about it.\n\nOnce you’ve tried it out we’d love to hear any feedback you have.\nTo share it with us, visit https://issues.jenkins.io and create an issue using the component atlassian-bitbucket-server-integration-plugin.","title":"Atlassian's new Bitbucket Server integration for Jenkins","tags":["bitbucket","pipeline","plugins"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#888888","images":{"fallback":{"src":"/gatsby-jenkins-io/static/6ccab92416b4e5b2a4498ee23e6d735c/19e71/dkjellin.jpg","srcSet":"/gatsby-jenkins-io/static/6ccab92416b4e5b2a4498ee23e6d735c/77b35/dkjellin.jpg 32w,\n/gatsby-jenkins-io/static/6ccab92416b4e5b2a4498ee23e6d735c/d4a57/dkjellin.jpg 64w,\n/gatsby-jenkins-io/static/6ccab92416b4e5b2a4498ee23e6d735c/19e71/dkjellin.jpg 128w,\n/gatsby-jenkins-io/static/6ccab92416b4e5b2a4498ee23e6d735c/68974/dkjellin.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/6ccab92416b4e5b2a4498ee23e6d735c/ef6ff/dkjellin.webp 32w,\n/gatsby-jenkins-io/static/6ccab92416b4e5b2a4498ee23e6d735c/8257c/dkjellin.webp 64w,\n/gatsby-jenkins-io/static/6ccab92416b4e5b2a4498ee23e6d735c/6766a/dkjellin.webp 128w,\n/gatsby-jenkins-io/static/6ccab92416b4e5b2a4498ee23e6d735c/22bfc/dkjellin.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"dkjellin","html":"<div class=\"paragraph\">\n<p>Daniel has been with <a href=\"https://www.atlassian.com\">Atlassian</a> over ten years working on a variety of different products. Lately he has led the work to write a new and improved integration between <a href=\"https://www.atlassian.com/software/bitbucket/enterprise/data-center\">Bitbucket Server</a> and Jenkins.</p>\n</div>","id":"dkjellin","irc":null,"linkedin":null,"name":"Daniel Kjellin","slug":"/blog/authors/dkjellin","twitter":null}]}},{"node":{"date":"2019-12-14T00:00:00.000Z","id":"646ce24b-257b-5341-bb67-a06739313fd5","slug":"/blog/2019/12/14/generic-webhook-trigger-plugin/","strippedHtml":"Table of Contents\n\nThe Problem\n\nCode Duplication And Security\nA Branch Is Not A Feature\nDocumentation\n\nThe Solution\n\nCode Duplication And Security\nA Branch Is Not A Feature\nDocumentation\n\nThis post will describe some common problems I’ve had with Jenkins and how I solved them by developing Generic Webhook Trigger Plugin.\n\nThe Problem\n\nI was often struggling with the same issues when working with Jenkins:\n\nCode duplication and security - Jenkinsfiles in every repository.\n\nA branch is not a feature - Parameterized jobs on master branch often mix parameters relevant for different features.\n\nPoorly documented trigger plugins - Proper documented services but poorly documented consuming plugins.\n\nCode Duplication And Security\n\nHaving Jenkinsfiles in every Git repository allows developers to let those files diverge. Developers pushes forward with their projects and it is hard to maintain patterns to share code.\n\nI have, almost, solved code duplication with shared libraries but it does not allow me to setup a strict pattern that must be followed. Any developer can still decide to not invoke the features provided by the shared library.\n\nThere is also the security aspect of letting developers run any code from the Jenkinsfiles. Developers might, for example, print passwords gathered from credentials. Letting developers execute any code on the Jenkins nodes just does not seem right to me.\n\nA Branch Is Not A Feature\n\nIn Bitbucket there are projects and each project has a collection of git repositories. Something like this:\n\nPROJ_1\n\nREPO_1\n\nREPO_2\n\nPROJ_2\n\nREPO_3\n\nLets think about some features we want to provide for these repositories:\n\nPull request verification\n\nBuilding snapshot (or pre release if you will)\n\nBuilding releases\n\nIf the developers are use to the repositories being organized like this in Bitbucket, should we not organize them the same way in Jenkins? And if they browse Jenkins should they not find one job per feature, like pull-request, snapshot and release? Each job with parameters only relevant for that feature. I think so! Like this:\n\n/ - Jenkins root\n\n/PROJ_1 - A folder, lists git repositories\n\n/PROJ_1/REPO_1 - A folder, lists jobs relevant for that repo.\n\n/PROJ_1/REPO_1/release - A job, performs releases.\n\n/PROJ_1/REPO_1/snapshot - A job, performs snapshot releases.\n\n/PROJ_1/REPO_1/pull-request - A job, verifies pull requests.\n\n…​\n\nIn this example, both snapshot and release jobs might work with the same git branch. The difference is the feature they provide. Their parameters can be well documented as you don’t have to mix parameters relevant for releases and those relevant for snapshots. This cannot be done with Multibranch Pipeline Plugin where you specify parameters as properties per branch.\n\nDocumentation\n\nWebhooks are often well documented in the services providing them. See:\n\nBitbucket Cloud\n\nBitbucket Server\n\nGitHub\n\nGitLab\n\nGogs and Gitea\n\nAssembla\n\nJira\n\nIt bothered me that, even if I understood these webhooks, I was unable to use them. Because I needed to perform development in the plugin I was using in order to provide whatever value from the webhook to the build. That process could take months from PR to actual release. Such a simple thing should really not be an issue.\n\nThe Solution\n\nMy solution is pretty much back to basics : We have an automation server (Jenkins) and we want to trigger it on external webhooks. We want to gather information from that webhook and provide it to our build. In order to support it I have created the Generic Webhook Trigger Plugin.\n\nThe latest docs are available in the repo and I also have a fully working example with GitLab implemented using configuration-as-code. See the repository here.\n\nCode Duplication And Security\n\nI establish a convention that all developers must follow. Instead of letting the developers explicitly invoke the infrastructure from Jenkinsfiles. There are rules to follow, like:\n\nAll git repositories should be built from the root of the repo.\n\nIf it contains a gradlew\n\nBuild is done with./gradlew build\n\nRelease is done with./gradlew release\n\n…​ and so on\n\nIf it contains a package.json\n\nBuild is done with npm run build\n\nRelease is done with npm run release\n\n…​ and so on\n\nWith these rules, pipelines can be totally generic and no Jenkinsfiles are needed in the repositories. Some git repositories may, for some reason, need to disable test cases. That can be solved by allowing repositories to add a special file, perhaps jenkins-settings.json, let the infrastructure discover and act on its content.\n\nThis also helps the developers even when not doing CI. When they clone a new, to them unknown, repository they will know what commands can be issued and their semantics.\n\nA Branch Is Not A Feature\n\nI implement:\n\nJenkins job configurations - With Job DSL.\n\nJenkins build process - With Pipelines and Shared Library.\n\nBy integrating with the git service from Job DSL I can automatically find the git repositories. I create jobs dynamically organized in folders. Also invoking the git service to setup webhooks triggering those jobs. The jobs are ordinary pipelines, not multibranch, and they don’t use Jenkinsfile from Git but instead Jenksinfile configured in the job using Job DSL. So that all job configurations and pipelines are under version control. This is all happening here.\n\nDocumentation\n\nThe plugin uses JSONPath, and also XPath, to extract values from JSON and provide them to the build. Letting the user pick whatever is needed from the webhook. It also has a regular expression filter to allow not triggering for some conditions.\n\nThe plugin is not very big, just being the glue between the webhook, JSONPath / XPath and regular expression. All these parts are very well documented already and I do my best supporting the plugin. That way this is a very well documented solution to use!","title":"Generic Webhook Trigger Plugin","tags":["webhooks","trigger","pipeline","security","scalability"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#98a8c8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/b56c7aed67e544184e7807e4f2d189f6/19e71/tomasbjerre.jpg","srcSet":"/gatsby-jenkins-io/static/b56c7aed67e544184e7807e4f2d189f6/77b35/tomasbjerre.jpg 32w,\n/gatsby-jenkins-io/static/b56c7aed67e544184e7807e4f2d189f6/d4a57/tomasbjerre.jpg 64w,\n/gatsby-jenkins-io/static/b56c7aed67e544184e7807e4f2d189f6/19e71/tomasbjerre.jpg 128w,\n/gatsby-jenkins-io/static/b56c7aed67e544184e7807e4f2d189f6/68974/tomasbjerre.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/b56c7aed67e544184e7807e4f2d189f6/ef6ff/tomasbjerre.webp 32w,\n/gatsby-jenkins-io/static/b56c7aed67e544184e7807e4f2d189f6/8257c/tomasbjerre.webp 64w,\n/gatsby-jenkins-io/static/b56c7aed67e544184e7807e4f2d189f6/6766a/tomasbjerre.webp 128w,\n/gatsby-jenkins-io/static/b56c7aed67e544184e7807e4f2d189f6/22bfc/tomasbjerre.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"https://bjurr.com/","github":"tomasbjerre","html":"<div class=\"paragraph\">\n<p>Tomas Bjerre is an experienced fullstack software developer. Been working full time since 2010 after graduating with a masters degree in computer science from Lund University (Faculty of Engineering, LTH). Is currently working full time and maintaining a bunch of Jenkins plugins on his spare time.</p>\n</div>","id":"tomasbjerre","irc":null,"linkedin":"tomasbjerre","name":"Tomas Bjerre","slug":"/blog/authors/tomasbjerre","twitter":null}]}},{"node":{"date":"2019-12-10T00:00:00.000Z","id":"ed404ce2-c53c-5f43-ae06-381e9a803bce","slug":"/blog/2019/12/10/introducing-aws-secrets-manager-credentials-provider-plugin/","strippedHtml":"API keys and secrets are difficult to handle safely, and probably something you avoid thinking about. In this post I’ll show how the new AWS Secrets Manager Credentials Provider plugin allows you to marshal your secrets into one place, and use them securely from Jenkins.\n\nWhen CI/CD pipelines moved to the public cloud, credential management did not evolve with them. If you’re in this situation, you may have seen a number of tactical workarounds to keep Jenkins builds talking to the services they depend on. The workarounds range from bad (hardcoding plaintext secrets into Git) to merely painful (wrangling Hiera EYAML), but their common feature is that they tend to make copies of secrets beyond the reach of automation. This increases their attack surface, makes routine key rotation impractical, and makes remediation difficult after a breach.\n\nThe good news is that there is a better way!\n\nAWS Secrets Manager is a comprehensive solution for secure secret storage. You define a secret just once for your whole AWS account, then you give your consumers permission to use the secrets. Secrets Manager lets you manage a secret entry (name and metadata) separately from its value, and it integrates with other AWS services that you already use:\n\nSecret entry management: Manual (Web console, AWS CLI) or with an infrastructure management tool ( Terraform, CloudFormation etc.)\n\nSecret value management: Manual (Web console, AWS CLI) or automatic (secret rotation Lambda function).\n\nAccess control: AWS IAM policies (for both applications and human operators).\n\nSecret encryption: Amazon KMS automatically encrypts the secret value. Use either the account’s default KMS key, or a customer-managed KMS key.\n\nAuditing: AWS CloudTrail and CloudWatch Events.\n\nA couple of teams in my company started to use Secrets Manager from Jenkins jobs by calling the AWS CLI, but this remained a niche approach as it was quite unwieldy. There was clearly an appetite to integrate key developer apps with a centralised secrets store, but production-ready integrations were needed for wider adoption. So this year I created the AWS Secrets Manager Credentials Provider plugin for Jenkins, with help from friends in the Jenkins community, to do exactly that.\n\nThis is how you set it up…​\n\nInstall the plugin from the Jenkins update center.\n\nGive Jenkins read-only access to Secrets Manager with an IAM policy.\n\n(Optional) Configure the plugin, either through the Global Configuration screen or Jenkins Configuration As Code.\n\nThis is how you use it…​\n\nCreate your build secrets in AWS Secrets Manager. (You can start by uploading secrets via the AWS CLI. More sophisticated methods of secret creation are also available.)\n\nView the credentials in the Jenkins UI, to check that Jenkins can see them.\n\nBind the credentials by ID in your Jenkins job.\n\nThe provider supports the following standard Jenkins credential types:\n\nSecret Text\n\nUsername With Password\n\nSSH User Private Key\n\nPKCS#12 Certificate\n\nAnd it has powerful advantages over quick-fix tactical solutions:\n\nYour Jenkins jobs consume the credentials with no knowledge of Secrets Manager, so they stay vendor-independent.\n\nThe provider caches relevant Secrets Manager API calls, for a quicker and more reliable experience.\n\nThe provider integrates with the ecosystem of existing Jenkins credential consumers, such as the Git and SSH Agent plugins.\n\nThe provider records credential usage in the central Jenkins credentials tracking log.\n\nJenkins can use multiple credentials providers concurrently, so you can incrementally migrate credentials to Secrets Manager while consuming other credentials from your existing providers.\n\nAfter the plugin’s first public release, developers at other companies adopted it too. It has had contributions so far from people at Elsevier, GoDaddy, and Northeastern University, as well as the fantastic Jenkins core team. We even got fan mail for our work!\n\nIn enterprise security, \"The important things are always simple. The simple things are always hard. The easy way is always mined.\" ( @thegrugq) It’s easy to buy a shiny ‘next generation' security appliance and drop it into your network. But it’s hard to embed the security fundamentals (like secrets management, OS patching, secure development) across your organisation. This Jenkins plugin is part of the effort [ 1 ] to take one of the persistent hard problems in security, and make it easier for everyone.\n\n1. If you’re on Azure or you run most of your workload on Kubernetes, check out the Azure Credentials Plugin and the Kubernetes Credentials Provider Plugin.","title":"Introducing the AWS Secrets Manager Credentials Provider for Jenkins","tags":["pipeline","plugins","aws","credentials","security"],"authors":[{"avatar":null,"blog":null,"github":"chriskilding","html":"","id":"chriskilding","irc":null,"linkedin":"chriskilding","name":"Chris Kilding","slug":"/blog/authors/chriskilding","twitter":null}]}}]}},"pageContext":{"tag":"pipeline","limit":8,"skip":0,"numPages":13,"currentPage":1}},
    "staticQueryHashes": ["3649515864"]}