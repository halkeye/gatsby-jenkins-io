{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/pipeline/page/8",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2016-12-31T00:00:00.000Z","id":"3267c8fe-f10e-530a-9caa-bae9eeb34fb0","slug":"/blog/2016/12/31/what-a-year/","strippedHtml":"I do not think it is an exaggeration to say: 2016 was the best year yet for the\nJenkins project. Since the first commit in 2006, the project has reached a\nnumber of significant milestones in its ten years but we have never experienced\nthe breadth of major milestones in such a short amount of time. From\nJenkins 2\nand\nBlue Ocean\nto the\nGoogle Summer of Code\nand\nJenkins World,\n\nI wanted to take a moment and celebrate the myriad of accomplishments which\ncouldn’t have happened without the help from everybody who participates in the\nJenkins project. The 1,300+ contributors to the\njenkinsci GitHub organization,\nthe 4,000+ members of the\ndevelopers mailing list,\nthe 8,000+ members of the\nusers mailing list,\nand countless others who have reported issues, submitted pull requests, and\npresented at meetups and conferences.\n\nJenkins 2\n\nThrough the course of 2016, the Jenkins project published 16\nLTS releases\nand 54\nWeekly releases.\nOf those 70 releases, the most notable may have been the\nJenkins 2.0 release\nwhich was published in April.\n\nJenkins 2 made Pipeline as Code front-and-center in the user experience,\nintroduced a new \"Getting Started\" experience, and included a number of other\nsmall UI improvements, all while maintaining backwards compatibility with\nexisting Jenkins environments.\n\nSince April, we have released a number of LTS\nreleases using Jenkins 2 as a baseline, meaning the Jenkins project no longer\nmaintains any 1.x release lines.\n\nThe\nPipeline\nefforts have continuted to gain steam since April, covered on this blog with a\nnumber of\nposts tagged \"pipeline\". Closing out 2016 with the\nannouncement of the beta for\nDeclarative Pipeline syntax\nwhich is expected in early 2017.\n\nBlue Ocean\n\nHot on the heels of Jenkins 2 announcement\"Blue Ocean, a new user experience for Jenkins\",\nwas\nopen sourced in May.\nBlue Ocean is a new project that rethinks the user experience of Jenkins.\nDesigned from the ground up for Jenkins Pipeline and compatible with Freestyle\njobs. The goal for the project is to reduce clutter and increase clarity for\nevery member of a team using Jenkins.\n\nThe Blue Ocean beta can be installed from the Update Center and can be run in\nproduction Jenkins environments alongside the existing UI. It adds the new user experience under\n/blue in the environment but does not disturb the existing UI.\n\nBlue Ocean is expected to reach \"1.0\" in the first half of 2017.\n\nAzure\n\nAlso in May of 2016, the Jenkins project announced an exciting\nPartnership with Microsoft\nto run our project infrastructure on\nAzure. While the migration of Jenkins project\ninfrastructure into Azure is still on-going, there have been some notable\nmilestones reached already:\n\nEnd-to-end TLS encrypted delivery for Debian/openSUSE/Red Hat repositories which are\nconfigured to use https://pkg.jenkins.io by the end-user.\n\nMajor capacity improvements to\nci.jenkins.io\nproviding on-demand Ubuntu and Windows build/test infrastructure.\n\nA full continuous delivery Pipeline for all Azure-based infrastructure using\nTerraform from Jenkins.\n\nThe migration to Azure is expected to complete in 2017.\n\nGoogle Summer of Code\n\nFor the first time in the history of the project, Jenkins was accepted into\nGoogle Summer of Code\n2016. Google Summer of Code (GSoC) is an annual, international, program\nwhich encourages college-aged students to participate with open source projects\nduring the summer break between classes. Students accepted into the program\nreceive a stipend, paid by Google, to work well-defined projects to improve or\nenhance the Jenkins project.\n\nIn exchange, numerous Jenkins community members volunteered as \"mentors\" for\nstudents to help integrate them into the open source community and succeed in\ncompleting their summer projects.\n\nA lot was learned during the summer which we look forward to applying to Google\nSummer of Code 2017\n\nJenkins World\n\nIn September, over one thousand people attended\nJenkins World,\nin Santa Clara, California.\n\nFollowing the event,\nLiam\nposted a series of blog posts which highlight some of the fantastic content\nshared by Jenkins users and contributors from around the world, such as:\n\nThe demos from the \"Experts\"\n\nSessions on Scaling Jenkins\n\nUsing Jenkins Pipeline\n\nThe Contributor Summit\n\nJenkins World was the first global event of its kind for Jenkins, it brought users\nand contributors together to exchange ideas on the current state of the\nproject, celebrate accomplishments of the past year, and look ahead at all the\nexiting enhancements coming down the pipe(line).\n\nIt was such a smashing success that\nJenkins World 2017\nis already scheduled for August 30-31st in San Francisco, California.\n\nJAM\n\nFinally, 2016 saw tremendous growth in the number of\nJenkins Area Meetups\n(JAMs) hosted around the world. JAMs are local meetups intended to bring\nJenkins users and contributors together for socializing and learning. JAMs are\norganized by local Jenkins community members who have a passion for sharing new\nJenkins concepts, patterns and tools.\n\nDriven by current Jenkins Events Officer,\nAlyssa Tong,\nand the dozens of passionate organizers, JAMs have become a great way to meet\nother Jenkins users near you.\n\nWhile we don’t yet have JAMs on each of the seven continents, you can always join the\nJenkins Online Meetup.\nThough we’re hoping more groups will be founded near you in 2017!\n\nI am personally grateful for the variety and volume of contributions made by\nthousands of people to the Jenkins project this year. I believe I can speak for\nproject founder,\nKohsuke Kawaguchi,\nin stating that the Jenkins community has grown beyond our anything we could\nhave imagined five years ago, let alone ten!\n\nThere are number of ways to\nparticipate\nin the Jenkins project, so if you didn’t have an opportunity to join in during\n2016, we hope to see you next year!","title":"Thank you for an amazing 2016","tags":["jam","jenkins2","pipeline","blueocean","azure","gsoc","new-year-blogpost"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-12-20T00:00:00.000Z","id":"07094990-1362-5018-bed6-12c7ddfa53ee","slug":"/blog/2016/12/20/jenkins-puppet-enterprise-plugin/","strippedHtml":"This is a guest post by Carl Caum,\nwho works at Puppet and created the\nPuppet Enterprise Pipeline plugin.\n\nDuring PuppetConf 2016, myself and Brian Dawson from CloudBees announced the\nplugin:puppet-enterprise-pipeline[Puppet Enterprise\nplugin for Jenkins Pipeline].\nLet’s take a look at how the plugin makes it trivial to use Puppet to perform\nsome or all of the deployment tasks in continuous delivery pipelines.\n\nJenkins Pipeline introduced an amazing world where the definition for a\npipeline is managed from the same version control repository as the code\ndelivered by the pipeline. This is a powerful idea, and one I felt complemented\nPuppet’s automation strengths. I wanted to make it trivial to control Puppet\nEnterprise’s orchestration and infrastructure code management capabilities, as\nwell as set hierarchical configuration data and use Puppet’s inventory data\nsystem as a source of truth – all from a Pipeline script. The result was the\nPuppet Enterprise plugin, which fully buys into the Pipeline ideals by\nproviding methods to control the different capabilities in Puppet Enterprise.\nThe methods provide ways to query\nPuppetDB, set\nHiera key/value pairs, deploy\nPuppet code environments with\nCode Management, and kick off orchestrated Puppet runs with the\nOrchestrator.\n\nThe Puppet Enterprise for Jenkins Pipeline plugin\n\nThe Puppet Enterprise for Jenkins Pipeline plugin itself has zero system\ndependencies. You need only to install the plugin from the update center. The\nplugin uses APIs available in Puppet Enterprise to do its work. Since the\nPuppetDB query, Code Management, and Orchestrator APIs are all\nbacked by Puppet Enterprise’s role-based access control (RBAC) system, it’s\neasy to restrict what pipelines are allowed to control in Puppet Enterprise. To\nlearn more about RBAC in Puppet Enterprise,\nread the docs here.\n\nConfiguring\n\nConfiguring the plugin is fairly straight forward. It takes three simple steps:\n\nSet the address of the Puppet server\n\nCreate a Jenkins credential with a Pupppet Enterprise RBAC authentication token\n\nConfigure the Hiera backend\n\nSet the Puppet Enterprise Server Address\n\nGo to Jenkins > Manage Jenkins > Puppet Enterprise page. Put the DNS address of\nthe Puppet server in the Puppet Master Address text field. Click the Test\nConnection button to verify the server is reachable, the Puppet CA certificate\nis retrievable, and HTTPS connections are successful. Once the test succeeds,\nClick Save.\n\nCreate a Jenkins Credentials Entry\n\nThe plugin uses the Jenkins built-in credentials system (the plain-credentials\nplugin) to store and refer RBAC tokens to Puppet Enterprise for authentication\nand authorization. First, generate an RBAC token in Puppet Enterprise by\nfollowing\nthe\ninstructions on the docs site. Next, create a new Jenkins Credentials item\nwith Kind Secret text and the Secret value the Puppet Enterprise RBAC\ntoken. It’s highly recommended to give the credential an ID value that’s\ndescriptive and identifiable. You’ll use it in your Pipeline scripts.\n\nIn your Jenkinsfile, use the puppet.credentials method to set all future Puppet\nmethods to use the RBAC token. For example:\n\npuppet.credentials 'pe-team-token'\n\nConfigure the Hiera Backend\n\nThe plugin exposes an HTTP API for performing Hiera data lookups for key/value\npairs managed by Pipeline jobs. To configure Hiera on the Puppet compile\nmaster(s) to query the Jenkins Hiera data store backend, use the\nhiera-http backend. On the\nPuppet Enterprise compile master(s), run the following commands:\n\n/opt/puppetlabs/puppet/bin/gem install hiera-http\n/opt/puppetlabs/bin/puppetserver gem install hiera-http\n\nNow you can configure the /etc/puppetlabs/puppet/hiera.yaml file. The following\nconfiguration instructs Hiera to first look to the Hiera yaml files in the\nPuppet code’s environment, then fall back to the http backend. The http backend\nwill first query the Hiera data store API looking for the key in the scope with\nthe same name as the node. If nothing’s found, look for the key in the node’s\nenvironments. You can use any Facter fact to match scope names.\n\n:backends:\n  - yaml\n  - http\n\n:http:\n  :host: jenkins.example.com\n  :port: 8080\n  :output: json\n  :use_auth: true\n  :auth_user:\n:auth_pass:\n:cache_timeout: 10\n  :failure: graceful\n  :paths:\n    - /hiera/lookup?path=%{clientcert}&key=%{key}\n    - /hiera/lookup?path=%{environment}&key=%{key}\n\nFinally, restart the pe-puppetserver process to pick up the new configs:\n\n/opt/puppetlabs/bin/puppet resource service pe-puppetserver ensure=stopped\n/opt/puppetlabs/bin/puppet resource service pe-puppetserver ensure=running\n\nHiera HTTP Authentication\n\nIf Jenkins' Global Security is configured to allow unauthenticated read-only\naccess, the 'use_auth', 'auth_pass', and 'auth_user' parameters are\nunnecessary. Otherwise, create a local Jenkins user that has permissions to\nview the Hiera Data Lookup page and use that user’s credentials for the\nhiera.yaml configuration.\n\nQuerying the infrastructure\n\nPuppetDB is an extensive data store that holds every bit of information Puppet\ngenerates and collects across every system Puppet is installed on. PuppetDB\nprovides a sweet query language called\nPQL. With PQL,\nyou can ask complex questions of your infrastructure such as \"How many\nproduction Red Hat systems are there with the openssl package installed?\" or\n\"What us-west-2c nodes with the MyApp role that were created in the last 24\nhours?\"\n\nThis can be a powerful tool for parts of your pipeline where you need to\nperform specific operations on subsets of the infrastructure like draining a\nloadbalancer.\n\nHere’s an example using the puppet.query method:\n\nresults = puppet.query '''\n  inventory[certname] {\n    facts.os.name = \"RedHat\" and\n    facts.ec2_metadata.placement.availability-zone = \"us-west-2c\" and\n    facts.uptime_hours < 24\n  }'''\n\nThe query returns an array of matching items. The results can be\niterated on, and even passed to a series of puppet.job calls. For example, the\nfollowing code will query all nodes in production that experienced a failure on\nthe last Puppet run.\n\nresults = puppet.query 'nodes { latest_report_status = \"failed\" and catalog_environment = \"production\"}'\n\nNote that once you can use closures in Pipeline scripts, doing the above\nexample will be much simpler.\n\nCreating an orchestrator job\n\nThe orchestration service in Puppet Enterprise is a tool to perform\norchestrated Puppet runs across as broad or as targeted an infrastructure as\nyou need at different parts of a pipeline. You can use the orchestrator to\nupdate applications in an environment, or update a specific list of nodes, or\nupdate nodes across a set of nodes that match certain criteria. In each\nscenario, Puppet will always push distributed changes in the correct order by\nrespecting the cross-node dependencies.\n\nTo create a job in the Puppet orchestrator from a Jenkins pipeline, use the\npuppet.job method. The puppet.job method will create a new orchestrator job,\nmonitor the job for completion, and determine if any Puppet runs failed. If\nthere were failures, the pipeline will fail.\n\nThe following are just some examples of how to run Puppet orchestration jobs against the infrastructure you need to target.\n\nTarget an entire environment:\n\npuppet.job 'production'\n\nTarget instances of an application in production:\n\npuppet.job 'production', application: 'Myapp'\n\nTarget a specific list of nodes:\n\npuppet.job 'production', nodes: ['db.example.com','appserver01.example.com','appserver02.example.com']\n\nTarget nodes matching a complex set if criteria:\n\npuppet.job 'production', query: 'inventory[certname] { facts.os.name = \"RedHat\" and facts.ec2_metadata.placement.availability-zone = \"us-west-2c\" and uptime_hours < 24 }'\n\nAs you can see, the puppet.job command means you can be as broad or as targeted\nas you need to be for different parts of your pipeline. There are many other\noptions you can add to the puppet.job method call, such as setting the Puppet\nruns to noop, or giving the orchestrator a maximum concurrency limit.\nLearn\nmore about the orchestrator here.\n\nUpdating Puppet code\n\nIf you’re using Code Management in Puppet Enterprise (and you should), you can\nensure that all the modules, site manifests, Hiera data, and roles and profiles\nare staged, synced, and ready across all your Puppet masters, direct from your\nJenkins pipeline.\n\nTo update Puppet code across all Puppet masters, use the puppet.codeDeploy method:\n\npuppet.codeDeploy 'staging'\n\nLearn more Code Management in Puppet Enterprise here.\n\nSetting Hiera values\n\nThe plugin includes an experimental feature to set Hiera key/value pairs. There\nare many cases where you need to promote information through a pipeline, such\nas a build version or artifact location. Doing so is very difficult in Puppet,\nsince data promotion almost always involves changing Hiera files and committing\nto version control.\n\nThe plugin exposes an HTTP API endpoint that Hiera can query using the\nhiera-http backend. With the backend configured on the Puppet master(s),\nkey/value pairs can be set to scopes. A scope is arbitrary and can be anything\nyou like, such as a Puppet environment, a node’s certname, or the name of a\nFacter fact like operatingsystem or domain.\n\nTo set a Hiera value from a pipeline, use the puppet.hiera method.\n\npuppet.hiera scope: 'staging', key: 'build-version', value: env.BUILD_ID\n\nNow you can set the same key with the same value to the production scope later\nin the pipeline, followed by a call to puppet.job to push the change out.\n\nExamples\n\nThe\nplugin’s\nGithub repository contains a set of example Pipeline scripts. Feel free to\nissue pull requests to add your own scripts!\n\nWhat’s next\n\nI’m pretty excited to see how this is going to help simplify continuous\ndelivery pipelines. I encourage everyone to get started with continuous\ndelivery today, even if it’s just a simple pipeline. As your practices evolve,\nyou can begin to add automated tests, automate away manual checkpoints, start\nto incorporate InfoSec tests, and include phases for practices like patch\nmanagement that require lots of manual approvals, verifications and rollouts.\nYou’ll be glad you did.","title":"Continuous Delivery with Jenkins and Puppet Enterprise","tags":["continuousdelivery","puppet","pipeline","puppetenterprise"],"authors":[{"avatar":null,"blog":null,"github":"ccaum","html":"","id":"ccaum","irc":null,"linkedin":null,"name":"Carl Caum","slug":"/blog/authors/ccaum","twitter":"ccaum"}]}},{"node":{"date":"2016-12-19T00:00:00.000Z","id":"584ac2c5-5d3d-5e1c-bcbe-5ef1ef71e42e","slug":"/blog/2016/12/19/declarative-pipeline-beta/","strippedHtml":"Last week we released version 0.7.1 of the\nPipeline-Model-Defintion\nplugin and wanted to crown it as the official Beta version of the Declarative\nPipeline syntax. Although it has been available in the update center\nsince August,\nwe continue to solidify the syntax. We feel this release is getting\nvery close to the final version and should not change much before 1.0. However,\nit is still a Beta so further tweaks are possible.\n\nA release (0.8.0) is planned for early January 2017 which will finalize the\nsyntax with the following changes:\nJENKINS-40524,\nJENKINS-40370,\nJENKINS-40462,\nJENKINS-40337\n\nWhat is Declarative Pipeline?\n\nAll the way back at Jenkins World in September, Andrew Bayer presented a\nsneak peak\nof a new syntax for constructing Pipelines. We are calling this new syntax\nDeclarative Pipeline to differentiate it from the existing Scripted Pipeline\nsyntax that has always been a part of Pipeline.\n\nAfter listening to many Jenkins users over the last year we felt that, while\nPipeline Script provides tremendous power, flexibility, and extensibility, the\nlearning curve for Scripted Pipeline was steep for users new to either Jenkins\nor Pipeline. Beginning users wanting to take advantage of all the features\nprovided by Pipeline and Jenkinsfiles were required to learn Scripted Pipeline\nor remain limited to the functionality provided by Freestyle jobs.\n\nDeclarative Pipeline does not replace Scripted Pipeline but extends Pipeline it\nwith a pre-defined structure to let users focus entirely on the steps\nrequired at each stage without needing to worry about scripting every aspect\nof the pipeline. Granular flow-control is extremely powerful and Scripted\nPipeline syntax will always be part of Pipeline but it’s not for everyone.\n\nDeclarative Pipeline enables all users to connect simple, declarative blocks\nthat define agents (including Docker), post actions, environment\nsettings, credentials and all stages that make up the pipeline. Best of all,\nbecause this Declarative syntax is part of Pipeline, all build steps and build\nwrappers available in Plugins or loaded from Shared Libraries are also\navailable as steps in Declarative.\n\nExample\n\nBelow is an example of a pipeline in Declarative syntax. You can also switch the view to show the same pipeline in Scripted syntax.\n The Declarative syntax has a more straightforward structure that is easier to grok by users not versed in Groovy.\n\n// Declarative //\npipeline {\n  agent  label:'has-docker', dockerfile: true\n  environment {\n    GIT_COMMITTER_NAME = \"jenkins\"\n    GIT_COMMITTER_EMAIL = \"jenkins@jenkins.io\"\n  }\n  stages {\n    stage(\"Build\") {\n      steps {\n        sh 'mvn clean install -Dmaven.test.failure.ignore=true'\n      }\n    }\n    stage(\"Archive\"){\n      steps {\n        archive \"*/target/**/*\"\n        junit '*/target/surefire-reports/*.xml'\n      }\n    }\n  }\n  post {\n    always {\n      deleteDir()\n    }\n    success {\n      mail to:\"me@example.com\", subject:\"SUCCESS: ${currentBuild.fullDisplayName}\", body: \"Yay, we passed.\"\n    }\n    failure {\n      mail to:\"me@example.com\", subject:\"FAILURE: ${currentBuild.fullDisplayName}\", body: \"Boo, we failed.\"\n    }\n  }\n}\n\n// Script //\nwithEnv([\"GIT_COMMITTER_NAME = jenkins\",\"GIT_COMMITTER_EMAIL = jenkins@jenkins.io\"]) {\n  node('has-docker') {\n    try {\n      checkout scm // checks out Dockerfile and source code\n      def myImage = docker.build 'my-environment:snapshot'\n      myImage.inside {\n        stage('Build') {\n          sh 'mvn clean install -Dmaven.test.failure.ignore=true'\n        }\n        stage('Archive') {\n          archive \"*/target/**/*\"\n          junit '*/target/surefire-reports/*.xml'\n        }\n      }\n      if (currentBuild.result == null || currentBuild.result == 'SUCCESS') {\n        mail to:\"me@example.com\", subject:\"SUCCESS: ${currentBuild.fullDisplayName}\", body: \"Yay, we passed.\"\n      }\n    }\n    catch (exc) {\n      mail to:\"me@example.com\", subject:\"FAILURE: ${currentBuild.fullDisplayName}\", body: \"Boo, we failed.\"\n    }\n    finally {\n      deleteDir()\n    }\n  }\n}\n\nHow can you help?\n\nInstall the lastest version of the\nPipeline-Model-Defintion plugin.\n\nRead the documentation:\nGetting Started and\nSyntax overview.\n(These documents will be incorporated into the Jenkins.io documentation.)\n\nConvert some of your existing Pipeline scripts into Declarative\n\nLog any issues or enhancements you have\nhere\nfor the syntax, the execution, or the documentation.\n\nAsk questions. You can send questions to the\nusers mailing list\nor visit the #jenkins channel on IRC.\n\nHow will this work with Blue Ocean?\n\nBlue Ocean is all about Pipelines in Jenkins. Running, displaying, and soon,\ncreating Pipelines.  Blue Ocean will be able to run and display Pipelines\nwritten in this new syntax just like any other Pipeline works today. However,\nbecause Declarative Pipeline includes a pre-defined structure, or model, it is\nnow possible to create and edit pipelines with a GUI editor.\n\nAlthough we plan to launch 1.0 of Declarative Pipeline before Blue Ocean 1.0 is\nofficially available, we expect to have a working Beta of the Editor available\nto play with. The combination of a simple syntax and an intuitive editor\nshould make creating Jenkins Pipelines a breeze.\n\nHappy Holidays\n\nI hope everyone has a great end of the year and a Happy New Year. With\nDeclarative Pipeline and\nBlue Ocean\nwe expect great things for Jenkins in 2017!","title":"Announcing the beta of Declarative Pipeline Syntax","tags":["pipeline","blueocean"],"authors":[{"avatar":null,"blog":null,"github":"HRMPW","html":"","id":"hrmpw","irc":null,"linkedin":null,"name":"Patrick Wolf","slug":"/blog/authors/hrmpw","twitter":"hrmpw"}]}},{"node":{"date":"2016-10-31T00:00:00.000Z","id":"dcf580a2-8de4-526e-8237-399ee53b3b39","slug":"/blog/2016/10/31/xunit-reporting/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nThe\nJUnit plugin\nis the go-to test result reporter for many Jenkins projects,\nbut the it is not the only one available.  The\nxUnit plugin\nis a viable alternative that supports JUnit and many other test result file formats.\n\nIntroduction\n\nNo matter the project, you need to gather and report test results.\nJUnit is one of the most widely supported formats for recording test results.\nFor a scenarios where your tests are stable and your framework can produce JUnit output,\nthis makes the JUnit plugin ideal for reporting results in Jenkins.\nIt will consume results from a specified file or path, create a report,\nand if it finds test failures it will set the the job state to \"unstable\" or \"failed\".\n\nThere are also plenty of scenarios where the JUnit plugin is not enough.\nIf your project has some failing tests that will take some time to fix,\nor if there are some flaky tests,\nthe JUnit plugin’s simplistic view of test failures may be difficult to work with.\n\nNo problem, the Jenkins plugin model lets us replace the JUnit\nplugin functionality with similar\nfunctionality from another plugin and Jenkins Pipeline lets us do this in safe\nstepwise fashion where we can test and debug each of our changes.\n\nIn this article, I will show you how to replace the JUnit plugin with the\nxUnit plugin in Pipeline code to address a few common test reporting scenarios.\n\nInitial Setup\n\nI’m going to use the \"JS-Nightwatch.js\" sample project from my\nprevious post to demonstrate a couple\ncommon scenarios that the xUnit handles better.\nI already have the latest\nJUnit plugin\nand\nxUnit plugin\ninstalled on my Jenkins server.\n\nI’ll be keeping my changes in\nlink: my fork\nof the \"JS-Nightwatch.js\" sample project on GitHub, under the\n\" blog/xunit\" branch.\n\nHere’s what the Jenkinsfile looked like at the end of that previous post and what\nthe report page looks like after a few runs:\n\nJenkinsfile\n\nnode {\n    stage \"Build\"\n    checkout scm\n\n    // Install dependencies\n    sh 'npm install'\n\n    stage \"Test\"\n    // Add sauce credentials\n    sauce('f0a6b8ad-ce30-4cba-bf9a-95afbc470a8a') {\n        // Start sauce connect\n        sauceconnect(options: '', useGeneratedTunnelIdentifier: false, verboseLogging: false) {\n\n            // List of browser configs we'll be testing against.\n            def platform_configs = [\n                'chrome',\n                'firefox',\n                'ie',\n                'edge'\n            ].join(',')\n\n            // Nightwatch.js supports color ouput, so wrap this step for ansi color\n            wrap([$class: 'AnsiColorBuildWrapper', 'colorMapName': 'XTerm']) {\n                // Run selenium tests using Nightwatch.js\n                // Ignore error codes. The junit publisher will cover setting build status.\n                sh \"./node_modules/.bin/nightwatch -e ${platform_configs} || true\"\n            }\n\n            junit 'reports/**'\n\n            step([$class: 'SauceOnDemandTestPublisher'])\n        }\n    }\n}\n\nSwitching from JUnit to xUnit\n\nI’ll start by replacing JUnit with xUnit in my pipeline.\nI use the Snippet Generator to create the step with the right parameters.\nThe main downside of using the xUnit plugin is that while it is Pipeline compatible,\nit still uses the more verbose step() syntax and has some very rough edges around that, too.\nI’ve filed\nJENKINS-37611\nbut in the meanwhile, we’ll work with what we have.\n\n// Original JUnit step\njunit 'reports/**'\n\n// Equivalent xUnit step - generated (reformatted)\nstep([$class: 'XUnitBuilder', testTimeMargin: '3000', thresholdMode: 1,\n    thresholds: [\n        [$class: 'FailedThreshold', failureNewThreshold: '', failureThreshold: '', unstableNewThreshold: '', unstableThreshold: '1'],\n        [$class: 'SkippedThreshold', failureNewThreshold: '', failureThreshold: '', unstableNewThreshold: '', unstableThreshold: '']],\n    tools: [\n        [$class: 'JUnitType', deleteOutputFiles: false, failIfNotNew: false, pattern: 'reports/**', skipNoTestFiles: false, stopProcessingIfError: true]]\n    ])\n\n// Equivalent xUnit step - cleaned\nstep([$class: 'XUnitBuilder',\n    thresholds: [[$class: 'FailedThreshold', unstableThreshold: '1']],\n    tools: [[$class: 'JUnitType', pattern: 'reports/**']]])\n\nIf I replace the junit step in my Jenkinsfile with that last step above,\nit produces a report and job result identical to the JUnit plugin but using the xUnit plugin.  Easy!\n\nnode {\n    stage \"Build\"\n    // ... snip ...\n\n    stage \"Test\"\n    // Add sauce credentials\n    sauce('f0a6b8ad-ce30-4cba-bf9a-95afbc470a8a') {\n        // Start sauce connect\n        sauceconnect(options: '', useGeneratedTunnelIdentifier: false, verboseLogging: false) {\n\n            // ... snip ...\n\n            // junit 'reports/**'\n            step([$class: 'XUnitBuilder',\n                thresholds: [[$class: 'FailedThreshold', unstableThreshold: '1']],\n                tools: [[$class: 'JUnitType', pattern: 'reports/**']]])\n\n            // ... snip ...\n        }\n    }\n}\n\nAccept a Baseline\n\nMost projects don’t start off with automated tests passing or even running.\nThey start with a people hacking and prototyping, and eventually they start to write tests.\nAs new tests are written, having tests checked-in, running, and failing can be valuable information.\nWith the xUnit plugin we can accept a baseline of failed cases and drive that number down over time.\n\nI’ll start by changing the Jenkinsfile to fail jobs only if the number of failures is greater than an expected baseline,\nin this case four failures. When I run the job with this change, the reported numbers remain the same, but the job passes.\n\nJenkinsfile\n\n// The rest of the Jenkinsfile is unchanged.\n// Only the xUnit step() call is modified.\nstep([$class: 'XUnitBuilder',\n    thresholds: [[$class: 'FailedThreshold', failureThreshold: '4']],\n    tools: [[$class: 'JUnitType', pattern: 'reports/**']]])\n\nNext, I can also check that the plugin reports the job as failed if more failures occur.\nSince this is sample code, I’ll do this by adding another failing test and checking the job\nreports as failed.\n\ntests/guineaPig.js\n\n// ... snip ...\n\n    'Guinea Pig Assert Title 0 - D': function(client) { /* ... */ },\n\n    'Guinea Pig Assert Title 0 - E': function(client) {\n        client\n            .url('https://saucelabs.com/test/guinea-pig')\n            .waitForElementVisible('body', 1000)\n            //.assert.title('I am a page title - Sauce Labs');\n            .assert.title('I am a page title - Sauce Labs - Cause a Failure');\n    },\n\n    afterEach: function(client, done) { /* ... */ }\n\n// ... snip ...\n\nIn a real project, we’d make fixes over a number of commits bringing the number of failures down and adjusting our baseline.\nSince this is a sample, I’ll just make all tests pass and set the job failure threshold for failed and skipped cases to zero.\n\nJenkinsfile\n\n// The rest of the Jenkinsfile is unchanged.\n// Only the xUnit step() call is modified.\nstep([$class: 'XUnitBuilder',\n    thresholds: [\n        [$class: 'SkippedThreshold', failureThreshold: '0'],\n        [$class: 'FailedThreshold', failureThreshold: '0']],\n    tools: [[$class: 'JUnitType', pattern: 'reports/**']]])\n\ntests/guineaPig.js\n\n// ... snip ...\n\n    'Guinea Pig Assert Title 0 - D': function(client) { /* ... */ },\n\n    'Guinea Pig Assert Title 0 - E': function(client) {\n        client\n            .url('https://saucelabs.com/test/guinea-pig')\n            .waitForElementVisible('body', 1000)\n            .assert.title('I am a page title - Sauce Labs');\n    },\n\n    afterEach: function(client, done) { /* ... */ }\n\n// ... snip ...\n\ntests/guineaPig_1.js\n\n// ... snip ...\n\n    'Guinea Pig Assert Title 1 - A': function(client) {\n        client\n            .url('https://saucelabs.com/test/guinea-pig')\n            .waitForElementVisible('body', 1000)\n            .assert.title('I am a page title - Sauce Labs');\n    },\n\n// ... snip ...\n\nAllow for Flakiness\n\nWe’ve all known the frustration of having one flaky test that fails once every ten jobs.\nYou want to keep it active so you can working isolating the source of the problem,\nbut you also don’t want to destablize your CI pipeline or reject commits that are actually okay.\nYou could move the test to a separate job that runs the \"flaky\" tests,\nbut in my experience that just leads to a job that is always in a failed state\nand a pile of flaky tests no one looks at.\n\nWith the xUnit plugin, we can keep the this flaky test in main test suite but allow\nthe our job to still pass.\n\nI’ll start by adding a sample flaky test.  After a few runs, we can see the test\nfails intermittently and causes the job to fail too.\n\ntests/guineaPigFlaky.js\n\n// New test file: tests/guineaPigFlaky.js\nvar https = require('https');\nvar SauceLabs = require(\"saucelabs\");\n\nmodule.exports = {\n\n    '@tags': ['guineaPig'],\n\n    'Guinea Pig Flaky Assert Title 0': function(client) {\n        var expectedTitle = 'I am a page title - Sauce Labs';\n        // Fail every fifth minute\n        if (Math.floor(Date.now() / (1000 * 60)) % 5 === 0) {\n            expectedTitle += \" - Cause failure\";\n        }\n\n        client\n            .url('https://saucelabs.com/test/guinea-pig')\n            .waitForElementVisible('body', 1000)\n            .assert.title(expectedTitle);\n    }\n\n    afterEach: function(client, done) {\n        client.customSauceEnd();\n\n        setTimeout(function() {\n            done();\n        }, 1000);\n\n    }\n\n};\n\nI can almost hear my teammates screaming in frustration just looking at this report.\nTo allow specific tests to be unstable but not others,\nI’m going to add a guard \"suite completed\" test to the suites that should be stable,\nand keep flaky test on it’s own.\nThen I’ll tell xUnit to allow for a number of failed tests, but no skipped ones.\nIf any test fails other than the ones I allow to be flaky,\nit will also result in one or more skipped tests and will fail the build.\n\n// The rest of the Jenkinsfile is unchanged.\n// Only the xUnit step() call is modified.\nstep([$class: 'XUnitBuilder',\n    thresholds: [\n        [$class: 'SkippedThreshold', failureThreshold: '0'],\n        // Allow for a significant number of failures\n        // Keeping this threshold so that overwhelming failures are guaranteed\n        //     to still fail the build\n        [$class: 'FailedThreshold', failureThreshold: '10']],\n    tools: [[$class: 'JUnitType', pattern: 'reports/**']]])\n\ntests/guineaPig.js\n\n// ... snip ...\n\n    'Guinea Pig Assert Title 0 - E': function(client) { /* ... */ },\n\n    'Guinea Pig Assert Title 0 - Suite Completed': function(client) {\n      // No assertion needed\n    },\n\n    afterEach: function(client, done) { /* ... */ }\n\n// ... snip ...\n\ntests/guineaPig_1.js\n\n// ... snip ...\n\n    'Guinea Pig Assert Title 1 - E': function(client) { /* ... */ },\n\n    'Guinea Pig Assert Title 1 - Suite Completed': function(client) {\n      // No assertion needed\n    },\n\n    afterEach: function(client, done) { /* ... */ }\n\n// ... snip ...\n\nAfter a few more runs, you can see the flaky test is still being flaky,\nbut it is no longer failing the build.  Meanwhile, if another test fails,\nit will cause the \"suite completed\" test to be skipped, failing the job.\nIf this were a real project, the test owner could instrument and eventually fix\nthe test.  When they were confident they had stabilized the test the could add\na \"suite completed\" test after it to enforce it passing without changes to other\ntests or framework.\n\nConclusion\n\nThis post has shown how to migrate from the JUnit plugin to the\nxUnit plugin on an existing project in Jenkins pipeline.  It also covered how to\nuse the features of xUnit plugin to get more meaningful and effective Jenkins\nreporting behavior.\n\nWhat I didn’t show was how many other formats xUnit supports - from CCPUnit to MSTest.  You can\nalso write your own XSL for result formats not on the known/supported list.\n\nLinks\n\nxUnit plugin\n\nbitwiseman/JS-Nightwatch.js\n\nsaucelabs-sample-test-frameworks","title":"xUnit and Pipeline","tags":["pipeline","plugins","xunit","nightwatch"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2016-10-16T00:00:00.000Z","id":"1b6c5cc3-a7d6-56f2-91fc-f717ed4f85fc","slug":"/blog/2016/10/16/stage-lock-milestone/","strippedHtml":"This is a guest post by Patrick Wolf,\nDirector of Product Management at CloudBees.\n\nRecently the Pipeline team began making several changes to improve the stage step and increase control of concurrent builds in Pipeline. Until now the stage step has been the catch-all for functionality related to the flow of builds through the Pipeline: grouping build steps into visualized stages, limiting concurrent builds, and discarding stale builds.\n\nIn order to improve upon each of these areas independently we decided to break this functionality into discrete steps rather than push more and more features into an already packed stage step.\n\nstage - the stage step remains but is now focused on grouping steps and providing boundaries for Pipeline segments.\n\nlock - the lock step throttles the number of concurrent builds in a defined section of the Pipeline.\n\nmilestone - the milestone step automatically discards builds that will finish out of order and become stale.\n\nSeparating these concerns into explicit, independent steps allows for much greater control of Pipelines and broadens the set of possible use cases.\n\nStage\n\nThe stage step is a primary building block in Pipeline, dividing the steps of a Pipeline into explicit units and helping to visualize the progress using the \"Stage View\" plugin or\"Blue Ocean\". Beginning with version 2.2 of \"Pipeline Stage Step\" plugin, the stage step now requires a block argument, wrapping all steps within the defined stage. This makes the boundaries of where each stage begins and ends obvious and predictable. In addition, the concurrency argument of stage has now been removed to make this step more concise; responsibility for concurrency control has been delegated to the lock step.\n\nstage('Build') {\n  doSomething()\n  sh \"echo $PATH\"\n}\n\nOmitting the block from stage and using the concurrency argument are now deprecated in Pipeline. Pipelines using this syntax will continue to function but will produce a warning in the console log:\n\nUsing the 'stage' step without a block argument is deprecated\n\nThis message is only a reminder to update your Pipeline scripts; none of your Pipelines will stop working. If we reach a point where the old syntax is to be removed we will make an announcement prior to the change. We do, however, recommend that you update your existing Pipelines to utilize the new syntax.\n\nnote: Stage View and Blue Ocean will both work with either the old stage syntax or the new.\n\nLock\n\nRather than attempt to limit the number of concurrent builds of a job using the stage, we now rely on the \"Lockable Resources\" plugin and the lock step to control this. The lock step limits concurrency to a single build and it provides much greater flexibility in designating where the concurrency is limited.\n\nlock can be used to constrain an entire stage or just a segment:\n\nstage('Build') {\n  doSomething()\n  lock('myResource') {\n    echo \"locked build\"\n  }\n}\n\nlock can be also used to wrap multiple stages into a single concurrency unit:\n\nlock('myResource') {\n  stage('Build') {\n    echo \"Building\"\n  }\n  stage('Test') {\n    echo \"Testing\"\n  }\n}\n\nMilestone\n\nThe milestone step is the last piece of the puzzle to replace functionality originally intended for stage and adds even more control for handling concurrent builds of a job. The lock step limits the number of builds running concurrently in a section of your Pipeline while the milestone step ensures that older builds of a job will not overwrite a newer build.\n\nConcurrent builds of the same job do not always run at the same rate. Depending on the network, the node used, compilation times, test times, etc. it is always possible for a newer build to complete faster than an older build. For example:\n\nBuild 1 is triggered\n\nBuild 2 is triggered\n\nBuild 2 builds faster than Build 1 and enters the Test stage sooner.\n\nRather than allowing Build 1 to continue and possibly overwrite the newer artifact produced in Build 2, you can use the milestone step to abort Build 1:\n\nstage('Build') {\n  milestone()\n  echo \"Building\"\n}\nstage('Test') {\n  milestone()\n  echo \"Testing\"\n}\n\nWhen using the input step or the lock step a backlog of concurrent builds can easily stack up, either waiting for user input or waiting for a resource to become free. The milestone step will automatically prune all older jobs that are waiting at these junctions.\n\nmilestone()\ninput message: \"Proceed?\"\nmilestone()\n\nBookending an input step like this allows you to select a specific build to proceed and automatically abort all antecedent builds.\n\nmilestone()\nlock(resource: 'myResource', inversePrecedence: true) {\n  echo \"locked step\"\n  milestone()\n}\n\nSimilarly a pair of milestone steps used with a lock will discard all old builds waiting for a shared resource. In this example, inversePrecedence: true instructs the lock to begin most recent waiting build first, ensuring that the most recent code takes precedence.\n\nPutting it all together\n\nEach of these steps can be used independently of the others to control one aspect of a Pipeline or they can be combined to provide powerful, fine-grained control of every aspect of multiple concurrent builds flowing through a Pipeline. Here is a very simple example utilizing all three:\n\nstage('Build') {\n  // The first milestone step starts tracking concurrent build order\n  milestone()\n  node {\n    echo \"Building\"\n  }\n}\n\n// This locked resource contains both Test stages as a single concurrency Unit.\n// Only 1 concurrent build is allowed to utilize the test resources at a time.\n// Newer builds are pulled off the queue first. When a build reaches the\n// milestone at the end of the lock, all jobs started prior to the current\n// build that are still waiting for the lock will be aborted\nlock(resource: 'myResource', inversePrecedence: true){\n  node('test') {\n    stage('Unit Tests') {\n      echo \"Unit Tests\"\n    }\n    stage('System Tests') {\n      echo \"System Tests\"\n    }\n  }\n  milestone()\n}\n\n// The Deploy stage does not limit concurrency but requires manual input\n// from a user. Several builds might reach this step waiting for input.\n// When a user promotes a specific build all preceding builds are aborted,\n// ensuring that the latest code is always deployed.\nstage('Deploy') {\n  input \"Deploy?\"\n  milestone()\n  node {\n    echo \"Deploying\"\n  }\n}\n\nFor a more complete and complex example utilizing all these steps in a Pipeline check out the Jenkinsfile provided with the Docker image for demonstrating Pipeline. This is a working demo that can be quickly set up and run.","title":"Controlling the Flow with Stage, Lock, and Milestone","tags":["pipeline","newfeatures"],"authors":[{"avatar":null,"blog":null,"github":"HRMPW","html":"","id":"hrmpw","irc":null,"linkedin":null,"name":"Patrick Wolf","slug":"/blog/authors/hrmpw","twitter":"hrmpw"}]}},{"node":{"date":"2016-09-19T00:00:00.000Z","id":"9f1352d7-4bb9-5af7-a7b7-3dab3d29a690","slug":"/blog/2016/09/19/blueocean-beta-declarative-pipeline-pipeline-editor/","strippedHtml":"At Jenkins World on Wednesday 14th of September, the Jenkins project was happy to\nintroduce the beta release of Blue Ocean. Blue Ocean is the new user experience\nfor Jenkins, built from the ground up to take advantage of Jenkins Pipeline.\nIt is an entire rethink of the the way that modern developers will use Jenkins.\n\nBlue Ocean is available today via the Jenkins Update Center for Jenkins users\nrunning 2.7.1 and above.\n\nGet the beta\n\nJust search for BlueOcean beta in the Update Center, install it,\nbrowse to the dashboard, and then click the Try BlueOcean UI button on the dashboard.\n\nWhats included?\n\nBack in April we open sourced Blue Ocean\nand shared our vision with the community. We’re very happy that all the things we showed you then have\nshipped in the beta (software projects run on time?!).\n\nFor a refresher on Blue Ocean, watch this short video:\n\nDeclarative Pipeline\n\nWe have heard from the community about the usability of Jenkins\nPipeline. Much of the feedback we received was to a desire to\nconfigure Pipelines rather than script them, and to make it easy for beginners\nto get started with their first Pipeline.\n\nThis is how Declarative Pipeline was born. We’ve introduced a new method whereby\nyou declare how you want your Pipeline to look rather than using Pipeline Script\n - it’s configuration rather than code.\n\nHere’s a small example of a Declarative Pipeline for nodejs that runs the whole\nPipeline inside a Docker container:\n\n// Declarative //\npipeline {\n  agent docker:'node:6.3'\n  stages {\n    stage('build') {\n      sh 'npm --version'\n      sh 'npm install'\n    }\n    stage ('test') {\n      sh 'npm test'\n    }\n  }\n}\n\n// Script //\nnode('docker') {\n  docker.image('node:6.3').inside {\n    stage('build') {\n      sh 'npm --version'\n      sh 'npm install'\n    }\n\n    stage('test') {\n      sh 'npm test'\n    }\n  }\n}\n\nDocker support in Declarative Pipeline allows you to version your application code,\nJenkins Pipeline configuration, and the environment where your pipeline will run,\nall in a single repository. It’s a crazy powerful combination.\n\nDeclarative Pipeline introduces the postBuild section that makes it\neasy to run things conditionally at the end of your Pipeline without the\ncomplexity of the try…​ catch of Pipeline script.\n\n// Declarative //\npostBuild {\n  always {\n    sh 'echo \"This will always run\"'\n  }\n  success {\n    sh 'echo \"This will run only if successful\"'\n  }\n  failure {\n    sh 'echo \"This will run only if failed\"'\n  }\n  unstable {\n    sh 'echo \"This will run only if the run was marked as unstable\"'\n  }\n  changed {\n    sh 'echo \"This will run only if the state of the Pipeline has changed\"'\n    sh 'echo \"For example, the Pipeline was previously failing but is now successful\"'\n    sh 'echo \"... or the other way around :)\"'\n  }\n}\n\n\n// Script //\nnode('docker') {\n  try {\n    stage('build') {\n      /* .. snip .. */\n    }\n    stage('test') {\n      /* .. snip .. */\n    }\n\n    sh 'echo \"This will run only if successful\"'\n  }\n  catch (exc) {\n    if (currentBuild.result == 'UNSTABLE') {\n      sh 'echo \"This will run only if the run was marked as unstable\"'\n    }\n    if (currentBuild.result == 'FAILURE') {\n      sh 'echo \"This will run only if failed\"'\n    }\n  }\n  finally {\n    sh 'echo \"This will always run\"'\n  }\n}\n\nAnd there is so much more!\n\nIf you have the Blue Ocean beta installed you already have Declarative Pipeline.\nWhile Declarative Pipeline is still alpha at the moment, we do encourage you to\nfollow our getting started guide,\n give us feedback on the Jenkins Users mailing list\nor file bugs against the 'pipeline-model-definition' component in JIRA.\n\nIntroducing the Pipeline Editor\n\nThe Pipeline Editor is a graphical user interface that gives Jenkins users the\nsimplest way yet to get started with creating Pipelines in Jenkins. It will also\nsave a lot of time for intermediate and advanced Jenkins users as a way to author\nPipelines.\n\nWhen you build your Pipeline in the Editor and click the save button, the editor\nwill commit a new Jenkinsfile back to your repository in the form of the new\nDeclarative Pipeline. When you want to edit again, Jenkins will read it from\nyour repository exactly how you saw it previously.\n\nThe Pipeline Editor is a work in progress and should arrive in a beta release soon.\n\nThank you\n\nThanks for reading our news from Jenkins World and be sure to check the blog\nfor regular updates!\n\nI’d also like to thank our amazing community for their feedback and support\nas we change the way software teams around the world use Jenkins. We couldn’t\ndo this without you.","title":"Announcing the Blue Ocean beta, Declarative Pipeline and Pipeline Editor","tags":["blueocean","ux","pipeline","jenkinsworld","jenkinsworld2016"],"authors":[{"avatar":null,"blog":null,"github":"i386","html":"","id":"i386","irc":null,"linkedin":null,"name":"James Dumay","slug":"/blog/authors/i386","twitter":"i386"}]}},{"node":{"date":"2016-08-29T00:00:00.000Z","id":"48307a4d-711a-56d1-885f-e9d1945fa4d5","slug":"/blog/2016/08/29/sauce-pipeline/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nTesting web applications across multiple browsers on different platforms can be challenging even for smaller applications.\nWith Jenkins and the\nSauce OnDemand Plugin,\nyou can wrangle that complexity by defining your Pipeline as Code.\n\nPipeline ♥ UI Testing, Too\n\nI recently started looking for a way to do browser UI testing for an open-source JavaScript project to which I contribute.\nThe project is targeted primarily at\nNode.js\nbut we’re committed to maintaining browser-client compatibility as well.\nThat means we should run tests on a matrix of browsers.\nSauce Labs\nhas an \"open-sauce\" program that provides free test instances to open-source projects.\nI decided to try using the\nSauce OnDemand Plugin\nand\nNightwatch.js\nto run Selenium tests on a sample project first, before trying a full-blown suite of tests.\n\nStarting from Framework\n\nI started off by following Sauce Labs' instructions on\n\" Setting up Sauce Labs with Jenkins\"\nas far as I could.\nI installed the\nJUnit and\nSauce OnDemand\nplugins, created an account with Sauce Labs, and\nadded my Sauce Labs credentials to Jenkins.\nFrom there I started to get a little lost.\nI’m new to Selenium and I had trouble understanding how to translate the instructions to my situation.\nI needed a working example that I could play with.\n\nHappily, there’s a whole range of sample projects in\n\" saucelabs-sample-test-frameworks\"\non GitHub, which show how to integrate Sauce Labs with various test frameworks, including Nightwatch.js.\nI forked the Nightwatch.js sample to\nbitwiseman/JS-Nightwatch.js\nand set to writing my Jenkinsfile.\nBetween the sample and the Sauce Labs instructions,\nI was able to write a pipeline that ran five tests on one browser via\nSauce Connect :\n\nnode {\n    stage \"Build\"\n    checkout scm\n\n    sh 'npm install' (1)\n\nstage \"Test\"\n    sauce('f0a6b8ad-ce30-4cba-bf9a-95afbc470a8a') { (2)\nsauceconnect(options: '', useGeneratedTunnelIdentifier: false, verboseLogging: false) { (3)\nsh './node_modules/.bin/nightwatch -e chrome --test tests/guineaPig.js || true' (4)\njunit 'reports/**' (5)\nstep([$class: 'SauceOnDemandTestPublisher']) (6)\n}\n    }\n}\n\n1\nInstall dependencies\n\n2\nUse my\npreviously added sauce credentials\n\n3\nStart up the\nSauce Connect\ntunnel to Sauce Labs\n\n4\nRun Nightwatch.js\n\n5\nUse JUnit to track results and show a trend graph\n\n6\nLink result details from Sauce Labs\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit url:'https://github.com/bitwiseman/JS-Nightwatch.js', branch: 'sauce-pipeline'.\n\nI ran this job a few times to get the JUnit report to show a trend graph.\n\nThis sample app generates the SauceOnDemandSessionID for each test, enabling the Jenkins Sauce OnDemand Plugin’s result publisher to link results to details Sauce Labs captured during the run.\n\nAdding Platforms\n\nNext I wanted to add a few more platforms to my matrix.\nThis would require changing both the test framework configuration and the pipeline.\nI’d need to add new named combinations of platform, browser, and browser version (called \"environments\") to the Nightwatch.js configuration file,\nand modify the pipeline to run tests in those new environments.\n\nThis is a perfect example of the power of pipeline as code.\nIf I were working with a separately configured pipeline,\nI’d have to make the change to the test framework, then change the pipeline manually.\nWith my pipeline checked in as code,\nI could change both in one commit,\npreventing errors resulting from pipeline configurations going out of sync from the rest of the project.\n\nI added three new environments to nightwatch.json :\n\n\"test_settings\" : {\n  \"default\": { /*----8 <----*/ },\n  \"chrome\": { /*----8 <----*/ },\n\n  \"firefox\": {\n    \"desiredCapabilities\": {\n      \"platform\": \"linux\",\n      \"browserName\": \"firefox\",\n      \"version\": \"latest\"\n    }\n  },\n  \"ie\": {\n    \"desiredCapabilities\": {\n      \"platform\": \"Windows 10\",\n      \"browserName\": \"internet explorer\",\n      \"version\": \"latest\"\n    }\n  },\n  \"edge\": {\n    \"desiredCapabilities\": {\n      \"platform\": \"Windows 10\",\n      \"browserName\": \"MicrosoftEdge\",\n      \"version\": \"latest\"\n    }\n  }\n}\n\nAnd I modified my Jenkinsfile to call them:\n\n//----8 (1)\n'chrome',\n        'firefox',\n        'ie',\n        'edge'\n    ].join(',')\n    // Run selenium tests using Nightwatch.js\n    sh \"./node_modules/.bin/nightwatch -e ${configs} --test tests/guineaPig.js\" (2)\n} //----8\n\n1\nUsing an array to improve readability and make it easy to add more platforms later.\n\n2\nChanged from single-quoted string to double-quoted to support variable substitution.\n\nTest frameworks have bugs too. Nightwatch.js (v0.9.8) generates incomplete JUnit files,\nreporting results without enough information in them to distinguish between platforms.\nI implemented a fix for it and\nsubmitted a PR to Nightwatch.js.\nThis blog shows output with that fix applied locally.\n\nAs expected, Jenkins picked up the new pipeline and ran Nightwatch.js on four platforms.\nSauce Labs of course recorded the results and correctly linked them into this build.\nNightwatch.js was already configured to use multiple worker threads to run tests against those platforms in parallel, and\nmy Sauce Labs account supported running them all at the same time,\nletting me cover four configurations in less that twice the time,\nand that added time was most due to individual new environments taking longer to complete.\nWhen I move to the actual project, this will let me run broad acceptance passes quickly.\n\nConclusion: To Awesome and Beyond\n\nConsidering the complexity of the system, I was impressed with how easy it was to integrate Jenkins with Sauce OnDemand to start testing on multiple browsers.\nThe plugin worked flawlessly with Jenkins Pipeline.\nI went ahead and ran some additional tests to show that failure reporting also behaved as expected.\n\n//----8 (1)\n//----8\n\n1\nRemoved --test filter to run all tests\n\nEpilogue: Pipeline vs. Freestyle\n\nJust for comparison here’s the final state of this job in Freestyle UI versus fully-commented pipeline code:\n\nThis includes the\nAnsiColor Plugin\nto support Nightwatch.js' default ANSI color output.\n\nFreestyle\n\nPipeline\n\nnode {\n    stage \"Build\"\n    checkout scm\n\n    // Install dependencies\n    sh 'npm install'\n\n    stage \"Test\"\n\n    // Add sauce credentials\n    sauce('f0a6b8ad-ce30-4cba-bf9a-95afbc470a8a') {\n        // Start sauce connect\n        sauceconnect(options: '', useGeneratedTunnelIdentifier: false, verboseLogging: false) {\n\n            // List of browser configs we'll be testing against.\n            def platform_configs = [\n                'chrome',\n                'firefox',\n                'ie',\n                'edge'\n            ].join(',')\n\n            // Nightwatch.js supports color ouput, so wrap this step for ansi color\n            wrap([$class: 'AnsiColorBuildWrapper', 'colorMapName': 'XTerm']) {\n\n                // Run selenium tests using Nightwatch.js\n                // Ignore error codes. The junit publisher will cover setting build status.\n                sh \"./node_modules/.bin/nightwatch -e ${platform_configs} || true\"\n            }\n\n            junit 'reports/**'\n\n            step([$class: 'SauceOnDemandTestPublisher'])\n        }\n    }\n}\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit url:'https://github.com/bitwiseman/JS-Nightwatch.js', branch: 'sauce-pipeline'.\n\nNot only is the pipeline as code more compact,\nit also allows for comments to further clarify what is being done.\nAnd as I noted earlier,\nchanges to this pipeline code are committed the same as changes to the rest of the project,\nkeeping everything synchronized, reviewable, and testable at any commit.\nIn fact, you can view the full set of commits for this blog post in the\nblog/sauce-pipeline\nbranch of the\nbitwiseman/JS-Nightwatch.js\nrepository.\n\nLinks\n\nSauce OnDemand Plugin\n\nbitwiseman/JS-Nightwatch.js\n\nsaucelabs-sample-test-frameworks","title":"Browser-testing with Sauce OnDemand and Pipeline","tags":["tutorial","pipeline","plugins","saucelabs","selenium","nightwatch"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2016-08-22T00:00:00.000Z","id":"5a42a05c-41f1-5105-866c-97942f6d788d","slug":"/blog/2016/08/22/ewm-stable-release/","strippedHtml":"This blog post is the last one from the series of\nGoogle Summer of Code 2016, External Workspace Manager Plugin project.\nThe previous posts are:\n\nIntroductory blog post\n\nAlpha release announcement\n\nBeta release announcement\n\nIn this post I would like to announce the 1.0.0 release of the External Workspace Manager Plugin version to the main\nupdate center.\n\nHere’s a highlight of the available features:\n\nWorkspace share and reuse across multiple jobs, running on different nodes\n\nAutomatic workspace cleanup\n\nProvide custom workspace path on the disk\n\nDisk Pool restrictions\n\nFlexible Disk allocation strategies\n\nAll the above are detailed, with usage examples, on the plugin’s\ndocumentation page.\n\nFuture work\n\nCurrently, there is work in progress for the workspace browsing feature (see pull request\n#37).\nAfterwards, I’m planning to integrate fingerprints, so that the user can view a specific workspace in which\nother jobs was used.\nA particular feature that would be nice to have is to integrate the plugin with at least one disk provider\n(e.g. Amazon EBS, Google Cloud Storage).\n\nMany other features and improvements are still to come, they are grouped in the phase 3 EPIC:\nJENKINS-37543.\nThe plugin’s repository is on GitHub.\nIf you’d like to come up with new features or ideas, contributions are very welcome.\n\nClosing\n\nThis was a Google Summer of Code 2016 project.\nA summary of the contributions that I’ve made to the Jenkins project during this time may be found\nhere.\nIt was a great experience, from which I learned a lot, and I’d wish I could repeat it every year.\n\nI’d like to thank to my mentors, Oleg Nenashev and\nMartin d’Anjou for all their support, good advices and help they gave me.\nAlso, thanks to the Jenkins contributors with which I have interacted and helped me during this period.\n\nIf you have any issues in setting up or using the plugin, please feel free to ask me on the plugin’s Gitter\nchat.\nAny feedback is welcome, and you may provide it either on the Gitter chat, or on\nJira by using the external-workspace-manager-plugin component.\n\nLinks\n\nProject repository\n\nWork product page\n\nGSoC page\n\nJenkins GSoC Page","title":"GSoC: External Workspace Manager for Pipeline is released","tags":["pipeline","plugins","external-workspace-manager","gsoc"],"authors":[{"avatar":null,"blog":null,"github":"alexsomai","html":"","id":"alexsomai","irc":null,"linkedin":null,"name":"Alexandru Somai","slug":"/blog/authors/alexsomai","twitter":"alex_somai"}]}}]}},"pageContext":{"tag":"pipeline","limit":8,"skip":56,"numPages":13,"currentPage":8}},
    "staticQueryHashes": ["3649515864"]}