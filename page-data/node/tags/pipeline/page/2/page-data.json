{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/pipeline/page/2",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2019-11-22T00:00:00.000Z","id":"e5f5bc1a-7d2f-5807-8b41-7478e232a4f1","slug":"/blog/2019/11/22/welcome-to-the-matrix/","strippedHtml":"I often find myself needing to run the same actions on a bunch of different configurations.\nUp to now, that meant I had to make multiple copies of the same stages in my pipelines.\nWhen I needed to make changes, I had to make the same changes in multiple places throughout my pipeline.\nMaintaining even a small number of configuration was difficult for larger pipelines.\n\nDeclarative Pipeline 1.5.0-beta1 (now available from the\nJenkins Experimental Update site) adds a new matrix section that lets me specify a list stages once and then run that same list in parallel on multiple configurations.\nLet’s take a look!\n\nSingle configuration pipeline\n\nI’ll start with a simple pipeline with build and test stages.\nI’m using echo steps as placeholders for my build and test actions.\n\nJenkinsfile\n\npipeline {\n    agent none\n    stages {\n        stage('BuildAndTest') {\n            agent any\n            stages {\n                stage('Build') {\n                    steps {\n                        echo 'Do Build'\n                    }\n                }\n                stage('Test') {\n                    steps {\n                        echo 'Do Test'\n                    }\n                }\n            }\n        }\n    }\n}\n\nPipeline for multiple platforms and browsers\n\nI’d like to run my build and tests on a combination of platforms and browsers.\nThe new matrix directive lets me specify a set of axes.\nEach axis has a name and a list of one or more values.\nWhen the pipeline is run, Jenkins will take those and run my stages on all possible combinations of values from each axis.\nAll cells in a matrix run in parallel (limited only by the number of available agents).\nStages within each cell are run sequentially.\n\nMy matrix has two axes: PLATFORM and BROWSER.\nI have three values for PLATFORM and four values for BROWSER resulting in my stages being run with twelve different combinations.\nI’ve changed my echo steps to use the axis values for each cell.\n\nJenkinsfile\n\npipeline {\n    agent none\n    stages {\n        stage('BuildAndTest') {\n            matrix {\n                agent any\n                axes {\n                    axis {\n                        name 'PLATFORM'\n                        values 'linux', 'windows', 'mac'\n                    }\n                    axis {\n                        name 'BROWSER'\n                        values 'firefox', 'chrome', 'safari', 'edge'\n                    }\n                }\n                stages {\n                    stage('Build') {\n                        steps {\n                            echo \"Do Build for ${PLATFORM} - ${BROWSER}\"\n                        }\n                    }\n                    stage('Test') {\n                        steps {\n                            echo \"Do Test for ${PLATFORM} - ${BROWSER}\"\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nLog output (truncated)\n\n...\n[Pipeline] stage\n[Pipeline] { (BuildAndTest)\n[Pipeline] parallel\n[Pipeline] { (Branch: Matrix - OS = 'linux', BROWSER = 'firefox')\n[Pipeline] { (Branch: Matrix - OS = 'windows', BROWSER = 'firefox')\n[Pipeline] { (Branch: Matrix - OS = 'mac', BROWSER = 'firefox')\n[Pipeline] { (Branch: Matrix - OS = 'linux', BROWSER = 'chrome')\n[Pipeline] { (Branch: Matrix - OS = 'windows', BROWSER = 'chrome')\n[Pipeline] { (Branch: Matrix - OS = 'mac', BROWSER = 'chrome')\n[Pipeline] { (Branch: Matrix - OS = 'linux', BROWSER = 'safari')\n[Pipeline] { (Branch: Matrix - OS = 'windows', BROWSER = 'safari')\n[Pipeline] { (Branch: Matrix - OS = 'mac', BROWSER = 'safari')\n[Pipeline] { (Branch: Matrix - OS = 'linux', BROWSER = 'edge') (hide)\n[Pipeline] { (Branch: Matrix - OS = 'windows', BROWSER = 'edge')\n[Pipeline] { (Branch: Matrix - OS = 'mac', BROWSER = 'edge')\n...\nDo Build for linux - safari\nDo Build for linux - firefox\nDo Build for windows - firefox\nDo Test for linux - firefox\nDo Build for mac - firefox\nDo Build for linux - chrome\nDo Test for windows - firefox\n...\n\nExcluding invalid combinations\n\nNow that I have my basic matrix created, I’ve noticed that I have some invalid combinations.\nMicrosoft Edge only runs on Windows and there isn’t a Linux version of Safari.\n\nI can remove invalid cells from my matrix using exclude directives. Each exclude has one or more axis directives with name and values.\nThe axis directives inside an exclude generate a set of combinations (similar to generating the matrix cells).\nThe matrix cells that match all the values from an exclude combination are removed from the matrix.\nIf I have more than one exclude directive, each are evaluated separately to remove cells.\n\nWhen dealing with a long lists of values to exclude, I can use notValues instead of values to specify axis values we don’t want excluded.\nYes, that’s a double negative, so it can get a little confusing.\nI try to use it only when I really need it.\n\nIn my sample pipeline below, I specifically exclude the linux, safari combination and I also exclude any platform that is not windows with the edge browser.\n\nThis pipeline uses two axes but there is no limit on the number of axis directives.\n\nAlso, in this pipeline each exclude specifies values for both axes, but that is not required.\nIf we wanted to run only \"linux\" cells, we could use the following exclude :\n\nexclude {\n    axis {\n        name 'PLATFORM'\n        notValues 'linux'\n    }\n}\n\npipeline {\n    agent none\n    stages {\n        stage('BuildAndTest') {\n            matrix {\n                agent any\n                axes {\n                    axis {\n                        name 'PLATFORM'\n                        values 'linux', 'windows', 'mac'\n                    }\n                    axis {\n                        name 'BROWSER'\n                        values 'firefox', 'chrome', 'safari', 'edge'\n                    }\n                }\n                excludes {\n                    exclude {\n                        axis {\n                            name 'PLATFORM'\n                            values 'linux'\n                        }\n                        axis {\n                            name 'BROWSER'\n                            values 'safari'\n                        }\n                    }\n                    exclude {\n                        axis {\n                            name 'PLATFORM'\n                            notValues 'windows'\n                        }\n                        axis {\n                            name 'BROWSER'\n                            values 'edge'\n                        }\n                    }\n                }\n                stages {\n                    stage('Build') {\n                        steps {\n                            echo \"Do Build for ${PLATFORM} - ${BROWSER}\"\n                        }\n                    }\n                    stage('Test') {\n                        steps {\n                            echo \"Do Test for ${PLATFORM} - ${BROWSER}\"\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nLog output (truncated)\n\n...\n[Pipeline] stage\n[Pipeline] { (BuildAndTest)\n[Pipeline] parallel\n[Pipeline] { (Branch: Matrix - OS = 'linux', BROWSER = 'firefox')\n[Pipeline] { (Branch: Matrix - OS = 'windows', BROWSER = 'firefox')\n[Pipeline] { (Branch: Matrix - OS = 'mac', BROWSER = 'firefox')\n[Pipeline] { (Branch: Matrix - OS = 'linux', BROWSER = 'chrome')\n[Pipeline] { (Branch: Matrix - OS = 'windows', BROWSER = 'chrome')\n[Pipeline] { (Branch: Matrix - OS = 'mac', BROWSER = 'chrome')\n[Pipeline] { (Branch: Matrix - OS = 'windows', BROWSER = 'safari')\n[Pipeline] { (Branch: Matrix - OS = 'mac', BROWSER = 'safari')\n[Pipeline] { (Branch: Matrix - OS = 'windows', BROWSER = 'edge')\n...\nDo Build for linux - firefox\n...\n\nControlling cell behavior at runtime\n\nInside the matrix directive I can also add \"per-cell\" directives.\nThese are the same directives that I would add to a stage and they let me control the behavior of each cell in the matrix.\nThese directives can use the axis values from their cell as part of their inputs, allowing me to customize the behavior of each cell to match its axis values.\n\nOn my Jenkins server I have configured agents with labels that match the OS for each agent (\"linux-agent\", \"windows-agent\", and \"mac-agent\").\nTo run each cell in my matrix on the appropriate operating system, I configure the label for that cell using Groovy string templating.\n\nmatrix {\n    axes { ... }\n    excludes { ... }\n    agent {\n        label \"${PLATFORM}-agent\"\n    }\n    stages { ... }\n    // ...\n}\n\nOccasionally I run my pipeline manually from the Jenkins Web UI.\nWhen I do that, I’d like to be able to select just one platform to run.\nThe axis and exclude directives define the static set of cells that make up the matrix.\nThat set of combinations is generated before the start of the run, before any parameters are processed.\nWhat this means is that I can’t add or remove cells from a matrix after the job has started.\n\nThe \"per-cell\" directives, on the other hand, are evaluated at runtime.\nI can use the \"per-cell\" when directive inside matrix to control which cells in the matrix are executed.\nI’ll add a choice parameter with the list of platforms, and add conditions to the when directive, which will either let all platforms execute, or only execute cells that match my selected platform.\n\npipeline {\n    parameters {\n        choice(name: 'PLATFORM_FILTER', choices: ['all', 'linux', 'windows', 'mac'], description: 'Run on specific platform')\n    }\n    agent none\n    stages {\n        stage('BuildAndTest') {\n            matrix {\n                agent {\n                    label \"${PLATFORM}-agent\"\n                }\n                when { anyOf {\n                    expression { params.PLATFORM_FILTER == 'all' }\n                    expression { params.PLATFORM_FILTER == env.PLATFORM }\n                } }\n                axes {\n                    axis {\n                        name 'PLATFORM'\n                        values 'linux', 'windows', 'mac'\n                    }\n                    axis {\n                        name 'BROWSER'\n                        values 'firefox', 'chrome', 'safari', 'edge'\n                    }\n                }\n                excludes {\n                    exclude {\n                        axis {\n                            name 'PLATFORM'\n                            values 'linux'\n                        }\n                        axis {\n                            name 'BROWSER'\n                            values 'safari'\n                        }\n                    }\n                    exclude {\n                        axis {\n                            name 'PLATFORM'\n                            notValues 'windows'\n                        }\n                        axis {\n                            name 'BROWSER'\n                            values 'edge'\n                        }\n                    }\n                }\n                stages {\n                    stage('Build') {\n                        steps {\n                            echo \"Do Build for ${PLATFORM} - ${BROWSER}\"\n                        }\n                    }\n                    stage('Test') {\n                        steps {\n                            echo \"Do Test for ${PLATFORM} - ${BROWSER}\"\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nIf I run this Pipeline from the Jenkins UI and set the PLATFORM_FILTER parameter to mac, I’ll get something like the output below:\n\nLog output (truncated - PLATFORM_FILTER = 'mac' )\n\n...\n[Pipeline] stage\n[Pipeline] { (BuildAndTest)\n[Pipeline] parallel\n[Pipeline] { (Branch: Matrix - OS = 'linux', BROWSER = 'firefox')\n[Pipeline] { (Branch: Matrix - OS = 'windows', BROWSER = 'firefox')\n[Pipeline] { (Branch: Matrix - OS = 'mac', BROWSER = 'firefox')\n[Pipeline] { (Branch: Matrix - OS = 'linux', BROWSER = 'chrome')\n[Pipeline] { (Branch: Matrix - OS = 'windows', BROWSER = 'chrome')\n[Pipeline] { (Branch: Matrix - OS = 'mac', BROWSER = 'chrome')\n[Pipeline] { (Branch: Matrix - OS = 'windows', BROWSER = 'safari')\n[Pipeline] { (Branch: Matrix - OS = 'mac', BROWSER = 'safari')\n[Pipeline] { (Branch: Matrix - OS = 'windows', BROWSER = 'edge')\n...\nStage \"Matrix - OS = 'linux', BROWSER = 'chrome'\" skipped due to when conditional\nStage \"Matrix - OS = 'linux', BROWSER = 'firefox'\" skipped due to when conditional\n...\nDo Build for mac - firefox\nDo Build for mac - chrome\nDo Build for mac - safari\n...\nStage \"Matrix - OS = 'windows', BROWSER = 'chrome'\" skipped due to when conditional\nStage \"Matrix - OS = 'windows', BROWSER = 'edge'\" skipped due to when conditional\n...\nDo Test for mac - safari\nDo Test for mac - firefox\nDo Test for mac - chrome\n\nCome join me at DevOps World | Jenkins World 2019 for \" Declarative Pipeline 2019: Tips, Tricks and What’s Next \".\nI’ll go over what’s been added to Pipeline in the last year (including matrix) and discuss ideas about where pipeline should go next.\n\nConclusion\n\nIn this blog post, we’ve looked at how to use the matrix directive to make concise but powerful declarative pipelines.\nAn equivalent pipeline created without matrix would easily be several times larger, and much harder to understand and maintain.\n\nMatrix is now available from the experimental update center.\nIt will be released to the main update center as soon as we’re done putting the finishing touches on the documentation and online help.\n\nLinks\n\nJenkins Experimental Update Center\n\nUsing the Jenkins Experimental Update Center","title":"Welcome to the Matrix","tags":["pipeline","plugins","declarative","matrix"],"authors":[{"avatar":{"childImageSharp":null},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"blog/author/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2019-09-10T00:00:00.000Z","id":"d5e0f766-e02d-50ec-a790-2201ba4d74b9","slug":"/blog/2019/09/10/introducing-the-jira-software-plugin-for-jenkins/","strippedHtml":"According to a recent survey we conducted, software & IT teams on average use 4+ tools to move code from development to customer-facing production. As a result, teams struggle with keeping the status of work updated and understanding the overall health of their delivery pipeline.\n\nTo solve this problem, I am excited to announce that we built an official Jenkins plugin for Jira Software Cloud. The plugin automatically associates build and deployment information from Jenkins with relevant Jira issues and exposes key information about your pipeline across Jira issues, boards and via JQL. This means you can use Jira Software to automatically update and track issues through your complete development pipeline, from backlog to release.\n\nI hope this plugin adds value to you and your team. If you are interested in contributing or forking this plug-in you can head over to our project on the Jenkins GitHub repo to get started.\n\nBetter collaboration between teams\n\nThis new information view is so powerful because historically it was dispersed across multiple tools only accessible to a few members of your team. Now anyone involved in the software delivery process can self-serve this information. For example, product managers, QA, and support teams can view which features have been deployed to customers and which are still waiting in staging environments.\n\nWith better information sharing between tools in your delivery stack, you can also improve cross-collaboration between teams. Teams such as QA and operations can collaborate in the software teams next sprint. For example, you can use build information in Jira Software to create a workflow between QA and developers and create a rapid feedback loop for testing at any point in your development process.\n\nUse Jira’s Querying Language for advanced views\n\nIn addition to building better ways to collaborate, these integrations also give your team deeper insight into the development pipeline from within Jira Software. You can now create powerful views into your delivery pipeline with JQL queries across multiple connected tools. For example, you can write a custom JQL query to report all Jira issues that have been deployed to production but still have an open PR.\n\ndeploymentEnvironmentType ~ “production“ AND development[pullrequests].open\n\nGet started\n\nIn Jira Software Cloud\n\nCreate OAuth credentials in Jira for Jenkins\n\nNavigate to Jira home > Jira settings > Apps.\n\nSelect OAuth credentials.\n\nSelect Create credentials.\n\nEnter the following details:\n\nApp name - Jenkins\n\nApp logo - A URL to the Jenkins logo, which will be used as an icon in the list of credentials. Eg: https://jenkins.yourcompany.com/logo.png\n\nServer base URL - The URL to your Jenkins server. Eg: https://jenkins.yourcompany.com\n\nIn Jenkins\n\nInstall the Jenkins plugin\n\nLogin to your Jenkins server and navigate to the Plugin Manager.\n\nSelect the 'Available' tab and search for 'Atlassian Jira Software Cloud' as the plugin name then install it.\n\nThe open-source plugin is hosted in the Jenkins GitHub account. You can check it out here.\n\nSet up Jenkins credentials\n\nIn Jenkins, go to Manage Jenkins > Configure System screen and scroll to the Jira Software Cloud integration section.\n\nSelect Add Jira Cloud Site > Jira Cloud Site. The Site name, ClientID, and Secret fields display.\n\nEnter the following details:\n\nSite name: The URL for your Jira Cloud site, for example yourcompany.atlassian.net.\n\nClient ID: Copy from OAuth credentials screen (Client ID column).\n\nSecret: Select Add > Jenkins.\n\nFor Kind, select Secret text.\n\nFor Secret, copy from OAuth credentials screen (Secret column).\n\nFor Description, provide a helpful description\n\nSelect Test settings to make sure your credentials are valid for your Jira site.\n\nHow to use the plugin\n\nTo start using the integration:\n\nGo into a specific pipeline in Jenkins ( Note: Your pipeline must be a 'Multibranch Pipeline' ).\n\nFrom the left-hand menu, select Pipeline Syntax.\n\nIn the Snippet Generator, select jiraSendDeploymentInfo or jiraSendBuildInfo from the dropdown list of Sample Steps and fill in the relevant details.\n\nSelect Generate Pipeline Script and copy/paste the output into your Jenkinsfile on the relevant Repository you are using. This will be used to notify Jira when you run that pipeline on that repo.\n\nFor sending build information\n\nThis is an example snippet of a very simple ‘build’ stage set up in a Jenkinsfile. After the pipeline is run, it will post the build information to your Jira Cloud site by looking at the branch name. If there is a Jira issue key (e.g. “TEST-123”) in the branch name, it will send the data over to Jira.\n\nJenkinsfile example\n\npipeline {\n     agent any\n     stages {\n         stage('Build') {\n             steps {\n                 echo 'Building...'\n             }\n             post {\n                 always {\n                     jiraSendBuildInfo site: 'example.atlassian.net'\n                 }\n             }\n         }\n     }\n }\n\nFor sending deployment information\n\nThis is an example snippet of two stages that run on any change to the staging or master branch. Again, we use a post step to send deployment data to Jira and the relevant issues. Here, the environmentId, environmentName, and environmentType need to be set to whatever you want to appear in Jira.\n\nJenkinsfile example\n\npipeline {\n     agent any\n     stages {\n         stage('Deploy - Staging') {\n             when {\n                 branch 'master'\n             }\n             steps {\n                 echo 'Deploying to Staging from master...'\n             }\n             post {\n                 always {\n                     jiraSendDeploymentInfo site: 'example.atlassian.net', environmentId: 'us-stg-1', environmentName: 'us-stg-1', environmentType: 'staging'\n                 }\n             }\n         }\n         stage('Deploy - Production') {\n            when {\n                branch 'master'\n            }\n            steps {\n                echo 'Deploying to Production from master...'\n            }\n            post {\n                always {\n                    jiraSendDeploymentInfo site: 'example.atlassian.net', environmentId: 'us-prod-1', environmentName: 'us-prod-1', environmentType: 'production'\n                }\n            }\n         }\n     }\n }\n\nThe entire Jenkinsfile may look something like this. This is only meant to represent an example of what the Jira snippets could look like within a stage or step.\n\nJenkinsfile example\n\npipeline {\n     agent any\n     stages {\n         stage('Build') {\n             steps {\n                 echo 'Building...'\n             }\n             post {\n                 always {\n                     jiraSendBuildInfo site: 'example.atlassian.net'\n                 }\n             }\n         }\n         stage('Deploy - Staging') {\n             when {\n                 branch 'master'\n             }\n             steps {\n                 echo 'Deploying to Staging from master...'\n             }\n             post {\n                 always {\n                     jiraSendDeploymentInfo site: 'example.atlassian.net', environmentId: 'us-stg-1', environmentName: 'us-stg-1', environmentType: 'staging'\n                 }\n             }\n         }\n         stage('Deploy - Production') {\n            when {\n                branch 'master'\n            }\n            steps {\n                echo 'Deploying to Production from master...'\n            }\n            post {\n                always {\n                    jiraSendDeploymentInfo site: 'example.atlassian.net', environmentId: 'us-prod-1', environmentName: 'us-prod-1', environmentType: 'production'\n                }\n            }\n         }\n     }\n }\n\nQuestions or feedback?\n\nIf you have any questions, please contact Atlassian support and they will route it to the correct team to help you.","title":"Introducing the Jira Software plugin for Jenkins","tags":["jira","plugin","pipeline"],"authors":[{"avatar":null,"blog":null,"github":"rafalmyslek","html":"","id":"rafalmyslek","irc":null,"linkedin":null,"name":"Rafal Myslek","slug":"blog/author/rafalmyslek","twitter":null}]}},{"node":{"date":"2019-08-23T00:00:00.000Z","id":"eddbf0dc-37bf-5f4d-8a6e-0bef8711db73","slug":"/blog/2019/08/23/introducing-gitlab-branch-source-plugin/","strippedHtml":"The GitLab Branch Source Plugin has come out of its beta stage and has been released to the Jenkins update center. It allows you to create job based on GitLab user or group or subgroup project(s). You can either:\n\nImport a single project’s branches as jobs from a GitLab user/group/subgroup (Multibranch Pipeline Job)\n\nImport all or a subset of projects as jobs from a GitLab user/group/subgroup (GitLab Group Job or GitLab Folder Organization)\n\nThe GitLab Group project scans the projects, importing the pipeline jobs it identifies based on the criteria provided. After a project is imported, Jenkins immediately runs the jobs based on the Jenkinsfile pipeline script and notifies the status to GitLab Pipeline Status. This plugin unlike other Branch Source Plugins provides GitLab server configuration which can be configured in Configure System. Jenkins Configuration as Code (JCasC) can also be used to configure the server. To learn more about server configuration see my previous blog post.\n\nRequirements\n\nJenkins - 2.176.2 (LTS)\n\nGitLab - v11.0+\n\nCreating a Job\n\nTo create a Multibranch Pipeline Job (with GitLab branch source) or GitLab Group Job, you must have GitLab Personal Access Token added to the server configuration. The credentials is used to fetch meta data of the project(s) and to set up hooks on GitLab Server. If the token has admin access you can also set up System Hooks while Web Hooks can be set up from any user token.\n\nCreate a Multibranch Pipeline Job\n\nGo to Jenkins > New Item > Multibranch Pipeline > Add Source > GitLab Project\n\nServer - Select your desired GitLab server from the dropdown, needs to be configured before creating this job.\n\nCheckout Credentials - Add credentials of type SSHPrivateKey or Username/Password if there are any private projects to be built by the plugin. If all projects are public then no checkout credentials required. Checkout credential is different from the credential (of type GitLab Personal Access Token) setup in GitLab server config.\n\nOwner - Can be a user, group or subgroup. Depending on this the Projects field is populated.\n\nProjects - Select the project you want to build from the dropdown.\n\nBehaviours - These traits are very powerful tool to configure the build logic and post build logic. We have defined new traits. You can see all the information in repository documentation.\n\nSave and wait for the branches indexing. You are free to navigate from here, the job progress is displayed to the left hand side.\n\nAfter the indexing, the imported project listed all the branches, merge requests and tags as jobs.\n\nOn visiting each job, you will find some action items on the left hand side:\n\nYou can trigger the job manually by selecting Build Now.\n\nYou can visiting the particular branch/merge request/tag on your GitLab Server by selecting the corresponding button.\n\nCreate a GitLab Group Job Type\n\nGo to Jenkins > New Item > GitLab Group\n\nYou can notice the configuration is very similar to Multibranch Pipeline Job with only Projects field missing. You can add all the projects inside your Owner i.e. User/Group/Subgroup. The form validation will check with your GitLab server if the owner is valid. You can add Discover subgroup project trait which allows you to discover this child projects of all subgroups inside a Group or Subgroup but this trait is not applicable to User. While indexing, web hook is created in each project. GitLab Api doesn’t support creation of Group web hooks so this plugin doesn’t support that feature which is only available in GitLab EE.\n\nYou can now explore your imported projects, configuring different settings on each of those folders if needed.\n\nGitLab Pipeline Status Notification\n\nGitLab is notified about build status from the point of queuing of jobs.\n\nSuccess - the job was successful\n\nFailure - the job failed and the merge request is not ready to be merged\n\nError - something unexpected happened; example: the job was aborted in Jenkins\n\nPending - the job is waiting in the build queue\n\nOn GitLab Pipeline status are hyperlinks to the corresponding Jenkins job build. To see the Pipeline Stages and the console output you will be required to visit your Jenkins server. We also planned to notify the pipeline stages to GitLab but it came with some drawbacks which has been addressed so far but there is future plan to add it as trait.\n\nYou can also skip notifying GitLab about the pipeline status by selecting Skip pipeline status notifications from the traits list.\n\nMerge Requests\n\nImplementing support for Merge Requests for the projects was challenging. First, MRs are of 2 types i.e. Origin branches and Forked Project branches so there had to be different implementation for each head. Second, MRs from forks can be from untrusted sources, so a new strategy Trust Members was implemented which allows CI to build MRs only from trusted users who have accesslevel of Developer / Maintainer / Owner.\n\nThird, MRs from forks do not support pipeline status notification due to GitLab issue, see this. You can add a trait Log Build Status as Comment on GitLab that allows you to add a sudo user (leave empty if you want owner user) to comment on the commit/tag/mrs the build result. To add a sudo user your token must have admin access. By default only failure/error are logged as comment but you can also enable logging of success build by ticking the checkbox.\n\nSometimes, Merge Requests fail due to external errors so you want to trigger rebuild of mr by commenting jenkins rebuild. To enable this trigger add the trait Trigger build on merge request comment. The comment body can be changed in the trait. For security reasons, commentor should have Developer / Maintainer / Owner accesslevel in the project.\n\nHooks\n\nWeb hooks are automatically created on your projects if configured to do so in server configuration. Web hooks are ensured to pass through a CSRF filter. Jenkins listens to web hooks on the path /gitlab-webhook/post. On GitLab web hooks are triggered on the following events:\n\nPush Event - when a commit or branch is pushed\n\nTag Event - when a new tag is created\n\nMerge Request Event - when a merge request is created/updated\n\nNote Event - when a comment is made on a merge request\n\nYou can also set up System Hooks on your GitLab server if your token has admin access. System hooks are triggered when new projects are created, Jenkins triggers a rescan of the new project based on the configuration and sets up web hook on it. Jenkins listens to system hooks on the path /gitlab-systemhook/post. On GitLab system hooks are triigered on Repository Update Events.\n\nYou can also use Override Hook Management mode trait to override the default hook management and choose if you want to use a different context (say Item) or disable it altogether.\n\nJob DSL and JCasC\n\nYou can use Job DSL to create jobs. Here’s an example of Job DSL script:\n\norganizationFolder('GitLab Organization Folder') {\n    description(\"GitLab org folder created with Job DSL\")\n    displayName('My Project')\n    // \"Projects\"\n    organizations {\n        gitLabSCMNavigator {\n            projectOwner(\"baymac\")\n            credentialsId(\"i<3GitLab\")\n            serverName(\"gitlab-3214\")\n            // \"Traits\" (\"Behaviours\" in the GUI) that are \"declarative-compatible\"\n            traits {\n                subGroupProjectDiscoveryTrait() // discover projects inside subgroups\n                gitLabBranchDiscovery {\n                    strategyId(3) // discover all branches\n                }\n                originMergeRequestDiscoveryTrait {\n                    strategyId(1) // discover MRs and merge them with target branch\n                }\n                gitLabTagDiscovery() // discover tags\n            }\n        }\n    }\n    // \"Traits\" (\"Behaviours\" in the GUI) that are NOT \"declarative-compatible\"\n    // For some 'traits, we need to configure this stuff by hand until JobDSL handles it\n    // https://issues.jenkins.io/browse/JENKINS-45504\n    configure {\n        def traits = it / navigators / 'io.jenkins.plugins.gitlabbranchsource.GitLabSCMNavigator' / traits\n        traits\n\nYou can also use JCasC to directly create job from a Job DSL script. For example see the plugin repository.\n\nHow to talk to us about bugs or new features?\n\nThis project uses Jenkins JIRA to track issues. You can file issues under gitlab-branch-source-plugin component.\n\nSend your mail in the Developer Mailing list.\n\nJoin our Gitter channel.\n\nFuture work\n\nActively maintain GitLab Branch Source Plugin and take feedbacks from users to improve the plugin’s user experience.\n\nExtend support for GitLab Pipeline to Blueocean.\n\nResources\n\nGitLab API Plugin\n\nGitLab API Plugin Wiki\n\nGitLab Branch Source Plugin\n\nProject Summary\n\nGitHub Branch Source Plugin Release\n\nThank you Jenkins and Google Summer of Code :)","title":"Introducing new GitLab Branch Source Plugin","tags":["gitlab","plugins","pipeline","multibranch","gsoc","gsoc2019"],"authors":[{"avatar":{"childImageSharp":null},"blog":null,"github":"baymac","html":"<div class=\"paragraph\">\n<p>GSoC 2019 student under Jenkins project. Maintainer of GitLab Branch Source Plugin. Software Developer at Udaan, India.</p>\n</div>","id":"baymac","irc":null,"linkedin":"parichay.barpanda","name":"Parichay Barpanda","slug":"blog/author/baymac","twitter":"baymac04"}]}},{"node":{"date":"2019-07-15T00:00:00.000Z","id":"41025c8c-be64-5725-9360-c21fc24cca93","slug":"/blog/2019/07/15/pipeline-config-history-plugin/","strippedHtml":"Pipelines are the efficient and modern way how to create jobs in Jenkins.\nTo recognize pipeline changes quickly and easily, we developed the Pipeline Configuration History plugin.\nThis plugin detects changes of pipelines and provides the user an option to view changes between two builds (diffs) of pipeline configurations visibly and traceably.\n\nHow everything started\n\nIt all started 10 years ago — with classical job types (e.g. Freestyle, Maven, etc.).\nEvery once in a while users contacted us because their jobs failed to build overnight.\nWhy did the job fail?\nWas the failure related to a job configuration change?\nThe users' typical answer was: \"We didn’t change anything!\", but is that really true?\nWe thought about this and decided to develop a plugin that helped us solve this problem.\nThis was the idea and the beginning of Job Configuration History.\n\nNow it was possible to view changes of job configurations (like other branches, JDK versions, etc.) and more often the reason for breaking builds were changes of job configurations.\n\nOver the years the plugin got developed and is still under development.\nNew functions were added, that not only view job configurations, but also changes of global and agent configurations.\nIt is also possible to recover old configuration versions.\nToday the plugin has more than 30,000 installations.\nFor many years JobConfigHistory relieves our daily work — with more than 3,000 Jenkins jobs!\nThen there was a new type of job: Pipelines.\n\nPipelines - something new was needed\n\nPipeline jobs are fundamentally different than classical job types .\nWhile classic job types are configured via the Jenkins GUI, Pipeline jobs are configured as code.\nEvery pipeline job indeed gets created via the Jenkins GUI, however that is not necessarily where the pipeline configuration is located.\nPipelines can be configured:\n\nDirectly in the Jenkins job as script.\nThe code gets inserted directly in the job configuration page.\n\nAs Jenkinsfile in the source code management system (SCM): The pipeline configuration is defined in a text file (Jenkinsfile) in the SCM.\nIn the job itself only the path to the repository of the Jenkinsfile is configured.\nDuring the build the Jenkinsfile gets checked out from the SCM and processed.\n\nAs a shared library: A part of the pipeline configuration gets moved to separate files that can be used by several jobs.\nThese files are also saved in the SCM.\nEven so a Jenkinsfile is still needed (or a pipeline script in the job).\n\nWith every save operation of the job configuration, JobConfigHistory creates a copy of the actual job configuration if something has changed.\nThat only works for pipeline jobs if the pipeline configuration is inserted in the job configuration page as script.\nChanges in the Jenkinsfile or the shared libraries are not detected by JobConfigHistory.\nYou have to use the SCM system to view changes of the Jenkinsfile or the shared libraries.\nIt is complex and time intensive to find a correlation between the time of a build and a change to the Jenkinsfile or shared library.\n\nThis new problem is much more than JobConfigHistory.  A new solution was needed to detect pipeline changes and show these changes in Jenkins.\nSo we developed Pipeline Configuration History.\n\nDuring every pipeline run the Jenkinsfile and related shared libraries are saved in the builds directory of the job.\nPipeline Configuration History saves changes of the pipeline files between the last run and the previous run as history events.\nTherefore when a pipeline job ceases to build successfully, you can check if something has changed on any used pipeline file.\nYou can also see the build where changes occurred.\n\nBecause a pipeline configuration can consist of several files where changes could have occurred, only files with changes between two builds are shown in the diff.\nThat makes the whole thing more compact and effective:\n\nBut sometimes you may want to show more than the differences between pipeline files.  You may want to see which pipeline files are in use or the content of those files when they were used.\nSo it’s possible to view all files and their content.\nIf required you can download them as well:\n\nConclusion\n\nWe use Pipeline Configuration History successfully in production. It has helped us from the very first day as we solved problems that occurred due to pipeline configuration changes.\nPipeline Configuration History won’t replace Job Configuration History.\nThe plugins have different use cases.\nMany times small changes on job or pipeline configurations also have big impacts.\nBecause of the correlation in time between changes of job or pipeline configurations and different build behavior, it is now possible to substantially reduce the time and effort to analyze build failures.\nThe Job Configuration History and Pipeline Configuration History plugins let us help our users in consulting and in solving issues.  We resolve problems much faster through easy access to the configuration history of jobs.  These plugins are essential for our daily work.","title":"Introducing the Pipeline Configuration History Plugin","tags":["pipeline","plugins"],"authors":[{"avatar":null,"blog":null,"github":"Jochen-A-Fuerbacher","html":"<div class=\"paragraph\">\n<p>Jochen is a software developer for development infrastructure at 1&amp;1 Telecommunication.\nHe has been working with Jenkins for many years and develops some Jenkins plugins.</p>\n</div>","id":"jochenafuerbacher","irc":null,"linkedin":null,"name":"Jochen A. Fürbacher","slug":"blog/author/jochenafuerbacher","twitter":null},{"avatar":null,"blog":null,"github":"stefanbrausch","html":"<div class=\"paragraph\">\n<p>Stefan is a software developer for development infrastructure at 1&amp;1 Telecommunication.\nHe has been working with Jenkins for many years and develops some Jenkins plugins.</p>\n</div>","id":"stefanbrausch","irc":null,"linkedin":null,"name":"Stefan Brausch","slug":"blog/author/stefanbrausch","twitter":null},{"avatar":null,"blog":null,"github":"RobinRSchulz","html":"<div class=\"paragraph\">\n<p>Robin is a software developer for development infrastructure at 1&amp;1 Telecommunication.\nHe has been working with Jenkins for many years and develops some Jenkins plugins.</p>\n</div>","id":"robinrschulz","irc":null,"linkedin":null,"name":"Robin Schulz","slug":"blog/author/robinrschulz","twitter":null}]}},{"node":{"date":"2019-07-05T00:00:00.000Z","id":"c2501d40-c7cb-5ca7-8a4c-4b96a13b434f","slug":"/blog/2019/07/05/jenkins-pipeline-stage-result-visualization-improvements/","strippedHtml":"Some changes have recently been released to give Pipeline authors some new tools to improve Pipeline visualizations in Blue Ocean, in particular to address the highly-voted issue JENKINS-39203, which causes all non-failing stages to be visualized as though they were unstable if the overall build result of the Pipeline was unstable. This issue made it difficult to quickly identify why a build was unstable, and forced users to read through builds logs and the Jenkinsfile to figure out what actually happened.\n\nIn order to fix this issue, we introduced a new Pipeline API that can be used to attach additional result information to individual Pipeline steps. Visualization tools like Blue Ocean use this new API when deciding how a given stage should be displayed. Steps like junit that used to set only the overall build result now additionally use the new API to set step-level result information. We created the new unstable and warnError steps so that Pipeline authors with more complicated use cases can still take advantage of this new API.\n\nThe core fixes for the issue are present in the following plugins, all of which require Jenkins 2.138.4 or newer:\n\nPipeline: API 2.34\n\nPipeline: Basic Steps 2.18 (requires a simultaneous update to Pipeline: Groovy 2.70)\n\nPipeline: Graph Analysis 1.10\n\nPipeline: Declarative 1.3.9\n\nBlue Ocean 1.17.0\n\nHere is a screenshot from Blue Ocean of a Pipeline using the unstable step where only the failing stage is marked as unstable:\n\nExamples\n\nHere are some examples of how to update your Pipelines to use the new improvements:\n\nUse the new warnError step to catch errors and mark the build and stage as unstable. warnError requires a single String parameter, which is a message to log when an error is caught. When warnError catches an error, it logs the message  and the error and sets the build and stage result to unstable. Using it looks like this:\n\nwarnError('Script failed!') {\n  sh('false')\n}\n\nUse the new unstable step to set the build and stage result to unstable. This step can be used as a direct replacement for currentBuild.result = 'UNSTABLE', and may be useful in cases where warnError is not flexible enough. unstable requires a single String parameter, which is a message to log when the step runs. Using it might look like this:\n\ntry {\n  sh('false')\n} catch (ex) {\n  unstable('Script failed!')\n}\n\nJUnit Plugin : Update to version 1.28 or newer to pick up fixes for the junit step so that it correctly marks the stage as unstable.\n\nWarnings Next Generation Plugin : Update to version 5.2.0 or newer to pick up fixes for the publishIssues and recordIssues steps so that they correctly mark the stage as unstable.\n\nOther Plugins : If your Pipeline is marked as unstable by a step in another plugin, please file a new issue with the component set to that plugin (after checking for duplicates), clearly describing which step has the problem and under what circumstances it occurs, and link to the developer section of this post as a reference for how the maintainer might be able to address the problem.\n\nLimitations\n\nIf you do not migrate to the unstable or warnError steps, or update plugins that set the build result to versions that integrate with the new API, then in cases where the build is unstable, Blue Ocean will not show any stages as unstable.\n\nEven after these changes, currentBuild.result continues to refer only to the overall build result. Unfortunately, it was not possible to adapt the currentBuild global variable to make it track step or stage-level results, since it is implemented as a global variable, which means it does not have any step-level context through which it could use the new API.\n\nPipeline Stage View Plugin has not yet been updated to use the new API, so these changes do not affect the visualization it provides.\n\nHistory\n\nJenkins Pipeline steps can complete in one of two ways: successfully, by returning a (possibly null) result, or unsuccessfully, by throwing an exception. When a step fails by throwing an exception, that exception propagates throughout the Pipeline until another step or Groovy code catches it, or it reaches the top level of the Pipeline, which causes the Pipeline itself to fail. Depending on the type of exception thrown, the final result of the Pipeline may be something other than failure (for example in some cases it will be aborted). Because of the way the exception propagates, it is easy for tools like Blue Ocean to identify steps (and therefore stages) which failed due to an exception.\n\nIn order for Pipelines to be able to interact with established Jenkins APIs, it was also necessary for Pipeline builds to have an overall build result that can be modified during the build. Among other things, this allows Pipelines to use build steps and wrappers that were originally written for use in Freestyle projects.\n\nIn some cases, it is desirable for a Pipeline step to be able to complete successfully so that the rest of the Pipeline continues normal execution, but for it to be able to note that some kind of error occurred so that visualizations are able to identify that something went wrong with the step, even though it didn’t fail completely. A good example of this is the junit step. This step looks at specified test results, and if there were any failures, marks the overall build result as unstable. This kind of behavior is problematic for visualization tools like Blue Ocean, because the step completed successfully, and there is no programmatic way to associate the overall build result with the step that ended up setting that result.\n\nLooking at JENKINS-39203 again, we see that there were essentially two options for the visualization. If the overall build result was unstable, either all steps that completed successfully could be shown as unstable, because they may have been the step that caused the build to become unstable, or they could be shown as successful, because we have no way to relate the setting of the build result to a specific step. In the end, the first option was chosen.\n\nTo work around this issue, some users tried to do things like throw exceptions and add try/catch blocks around stages that handle exceptions so that Blue Ocean would be able to use the exceptions to mark step and stage results as desired, and then by catching the exception the Pipeline would be able to continue normal execution. These kinds of workarounds were hard to understand, fragile, and did not work well (if at all) for Declarative Pipelines.\n\nDevelopers\n\nIf you are a developer of a plugin that integrates with Pipeline using a step, and want to take advantage of the new API so that your step can report an non-successful result without throwing an exception, please see this post to the Jenkins Developers mailing list, and respond there if you have any questions.","title":"Jenkins Pipeline Stage Result Visualization Improvements","tags":["pipeline","blueocean"],"authors":[{"avatar":null,"blog":null,"github":"dwnusbaum","html":"<div class=\"paragraph\">\n<p>Devin has worked on various areas of Jenkins for the past two years as a software engineer at CloudBees. He is currently a maintainer of core Jenkins Pipeline plugins and also works on Jenkins X Pipeline.</p>\n</div>","id":"dwnusbaum","irc":null,"linkedin":null,"name":"Devin Nusbaum","slug":"blog/author/dwnusbaum","twitter":null}]}},{"node":{"date":"2019-06-29T00:00:00.000Z","id":"7b06e02e-ca3c-5b58-8850-3e214fd8268e","slug":"/blog/2019/06/29/phase-1-multibranch-pipeline-support-for-gitlab/","strippedHtml":"This is one of the Jenkins project in GSoC 2019. We are working on adding support\nfor Multi-branch\nPipeline Jobs and Folder Organisation in GitLab. The plan is to create the following\nplugins:\n\nGitLab API Plugin - Wraps GitLab Java APIs.\n\nGitLab Branch Source Plugin - Contains two packages:\n\nio.jenkins.plugins.gitlabserverconfig - Manages server configuration and web hooks management.\nIdeally should reside inside another plugin with name GitLab Plugin. In future, this package should\nbe moved into a new plugin.\n\nio.jenkins.plugins.gitlabbranchsource - Adds GitLab Branch Source for Multi-branch Pipeline Jobs (including\nMerge Requests) and Folder organisation.\n\nPresent State\n\nFreeStyle Job and Pipeline(Single Branch) Job are fully supported.\n\nMulti-branch Pipeline Job is partially supported (no MRs detection).\n\nGitLab Folder Organisation is not supported.\n\nGoals of this project\n\nImplement a lightweight GitLab Plugin that depends on GitLab API Plugin.\n\nFollow convention of 3 separate plugins i.e. GitLab Plugin, GitLab API Plugin, GitLab Branch Source Plugin.\n\nImplement GitLab Branch Source Plugin with support for Multi-branch Pipeline Jobs.\n\nSupport new Jenkins features such as\nJenkins Code as Configuration (JCasC),\nIncremental Tools.\n\nClear & Efficient design.\n\nSupport new SCM Trait APIs.\n\nSupport Java 8 and above.\n\nBuilding the plugin\n\nNo binaries are available for this plugin as the plugin is in the very early alpha stage, and not ready for the general\npublic quite yet.  If you want to jump in early, you can try building it yourself from source.\n\nInstallation:\n\nCheckout source code to your local machine:\n\ngit clone https://github.com/baymac/gitlab-branch-source-plugin.git\n\ncd gitlab-branch-source-plugin\n\nInstall the plugin:\n\nmvn clean install\n\nmvn clean install -DskipTests # to skip tests\n\nRun the plugin:\n\nmvn hpi:run # runs a Jenkins instance at localhost:8080\n\nmvn hpi:run -Djetty.port= # to run on your desired port number\n\nIf you want to test it with your Jenkins server, after mvn clean install follow these steps in your Jenkins instance:\n\nSelect Manage Jenkins\n\nSelect Manage Plugins\n\nSelect Advanced tab\n\nIn Upload Plugin section, select Choose file\n\nSelect $ /target/gitlab-branch-source.hpi\n\nSelect Upload\n\nSelect Install without restart\n\nUsage\n\nAssuming plugin installation has done been already.\n\nSetting up GitLab Server Configuration on Jenkins\n\nOn jenkins, select Manage Jenkins\n\nSelect Configure System\n\nScroll down to find the GitLab section\n\nSelect Add GitLab Server | Select GitLab Server\n\nNow you will now see the GitLab Server Configuration options.\n\nThere are 4 fields that needs to be configured:\n\nName - Plugin automatically generates an unique server name for you. User may want to configure this field\nto suit their needs but should make sure it is sufficiently unique. We recommend to keep it as it is.\n\nServer URL - Contains the URL to your GitLab Server. By default it is set to \"https://gitlab.com\". User can\nmodify it to enter their GitLab Server URL e.g. https://gitlab.gnome.org/, http://gitlab.example.com:7990. etc.\n\nCredentials - Contains a list of credentials entries that are of type GitLab Personal Access Token. When\nno credential has been added it shows \"-none-\". User can add a credential by clicking \"Add\" button.\n\nWeb Hook - This field is a checkbox. If you want the plugin to setup a webhook on your GitLab project(s)\nrelated jobs, check this box. The plugin listens to a URL for the concerned GitLab project(s) and when an event\noccurs in the GitLab Server, the server sends an event trigger to the URL where the web hook is setup. If you\nwant continuous integration (or continuous delivery) on your GitLab project then you may want to automatically\nset it up.\n\nAdding a Personal Access Token Credentials (To automatically generate Personal Access Token see\nnext section):\n\nUser is required to add a GitLab Personal Access Token type credentials entry to securely persist the token\ninside Jenkins.\n\nGenerate a Personal Access Token on your GitLab Server:\n\nSelect profile dropdown menu from top-right corner\n\nSelect Settings\n\nSelect Access Token from left column\n\nEnter a name | Set Scope to api, read_user, read_repository\n\nSelect Create Personal Access Token\n\nCopy the token generated\n\nReturn to Jenkins | Select Add in Credentials field | Select Jenkins\n\nSet Kind to GitLab Personal Access Token\n\nEnter Token\n\nEnter a unique id in ID\n\nEnter a human readable description\n\nSelect Add\n\nTesting connection:\n\nSelect your desired token in the Credentials dropdown\n\nSelect Test Connection\n\nIt should return something like Credentials verified for user\n\nSelect Apply (at the bottom)\n\nGitLab Server is now setup on Jenkins\n\nCreating Personal Access Token within Jenkins\n\nAlternatively, users can generate a GitLab Personal Access Token within Jenkins itself and automatically add the\nGitLab Personal Access Token credentials to Jenkins server credentials.\n\nSelect Advanced at the bottom of GitLab Section\n\nSelect Manage Additional GitLab Actions\n\nSelect Convert login and password to token\n\nSet the GitLab Server URL\n\nThere are 2 options to generate token;\n\nFrom credentials - To select an already persisting Username Password Credentials or add an Username Password\ncredential to persist it.\n\nFrom login and password - If this is a one time thing then you can directly enter you credentials to the text boxes\nand the username/password credential is not persisted.\n\nAfter setting your username/password credential, select Create token credentials.\n\nThe token creator will create a Personal Access Token in your GitLab Server for the given user with the\nrequired scope and also create a credentials for the same inside Jenkins server. You can go back to the GitLab Server\nConfiguration to select the new credentials generated (select \"-none-\" first then new credentials will appear). For\nsecurity reasons this token is not revealed as plain text rather returns an id. It is a 128-bit long UUID-4 string\n(36 characters).\n\nConfiguration as Code\n\nNo need for messing around in the UI. Jenkins Configuration as Code (JCasC) or simply Configuration as Code Plugin\nallows you to configure Jenkins via a yaml file. If you are a first time user, you can learn more about JCasC\nhere.\n\nAdd configuration YAML:\n\nThere are multiple ways to load JCasC yaml file to configure Jenkins:\n\nJCasC by default searches for a file with the name jenkins.yaml in $JENKINS_ROOT.\n\nThe JCasC looks for an environment variable CASC_JENKINS_CONFIG which contains the path\nfor the configuration yaml file.\n\nA path to a folder containing a set of config files e.g. /var/jenkins_home/casc_configs.\n\nA full path to a single file e.g. /var/jenkins_home/casc_configs/jenkins.yaml.\n\nA URL pointing to a file served on the web e.g. /jenkins.yaml\" class=\"bare\">https:// /jenkins.yaml .\n\nYou can also set the configuration yaml path in the UI. Go to /configuration-as-code.\nEnter path or URL to jenkins.yaml and select Apply New Configuration.\n\nAn example of configuring GitLab server via jenkins.yaml :\n\ncredentials:\n  system:\n    domainCredentials:\n      - credentials:\n          - gitlabPersonalAccessToken:\n              scope: SYSTEM\n              id: \"i<3GitLab\"\n              token: \"XfsqZvVtAx5YCph5bq3r\" # gitlab personal access token\n\nunclassified:\n  gitLabServers:\n    servers:\n      - credentialsId: \"i<3GitLab\"\n        manageHooks: true\n        name: \"gitlab.com\"\n        serverUrl: \"https://gitlab.com\"\n\nFor better security, see handling secrets\nsection in JCasC\ndocumentation.\n\nFuture Scope of work\n\nThe second phase of GSoC will be utilized to develop GitLab Branch Source. The new feature is a work in progress, but\nthe codebase is unstable and requires lot of bugfixes. Some features like Multibranch Pipeline Jobs are functioning\nproperly. More about it at the end of second phase.\n\nIssue Tracking\n\nThis project uses Jenkins JIRA to track issues. You can file issues under\ngitlab-branch-source-plugin component.\n\nAcknowledgements\n\nThis plugin is built and maintained by the Google Summer of Code (GSoC) Team for\nMulti-branch Pipeline\nSupport for GitLab. A lot of inspiration was drawn from GitLab Plugin, Gitea Plugin and GitHub Plugin.\n\nOur team consists of: baymac, LinuxSuRen,\nMarky, Joseph,\nJustin, Jeff.\n\nWith support from: Oleg, Greg,\nOwen.\n\nAlso thanks to entire Jenkins community for contributing with technical expertise and inspiration.\n\nLinks\n\nPhase 1 demo\n\nPresentation slides\n\nGitLab API Plugin\n\nGitLab Branch Source Plugin\n\nGitLab API Plugin Wiki\n\nIssue Tracker for Phase 1\n\nBlog","title":"Multi-branch Pipeline Jobs Support for GitLab SCM","tags":["gitlab","plugins","pipeline","credentials","developer","gsoc","gsoc2019"],"authors":[{"avatar":{"childImageSharp":null},"blog":null,"github":"baymac","html":"<div class=\"paragraph\">\n<p>GSoC 2019 student under Jenkins project. Maintainer of GitLab Branch Source Plugin. Software Developer at Udaan, India.</p>\n</div>","id":"baymac","irc":null,"linkedin":"parichay.barpanda","name":"Parichay Barpanda","slug":"blog/author/baymac","twitter":"baymac04"}]}},{"node":{"date":"2019-05-09T00:00:00.000Z","id":"8e971fb0-7d4b-559d-a327-05b99fb4739e","slug":"/blog/2019/05/09/templating-engine/","strippedHtml":"Implementing DevSecOps practices at the enterprise scale is challenging. With multiple programming languages, automated testing frameworks, and security compliance tools being used by different applications within your organization, it becomes difficult to build and maintain pipelines for each team.\n\nMost pipelines are going to follow the same generic workflow regardless of which specific tech stack is employed by an application.  The Templating Engine Plugin (abbreviated as JTE for Jenkins Templating Engine) allows you to capture this efficiency by creating tool-agnostic, templated workflows to be reused by every team.\n\nAs technology consultants with clients in both the public and private sectors, at Booz Allen we found ourselves building DevSecOps pipelines from scratch for every new project.  Through developing the Jenkins Templating Engine, we’ve seen pipeline development decrease from months to days now that we can reuse tool integrations while bringing a new level of governance to Jenkins pipelines.\n\nPipeline Templating\n\nOrganizations benefit from letting application developers focus on what they do best: building applications. Supporting this means building a centralized DevOps team responsible for maintaining platform infrastructure and creating CI/CD pipelines utilized by development teams.\n\nWith the rise of microservice-based architectures, a centralized DevOps teams can support many different development teams simultaneously; all of whom may be leveraging different programming languages and automated testing tools.\n\nWhile the tools may differ between development teams, the workflow is often the same: unit test, static code analysis, build and publish an artifact, deploy it, and then perform different types of testing against the deployed application.\n\nThe Templating Engine Plugin allows you to remove the Jenkinsfile from each repository by defining a common workflow for teams to inherit.  Instead of an entire pipeline definition in each repository, teams supply a configuration file specifying which tools to use for the workflow.\n\nJTE in Action\n\nLet’s walk through a bare bones example to demonstrate the reusability of templates:\n\nExample Pipeline Template:\n\nunit_test()\nbuild()\nstatic_code_analysis()\n\nTemplates leverage Steps contributed by Libraries to outline a workflow teams must implement.  While a template does get executed just like any other Jenkinsfile (meaning that the standard scripted and declarative syntax is supported), the goal of a template should be to read like plain English and avoid any technical implementation.\n\nLeveraging templates in this way lets you separate the business logic (what should happen when) of your pipeline from the\ntechnical implementation (what’s actually going to happen).  The result of this is a CI/CD pipeline that’s proven to be\nsignificantly easier to manage when supporting multiple teams simultaneously.\n\nThe steps outlined by this template ( unit_test, build, and static_code_analysis) have been named generically on purpose. This way teams can specify different libraries to use while sharing the same pipeline.\n\nImplementing the Template\n\nImplementing a shareable pipeline with the Templating Engine requires a few key components:\n\nPipeline Template : Outline the workflow to be performed\n\nLibraries : Provide technical implementations of the steps of the workflow\n\nConfiguration Files : Specify which libraries to use and their configuration\n\nStep 1: Create a Pipeline Configuration Repository\n\nA Pipeline Configuration Repository is used to store common configurations and pipeline templates inherited by teams.\n\nThis example Pipeline Configuration Repository will later be configured as part of a Governance Tier : the mechanism in JTE that allows you to build hierarchical configurations representing your organization.\n\nA Governance Tier holds three things:\n\nPipeline Templates\n\nA list of Library Sources\n\nThe tier’s configuration file ( pipeline_config.groovy)\n\nThe pipeline templates and the configuration file for a Governance Tier are stored in the pipeline configuration repository.\n\nWhen configuring the Governance Tier in Jenkins, you will provide a source code management location for a repository that contains the above components as well as the base directory where these artifacts can be found.\n\nStep 2: Create the Pipeline Template\n\nNext, we’ll create a Jenkinsfile for the Governance Tier.  In JTE, the Jenkinsfile is the default pipeline template that an execution will use.\n\nJenkinsfile\n\nunit_test()\nbuild()\nstatic_code_analysis()\n\nStep 3: Create the Libraries\n\nThe Templating Engine Plugin has implemented a version of Jenkins Shared Libraries to enhance the reusability of libraries.  A library is a root directory within a source code repository that has been configured as a Library Source on a Governance Tier.\n\nIn our example, the pipeline template needs to perform unit testing, package an artifact, and run static code analysis.\n\nLet’s assume that we have some teams using gradle and some teams using maven to build and test their application but they will both use SonarQube to perform static code analysis.\n\nIn this scenario, we should create gradle, maven, and sonarqube libraries.\n\n|- gradle/\n  \\-- build.groovy\n  \\-- unit_test.groovy\n|- maven/\n  \\-- build.groovy\n  \\-- unit_test.groovy\n|- sonarqube/\n  \\-- static_code_analysis.groovy\n\nStep 4: Implement the Steps\n\nImplementing a library step is exactly the same as just writing regular global variables as part of the default Jenkins Shared Libraries.\n\nFor the purposes of this demonstration, we will just have each step print out the step name and contributing library.\n\ngradle/build.groovy\n\nvoid call(){\n    println \"gradle: build()\"\n}\n\nRead more about Library Development within JTE.\n\nStep 5: Create the Configuration Files\n\nThe configuration file for JTE is named pipeline_config.groovy.\n\nIn the Governance Tier we’ll create a configuration file specifying common configurations between the applications. In this case, both applications are using the sonarqube library:\n\npipeline_config.groovy\n\nlibraries{\n  merge = true // allow individual apps to contribute additional libraries\n  sonarqube\n}\n\nNext, we’ll create two more repositories representing the maven and gradle applications. Within those repositories all we’ll need is an application-specific pipeline_config.groovy file.\n\nThese repositories both contain an application pipeline_config.groovy configuration file.\n\nmaven app: pipeline_config.groovy\n\nlibraries{\n    maven\n}\n\ngradle app: pipeline_config.groovy\n\nlibraries{\n    gradle\n}\n\nStep 6: Configure the Governance Tier in Jenkins\n\nNow that we have a Pipeline Configuration Repository and a Library Source Repository, we can configure a Governance Tier in Jenkins:\n\nThis configuration shown in the image above can be found under Manage Jenkins >> Configure System\n\nThrough the Templating Engine, you can create a pipeline governance hierarchy matching your organization’s taxonomy by representing this structure via Folders in Jenkins.\n\nStep 7: Create a Multibranch Pipeline for Both Applications\n\nWhen creating Multibranch Pipeline Projects for each app, the Templating Engine plugin supplies a new Project Recognizer\ncalled Jenkins Templating Engine.  This sets the project to use the Templating Engine framework for all branches within the\nrepository.\n\nYou can also set the Jenkins Templating Engine project recognizer for a GitHub Organization project, enabling you to easily share the same pipeline across an entire Github Organization!\n\nStep 8: Run the Pipelines\n\nThat’s it!  Now, both applications will leverage the exact same pipeline template while having the flexibility to select which\ntools should be used during each phase of the workflow.\n\nBelow is sample output from the console log from both applications pipeline runs:\n\nGradle:\n\n[JTE] Obtained Template Configuration File pipeline_config.groovy from git https://github.com/steven-terrana/example-jte-configuration\n[JTE] Obtained Template Configuration File pipeline_config.groovy from git https://github.com/steven-terrana/example-jte-app-gradle.git\n[JTE] Loading Library sonarqube from git https://github.com/steven-terrana/example-jte-libraries.git\n[JTE] Loading Library gradle from git https://github.com/steven-terrana/example-jte-libraries.git\n...\n[JTE] Obtained Template Jenkinsfile from git https://github.com/steven-terrana/example-jte-configuration\n[JTE][Step - gradle/unit_test]\n[Pipeline] echo\ngradle: unit_test()\n[JTE][Step - gradle/build]\n[Pipeline] echo\ngradle: build()\n[JTE][Step - sonarqube/static_code_analysis]\n[Pipeline] echo\nsonarqube: static_code_analysis()\n[Pipeline] End of Pipeline\n\nMaven:\n\n[JTE] Obtained Template Configuration File pipeline_config.groovy from git https://github.com/steven-terrana/example-jte-configuration\n[JTE] Obtained Template Configuration File pipeline_config.groovy from git https://github.com/steven-terrana/example-jte-app-maven.git\n[JTE] Loading Library sonarqube from git https://github.com/steven-terrana/example-jte-libraries.git\n[JTE] Loading Library maven from git https://github.com/steven-terrana/example-jte-libraries.git\n...\n[JTE] Obtained Template Jenkinsfile from git https://github.com/steven-terrana/example-jte-configuration\n[JTE][Step - maven/unit_test]\n[Pipeline] echo\nmaven: unit_test()\n[JTE][Step - maven/build]\n[Pipeline] echo\nmaven: build()\n[JTE][Step - sonarqube/static_code_analysis]\n[Pipeline] echo\nsonarqube: static_code_analysis()\n[Pipeline] End of Pipeline\n\nBenefits of the Templating Engine\n\nApply Organizational Governance\n\nLeveraging the Templating Engine Plugin will allow you to define enterprise-scale, approved\nworkflows that can be used by teams regardless of what tools are being used.  This top-down\napproach makes scaling and enforcing DevSecOps principles significantly easier within your organization.\n\nOptimize Code Reuse\n\nThere’s really no need for every team in your organization to figure out how to do the same things over\nand over again.  At Booz Allen, we have seen pipeline development time decrease from months to days as\nwe have continuously reused and expanded upon our Templating Engine library portfolio as part of our Solutions\nDelivery Platform.\n\nSimplify Pipeline Maintainability\n\nOften DevOps engineers find themselves building and supporting pipelines for multiple development teams at\nthe same time.  By decoupling the workflow from the technical implementation and consolidating the pipeline\ndefinition to a centralized location, the Templating Engine plugin allows DevOps engineers to scale much faster.\n\nGet Involved!\n\nThe Templating Engine Plugin has been open sourced and made available in the Jenkins Update Center.\n\nWe always appreciate feedback and contributions! If you have an interesting use case or would like to ask questions, try the templating-engine-plugin on Gitter.\n\nAdvanced Features\n\nConfiguration File Conditional Inheritance\n\nExternalize Library Configurations\n\nAspect Oriented LifeCycle Hooks\n\nMultiple Pipeline Templates\n\nDefault Step Implementation\n\nConfiguration File DSL Sandboxing\n\nMore Resources\n\nFor this Demonstration\n\nPipeline Configuration Repository\n\nSample Libraries\n\nSample Maven Repository\n\nSample Gradle Repository\n\nAdditional Resources\n\nTemplating Engine Documentation\n\nSource Code\n\nBooz Allen’s SDP Pipeline Libraries\n\nBooz Allen Hamilton","title":"Introducing the Jenkins Templating Engine!","tags":["general","pipeline","plugin","pipeline-authoring"],"authors":[{"avatar":null,"blog":null,"github":"steven-terrana","html":"","id":"steven-terrana","irc":null,"linkedin":null,"name":"Steven Terrana","slug":"blog/author/steven-terrana","twitter":null}]}},{"node":{"date":"2019-02-06T00:00:00.000Z","id":"ed5dfb33-f5e6-565b-a671-f4ae47d2c691","slug":"/blog/2019/02/06/ssh-steps-for-jenkins-pipeline/","strippedHtml":"Pipeline-as-code or defining the deployment pipeline through code rather than manual job creation through UI, provides tremendous benefits for teams automating builds and deployment infrastructure across their environments.\n\nSource of image: https://jenkins.io/doc/book/pipeline/\n\nJenkins Pipelines\n\nJenkins is a well-known open source continuous integration and continuous deployment automation tool. With the latest 2.0 release, Jenkins introduced the Pipeline plugin that implements Pipeline-as-code. This plugin lets you define delivery pipelines using concise scripts which deal elegantly with jobs involving persistence and asynchrony.\n\nThe Pipeline-as-code’s script is also known as a Jenkinsfile.\n\nJenkinsfiles uses a domain specific language syntax based on the Groovy programming language. They are persistent files which can be checked in and version-controlled along with the rest of their project source code. This file can contain the complete set of encoded steps (steps, nodes, and stages) necessary to define the entire application life-cycle, becoming the intersecting point between development and operations.\n\nMissing piece of the puzzle\n\nOne of the most common steps defined in a basic pipeline job is the Deploy step. The deployment stage encompasses everything from publishing build artifacts to pushing code into pre-production and production environments. This deployment stage usually involves both development and operations teams logging onto various remote nodes to run commands and/or scripts to deploy code and configuration. While there are a couple of existing ssh plugins for Jenkins, they currently don’t support the functionality such as logging into nodes for pipelines. Thus, there was a need for a plugin that supports these steps.\n\nIntroducing SSH Steps\n\nRecently, our team at Cerner started working on a project to automate deployments through Jenkins pipelines to help facilitate running commands on over one thousand nodes. We looked at several options including existing plugins, internal shared Jenkins libraries, and others. In the end, we felt it was best to create and open source a plugin to fill this gap so that it can be used across Cerner and beyond.\n\nThe initial version of this new plugin SSH Steps supports the following:\n\nsshCommand : Executes the given command on a remote node.\n\nsshScript : Executes the given shell script on a remote node.\n\nsshGet : Gets a file/directory from the remote node to current workspace.\n\nsshPut : Puts a file/directory from the current workspace to remote node.\n\nsshRemove : Removes a file/directory from the remote node.\n\nUsage\n\nBelow is a simple demonstration on how to use above steps. More documentation can be found on GitHub.\n\ndef remote = [:]\nremote.name = \"node\"\nremote.host = \"node.abc.com\"\nremote.allowAnyHosts = true\n\nnode {\n    withCredentials([usernamePassword(credentialsId: 'sshUserAcct', passwordVariable: 'password', usernameVariable: 'userName')]) {\n        remote.user = userName\n        remote.password = password\n\n        stage(\"SSH Steps Rocks!\") {\n            writeFile file: 'test.sh', text: 'ls'\n            sshCommand remote: remote, command: 'for i in {1..5}; do echo -n \\\"Loop \\$i \\\"; date ; sleep 1; done'\n            sshScript remote: remote, script: 'test.sh'\n            sshPut remote: remote, from: 'test.sh', into: '.'\n            sshGet remote: remote, from: 'test.sh', into: 'test_new.sh', override: true\n            sshRemove remote: remote, path: 'test.sh'\n        }\n    }\n}\n\nConfiguring via YAML\n\nAt Cerner, we always strive to have simple configuration files for CI/CD pipelines whenever possible. With that in mind, my team built a wrapper on top of these steps from this plugin. After some design and analysis, we came up with the following YAML structure to run commands across various remote groups:\n\nconfig:\n  credentials_id: sshUserAcct\n\nremote_groups:\n  r_group_1:\n    - name: node01\n      host: node01.abc.net\n    - name: node02\n      host: node02.abc.net\n  r_group_2:\n    - name: node03\n      host: node03.abc.net\n\ncommand_groups:\n  c_group_1:\n    - commands:\n        - 'ls -lrt'\n        - 'whoami'\n    - scripts:\n        - 'test.sh'\n  c_group_2:\n    - gets:\n        - from: 'test.sh'\n          to: 'test_new.sh'\n    - puts:\n        - from: 'test.sh'\n          to: '.'\n    - removes:\n        - 'test.sh'\n\nsteps:\n  deploy:\n    - remote_groups:\n        - r_group_1\n      command_groups:\n        - c_group_1\n    - remote_groups:\n        - r_group_2\n      command_groups:\n        - c_group_2\n\nThe above example runs commands from c_group_1 on remote nodes within r_group_1 in parallel before it moves on to the next group using sshUserAcct (from the Jenkins Credentials store) to logon to nodes.\n\nShared Pipeline Library\n\nWe have created a shared pipeline library that contains a sshDeploy step to support the above mentioned YAML syntax. Below is the code snippet for the sshDeploy step from the library. The full version can be found here on Github.\n\n#!/usr/bin/groovy\ndef call(String yamlName) {\n    def yaml = readYaml file: yamlName\n    withCredentials([usernamePassword(credentialsId: yaml.config.credentials_id, passwordVariable: 'password', usernameVariable: 'userName')]) {\n        yaml.steps.each { stageName, step ->\n            step.each {\n                def remoteGroups = [:]\n                def allRemotes = []\n                it.remote_groups.each {\n                    remoteGroups[it] = yaml.remotes.\"$it\"\n                }\n\n                def commandGroups = [:]\n                it.command_groups.each {\n                    commandGroups[it] = yaml.commands.\"$it\"\n                }\n                def isSudo = false\n                remoteGroups.each { remoteGroupName, remotes ->\n                    allRemotes += remotes.collect { remote ->\n                        if(!remote.name)\n                            remote.name = remote.host\n                        remote.user = userName\n                        remote.password = password\n                        remote.allowAnyHosts = true\n                        remote.groupName = remoteGroupName\n                        remote\n                    }\n                }\n                if(allRemotes) {\n                    if(allRemotes.size() > 1) {\n                        def stepsForParallel = allRemotes.collectEntries { remote ->\n                            [\"${remote.groupName}-${remote.name}\" : transformIntoStep(stageName, remote.groupName, remote, commandGroups)]\n                        }\n                        stage(stageName) {\n                            parallel stepsForParallel\n                        }\n                    } else {\n                        def remote = allRemotes.first()\n                        stage(stageName + \"\\n\" + remote.groupName + \"-\" + remote.name) {\n                            transformIntoStep(stageName, remote.groupName, remote, commandGroups).call()\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nBy using the step (as described in the snippet above) from this shared pipeline library, a Jenkinsfile can be reduced to:\n\n@Library('ssh_deploy') _\n\nnode {\n  checkout scm\n  sshDeploy('dev/deploy.yml');\n}\n\nAn example execution of the above pipeline code in Blue Ocean looks like this:\n\nWrapping up\n\nSteps from the SSH Steps Plugin are deliberately generic enough that they can be used for various other use-cases as well, not just for deploying code. Using SSH Steps has significantly reduced the time we spend on deployments and has given us the possibility of easily scaling our deployment workflows to various environments.\n\nHelp us make this plugin better by contributing. Whether it is adding or suggesting a new feature, bug fixes, or simply improving documentation, contributions are always welcome.","title":"SSH Steps for Jenkins Pipeline","tags":["pipeline","plugins","ssh","steps"],"authors":[{"avatar":{"childImageSharp":null},"blog":null,"github":"nrayapati","html":"<div class=\"paragraph\">\n<p>Software Architect at <a href=\"https://www.cerner.com/\">Cerner Corporation</a>. Passionate about Agile, DevOps &amp; Continuous Delivery, and all things Automation.\nOSS Contributor, he is maintaining couple of Jenkins plugins since past several years. <a href=\"https://plugins.jenkins.io/ssh-steps\">SSH Steps</a> - <a href=\"https://plugins.jenkins.io/jira-steps\">JIRA Steps</a> - <a href=\"https://plugins.jenkins.io/hubot-steps\">Hubot Steps</a></p>\n</div>","id":"nrayapati","irc":null,"linkedin":null,"name":"Naresh Rayapati","slug":"blog/author/nrayapati","twitter":"nrayapati"}]}}]}},"pageContext":{"tag":"pipeline","limit":8,"skip":8,"numPages":13,"currentPage":2}},
    "staticQueryHashes": ["3649515864"]}