{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/pipeline/page/9",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2016-08-09T00:00:00.000Z","id":"e7eb5984-a870-5548-985b-25b9210fb2a0","slug":"/blog/2016/08/09/ewm-beta-version/","strippedHtml":"This blog post is a continuation of the External Workspace Manager Plugin related posts, starting with\nthe introductory blog post, and followed by\nthe alpha version release announcement.\n\nAs the title suggests, the beta version of the External Workspace Manager Plugin was launched!\nThis means that it’s available only in the\nExperimental Plugins Update Center.\n\nTake care when installing plugins from the Experimental Update Center, since they may change in\nbackward-incompatible ways.\nIt’s advisable not to use it for Jenkins production environments.\n\nThe plugin’s repository is on GitHub.\nThe complete plugin’s documentation can be accessed\nhere.\n\nWhat’s new\n\nBellow is a summary of the features added so far, since the alpha version.\n\nMultiple upstream run selection strategies\n\nIt has support for the\nRun Selector Plugin (which is still in beta),\nso you can provide different run selection strategies when allocating a disk from the upstream job.\n\nLet’s suppose that we have an upstream job that clones the repository and builds the project:\n\ndef extWorkspace = exwsAllocate 'diskpool1'\n\nnode ('linux') {\n    exws (extWorkspace) {\n        checkout scm\n        sh 'mvn clean install -DskipTests'\n    }\n}\n\nIn the downstream job, we run the tests on a different node, but we reuse the same workspace as the previous job:\n\ndef run = selectRun 'upstream'\ndef extWorkspace = exwsAllocate selectedRun: run\n\nnode ('test') {\n    exws (extWorkspace) {\n        sh 'mvn test'\n    }\n}\n\nThe selectRun in this example selects the last stable build from the upstream job.\nBut, we can be more explicit, and select a specific build number from the upstream job.\n\ndef run = selectRun 'upstream',\n selector: [$class: 'SpecificRunSelector', buildNumber: UPSTREAM_BUILD_NUMBER]\ndef extWorkspace = exwsAllocate selectedRun: run\n// ...\n\nWhen the selectedRun parameter is given to the exwsAllocate step, it will allocate the same workspace that was\nused by that run.\n\nThe Run Selector Plugin has several run selection strategies that are briefly explained\nhere.\n\nAutomatic workspace cleanup\n\nProvides an automatic workspace cleanup by integrating the\nWorkspace Cleanup Plugin.\nFor example, if we need to delete the workspace only if the build has failed, we can do the following:\n\ndef extWorkspace = exwsAllocate diskPoolId: 'diskpool1'\n\nnode ('linux') {\n    exws (extWorkspace) {\n        try {\n            checkout scm\n            sh 'mvn clean install'\n        } catch (e) {\n            currentBuild.result = 'FAILURE'\n            throw e\n        } finally {\n            step ([$class: 'WsCleanup', cleanWhenFailure: false])\n        }\n    }\n}\n\nMore workspace cleanup examples can be found at this\nlink.\n\nCustom workspace path\n\nAllows the user to specify a custom workspace path to be used when allocating workspace on the disk.\nThe plugin offers two alternatives for doing this:\n\nby defining a global workspace template for each Disk Pool\n\nThis can be defined in the Jenkins global config, External Workspace Definitions section.\n\nby defining a custom workspace path in the Pipeline script\n\nWe can use the Pipeline DSL to compute the workspace path.\nThen we pass this path as input parameter to the exwsAllocate step.\n\ndef customPath = \"${env.JOB_NAME}/${PULL_REQUEST_NUMBER}/${env.BUILD_NUMBER}\"\ndef extWorkspace = exwsAllocate diskPoolId: 'diskpool1', path: customPath\n// ...\n\nFor more details see the afferent\ndocumentation page.\n\nDisk Pool restrictions\n\nThe plugin comes with Disk Pool restriction strategies.\nIt does this by using the restriction capabilities provided by the\nJob Restrictions Plugin.\n\nFor example, we can restrict a Disk Pool to be allocated only if the Jenkins job in which it’s allocated was triggered\nby a specific user:\n\nOr, we can restrict the Disk Pool to be allocated only for those jobs whose name matches a well defined pattern:\n\nWhat’s next\n\nCurrently there is ongoing work for providing flexible disk allocation strategies.\nThe user will be able to define a default disk allocation strategy in the Jenkins global config.\nSo for example, we want to select the disk with the most usable space as default allocation strategy:\n\nIf needed, this allocation strategy may be overridden in the Pipeline code.\nLet’s suppose that for a specific job, we want to allocate the disk with the highest read speed.\n\ndef extWorkspace = exwsAllocate diskPoolId: 'diskpool1', strategy: fastestRead()\n// ...\n\nWhen this feature is completed, the plugin will enter a final testing phase.\nIf all goes to plan, a stable version should be released in about two weeks.\n\nIf you have any issues in setting up or using the plugin, please feel free to ask me on the plugin’s Gitter\nchat.\nAny feedback is welcome, and you may provide it either on the Gitter chat, or on\nJira by using the external-workspace-manager-plugin component.\n\nLinks\n\nProject repository\n\nProject intro blog post\n\nAlpha version announcement\n\nGSoC page\n\nJenkins GSoC Page","title":"GSoC: External Workspace Manager for Pipeline. Beta release is available","tags":["pipeline","plugins","external-workspace-manager","gsoc"],"authors":[{"avatar":null,"blog":null,"github":"alexsomai","html":"","id":"alexsomai","irc":null,"linkedin":null,"name":"Alexandru Somai","slug":"blog/author/alexsomai","twitter":"alex_somai"}]}},{"node":{"date":"2016-08-08T00:00:00.000Z","id":"649a2e8e-4f2f-56eb-99f1-8897de882a49","slug":"/blog/2016/08/08/docker-pipeline-environments/","strippedHtml":"This is a guest post by Michael Neale, long time open\nsource developer and contributor to the Blue Ocean\nproject.\n\nIf you are running parts of your pipeline on Linux, possibly the easiest way to\nget a clean reusable environment is to use:\nCloudBees\nDocker Pipeline plugin.\n\nIn this short post I wanted to show how you can avoid installing stuff on the agents, and have per project, or even per branch, customized build environments.\nYour environment, as well as your pipeline is defined and versioned alongside your code.\n\nI wanted to use the Blue Ocean project as an\nexample of a\nproject that uses the CloudBees Docker Pipeline plugin.\n\nEnvironment and Pipeline for JavaScript components\n\nThe Blue Ocean project has a few moving parts, one of\nwhich is called the \"Jenkins Design Language\".  This is a grab bag of re-usable\nCSS, HTML, style rules, icons and JavaScript components (using React.js) that\nprovide the look and feel for Blue Ocean.\n\nJavaScript and Web Development being what it is in 2016, many utilities are\nneed to assemble a web app.  This includes npm and all that it needs, less.js\nto convert Less to CSS, Babel to \"transpile\" versions of JavaScript to other\ntypes of JavaScript (don’t ask) and more.\n\nWe could spend time installling nodejs/npm on the agents, but why not just use\nthe official off the shelf docker image\nfrom Docker Hub?\n\nThe only thing that has to be installed and run on the build agents is the Jenkins agent, and a docker daemon.\n\nA simple pipeline using this approach would be:\n\nnode {\n        stage \"Prepare environment\"\n          checkout scm\n          docker.image('node').inside {\n            stage \"Checkout and build deps\"\n                sh \"npm install\"\n\n            stage \"Test and validate\"\n                sh \"npm install gulp-cli && ./node_modules/.bin/gulp\"\n          }\n}\n\nThis uses the stock \"official\" Node.js image from the Docker Hub, but doesn’t let us customize much about the environment.\n\nCustomising the environment, without installing bits on the agent\n\nBeing the forward looking and lazy person that I am, I didn’t want to have to\ngo and fish around for a Docker image every time a developer wanted something\nspecial installed.\n\nInstead, I put a Dockerfile in the root of the repo, alongside the Jenkinsfile :\n\nThe contents of the Dockerfile can then define the exact environment needed\nto build the project.  Sure enough, shortly after this, someone came along\nsaying they wanted to use Flow from Facebook (A\ntypechecker for JavaScript).  This required an additional native component to\nwork (via apt-get install).\n\nThis was achieved via a\npull\nrequest to both the Jenkinsfile and the Dockerfile at the same time.\n\nSo now our environment is defined by a Dockerfile with the following contents:\n\n# Lets not just use any old version but pick one\nFROM node:5.11.1\n\n# This is needed for flow, and the weirdos that built it in ocaml:\nRUN apt-get update && apt-get install -y libelf1\n\nRUN useradd jenkins --shell /bin/bash --create-home\nUSER jenkins\n\nThe Jenkinsfile pipeline now has the following contents:\n\nnode {\n    stage \"Prepare environment\"\n        checkout scm\n        def environment  = docker.build 'cloudbees-node'\n\n        environment.inside {\n            stage \"Checkout and build deps\"\n                sh \"npm install\"\n\n            stage \"Validate types\"\n                sh \"./node_modules/.bin/flow\"\n\n            stage \"Test and validate\"\n                sh \"npm install gulp-cli && ./node_modules/.bin/gulp\"\n                junit 'reports/**/*.xml'\n        }\n\n    stage \"Cleanup\"\n        deleteDir()\n}\n\nEven hip JavaScript tools can emit that weird XML format that test\nreporters can use, e.g. the junit result archiver.\n\nThe main change is that we have docker.build being called to produce the\nenvironment which is then used.  Running docker build is essentially a\n\"no-op\" if the image has already been built on the agent before.\n\nWhat’s it like to drive?\n\nWell, using Blue Ocean, to build Blue Ocean, yields a pipeline that visually\nlooks like this (a recent run I screen capped):\n\nThis creates a pipeline that developers can tweak on a pull-request basis,\nalong with any changes to the environment needed to support it, without having\nto install any packages on the agent.\n\nWhy not use docker commands directly?\n\nYou could of course just use shell commands to do things with Docker directly,\nhowever, Jenkins Pipeline keeps track of Docker images used in a Dockerfile\nvia the \"Docker Fingerprints\" link (which is good, should that image need to\nchange due to a security patch).\n\nLinks\n\nThe project used as as an example is here\n\nThe pipeline is defined by the Jenkinsfile\n\nThe environment is defined by the Dockerfile\n\nRead more on Docker Pipeline","title":"Don't install software, define your environment with Docker and Pipeline","tags":["pipeline","plugins","blueocean","ux","javascript","nodejs"],"authors":[{"avatar":{"childImageSharp":null},"blog":null,"github":"michaelneale","html":"<div class=\"paragraph\">\n<p>Michael is a CD enthusiast with a interest in User Experience.\nHe is a co-founder of CloudBees and a long time OSS developer, and can often be found\nlurking around the jenkins-dev mailing list or #jenkins on irc (same nick as twitter name).\nBefore CloudBees he worked at Red Hat.</p>\n</div>","id":"michaelneale","irc":null,"linkedin":null,"name":"Michael Neale","slug":"blog/author/michaelneale","twitter":"michaelneale"}]}},{"node":{"date":"2016-07-19T00:00:00.000Z","id":"ce2c6f83-1ed8-521b-a608-5f3680ad5166","slug":"/blog/2016/07/19/blue-ocean-update/","strippedHtml":"The team have been hard at work moving the needle forward on the Blue\nOcean 1.0 features. Many of the features we have been working on have\ncome a long way in the past few months but here’s a few highlights:\n\nGoodbye page refreshes, Hello Real Time updates!\n\nBuilding upon\nTom 's great work on\nServer Sent Events (SSE) both\nCliff and\nTom worked\non making the all the screens in Blue Ocean update without manual\nrefreshes.\n\nSSE is a great technology\nchoice for new web apps as it only pushes out\nevents to the client when things have changed on the server. That means\nthere’s a lot less traffic going between your browser and the Jenkins\nserver when compared to the continuous AJAX polling method that has been\ntypical of Jenkins in the past.\n\nNew Test Reporting UI\n\nKeith has\nbeen working with Vivek to\ndrive out a new set of extension points that allow us to build a new\nrest reporting UI in Blue Ocean. Today this works for JUnit test reports\nbut can be easily extended to work with other kinds of reports.\n\nPipeline logs are split into steps and update live\n\nThorsten and\nJosh have\nbeen hard at work breaking down the log into steps and making the live\nlog tailing follow the pipeline execution - which we’ve lovingly\nnicknamed the “karaoke mode”\n\nPipelines can be triggered from the UI\n\nTom has\nbeen on allowing users to trigger jobs from Blue Ocean, which is one\nless reason to go back to the Classic UI :)\n\nBlue Ocean has been released to the experimental update center\n\nMany of you have asked us questions about how you can try Blue Ocean\ntoday and have resorted to building the plugin yourself or running our\nDocker image.\n\nWe wanted to make the process of trying Blue Ocean in its unfinished\nstate by publishing the plugin to the experimental update center - it’s\navailable today!\n\nSo what is the Experimental Update Center? It is a mechanism for the\nJenkins developer community to share early previews of new plugins with\nthe broader user community. Plugins in this update center are\nexperimental and we strongly advise not running them on production or\nJenkins systems that you rely on for your work.\n\nThat means any plugin in this update center could eat your Jenkins data,\ncause slowdowns, degrade security or have their behavior change at no\nnotice.\n\nYou can learn how to\nactivate\nthe experimental update center on this post.\n\nStay tuned for more updates!","title":"Blue Ocean July development update ","tags":["blueocean","ux","pipeline"],"authors":[{"avatar":null,"blog":null,"github":"i386","html":"","id":"i386","irc":null,"linkedin":null,"name":"James Dumay","slug":"blog/author/i386","twitter":"i386"}]}},{"node":{"date":"2016-07-18T00:00:00.000Z","id":"523da562-6777-53bd-978f-45ab4cb9092c","slug":"/blog/2016/07/18/pipeline-notifications/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nRather than sitting and watching Jenkins for job status, I want Jenkins to send\nnotifications when events occur.  There are Jenkins plugins for\nSlack,\nHipChat,\nor even email\namong others.\n\nNote: Something is happening!\n\nI think we can all agree getting notified when events occur is preferable to\nhaving to constantly monitor them just in case.  I’m going to continue from\nwhere I left off in my\nprevious post with the\nhermann project.  I added a Jenkins\nPipeline with an HTML publisher for code coverage. This week, I’d like to make\nJenkins to notify me when builds start and when they succeed or fail.\n\nSetup and Configuration\n\nFirst, I select targets for my notifications. For this blog post, I’ll use sample\ntargets that I control.  I’ve created Slack and HipChat organizations called\n\"bitwiseman\", each with one member - me.  And for email I’m running a Ruby SMTP server called\nmailcatcher, that is perfect for local testing\nsuch as this.  Aside for these concessions, configuration would be much the\nsame in a non-demo situation.\n\nNext, I install and add server-wide configuration for the\nSlack,\nHipChat,\nand Email-ext\nplugins.  Slack and HipChat use API tokens - both products have integration\npoints on their side that generate tokens which I copy into my Jenkins\nconfiguration. Mailcatcher SMTP runs locally. I just point Jenkins\nat it.\n\nHere’s what the Jenkins configuration section for each of these looks like:\n\nOriginal Pipeline\n\nNow I can start adding notification steps. The same as\nlast week, I’ll use the\nJenkins Pipeline Snippet Generator\nto explore the step syntax for the notification plugins.\n\nHere’s the base pipeline before I start making changes:\n\nstage 'Build'\n\nnode {\n  // Checkout\n  checkout scm\n\n  // install required bundles\n  sh 'bundle install'\n\n  // build and run tests with coverage\n  sh 'bundle exec rake build spec'\n\n  // Archive the built artifacts\n  archive (includes: 'pkg/*.gem')\n\n  // publish html\n  // snippet generator doesn't include \"target:\"\n  // https://issues.jenkins.io/browse/JENKINS-29711.\n  publishHTML (target: [\n      allowMissing: false,\n      alwaysLinkToLastBuild: false,\n      keepAll: true,\n      reportDir: 'coverage',\n      reportFiles: 'index.html',\n      reportName: \"RCov Report\"\n    ])\n}\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit 'https://github.com/reiseburo/hermann.git'.\n\nJob Started Notification\n\nFor the first change, I decide to add a \"Job Started\" notification.  The\nsnippet generator and then reformatting makes this straightforward:\n\nnode {\n\n  notifyStarted()\n\n  /* ... existing build steps ... */\n}\n\ndef notifyStarted() {\n  // send to Slack\n  slackSend (color: '#FFFF00', message: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\")\n\n  // send to HipChat\n  hipchatSend (color: 'YELLOW', notify: true,\n      message: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\"\n    )\n\n  // send to email\n  emailext (\n      subject: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\",\n      body: \"\"\" STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':\nCheck console output at \" ${env.JOB_NAME} [${env.BUILD_NUMBER}]\"\"\"\",\n      recipientProviders: [[$class: 'DevelopersRecipientProvider']]\n    )\n}\n\nSince Pipeline is a Groovy-based DSL, I can use\nstring interpolation\nand variables to add exactly the details I want in my notification messages. When\nI run this I get the following notifications:\n\nJob Successful Notification\n\nThe next logical choice is to get notifications when a job succeeds.  I’ll\ncopy and paste based on the notifyStarted method for now and do some refactoring\nlater.\n\nnode {\n\n  notifyStarted()\n\n  /* ... existing build steps ... */\n\n  notifySuccessful()\n}\n\ndef notifyStarted() { /* .. */ }\n\ndef notifySuccessful() {\n  slackSend (color: '#00FF00', message: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\")\n\n  hipchatSend (color: 'GREEN', notify: true,\n      message: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\"\n    )\n\n  emailext (\n      subject: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\",\n      body: \"\"\" SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':\nCheck console output at \" ${env.JOB_NAME} [${env.BUILD_NUMBER}]\"\"\"\",\n      recipientProviders: [[$class: 'DevelopersRecipientProvider']]\n    )\n}\n\nAgain, I get notifications, as expected.  This build is fast enough,\nsome of them are even on the screen at the same time:\n\nJob Failed Notification\n\nNext I want to add failure notification.  Here’s where we really start to see the power\nand expressiveness of Jenkins pipeline.  A Pipeline is a Groovy script, so as we’d\nexpect in any Groovy script, we can handle errors using try-catch blocks.\n\nnode {\n  try {\n    notifyStarted()\n\n    /* ... existing build steps ... */\n\n    notifySuccessful()\n  } catch (e) {\n    currentBuild.result = \"FAILED\"\n    notifyFailed()\n    throw e\n  }\n}\n\ndef notifyStarted() { /* .. */ }\n\ndef notifySuccessful() { /* .. */ }\n\ndef notifyFailed() {\n  slackSend (color: '#FF0000', message: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\")\n\n  hipchatSend (color: 'RED', notify: true,\n      message: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\"\n    )\n\n  emailext (\n      subject: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\",\n      body: \"\"\" FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':\nCheck console output at \" ${env.JOB_NAME} [${env.BUILD_NUMBER}]\"\"\"\",\n      recipientProviders: [[$class: 'DevelopersRecipientProvider']]\n    )\n}\n\nCode Cleanup\n\nLastly, now that I have it all working, I’ll do some refactoring. I’ll unify\nall the notifications in one method and move the final success/failure notification\ninto a finally block.\n\nstage 'Build'\n\nnode {\n  try {\n    notifyBuild('STARTED')\n\n    /* ... existing build steps ... */\n\n  } catch (e) {\n    // If there was an exception thrown, the build failed\n    currentBuild.result = \"FAILED\"\n    throw e\n  } finally {\n    // Success or failure, always send notifications\n    notifyBuild(currentBuild.result)\n  }\n}\n\ndef notifyBuild(String buildStatus = 'STARTED') {\n  // build status of null means successful\n  buildStatus = buildStatus ?: 'SUCCESS'\n\n  // Default values\n  def colorName = 'RED'\n  def colorCode = '#FF0000'\n  def subject = \"${buildStatus}: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\"\n  def summary = \"${subject} (${env.BUILD_URL})\"\n  def details = \"\"\" STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':\nCheck console output at \" ${env.JOB_NAME} [${env.BUILD_NUMBER}]\"\"\"\"\n\n  // Override default values based on build status\n  if (buildStatus == 'STARTED') {\n    color = 'YELLOW'\n    colorCode = '#FFFF00'\n  } else if (buildStatus == 'SUCCESS') {\n    color = 'GREEN'\n    colorCode = '#00FF00'\n  } else {\n    color = 'RED'\n    colorCode = '#FF0000'\n  }\n\n  // Send notifications\n  slackSend (color: colorCode, message: summary)\n\n  hipchatSend (color: color, notify: true, message: summary)\n\n  emailext (\n      subject: subject,\n      body: details,\n      recipientProviders: [[$class: 'DevelopersRecipientProvider']]\n    )\n}\n\nYou have been notified!\n\nI now get notified twice per build on three different channels.  I’m not sure I\nneed to get notified this much for such a short build.  However, for a longer\nor complex CD pipeline, I might want exactly that.  If needed, I could even\nimprove this to handle other status strings and call it as needed throughout\nmy pipeline.\n\nLinks\n\nSlack Plugin\n\nHipChat Plugin\n\nEmail-ext Plugin\n\nJenkins Pipeline Snippet Generator","title":"Sending Notifications in Pipeline","tags":["tutorial","pipeline","plugins","notifications","slack","hipchat","emailext"],"authors":[{"avatar":{"childImageSharp":null},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"blog/author/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2016-07-01T00:00:00.000Z","id":"4848db1b-feac-54a0-8b3e-2a0f5e3fbfc6","slug":"/blog/2016/07/01/html-publisher-plugin/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nMost projects need more that just JUnit result reporting.  Rather than writing a\ncustom plugin for each type of report, we can use the\nHTML Publisher Plugin.\n\nLet’s Make This Quick\n\nI’ve found a Ruby project,\nhermann, I’d like to build using Jenkins Pipeline. I’d\nalso like to have the code coverage results published with each build job.  I could\nwrite a plugin to publish this data, but I’m in a bit of hurry and\nthe build already creates an HTML report file using SimpleCov\nwhen the unit tests run.\n\nSimple Build\n\nI’m going to use the\nHTML Publisher Plugin\nto add the HTML-formatted code coverage report to my builds.  Here’s a simple\npipeline for building the hermann\nproject.\n\nstage 'Build'\n\nnode {\n  // Checkout\n  checkout scm\n\n  // install required bundles\n  sh 'bundle install'\n\n  // build and run tests with coverage\n  sh 'bundle exec rake build spec'\n\n  // Archive the built artifacts\n  archive (includes: 'pkg/*.gem')\n}\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit 'https://github.com/reiseburo/hermann.git'.\n\nSimple enough, it builds, runs tests, and archives the package.\n\nNow I just need to add the step to publish the code coverage report.\nI know that rake spec creates an index.html file in the coverage directory.\nI’ve already installed the\nHTML Publisher Plugin.\nHow do I add the HTML publishing step to the pipeline?  The plugin page doesn’t\nsay anything about it.\n\nSnippet Generator to the Rescue\n\nDocumentation is hard to maintain and easy to miss, even more so in a system\nlike Jenkins with hundreds of plugins the each potential have one or more\ngroovy fixtures to add to the Pipeline.  The Pipeline Syntax\"Snippet Generator\" helps users\nnavigate this jungle by providing a way to generate a code snippet for any step using\nprovided inputs.\n\nIt offers a dynamically generated list of steps, based on the installed plugins.\nFrom that list I select the publishHTML step:\n\nThen it shows me a UI similar to the one used in job configuration.  I fill in\nthe fields, click \"generate\", and it shows me snippet of groovy generated from\nthat input.\n\nHTML Published\n\nI can use that snippet directly or as a template for further customization.\nIn this case, I’ll just reformat and copy it in at the end of my\npipeline.  (I ran into a minor bug\nin the snippet generated for this plugin step. Typing\nerror string in my search bar immediately found the bug and a workaround.)\n\n/* ...unchanged... */\n\n  // Archive the built artifacts\n  archive (includes: 'pkg/*.gem')\n\n  // publish html\n  // snippet generator doesn't include \"target:\"\n  // https://issues.jenkins.io/browse/JENKINS-29711.\n  publishHTML (target: [\n      allowMissing: false,\n      alwaysLinkToLastBuild: false,\n      keepAll: true,\n      reportDir: 'coverage',\n      reportFiles: 'index.html',\n      reportName: \"RCov Report\"\n    ])\n\n}\n\nWhen I run this new pipeline I am rewarded with an RCov Report link on left side,\nwhich I can follow to show the HTML report.\n\nI even added the keepAll setting to let I can also go back an look at reports on old jobs as\nmore come in.  As I said to to begin with, this is not as slick as what I\ncould do with a custom plugin, but it is much easier and works with any static\nHTML.\n\nLinks\n\nHTML Publisher Plugin\n\nJenkins Pipeline Snippet Generator","title":"Publishing HTML Reports in Pipeline","tags":["tutorial","pipeline","plugins","ruby"],"authors":[{"avatar":{"childImageSharp":null},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"blog/author/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2016-06-30T00:00:00.000Z","id":"6dc0bcbf-e180-50ae-86bf-0a881c810b38","slug":"/blog/2016/06/30/ewm-alpha-version/","strippedHtml":"Currently it’s quite difficult to share and reuse the same workspace between multiple jobs and across nodes.\nThere are some possible workarounds for achieving this, but each of them has its own drawback,\ne.g. stash/unstash pre-made artifacts, Copy Artifacts plugin or advanced job settings.\nA viable solution for this problem is the External Workspace Manager plugin, which facilitates workspace share and\nreuse across multiple Jenkins jobs and nodes.\nIt also eliminates the need to copy, archive or move files.\nYou can learn more about the design and goals of the External Workspace Manager project in\nthis introductory blog post.\n\nI’d like to announce that an alpha version of the External Manager Plugin has been released!\nIt’s now public available for testing.\nTo be able to install this plugin, you must follow the steps from the Experimental Plugins Update Center\nblog post.\n\nPlease be aware that it’s not recommended to use the Experimental Update Center in production installations of\nJenkins, since it may break it.\n\nThe plugin’s wiki page may be accessed\nhere.\nThe documentation that helps you get started with this plugin may be found on the\nREADME page.\nTo get an idea of what this plugin does, which are the features implemented so far and to see a working demo of it,\nyou can watch my mid-term presentation that is available here.\nThe slides for the presentation are shared on\nGoogle Slides.\n\nMy mentors, Martin and Oleg,\nand I have set up public meetings related to this plugin.\nYou are invited to join our discussions if you’d like to get more insight about the project.\nThe meetings are taking place twice a week on the Jenkins hangout,\nevery Monday at\n12 PM UTC\nand every Thursday at\n5 PM UTC.\n\nIf you have any issues in setting up or using the plugin, please feel free to ask me on the plugin’s Gitter\nchat.\nThe plugin is open-source, having the repository on\nGitHub, and you may contribute to it.\nAny feedback is welcome, and you may provide it either on the Gitter chat, or on\nJira by using the external-workspace-manager-plugin component.\n\nLinks\n\nProject repository\n\nPlugin wiki page\n\nMid-term presentation\n\nProject intro blog post\n\nGSoC page\n\nJenkins GSoC Page","title":"GSoC: External Workspace Manager Plugin alpha version","tags":["pipeline","plugins","external-workspace-manager","gsoc"],"authors":[{"avatar":null,"blog":null,"github":"alexsomai","html":"","id":"alexsomai","irc":null,"linkedin":null,"name":"Alexandru Somai","slug":"blog/author/alexsomai","twitter":"alex_somai"}]}},{"node":{"date":"2016-06-29T00:00:00.000Z","id":"72dc398a-0c3a-5b36-949e-7687e6ecbefc","slug":"/blog/2016/06/29/from-freestyle-to-pipeline/","strippedHtml":"This is a guest post by R. Tyler Croy, who is a\nlong-time contributor to Jenkins and the primary contact for Jenkins project\ninfrastructure. He is also a Jenkins Evangelist at\nCloudBees, Inc.\n\nFor ages I have used the \"Build After\" feature in Jenkins to cobble together\nwhat one might refer to as a \"pipeline\" of sorts. The Jenkins project itself, a\nmajor consumer of Jenkins, has used these daisy-chained Freestyle jobs to drive\na myriad of delivery pipelines in our infrastructure.\n\nOne such \"pipeline\" helped drive the complex process of generating the pretty\nblue charts on\nstats.jenkins.io.\nThis statistics generation process primarily performs two major tasks, on rather\nlarge sets of data:\n\nGenerate aggregate monthly \"census data.\"\n\nProcess the census data and create trend charts\n\nThe chained jobs allowed us to resume the independent stages of the pipeline,\nand allowed us to run different stages on different hardware (different\ncapabilities) as needed. Below is a diagram of what this looked like:\n\nThe infra_generate_monthly_json would run periodically creating the\naggregated census data, which would then be picked up by infra_census_push\nwhose sole responsibility was to take census data and publish it to the\nnecessary hosts inside the project’s infrastructure.\n\nThe second, semi-independent, \"pipeline\" would also run periodically. The\ninfra_statistics job’s responsibility was to use the census data, pushed\nearlier by infra_census_push, to generate the myriad of pretty blue charts\nbefore triggering the\ninfra_checkout_stats job which would make sure stats.jenkins.io was\nproperly updated.\n\nSuffice it to say, this \"pipeline\" had grown organically over a period time when\nmore advanced tools weren’t quite available.\n\nWhen we migrated to newer infrastructure for\nci.jenkins.io earlier this year I took the\nopportunity to do some cleaning up. Instead of migrating jobs verbatim, I pruned\nstale jobs and refactored a number of others into proper\nPipelines, statistics generation being an obvious\ntarget!\n\nOur requirements for statistics generation, in their most basic form, are:\n\nEnable a sequence of dependent tasks to be executed as a logical group (a\npipeline)\n\nEnable executing those dependent tasks on various pieces of infrastructure\nwhich support different requirements\n\nActually generate those pretty blue charts\n\nIf you wish to skip ahead, you can jump straight to the\nJenkinsfile\nwhich implements our new Pipeline.\n\nThe first iteration of the Jenkinsfile simply defined the conceptual stages we\nwould need:\n\nnode {\n    stage 'Sync raw data and census files'\n\n    stage 'Process raw logs'\n\n    stage 'Generate census data'\n\n    stage 'Generate stats'\n\n    stage 'Publish census'\n\n    stage 'Publish stats'\n}\n\nHow exciting! Although not terrifically useful. When I began actually\nimplementing the first couple stages, I noticed that the Pipeline might sync\ndozens of gigabytes of data every time it ran on a new agent in the cluster.\nWhile this problem will soon be solved by the\nExternal\nWorkspace Manager plugin, which is currently being developed. Until it’s ready,\nI chose to mitigate the issue by pinning the execution to a consistent agent.\n\n/* `census` is a node label for a single machine, ideally, which will be\n * consistently used for processing usage statistics and generating census data\n */\nnode('census && docker') {\n    /* .. */\n}\n\nRestricting a workload which previously used multiple agents to a single one\nintroduced the next challenge. As an infrastructure administrator, technically\nspeaking, I could just install all the system dependencies that I want on this\none special Jenkins agent. But what kind of example would that be setting!\n\nThe statistics generation process requires:\n\nJDK8\n\nGroovy\n\nA running MongoDB instance\n\nFortunately, with Pipeline we have a couple of useful features at our disposal:\ntool auto-installers and the\nCloudBees\nDocker Pipeline plugin.\n\nTool Auto-Installers\n\nTool Auto-Installers are exposed in Pipeline through the tool step and on\nci.jenkins.io we already had JDK8 and Groovy\navailable. This meant that the Jenkinsfile would invoke tool and Pipeline\nwould automatically install the desired tool on the agent executing the current\nPipeline steps.\n\nThe tool step does not modify the PATH environment variable, so it’s usually\nused in conjunction with the withEnv step, for example:\n\nnode('census && docker') {\n    /* .. */\n\n    def javaHome = tool(name: 'jdk8')\n    def groovyHome = tool(name: 'groovy')\n\n    /* Set up environment variables for re-using our auto-installed tools */\n    def customEnv = [\n        \"PATH+JDK=${javaHome}/bin\",\n        \"PATH+GROOVY=${groovyHome}/bin\",\n        \"JAVA_HOME=${javaHome}\",\n    ]\n\n    /* use our auto-installed tools */\n    withEnv(customEnv) {\n        sh 'java --version'\n    }\n\n    /* .. */\n}\n\nCloudBees Docker Pipeline plugin\n\nSatisfying the MongoDB dependency would still be tricky. If I caved in and installed\nMongoDB on a single unicorn agent in the cluster, what could I say the next time\nsomebody asked for a special, one-off, piece of software installed on our\nJenkins build agents?\n\nAfter doing my usual complaining and whining, I discovered that the CloudBees\nDocker Pipeline plugin provides the ability to run containers inside of a\nJenkinsfile. To make things even better, there are\nofficial MongoDB docker images readily\navailable on DockerHub!\n\nThis feature requires that the machine has a running Docker daemon which is\naccessible to the user running the Jenkins agent. After that, running a\ncontainer in the background is easy, for example:\n\nnode('census && docker') {\n    /* .. */\n\n    /* Run MongoDB in the background, mapping its port 27017 to our host's port\n     * 27017 so our script can talk to it, then execute our Groovy script with\n     * tools from our `customEnv`\n     */\n    docker.image('mongo:2').withRun('-p 27017:27017') { container ->\n        withEnv(customEnv) {\n            sh \"groovy parseUsage.groovy --logs ${usagestats_dir} --output ${census_dir} --incremental\"\n        }\n    }\n\n    /* .. */\n}\n\nThe beauty, to me, of this example is that you can pass a\nclosure to withRun which will\nexecute while the container is running. When the closure is finished executin,\njust the sh step in this case, the container is destroyed.\n\nWith that system requirement satisfied, the rest of the stages of the Pipeline\nfell into place. We now have a single source of truth, the\nJenkinsfile,\nfor the sequence of dependent tasks which need to be executed, accounting for\nvariations in systems requirements, and it actually generates\nthose pretty\nblue charts!\n\nOf course, a nice added bonus is the beautiful visualization of our\nnew Pipeline!\n\nLinks\n\nPipeline documentation\n\nCloudBees Docker Pipeline plugin documentation\n\nLive statistics Pipeline","title":"Migrating from chained Freestyle jobs to Pipelines","tags":["pipeline","infra"],"authors":[{"avatar":{"childImageSharp":null},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"blog/author/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-06-16T00:00:00.000Z","id":"9d5b4fb7-1151-5470-985c-045b0dd79455","slug":"/blog/2016/06/16/parallel-test-executor-plugin/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nIn this blog post, I’ll show you how to speed up your pipeline by using the\nParallel Test Executor Plugin.\n\nSo much to do, so little time…​\n\nIn my career, I’ve helped many teams move to continuous integration and delivery. One problem\nwe always encounter is how to run all the tests needed to ensure high-quality\nchanges while still keeping pipeline times reasonable and changes flowing\nsmoothly. More tests mean greater confidence, but also longer wait times.\nBuild systems may or may not support running tests in parallel, but they still only use one\nmachine even while other lab machines sit idle. In these cases, parallelizing\ntest execution across multiple machines is a great way to speed up pipelines.\nThe Parallel Test Executor plugin lets us leverage Jenkins do just that with no\ndisruption to the rest of the build system.\n\nSerial Test Execution\n\nFor this post, I’ll be running a pipeline based on the\nJenkins Git Plugin. I’ve modified\nthe Jenkinsfile from that project to allow us to compare execution times to our\nlater changes, and I’ve truncated the \"mvn\" utility method since it remains\nunchanged.  You can find the original file\nhere.\n\nnode {\n  stage 'Checkout'\n  checkout scm\n\n  stage 'Build'\n\n  /* Call the Maven build without tests. */\n  mvn \"clean install -DskipTests\"\n\n  stage 'Test'\n  runTests()\n\n  /* Save Results. */\n  stage 'Results'\n\n  /* Archive the build artifacts */\n  archive includes: 'target/*.hpi,target/*.jpi'\n}\n\nvoid runTests(def args) {\n  /* Call the Maven build with tests. */\n  mvn \"install -Dmaven.test.failure.ignore=true\"\n\n  /* Archive the test results */\n  junit '**/target/surefire-reports/TEST-*.xml'\n}\n\n/* Run Maven */\nvoid mvn(def args) { /* ... */ }\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit 'https://github.com/jenkinsci/git-plugin.git'.\n\nThis is a Maven project, so the Jenkinsfile is pretty simple.\nI’ve split the Maven build into separate “Build” and “Test”\nstages. Maven doesn’t support this split very well, it wants to run all\nthe steps of the lifecycle in order every time. So, I have to call Maven twice:\nfirst using the “skipTests” property to do only build steps in the first call,\nand then a second time with out that property to run tests.\n\nOn my quad-core machine, executing this pipeline takes about 13 minutes and 30\nseconds.  Of that time, it takes 13 minutes to run about 2.7 thousand tests in\nserial.\n\nParallel Test Execution\n\nThis looks like an ideal project for parallel test execution: a short build\nfollowed by a large number of serially executed tests that consume the most of\nthe pipeline time. There are a number of things I could try to speed this up.\nFor example, I could modify test harness to look for ways to parallelize\nthe test execution on this single machine. Or I could try speed up the tests\nthemselves. Both of those can be time-consuming and both risk destabilizing the\ntests. I’d need to know more about the project to do it well.\n\nI’ll avoid that risk by using Jenkins and the\nParallel Test Executor Plugin to\nparallelize the tests across multiple nodes instead. This will isolate the tests\nfrom each other, while still giving us speed gains from parallel execution.\n\nThe plugin reads the list of tests from the results archived in the previous execution of this\njob and splits that list into a specified number of sublists. I can then use\nthose sublists to execute the tests in parallel, passing a different sublist to\neach node.\n\nLet’s look at how this changes the pipeline:\n\nnode { /* ...unchanged... */ }\n\nvoid runTests(def args) {\n  /* Request the test groupings.  Based on previous test results. */\n  /* see https://wiki.jenkins.io/display/JENKINS/Parallel+Test+Executor+Plugin and demo on github\n  /* Using arbitrary parallelism of 4 and \"generateInclusions\" feature added in v1.8. */\n  def splits = splitTests parallelism: [$class: 'CountDrivenParallelism', size: 4], generateInclusions: true\n\n  /* Create dictionary to hold set of parallel test executions. */\n  def testGroups = [:]\n\n  for (int i = 0; i }. */\n    /*     includes = whether list specifies tests to include (true) or tests to exclude (false). */\n    /*     list = list of tests for inclusion or exclusion. */\n    /* The list of inclusions is constructed based on results gathered from */\n    /* the previous successfully completed job. One additional record will exclude */\n    /* all known tests to run any tests not seen during the previous run.  */\n    testGroups[\"split-${i}\"] = {  // example, \"split3\"\n      node {\n        checkout scm\n\n        /* Clean each test node to start. */\n        mvn 'clean'\n\n        def mavenInstall = 'install -DMaven.test.failure.ignore=true'\n\n        /* Write includesFile or excludesFile for tests.  Split record provided by splitTests. */\n        /* Tell Maven to read the appropriate file. */\n        if (split.includes) {\n          writeFile file: \"target/parallel-test-includes-${i}.txt\", text: split.list.join(\"\\n\")\n          mavenInstall += \" -Dsurefire.includesFile=target/parallel-test-includes-${i}.txt\"\n        } else {\n          writeFile file: \"target/parallel-test-excludes-${i}.txt\", text: split.list.join(\"\\n\")\n          mavenInstall += \" -Dsurefire.excludesFile=target/parallel-test-excludes-${i}.txt\"\n        }\n\n        /* Call the Maven build with tests. */\n        mvn mavenInstall\n\n        /* Archive the test results */\n        junit '**/target/surefire-reports/TEST-*.xml'\n      }\n    }\n  }\n  parallel testGroups\n}\n\n/* Run Maven */\nvoid mvn(def args) { /* ... */ }\n\nThat’s it!  The change is significant but it is all encapsulated in this one\nmethod in the Jenkinsfile.\n\nGreat (ish) Success!\n\nHere’s the results for the new pipeline with parallel test execution:\n\nThe tests ran almost twice as fast, without changes outside pipeline.  Great!\n\nHowever, I used 4 test executors, so why am I not seeing a 4x? improvement.\nA quick review of the logs shows the problem: A small number of tests are taking up\nto 5 minutes each to complete! This is actually good news. It means that I\nshould be able to see further improvement in pipeline throughput just by refactoring\nthose few long running tests into smaller parts.\n\nConclusion\n\nWhile I would like to have seen closer to a 4x improvement to match to number\nof executors, 2x is still perfectly respectable. If I were working on a group of projects\nwith similar pipelines, I’d be completely comfortable reusing these same changes\non my other project and I’d expect to similar improvement without any disruption to\nother tools or processes.\n\nLinks\n\nhttps://wiki.jenkins.io/display/JENKINS/Parallel+Test+Executor+Plugin","title":"Faster Pipelines with the Parallel Test Executor Plugin","tags":["tutorial","pipeline","plugins"],"authors":[{"avatar":{"childImageSharp":null},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"blog/author/lnewman","twitter":"bitwiseman"}]}}]}},"pageContext":{"tag":"pipeline","limit":8,"skip":64,"numPages":13,"currentPage":9}},
    "staticQueryHashes": ["3649515864"]}