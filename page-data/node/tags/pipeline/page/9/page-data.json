{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/pipeline/page/9",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2016-08-10T00:00:00.000Z","id":"4f2eb2b7-1bb1-501e-8b2c-1b8c4ac3e6e3","slug":"/blog/2016/08/10/rails-cd-with-pipeline/","strippedHtml":"This is a guest post by R. Tyler Croy, who is a\nlong-time contributor to Jenkins and the primary contact for Jenkins project\ninfrastructure. He is also a Jenkins Evangelist at\nCloudBees, Inc.\n\nWhen the Ruby on Rails framework debuted it\nchanged the industry in two noteworthy ways: it created a trend of opinionated web\napplication frameworks ( Django,\nPlay, Grails) and it\nalso strongly encouraged thousands of developers to embrace test-driven\ndevelopment along with many other modern best practices (source control, dependency\nmanagement, etc). Because Ruby, the language underneath Rails, is interpreted\ninstead of compiled there isn’t a \"build\" per se but rather tens, if not\nhundreds, of tests, linters and scans which are run to ensure the application’s\nquality. With the rise in popularity of Rails, the popularity of application\nhosting services with easy-to-use deployment tools like Heroku or\nEngine Yard rose too.\n\nThis combination of good test coverage and easily automated deployments\nmakes Rails easy to continuously deliver with Jenkins. In this post we’ll cover\ntesting non-trivial Rails applications with Jenkins\nPipeline and, as an added bonus, we will add security scanning via\nBrakeman and the\nBrakeman\nplugin.\n\nTopics\n\nPreparing the app\n\nPreparing Jenkins\n\nWriting the Pipeline\n\nSecurity scanning\n\nDeploying the good stuff\n\nWrap up\n\nFor this demonstration, I used Ruby Central 's\ncfp-app :\n\nA Ruby on Rails application that lets you manage your conference’s call for\nproposal (CFP), program and schedule. It was written by Ruby Central to run the\nCFPs for RailsConf and RubyConf.\n\nI chose this Rails app, not only because it’s a sizable application with lots\nof tests, but it’s actually the application we used to collect talk proposals\nfor the \" Community Tracks\" at this\nyear’s Jenkins World. For the most part,\ncfp-app is a standard Rails application. It uses\nPostgreSQL for its database,\nRSpec for its tests and\nRuby 2.3.x as its runtime.\n\nIf you prefer to just to look at the code, skip straight to the\nJenkinsfile.\n\nPreparing the app\n\nFor most Rails applications there are few, if any, changes needed to enable\ncontinuous delivery with Jenkins. In the case of\ncfp-app, I added two gems to get\nthe most optimal integration into Jenkins:\n\nci_reporter, for test report\nintegration\n\nbrakeman, for security scanning.\n\nAdding these was simple, I just needed to update the Gemfile and the\nRakefile in the root of the repository to contain:\n\nGemfile\n\n# .. snip ..\ngroup :test do\n  # RSpec, etc\n  gem 'ci_reporter'\n  gem 'ci_reporter_rspec'\n  gem \"brakeman\", :require => false\nend\n\nRakefile\n\n# .. snip ..\nrequire 'ci/reporter/rake/rspec'\n# Make sure we setup ci_reporter before executing our RSpec examples\ntask :spec => 'ci:setup:rspec'\n\nPreparing Jenkins\n\nWith the cfp-app project set up, next on the list is to ensure that Jenkins itself\nis ready. Generally I suggest running the latest LTS of\nJenkins; for this demonstration I used Jenkins 2.7.1 with the following\nplugins:\n\nPipeline plugin\n\nBrakeman plugin\n\nCloudBees\nDocker Pipeline plugin\n\nI also used the\nGitHub\nOrganization Folder plugin to automatically create pipeline items in my\nJenkins instance; that isn’t required for the demo, but it’s pretty cool to see\nrepositories and branches with a Jenkinsfile automatically show up in\nJenkins, so I recommend it!\n\nIn addition to the plugins listed above, I also needed at least one\nJenkins agent with the Docker daemon installed and\nrunning on it. I label these agents with \"docker\" to make it easier to assign\nDocker-based workloads to them in the future.\n\nAny Linux-based machine with Docker installed will work, in my case I was\nprovisioning on-demand agents with the\nAzure\nplugin which, like the\nEC2 plugin,\nhelps keep my test costs down.\n\nIf you’re using Amazon Web Services, you might also be interested in\nthis blog post from\nearlier this year unveiling the\nEC2\nFleet plugin for working with EC2 Spot Fleets.\n\nWriting the Pipeline\n\nTo make sense of the various things that the Jenkinsfile needs to do, I find\nit easier to start by simply defining the stages of my pipeline. This helps me\nthink of, in broad terms, what order of operations my pipeline should have.\nFor example:\n\n/* Assign our work to an agent labelled 'docker' */\nnode('docker') {\n    stage 'Prepare Container'\n    stage 'Install Gems'\n    stage 'Prepare Database'\n    stage 'Invoke Rake'\n    stage 'Security scan'\n    stage 'Deploy'\n}\n\nAs mentioned previously, this Jenkinsfile is going to rely heavily on the\nCloudBees\nDocker Pipeline plugin. The plugin provides two very important features:\n\nAbility to execute steps inside of a running Docker container\n\nAbility to run a container in the \"background.\"\n\nLike most Rails applications, one can effectively test the application with two\ncommands: bundle install followed by bundle exec rake. I already had some\nDocker images prepared with RVM and Ruby 2.3.0 installed,\nwhich ensures a common and consistent starting point:\n\nnode('docker') {\n    // .. 'stage' steps removed\n    docker.image('rtyler/rvm:2.3.0').inside { (1)\nrvm 'bundle install' (2)\nrvm 'bundle exec rake'\n    } (3)\n}\n\n1\nRun the named container. The inside method can take optional additional flags for the docker run command.\n\n2\nExecute our shell commands using our tiny sh step wrapper\nrvm . This ensures that the shell code is executed in the correct RVM environment.\n\n3\nWhen the closure completes, the container will be destroyed.\n\nUnfortunately, with this application, the bundle exec rake command will fail\nif PostgreSQL isn’t available when the process starts. This is where the\nsecond important feature of the CloudBees Docker Pipeline plugin comes\ninto effect: the ability to run a container in the \"background.\"\n\nnode('docker') {\n    // .. 'stage' steps removed\n    /* Pull the latest `postgres` container and run it in the background */\n    docker.image('postgres').withRun { container -> (1)\necho \"PostgreSQL running in container ${container.id}\" (2)\n} (3)\n}\n\n1\nRun the container, effectively docker run postgres\n\n2\nAny number of steps can go inside the closure\n\n3\nWhen the closure completes, the container will be destroyed.\n\nRunning the tests\n\nCombining these two snippets of Jenkins Pipeline is, in my opinion, where the\npower of the DSL\nshines:\n\nnode('docker') {\n    docker.image('postgres').withRun { container ->\n        docker.image('rtyler/rvm:2.3.0').inside(\"--link=${container.id}:postgres\") { (1)\nstage 'Install Gems'\n            rvm \"bundle install\"\n\n            stage 'Invoke Rake'\n            withEnv(['DATABASE_URL=postgres://postgres@postgres:5432/']) { (2)\nrvm \"bundle exec rake\"\n            }\n            junit 'spec/reports/*.xml' (3)\n}\n    }\n}\n\n1\nBy passing the --link argument, the Docker daemon will allow the RVM container to talk to the PostgreSQL container under the host name 'postgres'.\n\n2\nUse the withEnv step to set environment variables for everything that is in the closure. In this case, the cfp-app DB scaffolding will look for the DATABASE_URL variable to override the DB host/user/dbname defaults.\n\n3\nArchive the test reports generated by ci_reporter so that Jenkins can display test reports and trend analysis.\n\nWith this done, the basics are in place to consistently run the tests for\ncfp-app in fresh Docker containers for each execution of the pipeline.\n\nSecurity scanning\n\nUsing Brakeman, the security scanner for Ruby\non Rails, is almost trivially easy inside of Jenkins Pipeline, thanks to the\nBrakeman\nplugin which implements the publishBrakeman step.\n\nBuilding off our example above, we can implement the \"Security scan\" stage:\n\nnode('docker') {\n    /* --8 (1)\npublishBrakeman 'brakeman-output.tabs' (2)\n/* --8\n\n1\nRun the Brakeman security scanner for Rails and store the output for later in brakeman-output.tabs\n\n2\nArchive the reports generated by Brakeman so that Jenkins can display detailed reports with trend analysis.\n\nAs of this writing, there is work in progress\n( JENKINS-31202) to\nrender trend graphs from plugins like Brakeman on a pipeline project’s main\npage.\n\nDeploying the good stuff\n\nOnce the tests and security scanning are all working properly, we can start to\nset up the deployment stage. Jenkins Pipeline provides the variable\ncurrentBuild which we can use to determine whether our pipeline has been\nsuccessful thus far or not. This allows us to add the logic to only deploy when\neverything is passing, as we would expect:\n\nnode('docker') {\n    /* --8 (1)\nsh './deploy.sh' (2)\n}\n    else {\n        mail subject: \"Something is wrong with ${env.JOB_NAME} ${env.BUILD_ID}\",\n                  to: 'nobody@example.com',\n                body: 'You should fix it'\n    }\n    /* --8\n\n1\ncurrentBuild has the result property which would be 'SUCCESS', 'FAILED', 'UNSTABLE', 'ABORTED'\n\n2\nOnly if currentBuild.result is successful should we bother invoking our deployment script (e.g. git push heroku master)\n\nWrap up\n\nI have gratuitously commented the full\nJenkinsfile\nwhich I hope is a useful summation of the work outlined above. Having worked\non a number of Rails applications in the past, the consistency provided by\nDocker and Jenkins Pipeline above would have definitely improved those\nprojects' delivery times. There is still room for improvement however, which\nis left as an exercise for the reader. Such as: preparing new containers with\nall their\ndependencies\nbuilt-in instead of installing them at run-time. Or utilizing the parallel\nstep for executing RSpec across multiple Jenkins agents simultaneously.\n\nThe beautiful thing about defining your continuous delivery, and continuous\nsecurity, pipeline in code is that you can continue to iterate on it!","title":"Continuous Security for Rails apps with Pipeline and Brakeman","tags":["tutorial","ruby","pipeline","rails","brakeman","continuousdelivery"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-08-09T00:00:00.000Z","id":"e7eb5984-a870-5548-985b-25b9210fb2a0","slug":"/blog/2016/08/09/ewm-beta-version/","strippedHtml":"This blog post is a continuation of the External Workspace Manager Plugin related posts, starting with\nthe introductory blog post, and followed by\nthe alpha version release announcement.\n\nAs the title suggests, the beta version of the External Workspace Manager Plugin was launched!\nThis means that it’s available only in the\nExperimental Plugins Update Center.\n\nTake care when installing plugins from the Experimental Update Center, since they may change in\nbackward-incompatible ways.\nIt’s advisable not to use it for Jenkins production environments.\n\nThe plugin’s repository is on GitHub.\nThe complete plugin’s documentation can be accessed\nhere.\n\nWhat’s new\n\nBellow is a summary of the features added so far, since the alpha version.\n\nMultiple upstream run selection strategies\n\nIt has support for the\nRun Selector Plugin (which is still in beta),\nso you can provide different run selection strategies when allocating a disk from the upstream job.\n\nLet’s suppose that we have an upstream job that clones the repository and builds the project:\n\ndef extWorkspace = exwsAllocate 'diskpool1'\n\nnode ('linux') {\n    exws (extWorkspace) {\n        checkout scm\n        sh 'mvn clean install -DskipTests'\n    }\n}\n\nIn the downstream job, we run the tests on a different node, but we reuse the same workspace as the previous job:\n\ndef run = selectRun 'upstream'\ndef extWorkspace = exwsAllocate selectedRun: run\n\nnode ('test') {\n    exws (extWorkspace) {\n        sh 'mvn test'\n    }\n}\n\nThe selectRun in this example selects the last stable build from the upstream job.\nBut, we can be more explicit, and select a specific build number from the upstream job.\n\ndef run = selectRun 'upstream',\n selector: [$class: 'SpecificRunSelector', buildNumber: UPSTREAM_BUILD_NUMBER]\ndef extWorkspace = exwsAllocate selectedRun: run\n// ...\n\nWhen the selectedRun parameter is given to the exwsAllocate step, it will allocate the same workspace that was\nused by that run.\n\nThe Run Selector Plugin has several run selection strategies that are briefly explained\nhere.\n\nAutomatic workspace cleanup\n\nProvides an automatic workspace cleanup by integrating the\nWorkspace Cleanup Plugin.\nFor example, if we need to delete the workspace only if the build has failed, we can do the following:\n\ndef extWorkspace = exwsAllocate diskPoolId: 'diskpool1'\n\nnode ('linux') {\n    exws (extWorkspace) {\n        try {\n            checkout scm\n            sh 'mvn clean install'\n        } catch (e) {\n            currentBuild.result = 'FAILURE'\n            throw e\n        } finally {\n            step ([$class: 'WsCleanup', cleanWhenFailure: false])\n        }\n    }\n}\n\nMore workspace cleanup examples can be found at this\nlink.\n\nCustom workspace path\n\nAllows the user to specify a custom workspace path to be used when allocating workspace on the disk.\nThe plugin offers two alternatives for doing this:\n\nby defining a global workspace template for each Disk Pool\n\nThis can be defined in the Jenkins global config, External Workspace Definitions section.\n\nby defining a custom workspace path in the Pipeline script\n\nWe can use the Pipeline DSL to compute the workspace path.\nThen we pass this path as input parameter to the exwsAllocate step.\n\ndef customPath = \"${env.JOB_NAME}/${PULL_REQUEST_NUMBER}/${env.BUILD_NUMBER}\"\ndef extWorkspace = exwsAllocate diskPoolId: 'diskpool1', path: customPath\n// ...\n\nFor more details see the afferent\ndocumentation page.\n\nDisk Pool restrictions\n\nThe plugin comes with Disk Pool restriction strategies.\nIt does this by using the restriction capabilities provided by the\nJob Restrictions Plugin.\n\nFor example, we can restrict a Disk Pool to be allocated only if the Jenkins job in which it’s allocated was triggered\nby a specific user:\n\nOr, we can restrict the Disk Pool to be allocated only for those jobs whose name matches a well defined pattern:\n\nWhat’s next\n\nCurrently there is ongoing work for providing flexible disk allocation strategies.\nThe user will be able to define a default disk allocation strategy in the Jenkins global config.\nSo for example, we want to select the disk with the most usable space as default allocation strategy:\n\nIf needed, this allocation strategy may be overridden in the Pipeline code.\nLet’s suppose that for a specific job, we want to allocate the disk with the highest read speed.\n\ndef extWorkspace = exwsAllocate diskPoolId: 'diskpool1', strategy: fastestRead()\n// ...\n\nWhen this feature is completed, the plugin will enter a final testing phase.\nIf all goes to plan, a stable version should be released in about two weeks.\n\nIf you have any issues in setting up or using the plugin, please feel free to ask me on the plugin’s Gitter\nchat.\nAny feedback is welcome, and you may provide it either on the Gitter chat, or on\nJira by using the external-workspace-manager-plugin component.\n\nLinks\n\nProject repository\n\nProject intro blog post\n\nAlpha version announcement\n\nGSoC page\n\nJenkins GSoC Page","title":"GSoC: External Workspace Manager for Pipeline. Beta release is available","tags":["pipeline","plugins","external-workspace-manager","gsoc"],"authors":[{"avatar":null,"blog":null,"github":"alexsomai","html":"","id":"alexsomai","irc":null,"linkedin":null,"name":"Alexandru Somai","slug":"/blog/authors/alexsomai","twitter":"alex_somai"}]}},{"node":{"date":"2016-08-08T00:00:00.000Z","id":"649a2e8e-4f2f-56eb-99f1-8897de882a49","slug":"/blog/2016/08/08/docker-pipeline-environments/","strippedHtml":"This is a guest post by Michael Neale, long time open\nsource developer and contributor to the Blue Ocean\nproject.\n\nIf you are running parts of your pipeline on Linux, possibly the easiest way to\nget a clean reusable environment is to use:\nCloudBees\nDocker Pipeline plugin.\n\nIn this short post I wanted to show how you can avoid installing stuff on the agents, and have per project, or even per branch, customized build environments.\nYour environment, as well as your pipeline is defined and versioned alongside your code.\n\nI wanted to use the Blue Ocean project as an\nexample of a\nproject that uses the CloudBees Docker Pipeline plugin.\n\nEnvironment and Pipeline for JavaScript components\n\nThe Blue Ocean project has a few moving parts, one of\nwhich is called the \"Jenkins Design Language\".  This is a grab bag of re-usable\nCSS, HTML, style rules, icons and JavaScript components (using React.js) that\nprovide the look and feel for Blue Ocean.\n\nJavaScript and Web Development being what it is in 2016, many utilities are\nneed to assemble a web app.  This includes npm and all that it needs, less.js\nto convert Less to CSS, Babel to \"transpile\" versions of JavaScript to other\ntypes of JavaScript (don’t ask) and more.\n\nWe could spend time installling nodejs/npm on the agents, but why not just use\nthe official off the shelf docker image\nfrom Docker Hub?\n\nThe only thing that has to be installed and run on the build agents is the Jenkins agent, and a docker daemon.\n\nA simple pipeline using this approach would be:\n\nnode {\n        stage \"Prepare environment\"\n          checkout scm\n          docker.image('node').inside {\n            stage \"Checkout and build deps\"\n                sh \"npm install\"\n\n            stage \"Test and validate\"\n                sh \"npm install gulp-cli && ./node_modules/.bin/gulp\"\n          }\n}\n\nThis uses the stock \"official\" Node.js image from the Docker Hub, but doesn’t let us customize much about the environment.\n\nCustomising the environment, without installing bits on the agent\n\nBeing the forward looking and lazy person that I am, I didn’t want to have to\ngo and fish around for a Docker image every time a developer wanted something\nspecial installed.\n\nInstead, I put a Dockerfile in the root of the repo, alongside the Jenkinsfile :\n\nThe contents of the Dockerfile can then define the exact environment needed\nto build the project.  Sure enough, shortly after this, someone came along\nsaying they wanted to use Flow from Facebook (A\ntypechecker for JavaScript).  This required an additional native component to\nwork (via apt-get install).\n\nThis was achieved via a\npull\nrequest to both the Jenkinsfile and the Dockerfile at the same time.\n\nSo now our environment is defined by a Dockerfile with the following contents:\n\n# Lets not just use any old version but pick one\nFROM node:5.11.1\n\n# This is needed for flow, and the weirdos that built it in ocaml:\nRUN apt-get update && apt-get install -y libelf1\n\nRUN useradd jenkins --shell /bin/bash --create-home\nUSER jenkins\n\nThe Jenkinsfile pipeline now has the following contents:\n\nnode {\n    stage \"Prepare environment\"\n        checkout scm\n        def environment  = docker.build 'cloudbees-node'\n\n        environment.inside {\n            stage \"Checkout and build deps\"\n                sh \"npm install\"\n\n            stage \"Validate types\"\n                sh \"./node_modules/.bin/flow\"\n\n            stage \"Test and validate\"\n                sh \"npm install gulp-cli && ./node_modules/.bin/gulp\"\n                junit 'reports/**/*.xml'\n        }\n\n    stage \"Cleanup\"\n        deleteDir()\n}\n\nEven hip JavaScript tools can emit that weird XML format that test\nreporters can use, e.g. the junit result archiver.\n\nThe main change is that we have docker.build being called to produce the\nenvironment which is then used.  Running docker build is essentially a\n\"no-op\" if the image has already been built on the agent before.\n\nWhat’s it like to drive?\n\nWell, using Blue Ocean, to build Blue Ocean, yields a pipeline that visually\nlooks like this (a recent run I screen capped):\n\nThis creates a pipeline that developers can tweak on a pull-request basis,\nalong with any changes to the environment needed to support it, without having\nto install any packages on the agent.\n\nWhy not use docker commands directly?\n\nYou could of course just use shell commands to do things with Docker directly,\nhowever, Jenkins Pipeline keeps track of Docker images used in a Dockerfile\nvia the \"Docker Fingerprints\" link (which is good, should that image need to\nchange due to a security patch).\n\nLinks\n\nThe project used as as an example is here\n\nThe pipeline is defined by the Jenkinsfile\n\nThe environment is defined by the Dockerfile\n\nRead more on Docker Pipeline","title":"Don't install software, define your environment with Docker and Pipeline","tags":["pipeline","plugins","blueocean","ux","javascript","nodejs"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/19e71/michaelneale.jpg","srcSet":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/77b35/michaelneale.jpg 32w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/d4a57/michaelneale.jpg 64w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/19e71/michaelneale.jpg 128w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/ef6ff/michaelneale.webp 32w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/8257c/michaelneale.webp 64w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/6766a/michaelneale.webp 128w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"michaelneale","html":"<div class=\"paragraph\">\n<p>Michael is a CD enthusiast with a interest in User Experience.\nHe is a co-founder of CloudBees and a long time OSS developer, and can often be found\nlurking around the jenkins-dev mailing list or #jenkins on irc (same nick as twitter name).\nBefore CloudBees he worked at Red Hat.</p>\n</div>","id":"michaelneale","irc":null,"linkedin":null,"name":"Michael Neale","slug":"/blog/authors/michaelneale","twitter":"michaelneale"}]}},{"node":{"date":"2016-07-19T00:00:00.000Z","id":"ce2c6f83-1ed8-521b-a608-5f3680ad5166","slug":"/blog/2016/07/19/blue-ocean-update/","strippedHtml":"The team have been hard at work moving the needle forward on the Blue\nOcean 1.0 features. Many of the features we have been working on have\ncome a long way in the past few months but here’s a few highlights:\n\nGoodbye page refreshes, Hello Real Time updates!\n\nBuilding upon\nTom 's great work on\nServer Sent Events (SSE) both\nCliff and\nTom worked\non making the all the screens in Blue Ocean update without manual\nrefreshes.\n\nSSE is a great technology\nchoice for new web apps as it only pushes out\nevents to the client when things have changed on the server. That means\nthere’s a lot less traffic going between your browser and the Jenkins\nserver when compared to the continuous AJAX polling method that has been\ntypical of Jenkins in the past.\n\nNew Test Reporting UI\n\nKeith has\nbeen working with Vivek to\ndrive out a new set of extension points that allow us to build a new\nrest reporting UI in Blue Ocean. Today this works for JUnit test reports\nbut can be easily extended to work with other kinds of reports.\n\nPipeline logs are split into steps and update live\n\nThorsten and\nJosh have\nbeen hard at work breaking down the log into steps and making the live\nlog tailing follow the pipeline execution - which we’ve lovingly\nnicknamed the “karaoke mode”\n\nPipelines can be triggered from the UI\n\nTom has\nbeen on allowing users to trigger jobs from Blue Ocean, which is one\nless reason to go back to the Classic UI :)\n\nBlue Ocean has been released to the experimental update center\n\nMany of you have asked us questions about how you can try Blue Ocean\ntoday and have resorted to building the plugin yourself or running our\nDocker image.\n\nWe wanted to make the process of trying Blue Ocean in its unfinished\nstate by publishing the plugin to the experimental update center - it’s\navailable today!\n\nSo what is the Experimental Update Center? It is a mechanism for the\nJenkins developer community to share early previews of new plugins with\nthe broader user community. Plugins in this update center are\nexperimental and we strongly advise not running them on production or\nJenkins systems that you rely on for your work.\n\nThat means any plugin in this update center could eat your Jenkins data,\ncause slowdowns, degrade security or have their behavior change at no\nnotice.\n\nYou can learn how to\nactivate\nthe experimental update center on this post.\n\nStay tuned for more updates!","title":"Blue Ocean July development update ","tags":["blueocean","ux","pipeline"],"authors":[{"avatar":null,"blog":null,"github":"i386","html":"","id":"i386","irc":null,"linkedin":null,"name":"James Dumay","slug":"/blog/authors/i386","twitter":"i386"}]}},{"node":{"date":"2016-07-18T00:00:00.000Z","id":"523da562-6777-53bd-978f-45ab4cb9092c","slug":"/blog/2016/07/18/pipeline-notifications/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nRather than sitting and watching Jenkins for job status, I want Jenkins to send\nnotifications when events occur.  There are Jenkins plugins for\nSlack,\nHipChat,\nor even email\namong others.\n\nNote: Something is happening!\n\nI think we can all agree getting notified when events occur is preferable to\nhaving to constantly monitor them just in case.  I’m going to continue from\nwhere I left off in my\nprevious post with the\nhermann project.  I added a Jenkins\nPipeline with an HTML publisher for code coverage. This week, I’d like to make\nJenkins to notify me when builds start and when they succeed or fail.\n\nSetup and Configuration\n\nFirst, I select targets for my notifications. For this blog post, I’ll use sample\ntargets that I control.  I’ve created Slack and HipChat organizations called\n\"bitwiseman\", each with one member - me.  And for email I’m running a Ruby SMTP server called\nmailcatcher, that is perfect for local testing\nsuch as this.  Aside for these concessions, configuration would be much the\nsame in a non-demo situation.\n\nNext, I install and add server-wide configuration for the\nSlack,\nHipChat,\nand Email-ext\nplugins.  Slack and HipChat use API tokens - both products have integration\npoints on their side that generate tokens which I copy into my Jenkins\nconfiguration. Mailcatcher SMTP runs locally. I just point Jenkins\nat it.\n\nHere’s what the Jenkins configuration section for each of these looks like:\n\nOriginal Pipeline\n\nNow I can start adding notification steps. The same as\nlast week, I’ll use the\nJenkins Pipeline Snippet Generator\nto explore the step syntax for the notification plugins.\n\nHere’s the base pipeline before I start making changes:\n\nstage 'Build'\n\nnode {\n  // Checkout\n  checkout scm\n\n  // install required bundles\n  sh 'bundle install'\n\n  // build and run tests with coverage\n  sh 'bundle exec rake build spec'\n\n  // Archive the built artifacts\n  archive (includes: 'pkg/*.gem')\n\n  // publish html\n  // snippet generator doesn't include \"target:\"\n  // https://issues.jenkins.io/browse/JENKINS-29711.\n  publishHTML (target: [\n      allowMissing: false,\n      alwaysLinkToLastBuild: false,\n      keepAll: true,\n      reportDir: 'coverage',\n      reportFiles: 'index.html',\n      reportName: \"RCov Report\"\n    ])\n}\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit 'https://github.com/reiseburo/hermann.git'.\n\nJob Started Notification\n\nFor the first change, I decide to add a \"Job Started\" notification.  The\nsnippet generator and then reformatting makes this straightforward:\n\nnode {\n\n  notifyStarted()\n\n  /* ... existing build steps ... */\n}\n\ndef notifyStarted() {\n  // send to Slack\n  slackSend (color: '#FFFF00', message: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\")\n\n  // send to HipChat\n  hipchatSend (color: 'YELLOW', notify: true,\n      message: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\"\n    )\n\n  // send to email\n  emailext (\n      subject: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\",\n      body: \"\"\" STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':\nCheck console output at \" ${env.JOB_NAME} [${env.BUILD_NUMBER}]\"\"\"\",\n      recipientProviders: [[$class: 'DevelopersRecipientProvider']]\n    )\n}\n\nSince Pipeline is a Groovy-based DSL, I can use\nstring interpolation\nand variables to add exactly the details I want in my notification messages. When\nI run this I get the following notifications:\n\nJob Successful Notification\n\nThe next logical choice is to get notifications when a job succeeds.  I’ll\ncopy and paste based on the notifyStarted method for now and do some refactoring\nlater.\n\nnode {\n\n  notifyStarted()\n\n  /* ... existing build steps ... */\n\n  notifySuccessful()\n}\n\ndef notifyStarted() { /* .. */ }\n\ndef notifySuccessful() {\n  slackSend (color: '#00FF00', message: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\")\n\n  hipchatSend (color: 'GREEN', notify: true,\n      message: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\"\n    )\n\n  emailext (\n      subject: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\",\n      body: \"\"\" SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':\nCheck console output at \" ${env.JOB_NAME} [${env.BUILD_NUMBER}]\"\"\"\",\n      recipientProviders: [[$class: 'DevelopersRecipientProvider']]\n    )\n}\n\nAgain, I get notifications, as expected.  This build is fast enough,\nsome of them are even on the screen at the same time:\n\nJob Failed Notification\n\nNext I want to add failure notification.  Here’s where we really start to see the power\nand expressiveness of Jenkins pipeline.  A Pipeline is a Groovy script, so as we’d\nexpect in any Groovy script, we can handle errors using try-catch blocks.\n\nnode {\n  try {\n    notifyStarted()\n\n    /* ... existing build steps ... */\n\n    notifySuccessful()\n  } catch (e) {\n    currentBuild.result = \"FAILED\"\n    notifyFailed()\n    throw e\n  }\n}\n\ndef notifyStarted() { /* .. */ }\n\ndef notifySuccessful() { /* .. */ }\n\ndef notifyFailed() {\n  slackSend (color: '#FF0000', message: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\")\n\n  hipchatSend (color: 'RED', notify: true,\n      message: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\"\n    )\n\n  emailext (\n      subject: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\",\n      body: \"\"\" FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':\nCheck console output at \" ${env.JOB_NAME} [${env.BUILD_NUMBER}]\"\"\"\",\n      recipientProviders: [[$class: 'DevelopersRecipientProvider']]\n    )\n}\n\nCode Cleanup\n\nLastly, now that I have it all working, I’ll do some refactoring. I’ll unify\nall the notifications in one method and move the final success/failure notification\ninto a finally block.\n\nstage 'Build'\n\nnode {\n  try {\n    notifyBuild('STARTED')\n\n    /* ... existing build steps ... */\n\n  } catch (e) {\n    // If there was an exception thrown, the build failed\n    currentBuild.result = \"FAILED\"\n    throw e\n  } finally {\n    // Success or failure, always send notifications\n    notifyBuild(currentBuild.result)\n  }\n}\n\ndef notifyBuild(String buildStatus = 'STARTED') {\n  // build status of null means successful\n  buildStatus = buildStatus ?: 'SUCCESS'\n\n  // Default values\n  def colorName = 'RED'\n  def colorCode = '#FF0000'\n  def subject = \"${buildStatus}: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\"\n  def summary = \"${subject} (${env.BUILD_URL})\"\n  def details = \"\"\" STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':\nCheck console output at \" ${env.JOB_NAME} [${env.BUILD_NUMBER}]\"\"\"\"\n\n  // Override default values based on build status\n  if (buildStatus == 'STARTED') {\n    color = 'YELLOW'\n    colorCode = '#FFFF00'\n  } else if (buildStatus == 'SUCCESS') {\n    color = 'GREEN'\n    colorCode = '#00FF00'\n  } else {\n    color = 'RED'\n    colorCode = '#FF0000'\n  }\n\n  // Send notifications\n  slackSend (color: colorCode, message: summary)\n\n  hipchatSend (color: color, notify: true, message: summary)\n\n  emailext (\n      subject: subject,\n      body: details,\n      recipientProviders: [[$class: 'DevelopersRecipientProvider']]\n    )\n}\n\nYou have been notified!\n\nI now get notified twice per build on three different channels.  I’m not sure I\nneed to get notified this much for such a short build.  However, for a longer\nor complex CD pipeline, I might want exactly that.  If needed, I could even\nimprove this to handle other status strings and call it as needed throughout\nmy pipeline.\n\nLinks\n\nSlack Plugin\n\nHipChat Plugin\n\nEmail-ext Plugin\n\nJenkins Pipeline Snippet Generator","title":"Sending Notifications in Pipeline","tags":["tutorial","pipeline","plugins","notifications","slack","hipchat","emailext"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2016-07-01T00:00:00.000Z","id":"4848db1b-feac-54a0-8b3e-2a0f5e3fbfc6","slug":"/blog/2016/07/01/html-publisher-plugin/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nMost projects need more that just JUnit result reporting.  Rather than writing a\ncustom plugin for each type of report, we can use the\nHTML Publisher Plugin.\n\nLet’s Make This Quick\n\nI’ve found a Ruby project,\nhermann, I’d like to build using Jenkins Pipeline. I’d\nalso like to have the code coverage results published with each build job.  I could\nwrite a plugin to publish this data, but I’m in a bit of hurry and\nthe build already creates an HTML report file using SimpleCov\nwhen the unit tests run.\n\nSimple Build\n\nI’m going to use the\nHTML Publisher Plugin\nto add the HTML-formatted code coverage report to my builds.  Here’s a simple\npipeline for building the hermann\nproject.\n\nstage 'Build'\n\nnode {\n  // Checkout\n  checkout scm\n\n  // install required bundles\n  sh 'bundle install'\n\n  // build and run tests with coverage\n  sh 'bundle exec rake build spec'\n\n  // Archive the built artifacts\n  archive (includes: 'pkg/*.gem')\n}\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit 'https://github.com/reiseburo/hermann.git'.\n\nSimple enough, it builds, runs tests, and archives the package.\n\nNow I just need to add the step to publish the code coverage report.\nI know that rake spec creates an index.html file in the coverage directory.\nI’ve already installed the\nHTML Publisher Plugin.\nHow do I add the HTML publishing step to the pipeline?  The plugin page doesn’t\nsay anything about it.\n\nSnippet Generator to the Rescue\n\nDocumentation is hard to maintain and easy to miss, even more so in a system\nlike Jenkins with hundreds of plugins the each potential have one or more\ngroovy fixtures to add to the Pipeline.  The Pipeline Syntax\"Snippet Generator\" helps users\nnavigate this jungle by providing a way to generate a code snippet for any step using\nprovided inputs.\n\nIt offers a dynamically generated list of steps, based on the installed plugins.\nFrom that list I select the publishHTML step:\n\nThen it shows me a UI similar to the one used in job configuration.  I fill in\nthe fields, click \"generate\", and it shows me snippet of groovy generated from\nthat input.\n\nHTML Published\n\nI can use that snippet directly or as a template for further customization.\nIn this case, I’ll just reformat and copy it in at the end of my\npipeline.  (I ran into a minor bug\nin the snippet generated for this plugin step. Typing\nerror string in my search bar immediately found the bug and a workaround.)\n\n/* ...unchanged... */\n\n  // Archive the built artifacts\n  archive (includes: 'pkg/*.gem')\n\n  // publish html\n  // snippet generator doesn't include \"target:\"\n  // https://issues.jenkins.io/browse/JENKINS-29711.\n  publishHTML (target: [\n      allowMissing: false,\n      alwaysLinkToLastBuild: false,\n      keepAll: true,\n      reportDir: 'coverage',\n      reportFiles: 'index.html',\n      reportName: \"RCov Report\"\n    ])\n\n}\n\nWhen I run this new pipeline I am rewarded with an RCov Report link on left side,\nwhich I can follow to show the HTML report.\n\nI even added the keepAll setting to let I can also go back an look at reports on old jobs as\nmore come in.  As I said to to begin with, this is not as slick as what I\ncould do with a custom plugin, but it is much easier and works with any static\nHTML.\n\nLinks\n\nHTML Publisher Plugin\n\nJenkins Pipeline Snippet Generator","title":"Publishing HTML Reports in Pipeline","tags":["tutorial","pipeline","plugins","ruby"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2016-06-30T00:00:00.000Z","id":"6dc0bcbf-e180-50ae-86bf-0a881c810b38","slug":"/blog/2016/06/30/ewm-alpha-version/","strippedHtml":"Currently it’s quite difficult to share and reuse the same workspace between multiple jobs and across nodes.\nThere are some possible workarounds for achieving this, but each of them has its own drawback,\ne.g. stash/unstash pre-made artifacts, Copy Artifacts plugin or advanced job settings.\nA viable solution for this problem is the External Workspace Manager plugin, which facilitates workspace share and\nreuse across multiple Jenkins jobs and nodes.\nIt also eliminates the need to copy, archive or move files.\nYou can learn more about the design and goals of the External Workspace Manager project in\nthis introductory blog post.\n\nI’d like to announce that an alpha version of the External Manager Plugin has been released!\nIt’s now public available for testing.\nTo be able to install this plugin, you must follow the steps from the Experimental Plugins Update Center\nblog post.\n\nPlease be aware that it’s not recommended to use the Experimental Update Center in production installations of\nJenkins, since it may break it.\n\nThe plugin’s wiki page may be accessed\nhere.\nThe documentation that helps you get started with this plugin may be found on the\nREADME page.\nTo get an idea of what this plugin does, which are the features implemented so far and to see a working demo of it,\nyou can watch my mid-term presentation that is available here.\nThe slides for the presentation are shared on\nGoogle Slides.\n\nMy mentors, Martin and Oleg,\nand I have set up public meetings related to this plugin.\nYou are invited to join our discussions if you’d like to get more insight about the project.\nThe meetings are taking place twice a week on the Jenkins hangout,\nevery Monday at\n12 PM UTC\nand every Thursday at\n5 PM UTC.\n\nIf you have any issues in setting up or using the plugin, please feel free to ask me on the plugin’s Gitter\nchat.\nThe plugin is open-source, having the repository on\nGitHub, and you may contribute to it.\nAny feedback is welcome, and you may provide it either on the Gitter chat, or on\nJira by using the external-workspace-manager-plugin component.\n\nLinks\n\nProject repository\n\nPlugin wiki page\n\nMid-term presentation\n\nProject intro blog post\n\nGSoC page\n\nJenkins GSoC Page","title":"GSoC: External Workspace Manager Plugin alpha version","tags":["pipeline","plugins","external-workspace-manager","gsoc"],"authors":[{"avatar":null,"blog":null,"github":"alexsomai","html":"","id":"alexsomai","irc":null,"linkedin":null,"name":"Alexandru Somai","slug":"/blog/authors/alexsomai","twitter":"alex_somai"}]}},{"node":{"date":"2016-06-29T00:00:00.000Z","id":"72dc398a-0c3a-5b36-949e-7687e6ecbefc","slug":"/blog/2016/06/29/from-freestyle-to-pipeline/","strippedHtml":"This is a guest post by R. Tyler Croy, who is a\nlong-time contributor to Jenkins and the primary contact for Jenkins project\ninfrastructure. He is also a Jenkins Evangelist at\nCloudBees, Inc.\n\nFor ages I have used the \"Build After\" feature in Jenkins to cobble together\nwhat one might refer to as a \"pipeline\" of sorts. The Jenkins project itself, a\nmajor consumer of Jenkins, has used these daisy-chained Freestyle jobs to drive\na myriad of delivery pipelines in our infrastructure.\n\nOne such \"pipeline\" helped drive the complex process of generating the pretty\nblue charts on\nstats.jenkins.io.\nThis statistics generation process primarily performs two major tasks, on rather\nlarge sets of data:\n\nGenerate aggregate monthly \"census data.\"\n\nProcess the census data and create trend charts\n\nThe chained jobs allowed us to resume the independent stages of the pipeline,\nand allowed us to run different stages on different hardware (different\ncapabilities) as needed. Below is a diagram of what this looked like:\n\nThe infra_generate_monthly_json would run periodically creating the\naggregated census data, which would then be picked up by infra_census_push\nwhose sole responsibility was to take census data and publish it to the\nnecessary hosts inside the project’s infrastructure.\n\nThe second, semi-independent, \"pipeline\" would also run periodically. The\ninfra_statistics job’s responsibility was to use the census data, pushed\nearlier by infra_census_push, to generate the myriad of pretty blue charts\nbefore triggering the\ninfra_checkout_stats job which would make sure stats.jenkins.io was\nproperly updated.\n\nSuffice it to say, this \"pipeline\" had grown organically over a period time when\nmore advanced tools weren’t quite available.\n\nWhen we migrated to newer infrastructure for\nci.jenkins.io earlier this year I took the\nopportunity to do some cleaning up. Instead of migrating jobs verbatim, I pruned\nstale jobs and refactored a number of others into proper\nPipelines, statistics generation being an obvious\ntarget!\n\nOur requirements for statistics generation, in their most basic form, are:\n\nEnable a sequence of dependent tasks to be executed as a logical group (a\npipeline)\n\nEnable executing those dependent tasks on various pieces of infrastructure\nwhich support different requirements\n\nActually generate those pretty blue charts\n\nIf you wish to skip ahead, you can jump straight to the\nJenkinsfile\nwhich implements our new Pipeline.\n\nThe first iteration of the Jenkinsfile simply defined the conceptual stages we\nwould need:\n\nnode {\n    stage 'Sync raw data and census files'\n\n    stage 'Process raw logs'\n\n    stage 'Generate census data'\n\n    stage 'Generate stats'\n\n    stage 'Publish census'\n\n    stage 'Publish stats'\n}\n\nHow exciting! Although not terrifically useful. When I began actually\nimplementing the first couple stages, I noticed that the Pipeline might sync\ndozens of gigabytes of data every time it ran on a new agent in the cluster.\nWhile this problem will soon be solved by the\nExternal\nWorkspace Manager plugin, which is currently being developed. Until it’s ready,\nI chose to mitigate the issue by pinning the execution to a consistent agent.\n\n/* `census` is a node label for a single machine, ideally, which will be\n * consistently used for processing usage statistics and generating census data\n */\nnode('census && docker') {\n    /* .. */\n}\n\nRestricting a workload which previously used multiple agents to a single one\nintroduced the next challenge. As an infrastructure administrator, technically\nspeaking, I could just install all the system dependencies that I want on this\none special Jenkins agent. But what kind of example would that be setting!\n\nThe statistics generation process requires:\n\nJDK8\n\nGroovy\n\nA running MongoDB instance\n\nFortunately, with Pipeline we have a couple of useful features at our disposal:\ntool auto-installers and the\nCloudBees\nDocker Pipeline plugin.\n\nTool Auto-Installers\n\nTool Auto-Installers are exposed in Pipeline through the tool step and on\nci.jenkins.io we already had JDK8 and Groovy\navailable. This meant that the Jenkinsfile would invoke tool and Pipeline\nwould automatically install the desired tool on the agent executing the current\nPipeline steps.\n\nThe tool step does not modify the PATH environment variable, so it’s usually\nused in conjunction with the withEnv step, for example:\n\nnode('census && docker') {\n    /* .. */\n\n    def javaHome = tool(name: 'jdk8')\n    def groovyHome = tool(name: 'groovy')\n\n    /* Set up environment variables for re-using our auto-installed tools */\n    def customEnv = [\n        \"PATH+JDK=${javaHome}/bin\",\n        \"PATH+GROOVY=${groovyHome}/bin\",\n        \"JAVA_HOME=${javaHome}\",\n    ]\n\n    /* use our auto-installed tools */\n    withEnv(customEnv) {\n        sh 'java --version'\n    }\n\n    /* .. */\n}\n\nCloudBees Docker Pipeline plugin\n\nSatisfying the MongoDB dependency would still be tricky. If I caved in and installed\nMongoDB on a single unicorn agent in the cluster, what could I say the next time\nsomebody asked for a special, one-off, piece of software installed on our\nJenkins build agents?\n\nAfter doing my usual complaining and whining, I discovered that the CloudBees\nDocker Pipeline plugin provides the ability to run containers inside of a\nJenkinsfile. To make things even better, there are\nofficial MongoDB docker images readily\navailable on DockerHub!\n\nThis feature requires that the machine has a running Docker daemon which is\naccessible to the user running the Jenkins agent. After that, running a\ncontainer in the background is easy, for example:\n\nnode('census && docker') {\n    /* .. */\n\n    /* Run MongoDB in the background, mapping its port 27017 to our host's port\n     * 27017 so our script can talk to it, then execute our Groovy script with\n     * tools from our `customEnv`\n     */\n    docker.image('mongo:2').withRun('-p 27017:27017') { container ->\n        withEnv(customEnv) {\n            sh \"groovy parseUsage.groovy --logs ${usagestats_dir} --output ${census_dir} --incremental\"\n        }\n    }\n\n    /* .. */\n}\n\nThe beauty, to me, of this example is that you can pass a\nclosure to withRun which will\nexecute while the container is running. When the closure is finished executin,\njust the sh step in this case, the container is destroyed.\n\nWith that system requirement satisfied, the rest of the stages of the Pipeline\nfell into place. We now have a single source of truth, the\nJenkinsfile,\nfor the sequence of dependent tasks which need to be executed, accounting for\nvariations in systems requirements, and it actually generates\nthose pretty\nblue charts!\n\nOf course, a nice added bonus is the beautiful visualization of our\nnew Pipeline!\n\nLinks\n\nPipeline documentation\n\nCloudBees Docker Pipeline plugin documentation\n\nLive statistics Pipeline","title":"Migrating from chained Freestyle jobs to Pipelines","tags":["pipeline","infra"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}}]}},"pageContext":{"tag":"pipeline","limit":8,"skip":64,"numPages":13,"currentPage":9}},
    "staticQueryHashes": ["3649515864"]}