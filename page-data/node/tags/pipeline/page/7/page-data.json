{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/pipeline/page/7",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2017-02-15T00:00:00.000Z","id":"65c5d55a-edbc-5f25-8301-7393341de88e","slug":"/blog/2017/02/15/pipeline-editor-preview/","strippedHtml":"Back in September 2016 we announced the availability of the Blue Ocean beta\nand the forthcoming Visual Pipeline Editor. We are happy to announce that you can try\nthe Pipeline Editor preview release today.\n\nWhat is it?\n\nThe Visual Pipeline Editor is the simplest way for anyone wanting to get started with\ncreating Pipelines in Jenkins. It’s also a great way for advanced Jenkins users\nto start adopting pipeline. It allows developers to break up their pipeline into different\n stages and parallelize tasks that can occur at the same time - graphically.\n The rest is up to you.\n\nA pipeline you create visually will produce a Declarative Pipeline Jenkinsfile for you and\n the Jenkinsfile is stored within a Git repository where it is versioned with your application code.\n\nIf you are not sure what a Jenkins Pipeline or a Jenkinsfile is, why not check out the new guided tour to learn more about it?\n\nWhat are we doing next?\n\nWe are working hard to provide feature parity between the Declarative Pipeline syntax and the visual editor. The next phase is to integrate the editor into Blue Ocean so that you don’t have to leave the UI and commit the Jenkinsfile to your repository to complete authoring your pipeline.\n\nIn Blue Ocean, you will be able to edit a Jenkinsfile\nfor a branch directly from within the user interface using the Visual Pipeline Editor. When you are done authoring your pipeline, the pipeline definition will be saved back to your repository as a Jenkinsfile. You can edit the Pipeline again using the Visual Editor or from your favorite text editor.\n\nWe are hoping to deliver this level of integration into Blue Ocean and the\nVisual Pipeline Editor over the next few months, so be sure to check regularly for updates in\nthe Jenkins plugin manager.\n\nGet the Preview\n\nThe Visual Pipeline Editor is available in preview today.\n\nTo try it out today:\n\nInstall the Blue Ocean beta and Blue Ocean Pipeline Editor from the Jenkins plugin manager\n\nClick on the Open Blue Ocean button and then the Pipeline Editor in the main navigation\n\nWe are looking forward to your feedback to help make the Visual Pipeline Editor\nthe easiest way to get started with Jenkins Pipeline. To report bugs or to\nrequest features please follow the instructions on the project page.\n\nAnd don’t forget to join us on our Gitter community chat\n- drop by and say hello!","title":"Say Hello to the Blue Ocean Pipeline Editor","tags":["blueocean","editor","declarative","pipeline"],"authors":[]}},{"node":{"date":"2017-02-10T00:00:00.000Z","id":"06a04f0b-7823-5a11-8c3a-d385a336b68c","slug":"/blog/2017/02/10/declarative-html-publisher/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nDeclare Your Pipelines!\nDeclarative Pipeline 1.0 is here!\nThis is the second post in a series showing some of the cool features of\nDeclarative Pipeline.\n\nIn the\nprevious blog post,\nwe created a simple Declarative Pipeline.\nIn this blog post, we’ll go back and look at the Scripted Pipeline for the\nPublishing HTML Reports in Pipeline blog post.\nWe’ll convert that Pipeline to Declarative syntax (including properties), go\ninto more detail on the post section, and then we’ll use the agent\ndirective to switch our Pipeline to run in Docker.\n\nSetup\n\nFor this post, I’m going to use the\nblog/add-declarative/html\nbranch of\nmy fork of the\nhermann project.\nI’ve set up a Multibranch Pipeline and pointed it at my repository\nthe same as did it previous post.\nAlso the same as before, I’ve set this Pipeline’s Git configuration to\nautomatically \"Clean after checkout\".\n\nThis time we already have a Pipeline checked in.\nI’ll run it a few times to get a baseline.\n\nConverting to Declarative\n\nLet’s start by converting the Scripted Pipeline straight to Declarative.\n\n// Declarative //\npipeline {\n  agent any // <1> (2)\noptions {\n    // Keep the 10 most recent builds\n    buildDiscarder(logRotator(numToKeepStr:'10')) (3)\n}\n  stages {\n    stage ('Build') { (4)\nsteps {\n        // install required gems\n        sh 'bundle install'\n\n        // build and run tests with coverage\n        sh 'bundle exec rake build spec'\n\n        // Archive the built artifacts\n        archive includes: 'pkg/*.gem'\n\n        // publish html\n        publishHTML target: [\n            allowMissing: false,\n            alwaysLinkToLastBuild: false,\n            keepAll: true,\n            reportDir: 'coverage',\n            reportFiles: 'index.html',\n            reportName: 'RCov Report'\n          ]\n      }\n    }\n  }\n}\n// Scripted //\nproperties([[$class: 'BuildDiscarderProperty',\n                strategy: [$class: 'LogRotator', numToKeepStr: '10']]]) (3)\n\nnode { (1)\nstage ('Build') { (4)\n\n// Checkout\n    checkout scm (2)\n\n// install required gems\n    sh 'bundle install'\n\n    // build and run tests with coverage\n    sh 'bundle exec rake build spec'\n\n    // Archive the built artifacts\n    archive includes: 'pkg/*.gem'\n\n    // publish html\n    publishHTML [\n        allowMissing: false,\n        alwaysLinkToLastBuild: false,\n        keepAll: true,\n        reportDir: 'coverage',\n        reportFiles: 'index.html',\n        reportName: 'RCov Report'\n      ]\n\n  }\n}\n\n1\nSelect where to run this Pipeline, in this case \"any\" agent, regardless of label.\n\n2\nDeclarative automatically performs a checkout of source code on the agent,\nwhereas Scripted Pipeline users must explicitly call checkout scm.\n\n3\nSet the Pipeline option to preserve the ten most recent runs.\nThis overrides the default behavior from the Multibranch parent of this Pipeline.\n\n4\nRun the \"Build\" stage.\n\nNow that we have this Pipeline in Declarative form, let’s take a minute to do a\nlittle clean up.  We’ll split out the bundle actions a little more and move\nsteps into logically grouped stages.  Rather than having one monolithic \"Build\"\nstage, we’ll have details for each stage.  As long as we’re prettying things\nup, let’s switch to using Blue Ocean to view our\nbuilds, as well.\n\n// Declarative //\npipeline {\n  agent any\n  options {\n    // Keep the 10 most recent builds\n    buildDiscarder(logRotator(numToKeepStr:'10'))\n  }\n  stages {\n    stage ('Install') {\n      steps {\n        // install required gems\n        sh 'bundle install'\n      }\n    }\n    stage ('Build') {\n      steps {\n        // build\n        sh 'bundle exec rake build'\n\n        // Archive the built artifacts\n        archive includes: 'pkg/*.gem'\n      }\n    }\n    stage ('Test') {\n      steps {\n        // run tests with coverage\n        sh 'bundle exec rake spec'\n\n        // publish html\n        publishHTML target: [\n            allowMissing: false,\n            alwaysLinkToLastBuild: false,\n            keepAll: true,\n            reportDir: 'coverage',\n            reportFiles: 'index.html',\n            reportName: 'RCov Report'\n          ]\n      }\n    }\n  }\n}\n// Scripted //\n\nUsing post sections\n\nThis looks pretty good, but if we think about it\nthe archive and publishHTML steps are really post-stage actions.\nThey should only occur when the rest of their stage succeeds.\nAs our Pipeline gets more complex we might need to add actions that always happen\neven if a stage or the Pipeline as a whole fail.\n\nIn Scripted Pipeline, we would use try-catch-finally,\nbut we cannot do that in Declarative.\nOne of the defining features of the Declarative Pipeline\nis that it does not allow script-based control structures\nsuch as for loops, if-then-else blocks, or try-catch-finally blocks.\nOf course, internally Step implementations can still contain whatever conditional logic they want,\nbut the Declarative Pipeline cannot.\n\nInstead of free-form conditional logic,\nDeclarative Pipeline provides a set of Pipeline-specific controls:\nwhen directives, which we’ll look at in\na later blog post in this series, control whether to execute the steps in a stage,\nand\npost sections\ncontrol which actions to take based on result of a single stage\nor a whole Pipeline. post supports a number of\nrun conditions,\nincluding always (execute no matter what) and changed\n(execute when the result differs from previous run).\nWe’ll use success to run archive and publishHTML when their respective stages complete.\nWe’ll also use an always block with a placeholder for sending notifications,\nwhich I’ll implement in the next blog post.\n\n// Declarative //\npipeline {\n  agent any\n  options {\n    // Only keep the 10 most recent builds\n    buildDiscarder(logRotator(numToKeepStr:'10'))\n  }\n  stages {\n    stage ('Install') {\n      steps {\n        // install required gems\n        sh 'bundle install'\n      }\n    }\n    stage ('Build') {\n      steps {\n        // build\n        sh 'bundle exec rake build'\n      }\n\n      post {\n        success {\n          // Archive the built artifacts\n          archive includes: 'pkg/*.gem'\n        }\n      }\n    }\n    stage ('Test') {\n      steps {\n        // run tests with coverage\n        sh 'bundle exec rake spec'\n      }\n\n      post {\n        success {\n          // publish html\n          publishHTML target: [\n              allowMissing: false,\n              alwaysLinkToLastBuild: false,\n              keepAll: true,\n              reportDir: 'coverage',\n              reportFiles: 'index.html',\n              reportName: 'RCov Report'\n            ]\n        }\n      }\n    }\n  }\n  post {\n    always {\n      echo \"Send notifications for result: ${currentBuild.result}\"\n    }\n  }\n}\n// Scripted //\n\nSwitching agent to run in Docker\n\nagent can actually accept\nseveral other parameters instead of any.\nWe could filter on label \"some-label\", for example,\nwhich would be the equivalent of node ('some-label') in Scripted Pipeline.\nHowever, agent also lets us just as easily switch to using a Docker container,\nwhich replaces a more complicated set of changes in Scripted Pipeline:\n\npipeline {\n  agent {\n    // Use docker container\n    docker {\n      image 'ruby:2.3'\n    }\n  }\n  /* ... unchanged ... */\n}\n\nIf I needed to, I could add a label filter under docker\nto select a node to host the Docker container.\nI already have Docker available on all my agents, so I don’t need label -\nthis works as is.\nAs you can see below, the Docker container spins up at the start of the run\nand the pipeline runs inside it.  Simple!\n\nConclusion\n\nAt first glance, the Declarative Pipeline’s removal of control structures seems\nlike it would be too constrictive.  However, it replaces those structures with\nfacilities like the post section, that give us reasonable control over the\nflow our our Pipeline while still improving readability and maintainability.\nIn the next blog post, we’ll add notifications to this pipeline\nand look at how to use Shared Libraries with Declarative\nPipeline to share code and keep Pipelines easy to understand.\n\nLinks\n\nDeclarative Pipeline plugin\n\nDeclarative Pipeline Syntax Reference\n\nPipeline source for this post","title":"Declarative Pipeline: Publishing HTML Reports","tags":["tutorial","pipeline","declarative","plugins","ruby"],"authors":[]}},{"node":{"date":"2017-02-07T00:00:00.000Z","id":"df12ba62-13a1-5a1b-88f3-afc58f167e79","slug":"/blog/2017/02/07/declarative-maven-project/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nDeclare Your Pipelines!\nDeclarative Pipeline 1.0 is here!\nThis is first in a series of blog posts that will show some of the cool features of\nDeclarative Pipeline.\nFor several of these posts, I’ll be revisiting some of my\nprevious posts\non using various plugins with (Scripted) Pipeline,\nand seeing how those are implemented in Declarative Pipeline.\n\nTo start though, let’s get familiar with the basic structure of a Declarative Pipeline\nby creating a simple Pipeline for a Maven-based Java project - the\nJenkins JUnit plugin.\nWe’ll create a minimal Declarative Pipeline,\nadd the settings needed to install Maven and the JDK,\nand finally we’ll actually run Maven to build the plugin.\n\nSet up\n\nWith Declarative, it is still possible to run Pipelines edited directly in the\nJenkins web UI, but one of the key features of \"Pipeline as Code\" is\nchecking-in and being able to track changes.  For this post, I’m going to use\nthe\nblog/add-declarative-pipeline\nbranch of\nmy fork of the JUnit plugin.\nI’m going to set up a Multi-branch Pipeline and point it at my repository.\n\nI’ve also set this Pipeline’s Git configuration to automatically \"clean after\ncheckout\" and to only keep the ten most recent runs.\n\nWriting a Minimal Pipeline\n\nAs has been said before, Declarative Pipeline provides a more structured,\n\"opinionated\" way to create Pipelines. I’m going to start by creating a minimal\nDeclarative Pipeline and adding it to my branch.  Below is a minimal Pipeline\n(with annotations) that just prints a message:\n\n// Declarative //\npipeline { (1)\nagent any // <2> (3)\nstages { (4)\nstage('Build') { (5)\nsteps { (6)\necho 'This is a minimal pipeline.' (7)\n}\n        }\n    }\n}\n// Scripted //\nnode { (2)\ncheckout scm (3)\nstage ('Build') { (5)\necho 'This is a minimal pipeline.' (6)\n}\n}\n\n1\nAll Declarative Pipelines start with a pipeline section.\n\n2\nSelect where to run this Pipeline, in this case \"any\" agent, regardless of label.\n\n3\nDeclarative automatically performs a checkout of source code on the agent,\nwhereas Scripted Pipeline users must explicitly call checkout scm,\n\n4\nA Declarative Pipeline is defined as a series of stages.\n\n5\nRun the \"Build\" stage.\n\n6\nEach stage in a Declarative Pipeline runs a series of steps.\n\n7\nRun the echo step to print a message in the Console Output.\n\nIf you are familiar with Scripted Pipeline, you can toggle the above\nDeclarative code sample to show the Scripted equivalent.\n\nOnce I add the Pipeline above to my Jenkinsfile and run \"Branch Indexing\", my\nJenkins will pick it up and run run it.  We see that the Declarative Pipeline\nhas added stage called \"Declarative: Checkout SCM\":\n\nThis a \"dynamic stage\", one of several the kinds that Declarative Pipeline adds\nas needed for clearer reporting.  In this case, it is a stage in which the\nDeclarative Pipeline automatically checkouts out source code on the agent.\n\nAs you can see above, we didn’t have to tell it do any of this,\n\nConsole Output\n\n[Pipeline] node\nRunning on osx_mbp in /Users/bitwiseman/jenkins/agents/osx_mbp/workspace/blog_add-declarative-pipeline\n[Pipeline] {\n[Pipeline] stage\n[Pipeline] { (Declarative: Checkout SCM)\n[Pipeline] checkout\nCloning the remote Git repository\n{ ... truncated 20 lines ... }\n[Pipeline] }\n[Pipeline] // stage\n[Pipeline] stage\n[Pipeline] { (Build)\n[Pipeline] echo\nThis is a minimal pipeline\n[Pipeline] }\n[Pipeline] // stage\n[Pipeline] }\n[Pipeline] // node\n[Pipeline] End of Pipeline\nFinished: SUCCESS\n\nDeclarative Pipeline syntax is a little more verbose than the equivalent Scripted Pipeline,\nbut the added detail gives a clearer, more consistent view of what the Pipeline is supposed to do.\nIt also gives us a structure into which we can add more configuration details about this Pipeline.\n\nAdding Tools to Pipeline\n\nThe next thing we’ll add in this Pipeline is a tools section to let us use\nMaven.  The tools section is one of several sections we can add under\npipeline, which affect the configuration of the rest of the Pipeline.  (We’ll\nlook at the others, including agent, in later posts.) Each tool entry will\nmake whatever settings changes, such as updating PATH or other environment\nvariables, to make the named tool available in the current pipeline.  It will\nalso automatically install the named tool if that tool is configured to do so\nunder \"Managing Jenkins\" → \"Global Tool Configuration\".\n\n// Declarative //\npipeline {\n    agent any\n    tools { (1)\nmaven 'Maven 3.3.9' (2)\njdk 'jdk8' (3)\n}\n    stages {\n        stage ('Initialize') {\n            steps {\n                sh '''\n                    echo \"PATH = ${PATH}\"\n                    echo \"M2_HOME = ${M2_HOME}\"\n                ''' (4)\n}\n        }\n\n        stage ('Build') {\n            steps {\n                echo 'This is a minimal pipeline.'\n            }\n        }\n    }\n}\n// Scripted Not Defined //\n\n1\ntools section for adding tool settings.\n\n2\nConfigure this Pipeline to use the Maven version matching \"Maven 3.3.9\"\n(configured in \"Managing Jenkins\" → \"Global Tool Configuration\").\n\n3\nConfigure this Pipeline to use the Maven version matching \"jdk8\"\n(configured in \"Managing Jenkins\" → \"Global Tool Configuration\").\n\n4\nThese will show the values of PATH and M2_HOME environment variables.\n\nWhen we run this updated Pipeline the same way we ran the first, we see that\nthe Declarative Pipeline has added another stage called \"Declarative: Tool\nInstall\": In the console output, we see that during this particular stage \"Maven 3.3.9\" gets installed,\nand the PATH and M2_HOME environment variables are set:\n\nConsole Output\n\n{ ... truncated lines ... }\n[Pipeline] // stage\n[Pipeline] stage\n[Pipeline] { (Declarative: Tool Install)\n[Pipeline] tool\nUnpacking https://repo.maven.apache.org/maven2/org/apache/maven/apache-maven/3.3.9/apache-maven-3.3.9-bin.zip\nto /Users/bitwiseman/jenkins/agents/osx_mbp/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.3.9\non osx_mbp\n[Pipeline] envVarsForTool\n[Pipeline] tool\n[Pipeline] envVarsForTool\n[Pipeline] }\n[Pipeline] // stage\n{ ... }\nPATH = /Library/Java/JavaVirtualMachines/jdk1.8.0_92.jdk/Contents/Home/bin:/Users/bitwiseman/jenkins/agents/osx_mbp/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.3.9/bin:...\nM2_HOME = /Users/bitwiseman/jenkins/agents/osx_mbp/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.3.9\n{ ... }\n\nRunning a Maven Build\n\nFinally, running a Maven build is trivial.  The tools section already added\nMaven and JDK8 to the PATH, all we need to do is call mvn install.  It\nwould be nice if I could split the build and the tests into separate stages,\nbut Maven is famous for not liking when people do that, so I’ll leave it alone\nfor now.\n\nInstead, let’s load up the results of the build using the JUnit plugin,\nhowever the version that was just built, sorry.\n\n// Declarative //\npipeline {\n    agent any\n    tools {\n        maven 'Maven 3.3.9'\n        jdk 'jdk8'\n    }\n    stages {\n        stage ('Initialize') {\n            steps {\n                sh '''\n                    echo \"PATH = ${PATH}\"\n                    echo \"M2_HOME = ${M2_HOME}\"\n                '''\n            }\n        }\n\n        stage ('Build') {\n            steps {\n                sh 'mvn -Dmaven.test.failure.ignore=true install' (1)\n}\n            post {\n                success {\n                    junit 'target/surefire-reports/**/*.xml' (2)\n}\n            }\n        }\n    }\n}\n// Scripted //\nnode {\n    checkout scm\n\n    String jdktool = tool name: \"jdk8\", type: 'hudson.model.JDK'\n    def mvnHome = tool name: 'mvn'\n\n    /* Set JAVA_HOME, and special PATH variables. */\n    List javaEnv = [\n        \"PATH+MVN=${jdktool}/bin:${mvnHome}/bin\",\n        \"M2_HOME=${mvnHome}\",\n        \"JAVA_HOME=${jdktool}\"\n    ]\n\n    withEnv(javaEnv) {\n    stage ('Initialize') {\n        sh '''\n            echo \"PATH = ${PATH}\"\n            echo \"M2_HOME = ${M2_HOME}\"\n        '''\n    }\n    stage ('Build') {\n        try {\n            sh 'mvn -Dmaven.test.failure.ignore=true install'\n        } catch (e) {\n            currentBuild.result = 'FAILURE'\n        }\n    }\n    stage ('Post') {\n        if (currentBuild.result == null || currentBuild.result == 'SUCCESS') {\n            junit 'target/surefire-reports/**/*.xml' (2)\n}\n    }\n}\n\n1\nCall mvn, the version configured by the tools section will be first on the path.\n\n2\nIf the maven build succeeded, archive the JUnit test reports for display in the Jenkins web UI.\nWe’ll discuss the\npost section in detail in the next blog post.\n\nIf you are familiar with Scripted Pipeline, you can toggle the above\nDeclarative code sample to show the Scripted equivalent.\n\nBelow is the console output for this last revision:\n\nConsole Output\n\n{ ... truncated lines ... }\n+ mvn install\n[INFO] Scanning for projects...\n[WARNING] The POM for org.jenkins-ci.tools:maven-hpi-plugin:jar:1.119 is missing, no dependency information available\n[WARNING] Failed to build parent project for org.jenkins-ci.plugins:junit:hpi:1.20-SNAPSHOT\n[INFO]\n[INFO] ------------------------------------------------------------------------\n[INFO] Building JUnit Plugin 1.20-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO]\n[INFO] --- maven-hpi-plugin:1.119:validate (default-validate) @ junit ---\n[INFO]\n[INFO] --- maven-enforcer-plugin:1.3.1:display-info (display-info) @ junit ---\n[INFO] Maven Version: 3.3.9\n[INFO] JDK Version: 1.8.0_92 normalized as: 1.8.0-92\n[INFO] OS Info: Arch: x86_64 Family: mac Name: mac os x Version: 10.12.3\n[INFO]\n{ ... }\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 03:25 min\n[INFO] Finished at: 2017-02-06T22:43:41-08:00\n[INFO] Final Memory: 84M/1265M\n[INFO] ------------------------------------------------------------------------\n\nConclusion\n\nThe new Declarative syntax is a significant step forward for Jenkins Pipeline.\nIt trades some verbosity and constraints for much greater clarity and\nmaintainability.  In the coming weeks, I’ll be adding new blog posts\ndemonstrating various features of the Declarative syntax along with some recent\nJenkins Pipeline improvements.\n\nLinks\n\nDeclarative Pipeline\n\nDeclarative Pipeline Syntax Reference\n\nJenkins JUnit plugin","title":"Declarative Pipeline for Maven Projects","tags":["tutorial","pipeline","declarative","maven","java"],"authors":[]}},{"node":{"date":"2017-02-03T00:00:00.000Z","id":"eba78b86-bbf2-5022-a3c9-27efc09a20e1","slug":"/blog/2017/02/03/declarative-pipeline-ga/","strippedHtml":"This is a guest post by\nPatrick Wolf,\nDirector of Product Management at\nCloudBees\nand contributor to\nthe Jenkins project.\n\nI am very excited to announce the addition of\nDeclarative Pipeline syntax\n1.0 to\nJenkins Pipeline.\nWe think this new syntax will enable everyone involved in DevOps, regardless of expertise,\nto participate in the continuous delivery process. Whether creating, editing or reviewing\na pipeline, having a straightforward structure helps to understand and predict the\nflow of the pipeline and provides a common foundation across all pipelines.\n\nPipeline as Code\n\nPipeline as Code was one of the pillars of the Jenkins 2.0 release and an\nessential part of implementing continuous delivery (CD). Defining all of the\nstages of an application’s CD pipeline within a Jenkinsfile and checking it\ninto the repository with the application code provides all of the benefits\ninherent in source control management (SCM):\n\nRetain history of all changes to Pipeline\n\nRollback to a previous Pipeline version\n\nView diffs and merge changes to the Pipeline\n\nTest new Pipeline steps in branches\n\nRun the same Pipeline on a different Jenkins server\n\nGetting Started with Declarative Pipeline\n\nWe recommend people begin using it for all their Pipeline definitions in Jenkins.\nThe plugin has been available for use and testing starting with the 0.1 release that was debuted at\nJenkins World\nin September. Since then, it has already been installed in over 5,000 Jenkins\nenvironments.\n\nIf you haven’t tried Pipeline or have considered Pipeline in the past, I\nbelieve this new syntax is much more approachable with an easier adoption curve\nto quickly realize all of the benefits of Pipeline as Code. In addition, the\npre-defined structure of Declarative makes it possible to create and edit\nPipelines with a graphical user interface (GUI). The Blue Ocean team is\nactively working on a\nVisual Pipeline Editor\nwhich will be included in an upcoming release.\n\nIf you have already begun using Pipelines in Jenkins, I believe that this new\nalternative syntax can help expand that usage.\n\nThe original syntax for defining Pipelines in Jenkins is a Groovy DSL that\nallows most of the features of full\nimperative programming.\n\nThis syntax is still fully supported and is now\nreferred to as \"Scripted Pipeline Syntax\" to distinguish it from \"Declarative\nPipeline Syntax.\" Both use the same underlying execution engine in Jenkins and\nboth will generate the same results in\nPipeline Stage View\nor Blue Ocean visualizations. All existing\nPipeline steps,\nGlobal Variables, and\nShared Libraries\ncan be used in either. You can now create more cookie-cutter Pipelines and\nextend the power of Pipeline to all users regardless of Groovy expertise.\n\nDeclarative Pipeline Features\n\nSyntax Checking\n\nImmediate runtime syntax checking with explicit error messages.\n\nAPI endpoint for linting a Jenkinsfile.\n\nCLI command to lint a Jenkinsfile.\n\nDocker Pipeline integration\n\nRun all stages in a single container.\n\nRun each stage in a different container.\n\nEasy configuration\n\nQuickly define parameters for your Pipeline.\n\nQuickly define environment variables and credentials for your Pipeline.\n\nQuickly define options (such as timeout, retry, build discarding) for your Pipeline.\n\nRound trip editing with the Visual Pipeline Editor (watch for preview release shortly).\n\nConditional actions\n\nSend notifications or take actions depending upon success or failure.\n\nSkip stages based on branches, environment, or other Boolean expression.\nrelease shortly)\n\nWhere Can I Learn More?\n\nBe on the lookout for future blog posts detailing specific examples of\nscenarios or features in Declarative Pipeline. Andrew Bayer, one of the primary\ndevelopers behind Declarative Pipeline, will be presenting at\nFOSDEM\nin Brussels, Belgium this weekend. We have also scheduled an online\nJenkins Meetup (JAM)\nlater this month to demo the features of Declarative Pipeline and give a sneak\npeek at the upcoming Blue Ocean Pipeline Editor.\n\nIn the meantime, all the\nPipeline documentation\nhas been updated to incorporate a\nGuided Tour,\nand a\nSyntax Reference\nwith numerous examples to help you get on your way to using Pipeline.  Simply\nupgrade to the latest version, 2.5 or later of the Pipeline in Jenkins to\nenable all of these great features.","title":"Declarative Pipeline Syntax 1.0 is now available","tags":["pipeline","blueocean"],"authors":[]}},{"node":{"date":"2017-02-01T00:00:00.000Z","id":"aa509c7b-3f24-5cce-8519-dda84cd1233e","slug":"/blog/2017/02/01/pipeline-scalability-best-practice/","strippedHtml":"This is a guest post by Sam Van Oort,\nSoftware Engineer at CloudBees and contributor to\nthe Jenkins project.\n\nToday I’m going to show you best practices to write scalable and robust Jenkins Pipelines. This is drawn from a\ncombination of work with the internals of Pipeline and observations with large-scale users.\n\nPipeline code works beautifully for its intended role of automating\nbuild/test/deploy/administer tasks.  As it is pressed into more complex roles\nand unanticipated uses, some users hit issues.  In these cases, applying the\nbest practices can make the difference between:\n\nA single controller running\nhundreds\nof concurrent builds on low end hardware (4 CPU cores and 4 GB of\nheap)\n\nRunning a couple dozen builds and bringing a controller to its knees or\ncrashing it…​even with 16+ CPU cores and 20+ GB of heap!\n\nThis has been seen in the wild.\n\nFundamentals\n\nTo understand Pipeline behavior you must understand a few points about\nhow it executes.\n\nExcept for the steps themselves, all of the Pipeline logic, the Groovy conditionals, loops, etc execute on the controller. Whether simple or complex! Even inside a node block!\n\nSteps may use executors to do work where appropriate, but each\nstep has a small on-controller overhead too.\n\nPipeline code is written as Groovy but the execution model is\nradically transformed at compile-time to Continuation Passing Style\n(CPS).\n\nThis transformation provides valuable safety and durability\nguarantees for Pipelines, but it comes with trade-offs:\n\nSteps can invoke Java and execute fast and efficiently, but Groovy\nis much slower to run than normal.\n\nGroovy logic requires far more memory, because an object-based\nsyntax/block tree is kept in memory.\n\nPipelines persist the program and its state frequently to be able to\nsurvive failure of the controller.\n\nFrom these we arrive at a set of best practices to make pipelines more\neffective.\n\nBest Practices For Pipeline Code\n\nThink of Pipeline code as glue: just enough Groovy code to connect\ntogether the Pipeline steps and integrate tools, and no more.\n\nThis makes code easier to maintain, more robust against bugs, and\nreduces load on controllers.\n\nKeep it simple: limit the amount of complex logic embedded in the\nPipeline itself (similarly to a shell script) and avoid treating it as a\ngeneral-purpose programming language.\n\nPipeline restricts all variables to Serializable types, so keeping\nPipeline logic simple helps avoid a NotSerializableException - see\nappendix at the bottom.\n\nUse @NonCPS -annotated functions for slightly more complex work.\nThis means more involved processing, logic, and transformations. This\nlets you leverage additional Groovy & functional features for more\npowerful, concise, and performant code.\n\nThis still runs on controllers so be mindful of complexity, but is much\nfaster than native Pipeline code because it doesn’t provide durability\nand uses a faster execution model. Still, be mindful of the CPU cost and\noffload to executors for complex work (see below).\n\n@NonCPS functions can use a much broader subset of the Groovy\nlanguage, such as iterators and functional features, which makes them\nmore terse and fast to write.\n\n@NonCPS functions should not use Pipeline steps internally, however\nyou can store the result of a Pipeline step to a variable and use it\nthat as the input to a @NonCPS function.\n\nGotcha: It’s not guaranteed that use of a step will generate an\nerror (there is an open RFE to implement that), but you should not rely\non that behavior. You may see improper handling of exceptions, in\nparticular.\n\nWhile normal Pipeline is restricted to serializable local variables\n(see appendix at bottom), @NonCPS functions can use more complex,\nnonserializable types internally (for example regex matchers, etc). Parameters\nand return types should still be Serializable, however.\n\nGotcha: improper usages are not guaranteed to raise an error with\nnormal Pipeline (optimizations may mask the issue), but it is unsafe to\nrely on this behavior.\n\nPrefer external scripts/tools for complex or CPU-expensive\nprocessing rather than Groovy language features. This offloads work\nfrom the controller to external executors, allowing for easy scale-out of\nhardware resources. It is also generally easier to test because these\ncomponents can be tested in isolation without the full on-controller\nexecution environment.\n\nMany software vendors will provide easy command-line clients for\ntheir tools in various programming languages. These are often robust,\nperformant, and easy to use. Plugins offer another option (see below).\n\nShell or batch steps are often the easiest way to integrate these\ntools, which can be written in any language. For example: sh “java -jar\nclient.jar $endPointUrl $inputData” for a Java client, or sh “python\njiraClient.py $issueId $someParam” for a Python client.\n\nGotcha: especially avoid Pipeline XML or JSON parsing using Groovy’s XmlSlurper and JsonSlurper!  Strongly prefer command-line tools or scripts.\n\nThe Groovy implementations are complex and as a result more brittle in Pipeline use.\n\nXmlSlurper and JsonSlurper can carry a high memory and CPU cost in pipelines\n\nxmllint and xmlstartlet are command-line tools offering XML extraction via xpath\n\njq offers the same functionality for JSON\n\nThese extraction tools may be coupled to curl or wget for fetching information from an HTTP API\n\nExamples of other places to use command-line tools:\n\nTemplating large files\n\nNontrivial integration with external APIs (for bigger vendors,\nconsider a Jenkins plugin if a quality offering exists)\n\nSimulations/complex calculations\n\nBusiness logic\n\nConsider existing plugins for external integrations. Jenkins has a\nwealth of plugins, especially for source control, artifact management,\ndeployment systems, and systems automation. These can greatly reduce the\namount of Pipeline code to maintain. Well-written plugins may be\nfaster and more robust than Pipeline equivalents.\n\nConsider both plugins and command-line clients (above) — one may be\neasier than the other.\n\nPlugins may be of widely varying quality. Look at the number of installations and how frequently and recently updates appear in the changelog. Poorly-maintained plugins\nwith limited installations may actually be worse than writing a little\ncustom Pipeline code.\n\nAs a last resort, if there is a good-quality plugin that is not\nPipeline-enabled, it is fairly easy to write a Pipeline wrapper to\nintegrate it or write a custom step that will invoke it.\n\nAssume things will go wrong: don’t rely on workspaces being clean\nof the remnants from previous executions, clean explicitly where needed.\nMake use of timeouts and retry steps (that’s what they’re there for).\n\nWithin a git repository, git clean -fdx is a good way to\naccomplish this and reduces the amount of SCM cloning\n\nDO use parameterized Pipelines and variables to make your Pipeline\nscripts more reusable. Passing in parameters is especially helpful for\nhandling different environments and should be preferred to applying\nconditional lookup logic; however, try to limit parameterized pipelines invoking each other.\n\nTry to limit business logic embedded in Pipelines. To some extent\nthis is inevitable, but try to focus on tasks to complete instead,\nbecause this yields more maintainable, reusable, and often more\nperformant Pipeline code.\n\nOne code smell that points to a problem is many hard-coded\nconstants. Consider taking advantage of the options above to refactor\ncode for better composability.\n\nFor complex cases, consider using Jenkins integration options\n(plugins, Jenkins API calls, invoking input steps externally) to offload\nimplementation of more complex business rules to an external system if\nthey fit more naturally there.\n\nPlease, think of these as guidelines, not strict rules – Jenkins\nPipeline provides a great deal of power and flexibility, and it’s there\nto be used.\n\nBreaking enough of these rules at scale can cause controllers to fail by\nplacing an unsustainable load on them.\n\nFor additional guidance, I also recommend\nthis\nJenkins World talk\non how to engineer Pipelines for speed and performance:\n\nAppendix: Serializable vs. Non-Serializable Types:\n\nTo assist with Pipeline development, here are common serializable and\nnon-serializable types, to assist with deciding if your logic can be CPS\nor should be in a @NonCPS function to avoid issues.\n\nCommon Serializable Types (safe everywhere):\n\nAll primitive types and their object wrappers: byte, boolean, int,\ndouble, short, char\n\nStrings\n\nenums\n\nArrays of serializable types\n\nArrayLists and normal Groovy Lists\n\nSets: HashSet\n\nMaps: normal Groovy Map, HashMap, TreeMap\n\nExceptions\n\nURLs\n\nDates\n\nRegex Patterns (compiled patterns)\n\nCommon non-Serializable Types (only safe in @NonCPS functions):\n\nIterators: this is a common problem. You need to use C-style loop, i.e.\nfor(int i=0; i\n\nRegex Matchers (you can use the\nbuilt-in functions in String, etc, just not the Matcher itself)\n\nImportant: JsonObject, JsonSlurper, etc in Groovy 2+ (used in some 2.x+\nversions of Jenkins).\n\nThis is due to an internal implementation change — earlier versions may serialize.","title":"Best Practices for Scalable Pipeline Code","tags":["pipeline","performance","scalability"],"authors":[]}},{"node":{"date":"2017-01-19T00:00:00.000Z","id":"437a3a39-d6ca-5875-b27d-0189cefc4150","slug":"/blog/2017/01/19/converting-conditional-to-pipeline/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nIntroduction\n\nWith all the new developments in\nJenkins Pipeline (and\nDeclarative Pipeline on the horizon),\nit’s easy to forget what we did to create \"pipelines\" before\nPipeline.\nThere are number of plugins, some that have been around since the very beginning,\nthat enable users to create \"pipelines\" in Jenkins.\nFor example, basic job chaining worked well in many cases, and the\nParameterized Trigger plugin\nmade chaining more flexible.\nHowever, creating chained jobs with conditional behavior was\nstill one of the harder things to do in Jenkins.\n\nThe\nConditional BuildStep plugin\nis a powerful tool that has allowed Jenkins users to write Jenkins jobs with complex conditional logic.\nIn this post, we’ll take a look at how we might converting Freestyle jobs that\ninclude conditional build steps to Jenkins Pipeline.\nUnlike Freestyle jobs, implementing conditional operations in Jenkins Pipeline is trivial,\nbut matching the behavior of complex conditional build steps will require a bit more care.\n\nGraphical Programming\n\nThe Conditional BuildStep plugin lets users add conditional logic to Freestyle\njobs from within the Jenkins web UI.  It does this by:\n\nAdding two types of Conditional BuildStep (\"Single\" and \"Multiple\") -\nthese build steps contain one or more other build steps to be run when the configured\ncondition is met\n\nAdding a set of Condition operations -\nthese control whether the Conditional BuildStep execute the contained step(s)\n\nLeveraging the Token Macro facility -\nthese provide values to the Conditions for evaluation\n\nIn the example below, this project will run the shell script step when the value of the\nREQUESTED_ACTION token equals \"greeting\".\n\nHere’s the output when I run this project with REQUESTED_ACTION set to \"greeting\":\n\nRun condition [Strings match] enabling prebuild for step [Execute shell]\nStrings match run condition: string 1=[greeting], string 2=[greeting]\nRun condition [Strings match] enabling perform for step [Execute shell]\n[freestyle-conditional] $ /bin/sh -xe /var/folders/hp/f7yc_mwj2tq1hmbv_5n10v2c0000gn/T/hudson5963233933358491209.sh\n+ echo 'Hello, bitwiseman!'\nHello, bitwiseman!\nFinished: SUCCESS\n\nAnd when I pass the value \"silence\":\n\nRun condition [Strings match] enabling prebuild for step [Execute shell]\nStrings match run condition: string 1=[silence], string 2=[greeting]\nRun condition [Strings match] preventing perform for step [Execute shell]\nFinished: SUCCESS\n\nThis is a simple example but the conditional step can contain any regular build step.\nWhen combined with other plugins, it can control whether to send notifications,\ngather data from other sources, wait for user feedback, or call other projects.\n\nThe Conditional BuildStep plugin does a great job of leveraging strengths of\nthe Jenkins web UI, Freestyle jobs, and UI-based programming,\nbut it is also hampered by their limitations.\nThe Jenkins web UI can be clunky and confusing at times.\nLike the steps in any Freestyle job, these conditional steps are only\nstored and viewable in Jenkins.\nThey are not versioned with other product or build code and can’t be code reviewed.\nLike any number of UI-based programming tools, it has to make trade-offs between clarity\nand flexibility: more options or clearer presentation.\nThere’s only so much space on the screen.\n\nConverting to Pipeline\n\nJenkins Pipeline, on the other hand, enables users to implement their pipeline as code.\nPipeline code can be written directly in the Jenkins Web UI or in any text editor.\nIt is a full-featured programming language,\nwhich gives users access to much broader set of conditional statements\nwithout the restrictions of UI-based programming.\n\nSo, taking the example above, the Pipeline equivalent is:\n\n// Declarative //\npipeline {\n    agent any\n    parameters {\n        choice(\n            choices: ['greeting' , 'silence'],\n            description: '',\n            name: 'REQUESTED_ACTION')\n    }\n\n    stages {\n        stage ('Speak') {\n            when {\n                // Only say hello if a \"greeting\" is requested\n                expression { params.REQUESTED_ACTION == 'greeting' }\n            }\n            steps {\n                echo \"Hello, bitwiseman!\"\n            }\n        }\n    }\n}\n// Script //\nproperties ([\n    parameters ([\n        choice (\n            choices: ['greeting', 'silence'],\n            description: '',\n            name : 'REQUESTED_ACTION')\n    ])\n])\n\nnode {\n    stage ('Speak') {\n        // Only say hello if a \"greeting\" is requested\n        if (params.REQUESTED_ACTION == 'greeting') {\n            echo \"Hello, bitwiseman!\"\n        }\n    }\n}\n\nWhen I run this project with REQUESTED_ACTION set to \"greeting\", here’s the output:\n\n[Pipeline] node\nRunning on osx_mbp in /Users/bitwiseman/jenkins/agents/osx_mbp/workspace/pipeline-conditional\n[Pipeline] {\n[Pipeline] stage\n[Pipeline] { (Speak)\n[Pipeline] echo\nHello, bitwiseman!\n[Pipeline] }\n[Pipeline] // stage\n[Pipeline] }\n[Pipeline] // node\n[Pipeline] End of Pipeline\nFinished: SUCCESS\n\nWhen I pass the value \"silence\", the only change is \"Hello, bitwiseman!\" is not printed.\n\nSome might argue that the Pipeline code is a bit harder to understand on first reading.\nOthers would say the UI is just as confusing if not more so.\nEither way, the Pipeline representation is considerably more compact than the Jenkins UI presentation.\nPipeline also lets us add helpful comments, which we can’t do in the Freestyle UI.\nAnd we can easily put this Pipeline in a Jenkinsfile to be code-reviewed, checked-in, and versioned\nalong with the rest of our code.\n\nConditions\n\nThe previous example showed the \"Strings match\" condition and its Pipeline equivalent.\nLet’s look at couple more interesting conditions and their Jenkins Pipeline equivalents.\n\nBoolean condition\n\nYou might think that a boolean condition would be the simplest condition, but it isn’t.\nSince it works with string values from tokens, the Conditional BuildStep plugin offers\na number of ways to indicate true or false.\nTruth is a case insensitive match of one of the following:\n1 (the number one), Y, YES, T, TRUE, ON or RUN.\n\nPipeline can duplicate these, but depending on the scenario we might consider\nwhether a simpler expression would suffice.\n\nPipeline\n\n// Declarative //\nwhen {\n    // case insensitive regular expression for truthy values\n    expression { return token ==~ /(?i)(Y|YES|T|TRUE|ON|RUN)/ }\n}\nsteps {\n    /* step */\n}\n\n// Script //\n// case insensitive regular expression for truthy values\nif (token ==~ /(?i)(Y|YES|T|TRUE|ON|RUN)/) {\n    /* step */\n}\n\nLogical \"OR\" of conditions\n\nThis condition wraps other conditions.\nIt takes their results as inputs and performs a logical \"or\" of the results.\nThe AND and NOT conditions do the same, performing their respective operations.\n\nPipeline\n\n// Declarative //\nwhen {\n    // A or B\n    expression { return A || B }\n}\nsteps {\n    /* step */\n}\n\n// Script //\n// A or B\nif (A || B) {\n    /* step */\n}\n\nTokens\n\nTokens can be considerably more work than conditions.\nThere are more of them and they cover a much broader range of behaviors.\nThe previous example showed one of the simpler cases, accessing a build parameter,\nwhere the token has a direct equivalent in Pipeline.\nHowever, many tokens don’t have direct equivalents,\nsome take a parameters (adding to their complexity),\nand some provide information that is simply not exposed in Pipeline yet.\nSo, determining how to migrate tokens needs to be done on case-by-case basis.\n\nLet’s look at a few examples.\n\n\"FILE\" token\n\nExpands to the contents of a file. The file path is relative to the build workspace root.\n\n${FILE,path=\"PATH\"}\n\nThis token maps directly to the readFile step.\nThe only difference is the file path for readFile is relative to the\ncurrent working directory on the agent, but that is the workspace root by default.\nNo problem.\n\nPipeline\n\n// Declarative //\nwhen {\n    expression { return readFile('pom.xml').contains('mycomponent') }\n}\nsteps {\n    /* step */\n}\n\n// Script //\nif (readFile('pom.xml').contains('mycomponent')) {\n    /* step */\n}\n\nGIT_BRANCH\n\nExpands to the name of the branch that was built.\n\nParameters (descriptions omitted): all, fullName.\n\nThis information may or may not be exposed in Pipeline.  If you’re using the\nPipeline Multibranch plugin\nenv.BRANCH_NAME will give similar basic information, but doesn’t offer the parameters.\nThere are also\nseveral\nissues\nfiled around GIT_* tokens in Pipeline.\nUntil they are addressed fully, we can follow the pattern shown in\npipeline-examples,\nexecuting a shell to get the information we need.\n\nPipeline\n\nGIT_BRANCH = sh(returnStdout: true, script: 'git rev-parse --abbrev-ref HEAD').trim()\n\nCHANGES_SINCE_LAST_SUCCESS\n\nDisplays the changes since the last successful build.\n\nParameters (descriptions omitted):\nreverse, format, changesFormat, showPaths, pathFormat,\nshowDependencies, dateFormat, regex, replace, default.\n\nNot only is the information provided by this token not exposed in Pipeline,\nthe token has ten optional parameters, including format strings and regular expression\nsearches. There are a number of ways we might get similar information in Pipeline.\nEach have their own particular limitations and ways they differ from the token output.\nThen we’ll need to consider how each of the parameters changes the output.\nIf nothing else, translating this token is clearly beyond the scope of this post.\n\nSlightly More Complex Example\n\nLet’s do one more example that shows some of these conditions and tokens.\nThis time we’ll perform different build steps depending on what branch we’re building.\nWe’ll take two build parameters: BRANCH_PATTERN and FORCE_FULL_BUILD.\nBased on BRANCH_PATTERN, we’ll checkout a repository.\nIf we’re building on the master branch or the user checked FORCE_FULL_BUILD,\nwe’ll call three other builds in parallel\n( full-build-linux, full-build-mac, and full-build-windows),\nwait for them to finish, and report the result.\nIf we’re not building on the master branch and the user did not check FORCE_FULL_BUILD,\nwe’ll print a message saying we skipped the full builds.\n\nFreestyle\n\nHere’s the configuration for Freestyle version.\n(It’s pretty long.  Feel free to skip down to the Pipeline version):\n\nThe Pipeline version of this job determines the GIT_BRANCH branch by\nrunning a shell script that returns the current local branch name.\nThis means that the Pipeline version must checkout to a local branch (not a detached head).\n\nFreestyle version of this job does not require a local branch, GIT_BRANCH is set automatically.\nHowever, to maintain functional parity, the Freestyle version of this job includes\n\"Checkout to Specific Local Branch\" as well.\n\nPipeline\n\nHere’s the equivalent Pipeline:\n\nFreestyle version of this job is not stored in source control.\n\nIn general, the Pipeline version of this job would be stored in source control,\nwould checkout scm, and would run that same repository.\nHowever, to maintain functional parity, the Pipeline version shown does a checkout\nfrom source control but is not stored in that repository.\n\nPipeline\n\n// Script //\nproperties ([\n    parameters ([\n        string (\n            defaultValue: '*',\n            description: '',\n            name : 'BRANCH_PATTERN'),\n        booleanParam (\n            defaultValue: false,\n            description: '',\n            name : 'FORCE_FULL_BUILD')\n    ])\n])\n\nnode {\n    stage ('Prepare') {\n        checkout([$class: 'GitSCM',\n            branches: [[name: \"origin/${BRANCH_PATTERN}\"]],\n            doGenerateSubmoduleConfigurations: false,\n            extensions: [[$class: 'LocalBranch']],\n            submoduleCfg: [],\n            userRemoteConfigs: [[\n                credentialsId: 'bitwiseman_github',\n                url: 'https://github.com/bitwiseman/hermann']]])\n    }\n\n    stage ('Build') {\n        GIT_BRANCH = 'origin/' + sh(returnStdout: true, script: 'git rev-parse --abbrev-ref HEAD').trim()\n        if (GIT_BRANCH == 'origin/master' || params.FORCE_FULL_BUILD) {\n\n            // Freestyle build trigger calls a list of jobs\n            // Pipeline build() step only calls one job\n            // To run all three jobs in parallel, we use \"parallel\" step\n            // https://jenkins.io/doc/pipeline/examples/#jobs-in-parallel\n            parallel (\n                linux: {\n                    build job: 'full-build-linux', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                },\n                mac: {\n                    build job: 'full-build-mac', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                },\n                windows: {\n                    build job: 'full-build-windows', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                },\n                failFast: false)\n\n        } else {\n            echo 'Skipped full build.'\n        }\n    }\n}\n// Declarative //\npipeline {\n    agent any\n    parameters {\n        string (\n            defaultValue: '*',\n            description: '',\n            name : 'BRANCH_PATTERN')\n        booleanParam (\n            defaultValue: false,\n            description: '',\n            name : 'FORCE_FULL_BUILD')\n    }\n\n    stages {\n        stage ('Prepare') {\n            steps {\n                checkout([$class: 'GitSCM',\n                    branches: [[name: \"origin/${BRANCH_PATTERN}\"]],\n                    doGenerateSubmoduleConfigurations: false,\n                    extensions: [[$class: 'LocalBranch']],\n                    submoduleCfg: [],\n                    userRemoteConfigs: [[\n                        credentialsId: 'bitwiseman_github',\n                        url: 'https://github.com/bitwiseman/hermann']]])\n            }\n        }\n\n        stage ('Build') {\n            when {\n                expression {\n                    GIT_BRANCH = 'origin/' + sh(returnStdout: true, script: 'git rev-parse --abbrev-ref HEAD').trim()\n                    return GIT_BRANCH == 'origin/master' || params.FORCE_FULL_BUILD\n                }\n            }\n            steps {\n                // Freestyle build trigger calls a list of jobs\n                // Pipeline build() step only calls one job\n                // To run all three jobs in parallel, we use \"parallel\" step\n                // https://jenkins.io/doc/pipeline/examples/#jobs-in-parallel\n                parallel (\n                    linux: {\n                        build job: 'full-build-linux', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                    },\n                    mac: {\n                        build job: 'full-build-mac', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                    },\n                    windows: {\n                        build job: 'full-build-windows', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                    },\n                    failFast: false)\n            }\n        }\n        stage ('Build Skipped') {\n            when {\n                expression {\n                    GIT_BRANCH = 'origin/' + sh(returnStdout: true, script: 'git rev-parse --abbrev-ref HEAD').trim()\n                    return !(GIT_BRANCH == 'origin/master' || params.FORCE_FULL_BUILD)\n                }\n            }\n            steps {\n                echo 'Skipped full build.'\n            }\n        }\n    }\n}\n\nConclusion\n\nAs I said before, the Conditional BuildStep plugin is great.\nIt provides a clear, easy to understand way to add conditional logic to any Freestyle job.\nBefore Pipeline, it was one of the few plugins to do this and it remains one of the most popular plugins.\nNow that we have Pipeline, we can implement conditional logic directly in code.\n\nThis is blog post discussed how to approach converting conditional build steps to Pipeline\nand showed a couple concrete examples.  Overall, I’m pleased with the results so far.\nI found scenarios which could not easily be migrated to Pipeline, but even those\nare only more difficult, rather than impossible.\n\nThe next thing to do is add a section to the\nJenkins Handbook documenting the Pipeline\nequivalent of all of the Conditions and the most commonly used Tokens.\nLook for it soon!\n\nLinks\n\nConditional BuildStep plugin","title":"Converting Conditional Build Steps to Jenkins Pipeline","tags":["pipeline","freestyle","plugins","conditional-build-step","tutorial"],"authors":[]}},{"node":{"date":"2017-01-12T00:00:00.000Z","id":"2ff4e12b-c7b0-5650-8366-ce14906b5f15","slug":"/blog/2017/01/12/declarative-pipeline-beta-2/","strippedHtml":"This week, we released the second beta of the new\nDeclarative Pipeline syntax,\navailable in the Update Center now as version 0.8.1 of Pipeline: Model Definition.\nYou can read more about Declarative Pipeline\nin the blog post introducing the first beta\nfrom December, but we wanted to update you all on the syntax changes in the\nsecond beta. These syntax changes are the last compatibility-breaking changes to\nthe syntax before the 1.0 release planned for February, so you can safely start\nusing the 0.8.1 syntax now without needing to change it when 1.0 is released.\n\nA full syntax reference is available on the wiki as well.\n\nSyntax Changes\n\nChanged \"agent\" configuration to block structure\n\nIn order to support more detailed and clear configuration of agents, as well as\nmaking agent syntax more consistent with the rest of the Declarative Pipeline\nsyntax, we’ve moved the agent configuration into blocks. The agent any and\nagent none configurations work the same as previously, but label, docker\nand dockerfile now look like the following:\n\nJust specifying a label is simple.\n\n// Declarative //\nagent {\n    label \"some-label\"\n}\n// Script //\n\nIf you’re just specifying a Docker image, you can use this simple syntax.\n\n// Declarative //\nagent {\n    docker \"ubuntu:16.04\"\n}\n// Script //\n\nWhen you are specifying a label or other arguments, docker looks like this:\n\n// Declarative //\nagent {\n    docker {\n        image \"ubuntu:16.04\"\n        label \"docker-label\"\n        args \"-v /tmp:/tmp -p 8000:8000\"\n    }\n}\n// Script //\n\nWhen you’re building an image from \"Dockerfile\" in your repository and\ndon’t care what node is used or have additional arguments, you can again\nuse a simple syntax.\n\n// Declarative //\nagent {\n    dockerfile true\n}\n// Script //\n\nWhen you’re building an image from a different file, or have a label or other\narguments, use the following syntax:\n\n// Declarative //\nagent {\n    dockerfile {\n        filename \"OtherDockerfile\"\n        label \"docker-label\"\n        args \"-v /tmp:/tmp -p 8000:8000\"\n    }\n}\n// Script //\n\nImproved \"when\" conditions\n\nWe introduced the when section a couple releases ago, but have made some\nchanges to its syntax here in 0.8.1. We wanted to add some simpler ways to\nspecify common conditions, and that required we re-work the syntax accordingly.\n\nBranch\n\nOne of the most common conditions is running a stage only if you’re on a\nspecific branch. You can also use wildcards like \"*/master\".\n\n// Declarative //\nwhen {\n    branch \"master\"\n}\n// Script //\n\nEnvironment\n\nAnother built-in condition is the environment condition, which checks to see\nif a given environment variable is set to a given value.\n\n// Declarative //\nwhen {\n    environment name: \"SOME_ENV_VAR\", value: \"SOME_VALUE\"\n}\n// Script //\n\nExpression\n\nLastly, there’s the expression condition, which resolves an arbitrary\nPipeline expression. If the return value of that expression isn’t false or\nnull, the stage will execute.\n\n// Declarative //\nwhen {\n    expression {\n        echo \"Should I run?\"\n        return \"foo\" == \"bar\"\n    }\n}\n// Script //\n\n\"options\" replaces \"properties\" and \"wrappers\"\n\nWe’ve renamed the properties section to options, due to needing to add new\nDeclarative-specific options and to cut down on confusion. The options section\nis now where you’ll put general Pipeline options like buildDiscarder,\nDeclarative-specific options like skipDefaultCheckout, and block-scoped steps\nthat should wrap the execution of the entire build, like timeout or\ntimestamps.\n\n// Declarative //\n\noptions {\n    buildDiscarder(logRotator(numToKeepStr:'1'))\n    skipDefaultCheckout()\n    timeout(time: 5, unit: 'MINUTES')\n}\n// Script //\n\nHeading towards 1.0!\n\nWhile we may still add more functionality to the Declarative Pipeline syntax,\nwe won’t be making any changes to existing syntax for the 1.0 release. This\nmeans that any pipelines you write against the 0.8.1 syntax will keep working\nfor the foreseeable future without any changes. So if you’re already using\nDeclarative Pipelines, make sure to update your `Jenkinsfile`s after upgrading\nto 0.8.1, and if you haven’t been using Declarative Pipelines yet, install the\nPipeline: Model Definition plugin and\ngive them a try!","title":"Declarative Pipeline Syntax Beta 2 release","tags":["plugins","pipeline"],"authors":[]}},{"node":{"date":"2016-12-31T00:00:00.000Z","id":"3267c8fe-f10e-530a-9caa-bae9eeb34fb0","slug":"/blog/2016/12/31/what-a-year/","strippedHtml":"I do not think it is an exaggeration to say: 2016 was the best year yet for the\nJenkins project. Since the first commit in 2006, the project has reached a\nnumber of significant milestones in its ten years but we have never experienced\nthe breadth of major milestones in such a short amount of time. From\nJenkins 2\nand\nBlue Ocean\nto the\nGoogle Summer of Code\nand\nJenkins World,\n\nI wanted to take a moment and celebrate the myriad of accomplishments which\ncouldn’t have happened without the help from everybody who participates in the\nJenkins project. The 1,300+ contributors to the\njenkinsci GitHub organization,\nthe 4,000+ members of the\ndevelopers mailing list,\nthe 8,000+ members of the\nusers mailing list,\nand countless others who have reported issues, submitted pull requests, and\npresented at meetups and conferences.\n\nJenkins 2\n\nThrough the course of 2016, the Jenkins project published 16\nLTS releases\nand 54\nWeekly releases.\nOf those 70 releases, the most notable may have been the\nJenkins 2.0 release\nwhich was published in April.\n\nJenkins 2 made Pipeline as Code front-and-center in the user experience,\nintroduced a new \"Getting Started\" experience, and included a number of other\nsmall UI improvements, all while maintaining backwards compatibility with\nexisting Jenkins environments.\n\nSince April, we have released a number of LTS\nreleases using Jenkins 2 as a baseline, meaning the Jenkins project no longer\nmaintains any 1.x release lines.\n\nThe\nPipeline\nefforts have continuted to gain steam since April, covered on this blog with a\nnumber of\nposts tagged \"pipeline\". Closing out 2016 with the\nannouncement of the beta for\nDeclarative Pipeline syntax\nwhich is expected in early 2017.\n\nBlue Ocean\n\nHot on the heels of Jenkins 2 announcement\"Blue Ocean, a new user experience for Jenkins\",\nwas\nopen sourced in May.\nBlue Ocean is a new project that rethinks the user experience of Jenkins.\nDesigned from the ground up for Jenkins Pipeline and compatible with Freestyle\njobs. The goal for the project is to reduce clutter and increase clarity for\nevery member of a team using Jenkins.\n\nThe Blue Ocean beta can be installed from the Update Center and can be run in\nproduction Jenkins environments alongside the existing UI. It adds the new user experience under\n/blue in the environment but does not disturb the existing UI.\n\nBlue Ocean is expected to reach \"1.0\" in the first half of 2017.\n\nAzure\n\nAlso in May of 2016, the Jenkins project announced an exciting\nPartnership with Microsoft\nto run our project infrastructure on\nAzure. While the migration of Jenkins project\ninfrastructure into Azure is still on-going, there have been some notable\nmilestones reached already:\n\nEnd-to-end TLS encrypted delivery for Debian/openSUSE/Red Hat repositories which are\nconfigured to use https://pkg.jenkins.io by the end-user.\n\nMajor capacity improvements to\nci.jenkins.io\nproviding on-demand Ubuntu and Windows build/test infrastructure.\n\nA full continuous delivery Pipeline for all Azure-based infrastructure using\nTerraform from Jenkins.\n\nThe migration to Azure is expected to complete in 2017.\n\nGoogle Summer of Code\n\nFor the first time in the history of the project, Jenkins was accepted into\nGoogle Summer of Code\n2016. Google Summer of Code (GSoC) is an annual, international, program\nwhich encourages college-aged students to participate with open source projects\nduring the summer break between classes. Students accepted into the program\nreceive a stipend, paid by Google, to work well-defined projects to improve or\nenhance the Jenkins project.\n\nIn exchange, numerous Jenkins community members volunteered as \"mentors\" for\nstudents to help integrate them into the open source community and succeed in\ncompleting their summer projects.\n\nA lot was learned during the summer which we look forward to applying to Google\nSummer of Code 2017\n\nJenkins World\n\nIn September, over one thousand people attended\nJenkins World,\nin Santa Clara, California.\n\nFollowing the event,\nLiam\nposted a series of blog posts which highlight some of the fantastic content\nshared by Jenkins users and contributors from around the world, such as:\n\nThe demos from the \"Experts\"\n\nSessions on Scaling Jenkins\n\nUsing Jenkins Pipeline\n\nThe Contributor Summit\n\nJenkins World was the first global event of its kind for Jenkins, it brought users\nand contributors together to exchange ideas on the current state of the\nproject, celebrate accomplishments of the past year, and look ahead at all the\nexiting enhancements coming down the pipe(line).\n\nIt was such a smashing success that\nJenkins World 2017\nis already scheduled for August 30-31st in San Francisco, California.\n\nJAM\n\nFinally, 2016 saw tremendous growth in the number of\nJenkins Area Meetups\n(JAMs) hosted around the world. JAMs are local meetups intended to bring\nJenkins users and contributors together for socializing and learning. JAMs are\norganized by local Jenkins community members who have a passion for sharing new\nJenkins concepts, patterns and tools.\n\nDriven by current Jenkins Events Officer,\nAlyssa Tong,\nand the dozens of passionate organizers, JAMs have become a great way to meet\nother Jenkins users near you.\n\nWhile we don’t yet have JAMs on each of the seven continents, you can always join the\nJenkins Online Meetup.\nThough we’re hoping more groups will be founded near you in 2017!\n\nI am personally grateful for the variety and volume of contributions made by\nthousands of people to the Jenkins project this year. I believe I can speak for\nproject founder,\nKohsuke Kawaguchi,\nin stating that the Jenkins community has grown beyond our anything we could\nhave imagined five years ago, let alone ten!\n\nThere are number of ways to\nparticipate\nin the Jenkins project, so if you didn’t have an opportunity to join in during\n2016, we hope to see you next year!","title":"Thank you for an amazing 2016","tags":["jam","jenkins2","pipeline","blueocean","azure","gsoc","new-year-blogpost"],"authors":[]}}]}},"pageContext":{"tag":"pipeline","limit":8,"skip":48,"numPages":13,"currentPage":7}},
    "staticQueryHashes": ["3649515864"]}