{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/pipeline/page/10",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2016-05-31T00:00:00.000Z","id":"ad068db4-bb27-52c3-9916-bb8b132c6e53","slug":"/blog/2016/05/31/pipeline-snippetizer/","strippedHtml":"Those of you updating the Pipeline Groovy plugin\nto 2.3 or later will notice a change to the appearance of the configuration form.\nThe Snippet Generator tool is no longer a checkbox enabled inside the configuration page.\nRather, there is a link Pipeline Syntax which opens a separate page with several options.\n(The link appears in the project’s sidebar; Jenkins 2 users will not see the sidebar from the configuration screen,\nso as of 2.4 there is also a link beneath the Pipeline definition.)\n\nSnippet Generator continues to be available for learning the available\nPipeline steps and creating sample calls given various configuration options.\nThe new page also offers clearer links to static reference documentation, online\nPipeline documentation resources, and an IntelliJ IDEA code completion file\n(Eclipse support is unfinished).\n\nOne motivation for this change\n( JENKINS-31831) was to\ngive these resources more visual space and more prominence.  But another\nconsideration was that people using multibranch projects or organization folders\nshould be able to use Snippet Generator when setting up the project, before\nany code is committed.\n\nThose using\nPipeline\nMultibranch plugin or organization folder plugins should upgrade to 2.4 or\nlater to see these improvements as well.","title":"New display of Pipeline’s \"snippet generator\"","tags":["pipeline"],"authors":[{"avatar":null,"blog":null,"github":"jglick","html":"<div class=\"paragraph\">\n<p>Jesse has been developing Jenkins core and plugins for years.\nHe is the coauthor with Kohsuke of the core infrastructure of the Pipeline system.</p>\n</div>","id":"jglick","irc":null,"linkedin":null,"name":"Jesse Glick","slug":"/blog/authors/jglick","twitter":"tyvole"}]}},{"node":{"date":"2016-05-26T00:00:00.000Z","id":"bbd7c8d9-ad20-56ae-b97b-834547d227f7","slug":"/blog/2016/05/26/introducing-blue-ocean/","strippedHtml":"In recent years developers have become rapidly attracted to tools that are not\nonly functional but are designed to fit into their workflow seamlessly and are\na joy to use. This shift represents a higher standard of design and user\nexperience that Jenkins needs to rise to meet.\n\nWe are excited to share and invite the community to join us on a project we’ve\nbeen thinking about over the last few months called Blue Ocean.\n\nBlue Ocean is a project that rethinks the user experience of Jenkins, modelling\nand presenting the process of software delivery by surfacing information that’s\nimportant to development teams with as few clicks as possible, while still\nstaying true to the extensibility that is core to Jenkins.\n\nWhile this project is in the alpha stage of development, the intent is that\nJenkins users can install Blue Ocean side-by-side with the Jenkins Classic UI\nvia a plugin.\n\nNot all the features listed on this blog are complete but we will be hard at\nwork over the next few months preparing Blue Ocean for general use. We intend\nto provide regular updates on this blog as progress is made.\n\nBlue Ocean is open source today\nand we invite you to give us feedback and to contribute to the project.\n\nBlue Ocean will provide development teams:\n\nNew modern user experience\n\nThe UI aims to improve clarity, reduce clutter and navigational depth to make\nthe user experience very concise. A modern visual design gives developers much\nneeded relief throughout their daily usage and screens respond instantly to\nchanges on the server making manual page refreshes a thing of the past.\n\nAdvanced Pipeline visualisations with built-in failure diagnosis\n\nPipelines are visualised on screen along with the\nsteps and logs to allow simplified comprehension of the continuous delivery\npipeline – from the simple to the most sophisticated scenarios.\n\nScrolling through 10,000 line log files is a thing of the past. Blue Ocean\nbreaks down your log per step and calls out where your build failed.\n\nBranch and Pull Request awareness\n\nModern pipelines make use of multiple Git branches, and Blue Ocean is designed\nwith this in mind. Drop a Jenkinsfile into your Git\nrepository that defines your pipeline and Jenkins will automatically discover\nand start automating any  Branches and validating Pull Requests.\n\nJenkins will report the status of your pipeline right inside Github or\nBitbucket on all your commits, branches or pull requests.\n\nPersonalised View\n\nFavourite any pipelines, branches or pull requests and see them appear on your\npersonalised dashboard. Intelligence is being built into the dashboard. Jobs\nthat need your attention, say a Pipeline waiting for approval or a failing job\nthat you have recently changed, appear on the top of the dashboard.\n\nYou can read more about Blue Ocean and its goals on the\nproject page and developers should watch the\nDevelopers list for more information.\n\nFor Jenkins developers and plugin authors:\n\nJenkins Design “Language”\n\nThe Jenkins Design Language (JDL) is a set of standardised React components and\na style guide that help developers create plugins that retain the look and feel\nof Blue Ocean in an effortless way. We will be publishing more on the JDL,\nincluding the style guide and developer documentation, over the next few weeks.\n\nModern JavaScript toolchain\n\nThe Jenkins plugin tool chain has been extended so that developers can use\nES6,\nReact, NPM\nin their plugins without endless yak-shaving. Jenkins\njs-modules are already in use in\nJenkins today, and this builds on this, using the same tooling.\n\nClient side Extension points\n\nClient Side plugins use Jenkins plugin infrastructure. The Blue Ocean libraries\nbuilt on ES6 and React.js provide an extensible client side component model\nthat looks familiar to developers who have built Jenkins plugins before. Client\nside extension points can help isolate failure, so one bad plugin doesn’t take\na whole page down.\n\nServer Sent Events\n\nServer Sent Events\n(SSE) allow plugin developers to tap into changes of state on the server and make\ntheir UI update in real time ( watch this for a\ndemo).\n\nTo make Blue Ocean a success, we’re asking for help and support from Jenkins\ndevelopers and plugin authors. Please join in our Blue Ocean discussions on the\nJenkins Developer\nmailing list and the #jenkins-ux IRC channel on Freenode!\n\nLinks\n\nBlue Ocean project page\n\nBlue Ocean GitHub repository","title":"Introducing Blue Ocean: a new user experience for Jenkins","tags":["blueocean","ux","pipeline"],"authors":[{"avatar":null,"blog":null,"github":"i386","html":"","id":"i386","irc":null,"linkedin":null,"name":"James Dumay","slug":"/blog/authors/i386","twitter":"i386"}]}},{"node":{"date":"2016-05-25T00:00:00.000Z","id":"3bdee89e-566b-52d9-a3cc-f41507becadc","slug":"/blog/2016/05/25/update-plugin-for-pipeline/","strippedHtml":"This is a guest post by Chris Price.\nChris is a software engineer at Puppet, and has been\nspending some time lately on automating performance testing using the latest\nJenkins features.\n\nIn this blog post, I’m going to attempt to provide some step-by-step notes on\nhow to refactor an existing Jenkins plugin to make it compatible with the new\nJenkins Pipeline jobs.  Before we get to the fun stuff, though, a little\nbackground.\n\nHow’d I end up here?\n\nRecently, I started working on a project to automate some performance tests for\nmy company’s products.  We use the awesome Gatling load\ntesting tool for these tests, but we’ve largely been handling the testing very\nmanually to date, due to a lack of bandwidth to get them automated in a clean,\nmaintainable, extensible way.  We have a years-old Jenkins server where we use\nthe gatling jenkins\nplugin to track the\nhistory of certain tests over time, but the setup of the Jenkins instance was\nvery delicate and not easy to reproduce, so it had fallen into a state of\ndisrepair.\n\nOver the last few days I’ve been putting some effort into getting things more\nautomated and repeatable so that we can really maximize the value that we’re\ngetting out of the performance tests.  With some encouragement from the fine\nfolks in the #jenkins IRC channel, I ended up exploring\nthe JobDSL\nplugin and the new Pipeline jobs.  Combining those two\nthings with some Puppet code to provision a Jenkins server via the\njenkins puppet module gave me\na really nice way to completely automate my Jenkins setup and get a seed job in\nplace that would create my perf testing jobs.  And the Pipeline job format is\njust an awesome fit for what I wanted to do in terms of being able to easily\nmonitor the stages of my performance tests, and to make the job definitions\nmodular so that it would be really easy to create new performance testing jobs\nwith slight variations.\n\nSo everything’s going GREAT up to this point.  I’m really happy with how it’s\nall shaping up.  But then…​ (you knew there was a \"but\" coming, right?) I\nstarted trying to figure out how to add the\nGatling Jenkins\nplugin to the Pipeline jobs, and kind of ran into a wall.\n\nAs best as I could tell from my Googling, the plugin was probably going to\nrequire some modifications in order to be able to be used with Pipeline jobs.\nHowever, I wasn’t able to find any really cohesive documentation that\ndefinitively confirmed that or explained how everything fits together.\n\nEventually, I got it all sorted out.  So, in hopes of saving the next person a\nlittle time, and encouraging plugin authors to invest the time to get their\nplugins working with Pipeline, here are some notes about what I learned.\n\nSpoiler: if you’re just interested in looking at the individual git commits that\nI made on may way to getting the plugin working with Pipeline, have a look at\nthis github\nbranch.\n\nCreating a pipeline step\n\nThe main task that the Gatling plugin performs is to archive Gatling reports\nafter a run.  I figured that the end game for this exercise was that I was going\nto end up with a Pipeline \"step\" that I could include in my Pipeline scripts, to\ntrigger the archiving of the reports.  So my first thought was to look for an\nexisting plugin / Pipeline \"step\" that was doing something roughly similar, so\nthat I could use it as a model.  The Pipeline \"Snippet Generator\" feature\n(create a pipeline job, scroll down to the \"Definition\" section of its\nconfiguration, and check the \"Snippet Generator\" checkbox) is really helpful for\nfiguring out stuff like this; it is automatically populated with all of the\nsteps that are valid on your server (based on which plugins you have installed),\nso you can use it to verify whether or not your custom \"step\" is recognized, and\nalso to look at examples of existing steps.\n\nLooking through the list of existing steps, I figured that the archive step\nwas pretty likely to be similar to what I needed for the gatling plugin:\n\nSo, I started poking around to see what magic it was that made that archive\nstep show up there.  There are some mentions of this in the\npipeline-plugin\nDEVGUIDE.md and the\nworkflow-step-api-plugin\nREADME.md, but the real breakthrough for me was finding the definition of the\narchive step in the workflow-basic-steps-plugin source\ncode.\n\nWith that as an example, I was able to start poking at getting a\ngatlingArchive step to show up in the Snippet Generator.  The first thing that\nI needed to do was to update the gatling-plugin project’s pom.xml to depend\non a recent enough version of Jenkins, as well as specify dependencies on the\nappropriate pipeline\nplugins\n\nOnce that was out of the way, I noticed that the archive step had some tests\nwritten for it, using what looks to be a pretty awesome test API for pipeline\njobs and plugins.  Based on those archive\ntests,\nI added\na\nskeleton for a test for the gatlingArchive step that I was about to write.\n\nThen, I moved on to\nactually\ncreating the step.  The meat of the code was this:\n\npublic class GatlingArchiverStep extends AbstractStepImpl {\n    @DataBoundConstructor\n    public GatlingArchiverStep() {}\n\n    @Extension\n    public static class DescriptorImpl extends AbstractStepDescriptorImpl {\n        public DescriptorImpl() { super(GatlingArchiverStepExecution.class); }\n\n        @Override\n        public String getFunctionName() {\n            return \"gatlingArchive\";\n        }\n\n        @NonNull\n        @Override\n        public String getDisplayName() {\n            return \"Archive Gatling reports\";\n        }\n    }\n}\n\nNote that in that commit I also added a config.jelly file.  This is how you\ndefine the UI for your step, which will show up in the Snippet Generator.  In\nthe case of this Gatling step there’s really not much to configure, so my\nconfig.jelly is basically empty.\n\nWith that (and the rest of the code from that commit) in place, I was able to\nfire up the development Jenkins server (via mvn hpi:run, and note that you\nneed to go into the \"Manage Plugins\" screen on your development server and\ninstall the Pipeline plugin once before any of this will work) and visit the\nSnippet Generator to see if my step showed up in the dropdown:\n\nGREAT SUCCESS!\n\nThis step doesn’t actually do anything yet, but it’s recognized by Jenkins and\ncan be included in your pipeline scripts at that point, so, we’re on our way!\n\nThe step metastep\n\nThe step that we created above is a first-class DSL addition that can be used in\nPipeline scripts.  There’s another way to make your plugin work usable from a\nPipeline job, without making it a first-class build step.  This is by use of the\nstep\"metastep\", mentioned in the pipeline-plugin\nDEVGUIDE.\nWhen using this approach, you simply refactor your Builder or Publisher to\nextend SimpleBuildStep, and then you can reference the build step from the\nPipeline DSL using the step method.\n\nIn the Jenkins GUI, go to the config screen for a Pipeline job and click on the\nSnippet Generator checkbox.  Select 'step: General Build Step' from the\ndropdown, and then have a look at the options that appear in the 'Build Step'\ndropdown.  To compare with our previous work, let’s see what \"Archive the\nartifacts\" looks like:\n\nFrom the snippet generator we can see that it’s possible to trigger an Archive\naction with syntax like:\n\nstep([$class: 'ArtifactArchiver', artifacts: 'foo*', excludes: null])\n\nThis is the \"metastep\".  It’s a way to trigger any build action that implements\nSimpleBuildStep, without having to actually implement a real \"step\" that\nextends the Pipeline DSL like we did above.  In many cases, it might only make\nsense to do one or the other in your plugin; you probably don’t really need\nboth.\n\nFor the purposes of this tutorial, we’re going to do both.  For a couple of reasons:\n\nWhy the heck not?  :)  It’s a good demonstration of how the metastep stuff\nworks.\n\nBecause implementing the \"for realz\" step will be a lot easier if the Gatling\naction that we’re trying to call from our gatlingArchive() syntax is using the\nnewer Jenkins APIs that are required for subclasses of SimpleBuildStep.\n\nGatlingPublisher is the main build action that we’re interested in using in\nPipeline jobs.  So, with all of that in mind, here’s our next goal: get\nstep([$class: 'GatlingPublisher', …​) showing up in the Snippet Generator.\n\nThe javadocs for the SimpleBuildStep\nclass\nhave some notes on what you need to do when porting an existing Builder or\nPublisher over to implement the SimpleBuildStep interface.  In all\nlikelihood, most of what you’re going to end up doing is to replace occurrences\nof AbstractBuild with references to the Run class, and replace occurrences\nof AbstractProject with references to the Job class.  The APIs are pretty\nsimilar, so it’s not too hard to do once you understand that that’s the game.\nThere is some discussion of this in the pipeline-plugin\nDEVGUIDE.\n\nFor the Gatling plugin, my\ninitial\nefforts to port the GatlingPublisher over to implement SimpleBuildStep only\nrequired the AbstractBuild → Run refactor.\n\nAfter making these changes, I fired up the development Jenkins server, and, voila!\n\nSo, now, we can add a line like this to a Pipeline build script:\n\nstep([$class: 'GatlingPublisher', enabled: true])\n\nAnd it’ll effectively be the same as if we’d added the Gatling \"Post-Build\nAction\" to an old-school Freestyle project.\n\nWell…​ mostly.\n\nBuild Actions vs. Project Actions\n\nAt this point our modified Gatling plugin should work the same way as it always\ndid in a Freestyle build, but in a Pipeline build, it only partially works.\nSpecifically, the Gatling plugin implements two different \"Actions\" to surface\nthings in the Jenkins GUI: a \"Build\" action, which adds the Gatling icon to the\nleft sidebar in the GUI when you’re viewing an individual build in the build\nhistory of a job, and a \"Project\" action, which adds that same icon to the left\nsidebar of the GUI of the main page for a job.  The \"Project\" action also adds a\n\"floating panel\" on the main job page, which shows a graph of the historical\ndata for the Gatling runs.\n\nIn a Pipeline job, though, assuming we’ve added a call to the metastep, we’re\nonly seeing the \"Build\" actions.  Part of this is because, in the last round of\nchanges that I linked, we only modified the \"Build\" action, and not the\n\"Project\" action.  Running the metastep in a Pipeline job has no visible effect\nat all on the project/job page at this point.  So that’s what we’ll tackle next.\n\nThe key thing to know about getting \"Project\" actions working in a Pipeline job\nis that, with a Pipeline job, there is no way for Jenkins to know up front what\nsteps or actions are going to be involved in a job.  It’s only after the job\nruns once that Jenkins has a chance to introspect what all the steps were.  As\nsuch, there’s no list of Builders or Publishers that it knows about up front to\ncall getProjectAction on, like it would with a Freestyle job.\n\nThis is where\nSimpleBuildStep.LastBuildAction\ncomes into play.  This is an interface that you can add to your Build actions,\nwhich give them their own getProjectActions method that Jenkins recognizes and\nwill call when rendering the project page after the job has been run at least\nonce.\n\nSo, effectively, what we need to do is to\nget\nrid of the getProjectAction method on our Publisher class, modify the Build\naction to implement SimpleBuildStep.LastBuildAction, and encapsulate our\nProject action instances in the Build action.\n\nThe build action class now constructs an instance of the Project action and\nmakes it accessible via getProjectActions (which comes from the\nLastBuildAction interface):\n\npublic class GatlingBuildAction implements Action, SimpleBuildStep.LastBuildAction {\n    public GatlingBuildAction(Run build, List sims) {\n        this.build = build;\n        this.simulations = sims;\n\n        List projectActions = new ArrayList<>();\n        projectActions.add(new GatlingProjectAction(build.getParent()));\n        this.projectActions = projectActions;\n    }\n\n    @Override\n    public Collection getProjectActions() {\n        return this.projectActions;\n    }\n}\n\nAfter making these changes, if we run the development Jenkins server, we can see\nthat after the first successful run of the Pipeline job that calls the\nGatlingPublisher metastep, the Gatling icon indeed shows up in the sidebar on\nthe main project page, and the floating box with the graph shows up as well:\n\nMaking our DSL step do something\n\nSo at this point we’ve got the metastep syntax working from end-to-end, and\nwe’ve got a valid Pipeline DSL step ( gatlingArchive()) that we can use in our\nPipeline scripts without breaking anything…​ but our custom step doesn’t\nactually do anything.  Here’s the part where we tie it all together…​ and it’s\npretty easy!  All we need to do is to make our step \"Execution\" class\ninstantiate a Publisher and call perform on\nit.\n\nAs per the\nnotes\nin the pipeline-plugin DEVGUIDE, we can use the @StepContextParameter\nannotation to inject in the objects that we need to pass to the Publisher’s\nperform method:\n\npublic class GatlingArchiverStepExecution extends AbstractSynchronousNonBlockingStepExecution {\n\n    @StepContextParameter\n    private transient TaskListener listener;\n\n    @StepContextParameter\n    private transient FilePath ws;\n\n    @StepContextParameter\n    private transient Run build;\n\n    @StepContextParameter\n    private transient Launcher launcher;\n\n    @Override\n    protected Void run() throws Exception {\n        listener.getLogger().println(\"Running Gatling archiver step.\");\n\n        GatlingPublisher publisher = new GatlingPublisher(true);\n        publisher.perform(build, ws, launcher, listener);\n\n        return null;\n    }\n}\n\nAfter these changes, we can fire up the development Jenkins server, and hack up\nour Pipeline script to call gatlingArchive() instead of the metastep\nstep([$class: 'GatlingPublisher', enabled: true]) syntax.  One of these is\nnicer to type and read than the other, but I’ll leave that as an exercise for\nthe reader.\n\nFin\n\nWith that, our plugin now works just as well in the brave new Pipeline world as\nit did in the olden days of Freestyle builds.  I hope these notes save someone\nelse a little bit of time and googling on your way to writing (or porting) an\nawesome plugin for Jenkins Pipeline jobs!\n\nLinks\n\nJenkins Pipeline Overview\n\nPipeline Plugin Developer Guide\n\nJenkins Source Code\n\nWorkflow Step API Plugin\n\nWorkflow Basic Steps Plugin","title":"Refactoring a Jenkins plugin for compatibility with Pipeline jobs","tags":["core","pipeline","plugins"],"authors":[{"avatar":null,"blog":null,"github":"cprice404","html":"<div class=\"paragraph\">\n<p>Chris is a software engineer at Puppet, who mostly works on backend services\nfor Puppet itself, but occasionally gets to spend some time improving CI\nand automation using Jenkins.</p>\n</div>","id":"cprice404","irc":null,"linkedin":null,"name":"Chris Price","slug":"/blog/authors/cprice404","twitter":"cprice404"}]}},{"node":{"date":"2016-05-23T00:00:00.000Z","id":"85257451-d116-575c-b893-3c51d7386caa","slug":"/blog/2016/05/23/external-workspace-manager-plugin/","strippedHtml":"About myself\n\nMy name is Alexandru Somai.\nI’m following a major in Software Engineering at the Babes-Bolyai University of Cluj-Napoca, Romania.\nI have more than two years hands-on experience working in Software Development.\n\nI enjoy writing code in Java, Groovy and JavaScript.\nThe technologies and frameworks that I’m most familiar with are: Spring Framework, Spring Security, Hibernate,\nJMS, Web Services, JUnit, TestNG, Mockito.\nAs build tools and continuous integration, I’m using Maven and Jenkins.\nI’m a passionate software developer who is always learning, always looking for new challenges.\nI want to start contributing to the open source community and Google Summer of Code is a starting point for me.\n\nProject summary\n\nCurrently, Jenkins’ build workspace may become very large in size due to the fact that some compilers generate\nvery large volumes of data.\nThe existing plugins that share the workspace across builds are able to do this by copying the files from\none workspace to another, process which is inefficient.\nA solution is to have a Jenkins plugin that is able to manage and reuse the same workspace between multiple builds.\n\nAs part of the Google Summer of Code 2016 I will be working on\nthe External Workspace Manager plugin.\nMy mentors for this project are Oleg Nenashev\nand Martin d’Anjou.\nThis plugin aims to provide an external workspace management system.\nIt should facilitate workspace share and reuse across multiple Jenkins jobs.\nIt should eliminate the need to copy, archive or move files.\nThe plugin will be written for Pipeline jobs.\n\nUsage\n\nPrerequisites\n\nMultiple physical disks accessible from controller.\n\nThe same physical disks must be accessible from Jenkins Nodes (renamed to Agents in Jenkins 2.0).\n\nIn the Jenkins global configuration, define a disk pool (or many) that will contain the physical disks.\n\nIn each Node configuration, define the mounting point from the current node to each physical disk.\n\nThe following diagram gives you an overview of how an External Workspace Manager configuration may look like:\n\nExample one\n\nLet’s assume that we have one Jenkins job. In this job, we want to use the same workspace on multiple Jenkins nodes.\nOur pipeline code may look like this:\n\nstage ('Stage 1. Allocate workspace')\ndef extWorkspace = exwsAllocate id: 'diskpool1'\n\nnode ('linux') {\n    exws (extWorkspace) {\n        stage('Stage 2. Build on the build server')\n        git url: '...'\n        sh 'mvn clean install'\n    }\n}\n\nnode ('test') {\n    exws (extWorkspace) {\n        stage('Stage 3. Run tests on a test machine')\n        sh 'mvn test'\n    }\n}\n\nNote: The stage() steps are optional from the External Workspace Manager plugin perspective.\n\nStage 1. Allocate workspace\n\nThe exwsAllocate step selects a disk from diskpool1\n(default behavior: the disk with the most available size).\nOn that disk, let’s say disk1, it allocates a directory.\nThe computed directory path is: /physicalPathOnDisk/$JOB_NAME/$BUILD_NUMBER.\n\nFor example, Let’s assume that the $JOB_NAME is integration and the $BUILD_NUMBER is 14.\nThen, the resulting path is: /jenkins-project/disk1/integration/14.\n\nStage 2. Build on the build server\n\nAll the nodes labeled linux must have access to the disks defined in the disk pool.\nIn the Jenkins Node configurations we have defined the local paths that are the mounting points to each disk.\n\nThe exws step concatenates the node’s local path with the path returned by the exwsAllocate step.\nIn our case, the node labeled linux has its local path to disk1 defined as: /linux-node/disk1/.\nSo, the complete workspace path is: /linux-node/disk1/jenkins-project/disk1/integration/14.\n\nStage 3. Run tests on a test machine\n\nFurther, we want to run our tests on a different node, but we want to reuse the previously created workspace.\n\nIn the node labeled test we have defined the local path to disk1 as: /test-node/disk1/.\nBy applying the exws step, our tests will be able to run in the same workspace as the build.\nTherefore, the path is: /test-node/disk1/jenkins-project/disk1/integration/14.\n\nExample two\n\nLet’s assume that we have two Jenkins jobs, one called upstream and the other one called downstream.\nIn the upstream job, we clone the repository and build the project, and in the downstream job we run the tests.\nIn the downstream job we don’t want to clone and re-build the project, we need to use the same\nworkspace created in the upstream job.\nWe have to be able to do so without copying the workspace content from one location to another.\n\nThe pipeline code in the upstream job is the following:\n\nstage ('Stage 1. Allocate workspace in the upstream job')\ndef extWorkspace = exwsAllocate id: 'diskpool1'\n\nnode ('linux') {\n    exws (extWorkspace) {\n        stage('Stage 2. Build in the upstream job')\n           git url: '...'\n           sh 'mvn clean install'\n    }\n}\n\nAnd the downstream 's pipeline code is:\n\nstage ('Stage 3. Allocate workspace in the downstream job')\ndef extWorkspace = exwsAllocate id: 'diskpool1', upstream: 'upstream'\n\nnode ('test') {\n    exws (extWorkspace) {\n        stage('Stage 4. Run tests in the downstream job')\n        sh 'mvn test'\n    }\n}\n\nStage 1. Allocate workspace in the upstream job\n\nThe functionality is the same as in example one - stage 1.\nIn our case, the allocated directory on the physical disk is: /jenkins-project/disk1/upstream/14.\n\nStage 2. Build in the upstream job\n\nSame functionality as example one - stage 2.\nThe final workspace path is: /linux-node/disk1/jenkins-project/disk1/upstream/14.\n\nStage 3. Allocate workspace in the downstream job\n\nBy passing the upstream parameter to the exwsAllocate step,\nit selects the most recent stable upstream workspace (default behavior).\nThe workspace path pattern is like this: /physicalPathOnDisk/$UPSTREAM_NAME/$MOST_RECENT_STABLE_BUILD.\nLet’s assume that the last stable build number is 12, then the resulting path is:\n/jenkins-project/disk1/upstream/12.\n\nStage 4. Run tests in the downstream job\n\nThe exws step concatenates the node’s local path with the path returned by the exwsAllocate step in stage 3.\nIn this scenario, the complete path for running tests is: /test-node/disk1/jenkins-project/disk1/upstream/12.\nIt will reuse the workspace defined in the upstream job.\n\nAdditional details\n\nYou may find the complete project proposal, along with the design details, features, more examples and use cases,\nimplementation ideas and milestones in the design document.\nThe plugin repository will be available on GitHub.\n\nA prototype version of the plugin should be available in late June and the releasable version in late August.\nI will be holding plugin functionality demos within the community.\n\nI do appreciate any feedback.\nYou may add comments in the design document.\nIf you are interested to have a verbal conversation, feel free to join our regular meetings on Mondays at\n12:00 PM UTC\non the Jenkins hangout.\nI will be posting updates from time to time about the plugin status on the\nJenkins developers mailing list.\n\nLinks\n\nDesign document\n\nGSoC program\n\nJenkins GSoC Page\n\nProject repository","title":"GSoC Project Intro: External Workspace Manager Plugin","tags":["pipeline","plugins","gsoc"],"authors":[{"avatar":null,"blog":null,"github":"alexsomai","html":"","id":"alexsomai","irc":null,"linkedin":null,"name":"Alexandru Somai","slug":"/blog/authors/alexsomai","twitter":"alex_somai"}]}},{"node":{"date":"2016-04-22T00:00:00.000Z","id":"4e17de3e-4be0-59e9-b38e-1f1bbf9188e4","slug":"/blog/2016/04/22/pipeline-2.x/","strippedHtml":"Those of you who routinely apply all plugin updates may already have noticed that the version numbers of the plugins in the Pipeline suite have switched to a 2.x scheme. Besides aligning better with the upcoming Jenkins 2.0 core release, the plugins are now being released with independent lifecycles.\n\n“Pipeline 1.15” (the last in the 1.x line) included simultaneous releases of a dozen or so plugins with the 1.15 version number (and 1.15+ dependencies on each other). All these plugins were built out of a single workflow-plugin repository. While that was convenient in the early days for prototyping wide-ranging changes, it has become an encumbrance now that the Pipeline code is fairly mature, and more people are experimenting with additions and patches.\n\nAs of 2.0, all the plugins in the system live in their own repositories on GitHub—named to match the plugin code name, which in most cases uses the historical workflow term, so for example workflow-job-plugin. Some complex steps were moved into their own plugins, such as pipeline-build-step-plugin. The 1.x changelog is closed; now each plugin keeps a changelog in its own wiki, for example here for the Pipeline Job plugin.\n\nAmong other benefits, this change makes it easier to cut new plugin releases for even minor bug fixes or enhancements, or for developers to experiment with patches to certain plugins. It also opens the door for the “aggregator” plugin (called simply Pipeline) to pull in dependencies on other plugins that seem broadly valuable, like the stage view.\n\nThe original repository has been renamed pipeline-plugin and for now still holds some documentation, which might later be moved to jenkins.io.\n\nYou need not do anything special to “move” to the 2.x line; 1.642.x and later users can just accept all Pipeline-related plugin updates. Note that if you update Pipeline Supporting APIs you must update Pipeline, or at least install/update some related plugins as noted in the wiki.","title":"Pipeline 2.x plugins","tags":["pipeline","jenkins2"],"authors":[{"avatar":null,"blog":null,"github":"jglick","html":"<div class=\"paragraph\">\n<p>Jesse has been developing Jenkins core and plugins for years.\nHe is the coauthor with Kohsuke of the core infrastructure of the Pipeline system.</p>\n</div>","id":"jglick","irc":null,"linkedin":null,"name":"Jesse Glick","slug":"/blog/authors/jglick","twitter":"tyvole"}]}},{"node":{"date":"2016-04-21T00:00:00.000Z","id":"d9ce8340-eedf-5dba-874d-c9eba3f8e717","slug":"/blog/2016/04/21/dsl-plugins/","strippedHtml":"In this post I will show how you can make your own DSL extensions and distribute\nthem as a plugin, using Pipeline Script.\n\nA quick refresher\n\nPipeline has a well kept secret: the ability to add your own DSL\nelements. Pipeline is itself a DSL, but you can extend it.\n\nThere are 2 main reasons I can think you may want to do this:\n\nYou want to reduce boilerplate by encapsulating common snippets/things you do\nin one DSL statement.\n\nYou want to provide a DSL that provides a prescriptive way that your builds\nwork - uniform across your organisations Jenkinsfiles.\n\nA DSL could look as simple as\n\nacmeBuild {\n    script = \"./bin/ci\"\n    environment = \"nginx\"\n    team = \"evil-devs\"\n    deployBranch = \"production\"\n}\n\nThis could be the entirety of your Jenkinsfile!\n\nIn this \"simple\" example, it could actually be doing a multi stage build with\nretries, in a specified docker container, that deploys only from the production\nbranch.  Detailed notifications are sent to the right team on important events\n(as defined by your org).\n\nTraditionally this is done via the\nglobal\nlibrary.  You take a snippet of DSL you want to want to make into a DSL, and\ndrop it in the git repo that is baked into Jenkins.\n\nA great trivial\nexample\nis this:\n\njenkinsPlugin {\n    name = 'git'\n}\n\nWhich is enabled by git pushing the following into vars/jenkinsPlugin.groovy\n\nThe name of the file is the name of the DSL expression you use in the Jenkinsfile\n\ndef call(body) {\n    def config = [:]\n    body.resolveStrategy = Closure.DELEGATE_FIRST\n    body.delegate = config\n    body()\n\n    // This is where the magic happens - put your pipeline snippets in here, get variables from config.\n    node {\n        git url: \"https://github.com/jenkinsci/${config.name}-plugin.git\"\n        sh \"mvn install\"\n        mail to: \"...\", subject: \"${config.name} plugin build\", body: \"...\"\n    }\n}\n\nYou can imagine many more pipelines, or even archetypes/templates of pipelines\nyou could do in this way, providing a really easy Jenkinsfile syntax for your\nusers.\n\nMaking it a plugin\n\nUsing the global DSL library is a handy thing if you have a single Jenkins, or\nwant to keep the DSLs local to a Jenkins instance.  But what if you want to\ndistribute it around your org, or, perhaps it is general purpose enough you want\nto share it with the world?\n\nWell this is possible, by wrapping it in a plugin. You use the same pipeline\nsnippet tricks you use in the global lib, but put it in the dsl directory of a\nplugin.\n\nMy simple\nbuild plugin shows how it is done.  To make your own plugin:\n\nCreate a new plugin project, either fork the simple build one, or add a\ndependency to it in your pom.xml / build.gradle file\n\nPut your dsl in the resources directory in a similar fashion to\nthis\n(note the \"package dsl\" declaration at the top)\n\nCreate the equivalent extension that just points to the DSL by name like\nthis\nThis is mostly \"boiler plate\" but it tells Jenkins there is a GlobalVariable extension available when Pipelines run.\n\nDeploy it to an Jenkins Update Center to share with your org, or everyone!\n\nThe advantage of delivering this DSL as a plugin is that it has a version (you\ncan also put tests in there), and distributable just like any other plugin.\n\nFor the more advanced, Andrew Bayer has a Simple\nTravis Runner plugin that\ninterprets and runs\ntravis.yml files which is also implemented in pipeline.\n\nSo, approximately, you can build plugins for pipeline that extend pipeline, in\npipeline script (with a teeny bit of boiler plate).\n\nEnjoy!","title":"Making your own DSL with plugins, written in Pipeline script","tags":["jenkins","dsl","pipeline","plugins"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/19e71/michaelneale.jpg","srcSet":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/77b35/michaelneale.jpg 32w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/d4a57/michaelneale.jpg 64w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/19e71/michaelneale.jpg 128w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/ef6ff/michaelneale.webp 32w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/8257c/michaelneale.webp 64w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/6766a/michaelneale.webp 128w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"michaelneale","html":"<div class=\"paragraph\">\n<p>Michael is a CD enthusiast with a interest in User Experience.\nHe is a co-founder of CloudBees and a long time OSS developer, and can often be found\nlurking around the jenkins-dev mailing list or #jenkins on irc (same nick as twitter name).\nBefore CloudBees he worked at Red Hat.</p>\n</div>","id":"michaelneale","irc":null,"linkedin":null,"name":"Michael Neale","slug":"/blog/authors/michaelneale","twitter":"michaelneale"}]}},{"node":{"date":"2016-04-15T00:00:00.000Z","id":"fa2f2d1b-91d5-5e2e-9783-72c7059face6","slug":"/blog/2016/04/15/the-need-for-pipeline/","strippedHtml":"This is a cross-post of\nan article authored\nby Viktor Farcic on the\nCloudBees blog. Viktor is also the author\nof The DevOps 2.0 Toolkit, which\nexplores Jenkins, the Pipeline plugin, and the ecosystem\naround it in much more detail.\n\nOver the years, Jenkins has become the undisputed ruler among continuous\nintegration (CI), delivery and deployment (CD) tools. It, in a way, defined the\nCI/CD processes we use today. As a result of its leadership, many other products\nhave tried to overthrow it from its position. Among others, we got Bamboo and\nTeam City attempting to get a piece of the market. At the same time, new\nproducts emerged with a service approach (as opposed to on-premises). Some of\nthem are Travis, CircleCI and Shippable. Be that as it may, none managed to get\neven close to Jenkins' adoption. Today, depending on the source we use, Jenkins\nholds between 50-70% of the whole CI/CD tools market. The reason behind such a\nhigh percentage is its dedication to open source principles set from the very\nbeginning by Kohsuke Kawaguchi. Those same principles were the reason he forked\nJenkins from Hudson. The community behind the project, as well as commercial\nentities behind enterprise versions, are continuously improving the way it works\nand adding new features and capabilities. They are redefining not only the way\nJenkins behaves but also the CI/CD practices in a much broader sense. One of\nthose new features is the Jenkins Pipeline plugin. Before we\ndive into it, let us take a step back and discuss the reasons that led us to\ninitiate the move away from Freestyle jobs and towards the Pipeline.\n\nThe Need for Change\n\nOver time, Jenkins, like most other self-hosted CI/CD tools, tends to accumulate\na vast number of jobs. Having a lot of them causes quite an increase in\nmaintenance cost. Maintaining ten jobs is easy. It becomes a bit harder (but\nstill bearable) to manage a hundred. When the number of jobs increases to\nhundreds or even thousands, managing them becomes very tedious and time\ndemanding.\n\nIf you are not proficient with Jenkins (or other CI/CD tools) or you do not work\nfor a big project, you might think that hundreds of jobs is excessive. The truth\nis that such a number is reached over a relatively short period when teams\nare practicing continuous delivery or deployment. Let’s say that an average\nCD flow has the following set of tasks that should be run on each commit:\nbuilding, pre-deployment testing, deployment to a staging environment,\npost-deployment testing and deployment to production. That’s five groups of\ntasks that are often divided into, at least, five separate Jenkins jobs. In\nreality, there are often more than five jobs for a single CD flow, but let\nus keep it an optimistic estimate. How many different CD flows does a medium\nsized company have? With twenty, we are already reaching a three digits\nnumber. That’s quite a lot of  jobs to cope with even though the estimates\nwe used are too optimistic for all but the smallest entities.\n\nNow, imagine that we need to change all those jobs from, let’s say, Maven to\nGradle. We can choose to start modifying them through the Jenkins UI, but that\ntakes too much time. We can apply changes directly to Jenkins XML files that\nrepresent those jobs but that is too complicated and error prone. Besides,\nunless we write a script that will do the modifications for us, we would\nprobably not save much time with this approach. There are quite a few plugins\nthat can help us to apply changes to multiple jobs at once, but none of them is\ntruly successful (at least among free plugins). They all suffer from one\ndeficiency or another. The problem is not whether we have the tools to perform\nmassive changes to our jobs, but whether jobs are defined in a way that they can\nbe easily maintained.\n\nBesides the sheer number of Jenkins jobs, another critical Jenkins' pain point\nis centralization. While having everything in one location provides a lot of\nbenefits (visibility, reporting and so on), it also poses quite a few\ndifficulties. Since the emergence of agile methodologies, there’s been a huge\nmovement towards self-sufficient teams. Instead of horizontal organization with\nseparate development, testing, infrastructure, operations and other groups, more\nand more companies are moving (or already moved) towards self-sufficient teams\norganized vertically. As a result, having one centralized place that defines all\nthe CD flows becomes a liability and often impedes us from splitting teams\nvertically based on projects. Members of a team should be able to collaborate\neffectively without too much reliance on other teams or departments. Translated\nto CD needs, that means that each team should be able to define the deployment\nflow of the application they are developing.\n\nFinally, Jenkins, like many other tools, relies heavily on its UI. While that is\nwelcome and needed as a way to get a visual overview through dashboards and\nreports, it is suboptimal as a way to define the delivery and deployment flows.\nJenkins originated in an era when it was fashionable to use UIs for everything.\nIf you worked in this industry long enough you probably saw the swarm of tools\nthat rely completely on UIs, drag & drop operations and a lot of forms that\nshould be filled. As a result, we got tools that produce artifacts that cannot\nbe easily stored in a code repository and are hard to reason with when anything\nbut simple operations are to be performed. Things changed since then, and now we\nknow that many things (deployment flow being one of them) are much easier to\nexpress through code. That can be observed when, for example, we try to define a\ncomplex flow through many Jenkins jobs. When deployment complexity requires\nconditional executions and some kind of a simple intelligence that depends on\nresults of different steps, chained jobs are truly complicated and often\nimpossible to create.\n\nAll things considered, the major pain points Jenkins had until recently are as\nfollows.\n\nTendency to create a vast number of jobs\n\nRelatively hard and costly maintenance\n\nCentralization of everything\n\nLack of powerful and easy ways to specify deployment flow through code\n\nThis list is, by no means, unique to Jenkins. Other CI/CD tools have at least\none of the same problems or suffer from deficiencies that Jenkins solved a long\ntime ago. Since the focus of this article is Jenkins, I won’t dive into a\ncomparison between the CI/CD tools.\n\nLuckily, all those, and many other deficiencies are now a thing of the past.\nWith the emergence of the\nPipeline\nplugin and many others that were created on\ntop of it, Jenkins entered a new era and proved itself as a dominant player in\nthe CI/CD market. A whole new ecosystem was born, and the door was opened for\nvery exciting possibilities in the future.\n\nBefore we dive into the Jenkins Pipeline and the toolset that surrounds it, let\nus quickly go through the needs of a modern CD flow.\n\nContinuous Delivery or Deployment Flow with Jenkins\n\nWhen embarking on the CD journey for the first time, newcomers tend to think\nthat the tasks that constitute the flow are straightforward and linear. While\nthat might be true with small projects, in most cases things are much more\ncomplicated than that. You might think that the flow consists of building,\ntesting and deployment, and that the approach is linear and follows the\nall-or-nothing rule. Build invokes testing and testing invokes deployment. If\none of them fails, the developer gets a notification, fixes the problem and\ncommits the code that will initiate the repetition of the process.\n\nIn most instances, the process is far more complex. There are many tasks to run,\nand each of them might produce a failure. In some cases, a failure should only\nstop the process. However, more often than not, some additional logic should be\nexecuted as part of the after-failure cleanup. For example, what happens if\npost-deployment tests fail after a new release was deployed to production? We\ncannot just stop the flow and declare the build a failure. We might need to\nrevert to the previous release, rollback the proxy, de-register the service and\nso on. I won’t go into many examples of situations that require complex flow\nwith many tasks, conditionals that depend on results, parallel execution and so\non. Instead, I’ll share a diagram of one of the flows I worked on.\n\nSome tasks are run in one of the testing servers (yellow) while others are run\non the production cluster (blue). While any task might produce an error, in some\ncases such an outcome triggers a separate set of tasks. Some parts of the flow\nare not linear and depend on task results. Some tasks should be executed in\nparallel to improve the overall time required to run them. The list goes on and\non. Please note that this discussion is not about the best way to execute the\ndeployment flow but only a demonstration that the complexity can be, often, very\nhigh and cannot be solved by a simple chaining of Freestyle jobs. Even in cases\nwhen such chaining is possible, the maintenance cost tends to be very high.\n\nOne of the CD objectives we are unable to solve through chained jobs, or is\nproved to be difficult to implement, is conditional logic. In many cases, it is\nnot enough to simply chain jobs in a linear fashion. Often, we do not want only\nto create a job A that, once it’s finished running, executes job B, which, in\nturn, invokes job C. In real-world situations, things are more complicated than\nthat. We want to run some tasks (let’s call them job A), and, depending on the\nresult, invoke jobs B1 or B2, then run in parallel C1, C2 and C3, and, finally,\nexecute job D only when all C jobs are finished successfully. If this were a\nprogram or a script, we would have no problem accomplishing something like that,\nsince all modern programming languages allow us to employ conditional logic in a\nsimple and efficient way. Chained Jenkins jobs, created through its UI, pose\ndifficulties to create even a simple conditional logic. Truth be told, some\nplugins can help us with conditional logic. We have Conditional Build Steps,\nParameterised Trigger, Promotions and others. However, one of the major issues\nwith these plugins is configuration. It tends to be scattered across multiple\nlocations, hard to maintain and with little visibility.\n\nResource allocation needs a careful thought and is, often, more complicated than\na simple decision to run a job on a predefined agent. There are cases when agent\nshould be decided dynamically, workspace should be defined during runtime and\ncleanup depends on a result of some action.\n\nWhile a continuous deployment process means that the whole pipeline ends with\ndeployment to production, many businesses are not ready for such a goal or have\nuse-cases when it is not appropriate. Any other process with a smaller scope, be\nit continuous delivery or continuous integration, often requires some human\ninteraction. A step in the pipeline might need someone’s confirmation, a failed\nprocess might require a manual input about reasons for the failure, and so on.\nThe requirement for human interaction should be an integral part of the pipeline\nand should allow us to pause, inspect and resume the flow. At least, until we\nreach the true continuous deployment stage.\n\nThe industry is, slowly, moving towards microservices architectures. However,\nthe transformation process might take a long time to be adopted, and even more\nto be implemented. Until then, we are stuck with monolithic applications that\noften require a long time for deployment pipelines to be fully executed. It is\nnot uncommon for them to run for a couple of hours, or even days. In such cases,\nfailure of the process, or the whole node the process is running on, should not\nmean that everything needs to be repeated. We should have a mechanism to\ncontinue the flow from defined checkpoints, thus avoiding costly repetition,\npotential delays and additional costs. That is not to say that long-running\ndeployment flows are appropriate or recommended. A well-designed CD process\nshould run within minutes, if not seconds. However, such a process requires not\nonly the flow to be designed well, but also the architecture of our applications\nto be changed. Since, in many cases, that does not seem to be a viable option,\nresumable points of the flow are a time saver.\n\nAll those needs, and many others, needed to be addressed in Jenkins if it was to\ncontinue being a dominant CI/CD tool. Fortunately, developers behind the project\nunderstood those needs and, as a result, we got the Jenkins Pipeline plugin. The\nfuture of Jenkins lies in a transition from Freestyle chained jobs to a single\npipeline expressed as code. Modern delivery flows cannot be expressed and easily\nmaintained through UI drag 'n drop features, nor through chained jobs. They can\nneither be defined through YAML (Yet Another Markup Language) definitions\nproposed by some of the newer tools (which I’m not going to name). We need to go\nback to code as a primary way to define not only the applications and services\nwe are developing but almost everything else. Many other types of tools adopted\nthat approach, and it was time for us to get that option for CI/CD processes as\nwell.","title":"The Need For Jenkins Pipeline","tags":["jenkins2","pipeline"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-04-14T00:00:00.000Z","id":"44aa3a43-715d-549b-b0d2-cc5c5bd94725","slug":"/blog/2016/04/14/replay-with-pipeline/","strippedHtml":"This is a cross-post of\nan article authored by\nPipeline plugin maintainer Jesse Glick on the\nCloudBees blog.\n\nFor those of you not checking their Updates tab obsessively, Pipeline 1.14 [up\nto 2.1 now] was\nreleased\na couple of weeks ago and I wanted to highlight the major feature in this\nrelease: JENKINS-32727,\nor replay. Some folks writing \"Jenkinsfiles\" in the field had grumbled that it\nwas awkward to develop the script incrementally, especially compared to jobs\nusing inline scripts stored in the Jenkins job configuration: to try a change to\nthe script, you had to edit Jenkinsfile in SCM, commit it (perhaps to a\nbranch), and then go back to Jenkins to follow the output. Now this is a little\neasier. If you have a Pipeline build which did not proceed exactly as you\nexpected, for reasons having to do with Jenkins itself (say, inability to find &\npublish test results, as opposed to test failures you could reproduce locally),\ntry clicking the Replay link in the build’s sidebar. The quickest way to try\nthis for yourself is to run the\nstock CD demo in its\nlatest release:\n\n$ docker run --rm -p 2222:2222 -p 8080:8080 -p 8081:8081 -p 9418:9418 -ti jenkinsci/workflow-demo:1.14-3\n\nWhen you see the page Replay\n#1 , you are shown two\n(Groovy) editor boxes: one for the main\nJenkinsfile , one for a library script\nit loaded\n( servers.groovy , introduced to help demonstrate this feature). You\ncan make edits to either or both. For example, the original demo allocates a\ntemporary web application with a random name like\n9c89e9aa-6ca2-431c-a04a-6599e81827ac for the duration of the functional tests.\nPerhaps you wished to prefix the application name with tmp- to make it obvious\nto anyone encountering the Jetty index page that these\nURLs are transient. So in the second text area, find the line\n\ndef id = UUID.randomUUID().toString()\n\nand change it to read\n\ndef id = \"tmp-${UUID.randomUUID()}\"\n\nthen click Run. In\nthe new build’s log\nyou will now see\n\nReplayed #1\n\nand later something like\n\n… test -Durl=http://localhost:8081/tmp-812725bb-74c6-41dc-859e-7d9896b938c3/ …\n\nwith the improved URL format. Like the result? You will want to make it\npermanent. So jump to the [second build’s index\npage]( http://localhost:8080/job/cd/branch/master/2/) where you will see a note\nthat this build > Replayed #1 (diff) If you\nclick on diff you\nwill see:\n\n--- old/Script1\n+++ new/Script1\n@@ -8,7 +8,7 @@\n }\n\n def runWithServer(body) {\n-    def id = UUID.randomUUID().toString()\n+    def id = \"tmp-${UUID.randomUUID()}\"\n     deploy id\n     try {\n         body.call id\n\nso you can know exactly what you changed from the last-saved version. In fact if you replay #2 and change tmp to temp in the loaded script, in the diff view for #3 you will see the diff from the first build, the aggregate diff:\n\n--- old/Script1\n+++ new/Script1\n@@ -8,7 +8,7 @@\n }\n\n def runWithServer(body) {\n-    def id = UUID.randomUUID().toString()\n+    def id = \"temp-${UUID.randomUUID()}\"\n     deploy id\n     try {\n         body.call id\n\nAt this point you could touch up the patch to refer to servers.groovy\n( JENKINS-31838), git\napply it to a clone of your repository, and commit. But why go to the trouble\nof editing Groovy in the Jenkins web UI and then manually copying changes back\nto your IDE, when you could stay in your preferred development environment from\nthe start?\n\n$ git clone git://localhost/repo\nCloning into 'repo'...\nremote: Counting objects: 23, done.\nremote: Compressing objects: 100% (12/12), done.\nremote: Total 23 (delta 1), reused 0 (delta 0)\nReceiving objects: 100% (23/23), done.\nResolving deltas: 100% (1/1), done.\nChecking connectivity... done.\n$ cd repo\n$ $EDITOR servers.groovy\n# make the same edit as previously described\n$ git diff\ndiff --git a/servers.groovy b/servers.groovy\nindex 562d92e..63ea8d6 100644\n--- a/servers.groovy\n+++ b/servers.groovy\n@@ -8,7 +8,7 @@ def undeploy(id) {\n }\n\n def runWithServer(body) {\n-    def id = UUID.randomUUID().toString()\n+    def id = \"tmp-${UUID.randomUUID()}\"\n     deploy id\n     try {\n         body.call id\n$ ssh -p 2222 -o StrictHostKeyChecking=no localhost replay-pipeline cd/master -s Script1 webapp-naming\n\nUsing the replay-pipeline CLI command (in this example via\nSSH)\nyou can prepare, test, and commit changes to your Pipeline script code without\ncopying anything to or from a browser. That is all for now. Enjoy!","title":"Replay a Pipeline with script edits","tags":["jenkins2","pipeline"],"authors":[]}}]}},"pageContext":{"tag":"pipeline","limit":8,"skip":72,"numPages":13,"currentPage":10}},
    "staticQueryHashes": ["3649515864"]}