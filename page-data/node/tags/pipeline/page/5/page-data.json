{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/pipeline/page/5",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2018-02-22T00:00:00.000Z","id":"d0c104fc-7582-5b71-ae72-ad7781b4796d","slug":"/blog/2018/02/22/cheetah/","strippedHtml":"Table of Contents\n\nIntroducing \"Project Cheetah\"\nYes, but what does it DO?\nHow Do I Set Speed/Durability Settings?\n\n1. Globally, you can choose a global default durability setting:\n2. Each Pipeline can get a custom Durability Setting:\n3. Multibranch Projects can use a new BranchProperty to customize the Durability Setting.\n\nWill Performance-Optimized Mode Help Me?\nOther Goodies\nHow Did You Do It?\nWhat Next?\n\nSince it launched, Pipeline has had a bit of a Dr. Jekyll and Mr. Hyde performance problem.  In certain circumstances, Pipeline can turn from a mild-mannered CI/CD assistant into a monster.  It will happily eat storage read/write capacity like popcorn without caring about the other concerns of our friendly butler.  When combined with other additional factors, this can result in real-world stability problems.  For example, combining slow storage with a spike in running Pipelines has brought down production Jenkins at more than one organization.  Similarly, users see issues if a busy controller gets hit with an extra source of stress; past culprits have been heavy automated (ab)use of Jenkins APIs, now-solved user lookup bugs, backup jobs, and plugins run crazy that load excessive numbers of builds.  Symptoms ranged from visible slowdowns in the UI to unresponsive jobs and \"hung\" controllers.\n\nNow I’m not saying this to scare people or to criticize the technology we’ve built. Implementing Pipeline scalability best practices coupled with SSD storage keeps Jenkins in a happy place.  We just need context on the weaknesses to see why it’s important to address them.\n\nIntroducing \"Project Cheetah\"\n\nToday we’re announcing the first major results of \"Project Cheetah\", our long-running effort to address these challenges and improve Pipeline scalability.  More broadly, Cheetah aims to help in 3 places:\n\nSmall-scale containers: Pipeline needs to run leanly in resource-constrained containers, to enable easy scale-out without consuming excessive resources on shared container hosts.\n\nEnterprise systems: Pipeline needs to effectively serve high-scale Jenkins instances that are central to many large companies.\n\nGeneral case: run Pipelines a bit more quickly on average, and allow users to get much-stronger performance in worst-case scenarios.\n\nThese changes are implemented across many of the Pipeline plugins.\n\nYes, but what does it DO?\n\nProject Cheetah offers several things, but the most important is Durability Settings for all Pipelines, and especially the Performance-Optimized setting.  This setting avoids several potentially unexpected performance \"surprises\" that may strike users.  In the general case, it greatly reduces the disk IO needs for Pipeline.  How much?  Below is a graph of storage utilization with legacy Pipeline versions (think early 2017) and with the latest version using the Performance-Optimized mode.  These are tested on an AWS instance backed by an EBS volume provisioned with 300 IOPs.\n\nBefore and After:\n\nAs you can see, storage utilization goes down by a lot.  While the exact number will vary, across the benchmark testcases this results in Pipeline throughput of 2x to 6x the previous before becoming IO-bound. This also increases stability of Jenkins controllers because they will tolerate unexpected load.\n\nThis comes with a major drop in CPU IOWait as well:\n\nAnd of course the rate at which data is written to disk and number of writes/s is also reduced:\n\nFor enterprise users, timing stats often show 10-20% of normal builds is serializing the Program and writing the record of steps run (\"FlowNodes\") - the performance optimized durability setting will cut this to almost nothing (for standard pipelines, 1/100 or less) - so builds will complete faster, especially complex ones.\n\nPlease see the Pipeline Scalability documentation for deeper information on the new Durability Settings, how to use them, and which plugin versions are required to gain these features.\n\nAlso, users may see a reduction in hung Pipelines because new test utilities made it possible to identify and correct a variety of bugs.\n\nHow Do I Set Speed/Durability Settings?\n\nThere are 3 ways to configure the durability setting:\n\n1. Globally, you can choose a global default durability setting:\n\nUnder \"Manage Jenkins\" > \"Configure System\", labelled \"Pipeline Speed/Durability Settings\".  You can override these with the more specific settings below.\n\n2. Each Pipeline can get a custom Durability Setting:\n\nThis is one of the job properties located at the top of the job configuration, labelled \"Custom Pipeline Speed/Durability Level.\" This overrides the global setting. Or, use a \"properties\" step - the setting will apply to the NEXT run after the step is executed (same result).\n\n// Script //\nproperties([durabilityHint('PERFORMANCE_OPTIMIZED')])\n// Declarative //\npipeline {\n    agent any\n    stages {\n        stage('Example') {\n            steps {\n                echo 'Hello World'\n            }\n        }\n    }\n    options {\n        durabilityHint('PERFORMANCE_OPTIMIZED')\n    }\n}\n\n3. Multibranch Projects can use a new BranchProperty to customize the Durability Setting.\n\nUnder the SCM you can configure a custom Branch Property Strategy and add a property for Custom Pipeline Speed/Durability Level.  This overrides the global Durability Setting and will apply to each branch at the next run.  You can also use a \"properties\" step to override the setting, but remember that you may have to run the step again to undo this.\n\nDurability settings will take effect with the next applicable Pipeline run, not immediately.  The setting will be displayed in the log.\n\nThere is a slight durability trade-off for using the Performance-Optimized mode — the appropriate section of the Pipeline Scalability documentation has the specifics.\nFor most uses we do not expect this to be important, but there are a few specific cases where users may wish to use a slower/higher-durability setting. The Best Practices are documented.\n\nWe recommend using Performance-Optimized by default, but because it does represent a slight behavioral change the initial \"Cheetah\" plugin releases defaults to maintain previous behavior. We expect to switch this default in the future with appropriate notice once people have a chance to get used to the new settings.\n\nWill Performance-Optimized Mode Help Me?\n\nYes, if your Jenkins instance uses NFS, magnetic storage, runs many Pipelines at once, or shows high iowait (above 5%)\n\nYes, if you are running Pipelines with many steps (more than several hundred).\n\nYes, if your Pipeline stores large files or complex data to variables in the script, keeps that variable in scope for future use, and then runs steps.  This sounds oddly specific but happens more than you’d expect.\n\nFor example: readFile step with a large XML/JSON file, or using configuration information from parsing such a file with One of the Utility Steps.\n\nAnother common pattern is a \"summary\" object containing data from many branches (logs, results, or statistics). Often this is visible because you’ll be adding to it often via an add/append or Map.put() operations.\n\nLarge arrays of data or Maps of configuration information are another common example of this situation.\n\nNo, if your Pipelines spend almost all their time waiting for a few shell/batch steps to finish.  This ISN’T a magic \"go fast\" button for everything!\n\nNo, if Pipelines are writing massive amounts of data to logs (logging is unchanged).\n\nNo, if you are not using Pipelines, or your system is loaded down by other factors.\n\nNo, if you don’t enable higher-performance modes for pipelines.  See above for how!\n\nOther Goodies\n\nUsers can now set an optional job property so that individual Pipelines fail cleanly rather than resuming upon restarting the controller.  This is useful for niche cases where some Pipelines are considered disposable and users would value a clean restart over Pipeline durability.\n\nWe’ve reduced classloading and reflection quite significantly, which improves scaling and reduces CPU use:\n\nScript Security (as of version 1.41) has gotten optimizations to reduce the performance overhead of Sandbox mode and eliminate lock contention so Pipeline multithreads better.\n\nPipeline Step data uses up less space on disk (regardless of the durability setting) - this should be 30% smaller.  Assume it’s a few MB per 1000 steps - but for every build after the change.\n\nEven in the low-performance/high-durability modes, some redundant writes have been removed, which decreases the number of writes by 10-20%.\n\nHow Did You Do It?\n\nThat’s probably material for another blog post or Jenkins World talk.\n\nThe short answer is: first we built a tool to simulate a full production environment and provide detailed metrics collection at scale.  Then we profiled Jenkins to identify bottlenecks and attacked them.  Rinse and repeat.\n\nWhat Next?\n\nThe next big change, which I’m calling Cheetah Part 2 is to address Pipeline’s logging. For every Step run, Pipeline writes one or more small log files. These log files are then copied into the build log content, but are retained to make it possible to easily fetch logs for each step.\n\nThis copying process means every log line is written twice, greatly reducing performance, and writing to many small files is orders of magnitude slower than appending to one big log file.\n\nWe’re going to remove this duplication and data fragmentation and use a more efficient mechanism to find per-step logs. This should further improve the ability to run Pipelines on NFS mounts and hard-drive-backed storage, and should significantly improve performance at scale.\n\nBesides this, there’s a variety of different tactical improvements to improve scaling behavior and reduce resource needs.\n\nThe Project Cheetah work doesn’t free users to completely ignore Pipeline scaling best practices and previous suggestions.  Nor does it eliminate the need for efficient GC settings.  But this and other enhancements from the last year can significantly improve the storage situation for most users and reduce the penalties for worst-case behaviors.  When you add all the pieces together, the result is a faster, leaner, more reliable Pipeline experience.","title":"Project Cheetah - Faster, Leaner Pipeline That Can Keep Up With Demand","tags":["pipeline","performance","scalability"],"authors":[{"avatar":null,"blog":null,"github":"svanoort","html":"","id":"svanoort","irc":null,"linkedin":null,"name":"Sam Van Oort","slug":"/blog/authors/svanoort","twitter":null}]}},{"node":{"date":"2017-12-15T00:00:00.000Z","id":"c2ac3d30-6b1c-5338-a437-133b5ebc730e","slug":"/blog/2017/12/15/auto-convert-freestyle-jenkins-jobs-to-coded-pipeline/","strippedHtml":"This is a guest post by Sanil Pillai, Director of Labs & Strategic Insights, Infostretch\n\nInfostretch has created a\nplugin for teams\nupgrading from Freestyle Jobs to Pipelines as code with Jenkins Pipeline.\nThis new plugin streamlines the process and accelerates\npipeline on-boarding for any new set of applications. Previously, when\nupgrading to Jenkins Pipeline, converting Freestyle Jobs required developers\nto drill down on each one of those hundreds (or thousands!)  of jobs to understand\ntools, configurations, URLs, parameters, and more before rewriting them in\nPipeline syntax. This process is very manual,\nerror-prone, lengthy, and not cost-effective. Beyond saving time, the new\nplugin also assures adherence to proper coding standards and separates complex\nbusiness logic and standards declaration from execution flow.\n\nKey features:\n\nConvert single freestyle job to pipeline\n\nConvert chain of freestyle jobs to single pipeline\n\nWorks with both Jenkins and CloudBees Jenkins Enterprise\n\nPlugin can be customized to support any Freestyle plugin and an\norganization’s Pipeline Shared Library,\nor Groovy coding standards.\n\nWorks with CloudBees' Role-based Access Control to help the new Pipelines\ncomply with existing security policies.\n\nDirect migration of properties such as \"Build with Parameters\" to newly\ncreated Pipelines.\n\nDirect migration of Agent on which job is to be run with support for multiple agent labels across different downstream jobs\n\nEnvironment properties: JDK, NodeJS\n\nSupports Git SCM.\n\nBuild steps: Maven, Ant, Shell, Batch, and Ansible Playbook.\n\nPost build actions: artifact archiver, simple mailer, TestNG reports, JUnit reports, checkstyle publisher\n\nNow, let’s take a look at how to get started:\n\nClick on a link at Root level or Folder level or Job level.\n\nSelect the job from the drop-down list that is the beginning point of the\n\"chain\". If job level link is clicked, this drop-down list will not be visible.\n\nProvide the new pipeline job name. If this is not specified, the plugin will\nattempt to create a new pipeline job with the naming convention of\n\"oldname-pipeline\".\n\nCheck \"Recursively convert downstream jobs if any?\" if you wish to have all the\ndownstream jobs converted into this new pipeline. The plugin will write all the\nlogic of current and downstream jobs into a single pipeline.\n\nCheck \"Commit Jenkinsfile?\" if you would like the plugin to create a\nJenkinsfile and commit it back to the SCM. The plugin will commit the\nJenkinsfile at the root of the SCM repository it finds in the first job\n(selected in step 1 above). It will attempt to commit to this repo using the\ncredentials it finds in the first job.\n\nDo note that the plugin will checkout the repo in to a temporary workspace on\nthe controller (JENKINS_HOME/plugins/convert-to-    pipeline/ws). Once the\nconversion is complete and Jenkinsfile is committed back to the repo, the\nworkspace will be deleted.\n\nClick \"Convert\" to convert the Freestyle job configurations to a single\nscripted pipeline job. Once the conversion is complete and the new job is\ncreated, you will be redirected to the newly created pipeline job.\n\nThat’s it!\n\nTo learn more about plugin usage, customization and to see a demo\nclick here\nto watch the webinar replay on-demand.","title":"Auto-Convert Freestyle Jobs to Jenkins Pipeline","tags":["pipeline","freestyle"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"<div class=\"paragraph\">\n<p>Director of Labs &amp; Strategic Insights, Infostretch</p>\n</div>","id":"spillai","irc":null,"linkedin":null,"name":"Sanil Pillai","slug":"/blog/authors/spillai","twitter":null}]}},{"node":{"date":"2017-11-27T00:00:00.000Z","id":"fd21dbc6-9b4b-5092-b232-0feabaf928ce","slug":"/blog/2017/11/27/tutorials-in-the-jenkins-user-documentation/","strippedHtml":"Regular perusers of the Jenkins User Documentation may have noticed\nthe presence of the Tutorials part (between the Guided Tour and User\nHandbook) that appeared in the last couple of months and gradually began to get\npopulated with much of my recent work, writing Jenkins tutorials.\n\nMy name’s Giles and I’ve been a technical writer in the software development\nfield for several years now. I’ve always been passionate about technical writing\nand more recently, the technologies that go into developing written content and\nautomating its generation - like Jenkins! I was a former Atlassian and recently\njoined CloudBees as a Senior Technical Writer, working remotely from the \"Sydney\nOffice\", with my current focus on the Jenkins User Documentation.\n\nWhy tutorials?\n\nMy exposure to Jenkins and its usage over the years has been patchy at best.\nDuring this time, however, I’ve had some degree of experience as a user of\nvarious continuous delivery (CD) tools like Jenkins and am reasonably familiar\nwith the advantages these tools can offer software development teams.\n\nI’ve also found that while many software developers are familiar with the\nbroader concept of \"developer operations\" (or simply \"devops\"), fewer seem\nfamiliar with the concepts of CD and related tools to facilitate devops within\norganizations.\n\nThe CD process is based on the fundamental flow of building the application\ntesting it delivering it, where typically:\n\nThe building part involves compiling the application and/or ensuring all\nnecessary libraries and dependencies are in place for the application to run\nas intended.\n\nThe testing part involves testing the built application with automated tests\nto ensure that changes implemented by developers function as expected.\n\nThe delivering part involves packaging or presenting the application in a\nway that can be delivered to customers or other users for any kind of purpose.\n\nNow, as one of the major contributors to the Jenkins User Documentation (and\nfaced with a reasonably steep learning curve), it quickly became apparent about\nthe lack of accessible documentation to hand-hold people relatively new to\nJenkins through this CD process. I couldn’t find anything in the Jenkins User\nDocumentation to demonstrate how Jenkins implements this process on a simple\napp that delivers an end result.\n\nWith the guidance and assistance of helpful colleagues, I therefore decided to\nembark on creating a series of Jenkins tutorials to help fill these\ndocumentation and knowledge gaps. These tutorials are based on Daniele Procida’s\ndescription of how tutorials should be presented in his blog post\n\" What nobody tells you about\ndocumentation\").\n\nIntroductory tutorials\n\nThe first set of tutorials on the Tutorials overview page\ndemonstrate how to implement this fundamental CD process in Jenkins on a simple\napplication for a given technology stack.\n\nSo far, there’s one for\nJava with Maven and another\nfor Node.js and\nReact with npm. Another for Python will be added to this list in the near\nfuture.\n\nThese tutorials define your application’s entire CD process (i.e. your Pipeline)\nin a Jenkinsfile, whose Groovy-like Declarative Pipeline syntax is checked in\nto your Git source repository. Managing your Pipeline with your application’s\nsource code like this forms the fundamentals of \"Pipeline as code\".\n\nThe Introductory tutorials also cover how to use some powerful features of\nJenkins, like Blue Ocean,\nwhich makes it easy to connect to an existing cloud, web or locally hosted Git\nrepository and create your Pipeline with limited knowledge of Pipeline syntax.\n\nAdvanced tutorials\n\nAlso soon to be released will be the first Advanced tutorial on building\nmultibranch Pipelines in Jenkins. This tutorial takes the \"Pipeline as code\"\nconcept to a new level, where a single Jenkinsfile (defining the entire CD\nprocess across all branches of your application’s Git repository) consists of\nmultiple stages which are selectively executed based on the branch that Jenkins\nis building.\n\nAdditional tutorials that demonstrate more advanced features of Jenkins and how\nto manage your Pipelines with greater sophistication and flexibility will be\nadded to this section in future.\n\nSumming up\n\nYou can access all currently available tutorials from the\nTutorials overview page in the Jenkins User Documentation.\nIt’s worthwhile checking that page from time to time as it’ll be updated\nwhenever a new tutorial is published.\n\nAlso, if you have any suggestions for tutorials or other content you’d like to\nsee in the documentation, please post your suggestions in the\nJenkins\nDocumentation Google Group, which you can also post (and reply) to by emailing\njenkinsci-docs@googlegroups.com.\n\nThe Sydney Office team meeting at Carriageworks - from left to right, Giles\nGaskell, Nicholae Pascu, Michael Neale and James Dumay","title":"Introducing Tutorials in the Jenkins User Documentation","tags":["tutorial","blueocean","pipeline"],"authors":[{"avatar":null,"blog":null,"github":"gilesgas","html":"","id":"gilesgas","irc":null,"linkedin":null,"name":"Giles Gaskell","slug":"/blog/authors/gilesgas","twitter":"giles_gas"}]}},{"node":{"date":"2017-10-02T00:00:00.000Z","id":"3c59e404-8125-5869-a3c8-98dd83976f14","slug":"/blog/2017/10/02/pipeline-templates-with-shared-libraries/","strippedHtml":"This is a guest post by Philip Stroh, Software Architect at\nTimoCom.\n\nWhen building multiple microservices - e.g. with Spring Boot - the integration\nand delivery pipelines of your services will most likely be very similar.\nSurely, you don’t want to copy-and-paste Pipeline code from one Jenkinsfile\nto another if you develop a new service or if there are adaptions in your\ndelivery process. Instead you would like to define something like a pipeline\n\"template\" that can be applied easily to all of your services.\n\nThe requirement for a common pipeline that can be used in multiple projects does not only emerge in microservice architectures. It’s valid for all areas where applications are\nbuilt on a similar technology stack or deployed in a standardized way (e.g. pre-packages as containers).\n\nIn this blog post I’d like to outline the possibility to create such a pipeline \"template\" using Jenkins Shared Libraries. If\nyou’re not yet familiar with Shared Libraries I’d recommend having a look at\nthe documentation.\n\nThe following code shows a (simplified) integration and delivery Pipeline for a\nSpring Boot application in declarative syntax.\n\nJenkinsFile\n\npipeline {\n    agent any\n    environment {\n        branch = 'master'\n        scmUrl = 'ssh://git@myScmServer.com/repos/myRepo.git'\n        serverPort = '8080'\n        developmentServer = 'dev-myproject.mycompany.com'\n        stagingServer = 'staging-myproject.mycompany.com'\n        productionServer = 'production-myproject.mycompany.com'\n    }\n    stages {\n        stage('checkout git') {\n            steps {\n                git branch: branch, credentialsId: 'GitCredentials', url: scmUrl\n            }\n        }\n\n        stage('build') {\n            steps {\n                sh 'mvn clean package -DskipTests=true'\n            }\n        }\n\n        stage ('test') {\n            steps {\n                parallel (\n                    \"unit tests\": { sh 'mvn test' },\n                    \"integration tests\": { sh 'mvn integration-test' }\n                )\n            }\n        }\n\n        stage('deploy development'){\n            steps {\n                deploy(developmentServer, serverPort)\n            }\n        }\n\n        stage('deploy staging'){\n            steps {\n                deploy(stagingServer, serverPort)\n            }\n        }\n\n        stage('deploy production'){\n            steps {\n                deploy(productionServer, serverPort)\n            }\n        }\n    }\n    post {\n        failure {\n            mail to: 'team@example.com', subject: 'Pipeline failed', body: \"${env.BUILD_URL}\"\n        }\n    }\n}\n\nThis Pipeline builds the application, runs unit as well as integration tests and deploys the application to\nseveral environments. It uses a global variable \"deploy\" that is provided within a Shared Library. The deploy method\ncopies the JAR-File to a remote server and starts the application. Through the handy REST endpoints of Spring Boot\nActuator a previous version of the application is stopped beforehand. Afterwards the deployment is verified via the\nhealth status monitor of the application.\n\nvars/deploy.groovy\n\ndef call(def server, def port) {\n    httpRequest httpMode: 'POST', url: \"http://${server}:${port}/shutdown\", validResponseCodes: '200,408'\n    sshagent(['RemoteCredentials']) {\n        sh \"scp target/*.jar root@${server}:/opt/jenkins-demo.jar\"\n        sh \"ssh root@${server} nohup java -Dserver.port=${port} -jar /opt/jenkins-demo.jar &\"\n    }\n    retry (3) {\n        sleep 5\n        httpRequest url:\"http://${server}:${port}/health\", validResponseCodes: '200', validResponseContent: '\"status\":\"UP\"'\n    }\n}\n\nThe common approach to reuse pipeline code is to put methods like \"deploy\" into\na Shared Library. If we now start developing the next application of the same\nfashion we can use this method for deployments as well. But often there are\neven more similarities within projects of one company. E.g. applications are\nbuilt, tested and deployed in the same way into the same environments\n(development, staging and production). In this case it is possible to define\nthe whole Pipeline as a global variable within a Shared Library. The next code\nsnippet defines a Pipeline \"template\" for all of our Spring Boot applications.\n\nvars/myDeliveryPipeline.groovy\n\ndef call(Map pipelineParams) {\n\n    pipeline {\n        agent any\n        stages {\n            stage('checkout git') {\n                steps {\n                    git branch: pipelineParams.branch, credentialsId: 'GitCredentials', url: pipelineParams.scmUrl\n                }\n            }\n\n            stage('build') {\n                steps {\n                    sh 'mvn clean package -DskipTests=true'\n                }\n            }\n\n            stage ('test') {\n                steps {\n                    parallel (\n                        \"unit tests\": { sh 'mvn test' },\n                        \"integration tests\": { sh 'mvn integration-test' }\n                    )\n                }\n            }\n\n            stage('deploy developmentServer'){\n                steps {\n                    deploy(pipelineParams.developmentServer, pipelineParams.serverPort)\n                }\n            }\n\n            stage('deploy staging'){\n                steps {\n                    deploy(pipelineParams.stagingServer, pipelineParams.serverPort)\n                }\n            }\n\n            stage('deploy production'){\n                steps {\n                    deploy(pipelineParams.productionServer, pipelineParams.serverPort)\n                }\n            }\n        }\n        post {\n            failure {\n                mail to: pipelineParams.email, subject: 'Pipeline failed', body: \"${env.BUILD_URL}\"\n            }\n        }\n    }\n}\n\nNow we can setup the Pipeline of one of our applications with the following method call:\n\nJenkinsfile\n\nmyDeliveryPipeline(branch: 'master', scmUrl: 'ssh://git@myScmServer.com/repos/myRepo.git',\n                   email: 'team@example.com', serverPort: '8080',\n                   developmentServer: 'dev-myproject.mycompany.com',\n                   stagingServer: 'staging-myproject.mycompany.com',\n                   productionServer: 'production-myproject.mycompany.com')\n\nThe Shared library documentation mentions the ability to encapsulate\nsimilarities between several Pipelines with a global variable. It shows how we\ncan enhance our template approach and build a higher-level DSL step:\n\nvars/myDeliveryPipeline.groovy\n\ndef call(body) {\n    // evaluate the body block, and collect configuration into the object\n    def pipelineParams= [:]\n    body.resolveStrategy = Closure.DELEGATE_FIRST\n    body.delegate = pipelineParams\n    body()\n\n    pipeline {\n        // our complete declarative pipeline can go in here\n        ...\n    }\n}\n\nNow we can even use our own DSL-step to set up the integration and deployment Pipeline of our project:\n\nJenkinsfile\n\nmyDeliveryPipeline {\n    branch = 'master'\n    scmUrl = 'ssh://git@myScmServer.com/repos/myRepo.git'\n    email = 'team@example.com'\n    serverPort = '8080'\n    developmentServer = 'dev-myproject.mycompany.com'\n    stagingServer = 'staging-myproject.mycompany.com'\n    productionServer = 'production-myproject.mycompany.com'\n}\n\nThe blog post showed how a common Pipeline template can be developed using the\nShared Library functionality in Jenkins. The approach allows to create a\nstandard Pipeline that can be reused by applications that are built in a\nsimilar way.\n\nIt works for Declarative and Scripted Pipelines as well. For declarative\npipelines the ability to define a Pipeline block in a Shared Library is\nofficial supported since version 1.2 (see the recent blog post on\nDeclarative Pipeline 1.2).","title":"Share a standard Pipeline across multiple projects with Shared Libraries","tags":["pipeline","declarative","microservices"],"authors":[{"avatar":null,"blog":null,"github":"pstrh","html":"","id":"pstrh","irc":null,"linkedin":null,"name":"Philip Stroh","slug":"/blog/authors/pstrh","twitter":null}]}},{"node":{"date":"2017-09-25T00:00:00.000Z","id":"ec7b8ed5-f69c-5e84-9b65-735961d0c5cf","slug":"/blog/2017/09/25/declarative-1.2-released/","strippedHtml":"After a few months of work on its key features, I’m happy to announce the\n1.2 release of\nDeclarative Pipeline!\nOn behalf of the contributors developing Pipeline, I thought it would be\nhelpful to discuss three of the key changes.\n\nParallel Stages\n\nFirst, we’ve added syntax support for parallel stages. In earlier versions of\nDeclarative Pipeline, the only way to run chunks of Pipeline code in parallel\nwas to use the parallel step inside the steps block for a stage, like this:\n\n/* .. snip .. */\nstage('run-parallel-branches') {\n  steps {\n    parallel(\n      a: {\n        echo \"This is branch a\"\n      },\n      b: {\n        echo \"This is branch b\"\n      }\n    )\n  }\n}\n/* .. snip .. */\n\nWhile this works, it doesn’t integrate well with the rest of the Declarative\nPipeline syntax. For example, to run each parallel branch on a different agent,\nyou need to use a node step, and if you do that, the output of the parallel\nbranch won’t be available for post directives (at a stage or pipeline\nlevel). Basically the old parallel step required you to use Scripted Pipeline\nwithin a Declarative Pipeline.\n\nBut now with Declarative Pipeline 1.2, we’ve introduced a true Declarative\nsyntax for running stages in parallel:\n\nJenkinsfile\n\npipeline {\n    agent none\n    stages {\n        stage('Run Tests') {\n            parallel {\n                stage('Test On Windows') {\n                    agent {\n                        label \"windows\"\n                    }\n                    steps {\n                        bat \"run-tests.bat\"\n                    }\n                    post {\n                        always {\n                            junit \"**/TEST-*.xml\"\n                        }\n                    }\n                }\n                stage('Test On Linux') {\n                    agent {\n                        label \"linux\"\n                    }\n                    steps {\n                        sh \"run-tests.sh\"\n                    }\n                    post {\n                        always {\n                            junit \"**/TEST-*.xml\"\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nYou can now specify either steps or parallel for a stage, and within\nparallel, you can specify a list of stage directives to run in parallel,\nwith all the configuration you’re used to for a stage in Declarative\nPipeline. We think this will be really useful for cross-platform builds and\ntesting, as an example. Support for parallel stages will be in the\nsoon-to-be-released Blue Ocean Pipeline Editor 1.3 as well.\n\nYou can find more documentation on parallel stages in the\nUser Handbook.\n\nDefining Declarative Pipelines in Shared Libraries\n\nUntil the 1.2 release, Declarative Pipelines did not officially support\ndefining your pipeline blocks in a shared library. Some of you may have tried\nthat out and found that it could work in some cases, but since it was never an\nofficially supported feature, it was vulnerable to breaking due to necessary\nchanges for the supported use cases of Declarative. But with 1.2, we’ve added\nofficial support for defining pipeline blocks in src/.groovy files in your\nshared libraries. Within your src/.groovy file’s call method, you can\ncall pipeline { …​ }, or possibly different pipeline { …​ } blocks\ndepending on if conditions and the like. Note that only one pipeline { …​ }\nblock can actually be executed per run - you’ll get an error if a second one\ntries to execute!\n\nMajor Improvements to Parsing and Environment Variables\n\nHopefully, you’ll never actually care about this change, but we’re very happy\nabout it nonetheless. The original approach used for actually taking the\npipeline { …​ } block and executing its contents was designed almost two\nyears ago, and wasn’t very well suited to how you all are actually using\nDeclarative Pipelines. In our attempts to work around some of those limitations,\nwe made the parsing logic even more complicated and fragile, resulting in an\nimpressive\nnumber of bugs, mainly relating to inconsistencies and bad behavior with\nenvironment variables.\n\nIn Declarative 1.2, we’ve replaced the runtime parsing logic completely with a\nfar more robust system, which also happens to fix most of those bugs at the\nsame time! While not every issue has been resolved, you may find that you can\nuse environment variables in more places, escaping is more consistent,\nWindows paths are no longer handled incorrectly, and a lot more. Again, we’re\nhoping you’ve never had the misfortune to run into any of these bugs, but if\nyou have, well, they’re fixed now, and it’s going to be a lot easier for us to\nfix any future issues that may arise relating to environment variables, when\nexpressions, and more. Also, the parsing at the very beginning of your build\nmay be about 0.5 seconds faster. =)\n\nMore to Come!\n\nWhile we don’t have any concrete plans for what will be going into Declarative\nPipelines 1.3, rest assured that we’ve got some great new features in mind, as\nwell as our continuing dedication to fixing the bugs you encounter and report.\nSo please do keep opening tickets for\nissues and feature requests. Thanks!","title":"Parallel stages with Declarative Pipeline 1.2","tags":["pipeline","declarative"],"authors":[{"avatar":null,"blog":null,"github":"abayer","html":"<div class=\"paragraph\">\n<p>Andrew was a core committer to Hudson and the author of numerous plugins.</p>\n</div>","id":"abayer","irc":null,"linkedin":null,"name":"Andrew Bayer","slug":"/blog/authors/abayer","twitter":"abayer"}]}},{"node":{"date":"2017-09-08T00:00:00.000Z","id":"529abe58-b0d2-5d12-931f-675712ff69a6","slug":"/blog/2017/09/08/enumerators-in-pipeline/","strippedHtml":"While at Jenkins World, Kohsuke Kawaguchi\npresented two long-time Jenkins contributors with a\n\" Small Matter of Programming\"\naward: Andrew Bayer and\nJesse Glick. \"Small Matter of Programming\"\nbeing:\n\na phrase used to ironically indicate that a suggested feature or design change\nwould in fact require a great deal of effort; it often implies that the person\nproposing the feature underestimates its cost.\n\n— Wikipedia\n\nIn this context the \"Small Matter\" relates to Jenkins\nPipeline and a very simple snippet of Scripted Pipeline:\n\n[1, 2, 3].each { println it }\n\nFor a long time in Scripted Pipeline, this simply did not work as users would\nexpect it. Originally filed as\nJENKINS-26481 in 2015,\nit became one of the most voted for, and watched, tickets in the entire issue\ntracker until it was ultimately fixed earlier this year.\n\nAt least some closures are executed only once inside of Groovy CPS DSL scripts\nmanaged by the workflow plugin.\n\n— Original bug description by Daniel Tschan\n\nAt a high level, what has been confusing for many users is that Scripted\nPipeline looks like a Groovy, it quacks like a Groovy, but it’s not exactly\nGroovy. Rather, there’s an custom Groovy interpreter\n( CPS) that executes the\nScripted Pipeline in a manner which provides the durability/resumability that\ndefines Jenkins Pipeline.\n\nWithout diving into too much detail, refer to the pull requests linked to\nJENKINS-26481 for that, the code snippet above was particularly challenging to\nrectify inside the Pipeline execution layer. As one of the chief architects for\nJenkins Pipeline, Jesse made a number of changes around the problem in 2016,\nbut it wasn’t until early 2017 when Andrew, working on Declarative Pipeline,\nstarted to identify a number of areas of improvement in CPS and provided\nmultiple patches and test cases.\n\nAs luck would have it, combining two of the sharpest minds in the Jenkins\nproject resulted in the \"Small Matter of Programming\" being finished, and\nreleased in May of this year with Pipeline: Groovy 2.33.\n\nPlease join me in congratulating, and thanking, Andrew and Jesse for their\ndiligent and hard work smashing one of the most despised bugs in Jenkins\nhistory :).","title":"Closure on enumerators in Pipeline","tags":["pipeline","jenkinsworld"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2017-08-18T00:00:00.000Z","id":"e2e86b67-d732-51e3-9127-44de5e51de0e","slug":"/blog/2017/08/18/declarative-pipelines-at-jenkinsworld/","strippedHtml":"This is a guest post by Andrew Bayer, who is\none of the authors of the\nDeclarative Pipeline plugin,\nand is a software engineer on the Pipeline team at\nCloudBees, Inc.\n\nA year ago at Jenkins World 2016, we unveiled Declarative Pipeline, a\nstructured way to define your Pipeline. It’s been a great year for Declarative\nand Pipeline in general, with the release of Declarative Pipeline 1.0 in\nFebruary, multiple releases since then, the introduction of\ndocumentation on Pipeline at jenkins.io,\nwith a focus on Declarative, and more. Given everything that’s happened over\nthe last year, we thought it’d be good to let you all know what you can expect\nto see and hear about Declarative Pipeline at this year’s Jenkins World.\n\nFirst, on Thursday, August 31, I’ll be giving a talk on Declarative Pipeline\nwith Robert Sandell, one of my coworkers\nhere at CloudBees and another author of Declarative Pipeline. We’ll be\ncovering what’s happened with Declarative over the last year, new features\nadded since the 1.0 release, such as the libraries directive and more when\nconditions, what’s planned for the upcoming 1.2 release (which is planned for\nshortly after Jenkins World!), including parallel stage s, and what’s on the\nroadmap for the future. In addition, we’ll be demoing some of the features in\n1.2, and providing some pointers on best practices for writing your Declarative\nPipeline.\n\nAlso on Thursday, Stephen Donner from Mozilla\nwill be giving a demo showing Mozilla’s usage of Declarative Pipeline and\nshared libraries at the Community Booth - Mozilla has been doing great work\nwith Declarative, and I’m excited to see their usage in more detail and hear\nStephen talk about their experience!\n\nIn addition, Robert, Stephen, and myself will all be at Jenkins World both days\nof the main sessions, and Robert and myself will also be at the\nContributor Summit\non Tuesday. We’d love to hear your thoughts on Declarative and will be happy to\nanswer any questions that we can. Looking forward to seeing you all!\n\nAndrew Bayer and Robert Sandell will be talking about the latest on\nDeclarative Pipeline in Jenkins\nat Jenkins World in August,\nregister with the code JWFOSS for a 30% discount off your pass.","title":"Declarative Pipeline at Jenkins World","tags":["plugins","pipeline"],"authors":[{"avatar":null,"blog":null,"github":"abayer","html":"<div class=\"paragraph\">\n<p>Andrew was a core committer to Hudson and the author of numerous plugins.</p>\n</div>","id":"abayer","irc":null,"linkedin":null,"name":"Andrew Bayer","slug":"/blog/authors/abayer","twitter":"abayer"}]}},{"node":{"date":"2017-08-10T00:00:00.000Z","id":"7a11360a-42a4-5d64-8401-89a9e4f35904","slug":"/blog/2017/08/10/kubernetes-with-pipeline-acs/","strippedHtml":"This is a guest post by Pui Chee Chen,\nProduct Manager at Microsoft working on\nAzure\nDevOps open source integrations.\n\nRecently, we improved the Azure Credential plugin by\nadding a custom binding for Azure Credentials which allows you to use an\nAzure\nservice principal (the analog to a service or system account) via  the\nCredentials Binding plugin. This means it’s now trivial to run Azure CLI\ncommands from a Jenkins Pipeline. We also recently published the first version\nof the Azure App Service plugin which makes it very\neasy to deploy\nAzure Web\nApps directly from Jenkins Pipeline. While we’ll have\nmuch more to discuss in our Jenkins World presentation on\nAzure\nDevOps open source integrations, in this blog post I wanted to share some good\nsnippets of what is possible today with Jenkins Pipeline and Azure.\n\nFirst, a simple example using the Azure CLI to list resources in the\nsubscription:\n\n// Scripted //\nnode {\n    /* .. snip .. */\n    stage('Deploy') {\n        withCredentials([azureServicePrincipal('principal-credentials-id')]) {\n            sh 'az login --service-principal -u $AZURE_CLIENT_ID -p $AZURE_CLIENT_SECRET -t $AZURE_TENANT_ID'\n            sh 'az account set -s $AZURE_SUBSCRIPTION_ID'\n            sh 'az resource list'\n        }\n    }\n}\n// Declarative //\n\nazureServicePrincipal() cannot be used in Declarative Pipeline until\nJENKINS-46103 is\nresolved.\n\nOnce a Pipeline can interact with Azure, there are countless ways one could\nimplement continuous delivery with Jenkins and Azure. From a deploying a simple\nwebapp with the\nAzure\nApp Service plugin and the azureWebAppPublish step, or a more advanced\ncontainer-based delivery pipeline to deliver new containers to\nKubernetes\nvia Azure Container Service.\n\nWith the Docker Pipeline plugin and a little bit of\nextra scripting, a Jenkins Pipeline can also build and publish a Docker\ncontainer to an\nAzure\nContainer Registry :\n\n// Scripted //\nimport groovy.json.JsonSlurper\n\nnode {\n    def container\n    def acrSettings\n\n    withCredentials([azureServicePrincipal('principal-credentials-id')]) {\n        stage('Prepare Environment') {\n            sh 'az login --service-principal -u $AZURE_CLIENT_ID -p $AZURE_CLIENT_SECRET -t $AZURE_TENANT_ID'\n            sh 'az account set -s $AZURE_SUBSCRIPTION_ID'\n            acrSettings = new JsonSlurper().parseText(\n                                            sh(script: \"az acs show -o json -n my-acr\", returnStdout: true))\n        }\n\n        stage('Build') {\n            container = docker.build(\"${acrSettings.loginServer}/my-app:${env.BUILD_ID}\")\n        }\n\n        stage('Publish') {\n            /* https://issues.jenkins.io/browse/JENKINS-46108 */\n            sh \"docker login -u ${AZURE_CLIENT_ID} -p ${AZURE_CLIENT_SECRET} ${acrSettings.loginServer}\"\n            container.push()\n        }\n\n        stage('Deploy') {\n            echo 'Orchestrating a new deployment with kubectl is a simple exercise left to the reader ;)'\n        }\n    }\n}\n// Declarative //\n\nIf you have been following our\nAzure Blog, you may\nhave noticed we have shipped a lot of updates to provide better support for\nAzure on Jenkins, and vice versa, such as:\n\nHosted Jenkins. New\nSolution\nTemplate in Azure Marketplace lets you spin up a\nJenkins Controller on Azure in minutes. Not only is it easy and fast, the solution\ntemplate gives you option to scale up by selecting the VM disk type and size.\nAnd guess what? You can even select the Jenkins release type you want to use -\nLTS, weekly build or Azure verified - all under your control.\n\nContinuous integration experience. In the latest version of our\nAzure VM Agents plugin, we improved the user\nexperience and added the option to let you to select Managed Disk for disk\ntype (which is currently used extensively on\nci.jenknis.io. You no longer need to worry about\nexceeding the number of VMs on your subscription.\n\nContinuous deployment experience. Now, if\nAzure CLI is not your cup of tea, we released our first plugin to provide\ncontinuous deployment support to Azure App Service. The plugin supports all\nlanguages Azure App Service supports. We even have a walkthrough\nhere in the\nbrand new Jenkins Hub where you can find all Jenkins on Azure resources.\n\nPipeline readiness. Also, all Azure plugins are and will be pipeline ready.\nHave you been leveraging our\nAzure Storage plugin in your Pipeline?\n\nSo, what’s next? We have a big surprise in store at Jenkins World! :)\n\nWe are serious about supporting open source and the open source community.\nBe sure to catch our talk on\nAzure\nDevOps open source integrations.\nSee you at\nJenkins World 2017!\n\nJoin the Azure DevOps team at\nJenkins World in August,\nregister with the code JWFOSS for a 30% discount off your pass.","title":"CI/CD with Jenkins Pipeline and Azure","tags":["plugins","kubernetes","pipeline"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}}]}},"pageContext":{"tag":"pipeline","limit":8,"skip":32,"numPages":13,"currentPage":5}},
    "staticQueryHashes": ["3649515864"]}