{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/pipeline/page/5",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2017-12-15T00:00:00.000Z","id":"c2ac3d30-6b1c-5338-a437-133b5ebc730e","slug":"/blog/2017/12/15/auto-convert-freestyle-jenkins-jobs-to-coded-pipeline/","strippedHtml":"This is a guest post by Sanil Pillai, Director of Labs & Strategic Insights, Infostretch\n\nInfostretch has created a\nplugin for teams\nupgrading from Freestyle Jobs to Pipelines as code with Jenkins Pipeline.\nThis new plugin streamlines the process and accelerates\npipeline on-boarding for any new set of applications. Previously, when\nupgrading to Jenkins Pipeline, converting Freestyle Jobs required developers\nto drill down on each one of those hundreds (or thousands!)  of jobs to understand\ntools, configurations, URLs, parameters, and more before rewriting them in\nPipeline syntax. This process is very manual,\nerror-prone, lengthy, and not cost-effective. Beyond saving time, the new\nplugin also assures adherence to proper coding standards and separates complex\nbusiness logic and standards declaration from execution flow.\n\nKey features:\n\nConvert single freestyle job to pipeline\n\nConvert chain of freestyle jobs to single pipeline\n\nWorks with both Jenkins and CloudBees Jenkins Enterprise\n\nPlugin can be customized to support any Freestyle plugin and an\norganization’s Pipeline Shared Library,\nor Groovy coding standards.\n\nWorks with CloudBees' Role-based Access Control to help the new Pipelines\ncomply with existing security policies.\n\nDirect migration of properties such as \"Build with Parameters\" to newly\ncreated Pipelines.\n\nDirect migration of Agent on which job is to be run with support for multiple agent labels across different downstream jobs\n\nEnvironment properties: JDK, NodeJS\n\nSupports Git SCM.\n\nBuild steps: Maven, Ant, Shell, Batch, and Ansible Playbook.\n\nPost build actions: artifact archiver, simple mailer, TestNG reports, JUnit reports, checkstyle publisher\n\nNow, let’s take a look at how to get started:\n\nClick on a link at Root level or Folder level or Job level.\n\nSelect the job from the drop-down list that is the beginning point of the\n\"chain\". If job level link is clicked, this drop-down list will not be visible.\n\nProvide the new pipeline job name. If this is not specified, the plugin will\nattempt to create a new pipeline job with the naming convention of\n\"oldname-pipeline\".\n\nCheck \"Recursively convert downstream jobs if any?\" if you wish to have all the\ndownstream jobs converted into this new pipeline. The plugin will write all the\nlogic of current and downstream jobs into a single pipeline.\n\nCheck \"Commit Jenkinsfile?\" if you would like the plugin to create a\nJenkinsfile and commit it back to the SCM. The plugin will commit the\nJenkinsfile at the root of the SCM repository it finds in the first job\n(selected in step 1 above). It will attempt to commit to this repo using the\ncredentials it finds in the first job.\n\nDo note that the plugin will checkout the repo in to a temporary workspace on\nthe controller (JENKINS_HOME/plugins/convert-to-    pipeline/ws). Once the\nconversion is complete and Jenkinsfile is committed back to the repo, the\nworkspace will be deleted.\n\nClick \"Convert\" to convert the Freestyle job configurations to a single\nscripted pipeline job. Once the conversion is complete and the new job is\ncreated, you will be redirected to the newly created pipeline job.\n\nThat’s it!\n\nTo learn more about plugin usage, customization and to see a demo\nclick here\nto watch the webinar replay on-demand.","title":"Auto-Convert Freestyle Jobs to Jenkins Pipeline","tags":["pipeline","freestyle"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"<div class=\"paragraph\">\n<p>Director of Labs &amp; Strategic Insights, Infostretch</p>\n</div>","id":"spillai","irc":null,"linkedin":null,"name":"Sanil Pillai","slug":"/blog/authors/spillai","twitter":null}]}},{"node":{"date":"2017-11-27T00:00:00.000Z","id":"fd21dbc6-9b4b-5092-b232-0feabaf928ce","slug":"/blog/2017/11/27/tutorials-in-the-jenkins-user-documentation/","strippedHtml":"Regular perusers of the Jenkins User Documentation may have noticed\nthe presence of the Tutorials part (between the Guided Tour and User\nHandbook) that appeared in the last couple of months and gradually began to get\npopulated with much of my recent work, writing Jenkins tutorials.\n\nMy name’s Giles and I’ve been a technical writer in the software development\nfield for several years now. I’ve always been passionate about technical writing\nand more recently, the technologies that go into developing written content and\nautomating its generation - like Jenkins! I was a former Atlassian and recently\njoined CloudBees as a Senior Technical Writer, working remotely from the \"Sydney\nOffice\", with my current focus on the Jenkins User Documentation.\n\nWhy tutorials?\n\nMy exposure to Jenkins and its usage over the years has been patchy at best.\nDuring this time, however, I’ve had some degree of experience as a user of\nvarious continuous delivery (CD) tools like Jenkins and am reasonably familiar\nwith the advantages these tools can offer software development teams.\n\nI’ve also found that while many software developers are familiar with the\nbroader concept of \"developer operations\" (or simply \"devops\"), fewer seem\nfamiliar with the concepts of CD and related tools to facilitate devops within\norganizations.\n\nThe CD process is based on the fundamental flow of building the application\ntesting it delivering it, where typically:\n\nThe building part involves compiling the application and/or ensuring all\nnecessary libraries and dependencies are in place for the application to run\nas intended.\n\nThe testing part involves testing the built application with automated tests\nto ensure that changes implemented by developers function as expected.\n\nThe delivering part involves packaging or presenting the application in a\nway that can be delivered to customers or other users for any kind of purpose.\n\nNow, as one of the major contributors to the Jenkins User Documentation (and\nfaced with a reasonably steep learning curve), it quickly became apparent about\nthe lack of accessible documentation to hand-hold people relatively new to\nJenkins through this CD process. I couldn’t find anything in the Jenkins User\nDocumentation to demonstrate how Jenkins implements this process on a simple\napp that delivers an end result.\n\nWith the guidance and assistance of helpful colleagues, I therefore decided to\nembark on creating a series of Jenkins tutorials to help fill these\ndocumentation and knowledge gaps. These tutorials are based on Daniele Procida’s\ndescription of how tutorials should be presented in his blog post\n\" What nobody tells you about\ndocumentation\").\n\nIntroductory tutorials\n\nThe first set of tutorials on the Tutorials overview page\ndemonstrate how to implement this fundamental CD process in Jenkins on a simple\napplication for a given technology stack.\n\nSo far, there’s one for\nJava with Maven and another\nfor Node.js and\nReact with npm. Another for Python will be added to this list in the near\nfuture.\n\nThese tutorials define your application’s entire CD process (i.e. your Pipeline)\nin a Jenkinsfile, whose Groovy-like Declarative Pipeline syntax is checked in\nto your Git source repository. Managing your Pipeline with your application’s\nsource code like this forms the fundamentals of \"Pipeline as code\".\n\nThe Introductory tutorials also cover how to use some powerful features of\nJenkins, like Blue Ocean,\nwhich makes it easy to connect to an existing cloud, web or locally hosted Git\nrepository and create your Pipeline with limited knowledge of Pipeline syntax.\n\nAdvanced tutorials\n\nAlso soon to be released will be the first Advanced tutorial on building\nmultibranch Pipelines in Jenkins. This tutorial takes the \"Pipeline as code\"\nconcept to a new level, where a single Jenkinsfile (defining the entire CD\nprocess across all branches of your application’s Git repository) consists of\nmultiple stages which are selectively executed based on the branch that Jenkins\nis building.\n\nAdditional tutorials that demonstrate more advanced features of Jenkins and how\nto manage your Pipelines with greater sophistication and flexibility will be\nadded to this section in future.\n\nSumming up\n\nYou can access all currently available tutorials from the\nTutorials overview page in the Jenkins User Documentation.\nIt’s worthwhile checking that page from time to time as it’ll be updated\nwhenever a new tutorial is published.\n\nAlso, if you have any suggestions for tutorials or other content you’d like to\nsee in the documentation, please post your suggestions in the\nJenkins\nDocumentation Google Group, which you can also post (and reply) to by emailing\njenkinsci-docs@googlegroups.com.\n\nThe Sydney Office team meeting at Carriageworks - from left to right, Giles\nGaskell, Nicholae Pascu, Michael Neale and James Dumay","title":"Introducing Tutorials in the Jenkins User Documentation","tags":["tutorial","blueocean","pipeline"],"authors":[{"avatar":null,"blog":null,"github":"gilesgas","html":"","id":"gilesgas","irc":null,"linkedin":null,"name":"Giles Gaskell","slug":"/blog/authors/gilesgas","twitter":"giles_gas"}]}},{"node":{"date":"2017-10-02T00:00:00.000Z","id":"3c59e404-8125-5869-a3c8-98dd83976f14","slug":"/blog/2017/10/02/pipeline-templates-with-shared-libraries/","strippedHtml":"This is a guest post by Philip Stroh, Software Architect at\nTimoCom.\n\nWhen building multiple microservices - e.g. with Spring Boot - the integration\nand delivery pipelines of your services will most likely be very similar.\nSurely, you don’t want to copy-and-paste Pipeline code from one Jenkinsfile\nto another if you develop a new service or if there are adaptions in your\ndelivery process. Instead you would like to define something like a pipeline\n\"template\" that can be applied easily to all of your services.\n\nThe requirement for a common pipeline that can be used in multiple projects does not only emerge in microservice architectures. It’s valid for all areas where applications are\nbuilt on a similar technology stack or deployed in a standardized way (e.g. pre-packages as containers).\n\nIn this blog post I’d like to outline the possibility to create such a pipeline \"template\" using Jenkins Shared Libraries. If\nyou’re not yet familiar with Shared Libraries I’d recommend having a look at\nthe documentation.\n\nThe following code shows a (simplified) integration and delivery Pipeline for a\nSpring Boot application in declarative syntax.\n\nJenkinsFile\n\npipeline {\n    agent any\n    environment {\n        branch = 'master'\n        scmUrl = 'ssh://git@myScmServer.com/repos/myRepo.git'\n        serverPort = '8080'\n        developmentServer = 'dev-myproject.mycompany.com'\n        stagingServer = 'staging-myproject.mycompany.com'\n        productionServer = 'production-myproject.mycompany.com'\n    }\n    stages {\n        stage('checkout git') {\n            steps {\n                git branch: branch, credentialsId: 'GitCredentials', url: scmUrl\n            }\n        }\n\n        stage('build') {\n            steps {\n                sh 'mvn clean package -DskipTests=true'\n            }\n        }\n\n        stage ('test') {\n            steps {\n                parallel (\n                    \"unit tests\": { sh 'mvn test' },\n                    \"integration tests\": { sh 'mvn integration-test' }\n                )\n            }\n        }\n\n        stage('deploy development'){\n            steps {\n                deploy(developmentServer, serverPort)\n            }\n        }\n\n        stage('deploy staging'){\n            steps {\n                deploy(stagingServer, serverPort)\n            }\n        }\n\n        stage('deploy production'){\n            steps {\n                deploy(productionServer, serverPort)\n            }\n        }\n    }\n    post {\n        failure {\n            mail to: 'team@example.com', subject: 'Pipeline failed', body: \"${env.BUILD_URL}\"\n        }\n    }\n}\n\nThis Pipeline builds the application, runs unit as well as integration tests and deploys the application to\nseveral environments. It uses a global variable \"deploy\" that is provided within a Shared Library. The deploy method\ncopies the JAR-File to a remote server and starts the application. Through the handy REST endpoints of Spring Boot\nActuator a previous version of the application is stopped beforehand. Afterwards the deployment is verified via the\nhealth status monitor of the application.\n\nvars/deploy.groovy\n\ndef call(def server, def port) {\n    httpRequest httpMode: 'POST', url: \"http://${server}:${port}/shutdown\", validResponseCodes: '200,408'\n    sshagent(['RemoteCredentials']) {\n        sh \"scp target/*.jar root@${server}:/opt/jenkins-demo.jar\"\n        sh \"ssh root@${server} nohup java -Dserver.port=${port} -jar /opt/jenkins-demo.jar &\"\n    }\n    retry (3) {\n        sleep 5\n        httpRequest url:\"http://${server}:${port}/health\", validResponseCodes: '200', validResponseContent: '\"status\":\"UP\"'\n    }\n}\n\nThe common approach to reuse pipeline code is to put methods like \"deploy\" into\na Shared Library. If we now start developing the next application of the same\nfashion we can use this method for deployments as well. But often there are\neven more similarities within projects of one company. E.g. applications are\nbuilt, tested and deployed in the same way into the same environments\n(development, staging and production). In this case it is possible to define\nthe whole Pipeline as a global variable within a Shared Library. The next code\nsnippet defines a Pipeline \"template\" for all of our Spring Boot applications.\n\nvars/myDeliveryPipeline.groovy\n\ndef call(Map pipelineParams) {\n\n    pipeline {\n        agent any\n        stages {\n            stage('checkout git') {\n                steps {\n                    git branch: pipelineParams.branch, credentialsId: 'GitCredentials', url: pipelineParams.scmUrl\n                }\n            }\n\n            stage('build') {\n                steps {\n                    sh 'mvn clean package -DskipTests=true'\n                }\n            }\n\n            stage ('test') {\n                steps {\n                    parallel (\n                        \"unit tests\": { sh 'mvn test' },\n                        \"integration tests\": { sh 'mvn integration-test' }\n                    )\n                }\n            }\n\n            stage('deploy developmentServer'){\n                steps {\n                    deploy(pipelineParams.developmentServer, pipelineParams.serverPort)\n                }\n            }\n\n            stage('deploy staging'){\n                steps {\n                    deploy(pipelineParams.stagingServer, pipelineParams.serverPort)\n                }\n            }\n\n            stage('deploy production'){\n                steps {\n                    deploy(pipelineParams.productionServer, pipelineParams.serverPort)\n                }\n            }\n        }\n        post {\n            failure {\n                mail to: pipelineParams.email, subject: 'Pipeline failed', body: \"${env.BUILD_URL}\"\n            }\n        }\n    }\n}\n\nNow we can setup the Pipeline of one of our applications with the following method call:\n\nJenkinsfile\n\nmyDeliveryPipeline(branch: 'master', scmUrl: 'ssh://git@myScmServer.com/repos/myRepo.git',\n                   email: 'team@example.com', serverPort: '8080',\n                   developmentServer: 'dev-myproject.mycompany.com',\n                   stagingServer: 'staging-myproject.mycompany.com',\n                   productionServer: 'production-myproject.mycompany.com')\n\nThe Shared library documentation mentions the ability to encapsulate\nsimilarities between several Pipelines with a global variable. It shows how we\ncan enhance our template approach and build a higher-level DSL step:\n\nvars/myDeliveryPipeline.groovy\n\ndef call(body) {\n    // evaluate the body block, and collect configuration into the object\n    def pipelineParams= [:]\n    body.resolveStrategy = Closure.DELEGATE_FIRST\n    body.delegate = pipelineParams\n    body()\n\n    pipeline {\n        // our complete declarative pipeline can go in here\n        ...\n    }\n}\n\nNow we can even use our own DSL-step to set up the integration and deployment Pipeline of our project:\n\nJenkinsfile\n\nmyDeliveryPipeline {\n    branch = 'master'\n    scmUrl = 'ssh://git@myScmServer.com/repos/myRepo.git'\n    email = 'team@example.com'\n    serverPort = '8080'\n    developmentServer = 'dev-myproject.mycompany.com'\n    stagingServer = 'staging-myproject.mycompany.com'\n    productionServer = 'production-myproject.mycompany.com'\n}\n\nThe blog post showed how a common Pipeline template can be developed using the\nShared Library functionality in Jenkins. The approach allows to create a\nstandard Pipeline that can be reused by applications that are built in a\nsimilar way.\n\nIt works for Declarative and Scripted Pipelines as well. For declarative\npipelines the ability to define a Pipeline block in a Shared Library is\nofficial supported since version 1.2 (see the recent blog post on\nDeclarative Pipeline 1.2).","title":"Share a standard Pipeline across multiple projects with Shared Libraries","tags":["pipeline","declarative","microservices"],"authors":[{"avatar":null,"blog":null,"github":"pstrh","html":"","id":"pstrh","irc":null,"linkedin":null,"name":"Philip Stroh","slug":"/blog/authors/pstrh","twitter":null}]}},{"node":{"date":"2017-09-25T00:00:00.000Z","id":"ec7b8ed5-f69c-5e84-9b65-735961d0c5cf","slug":"/blog/2017/09/25/declarative-1.2-released/","strippedHtml":"After a few months of work on its key features, I’m happy to announce the\n1.2 release of\nDeclarative Pipeline!\nOn behalf of the contributors developing Pipeline, I thought it would be\nhelpful to discuss three of the key changes.\n\nParallel Stages\n\nFirst, we’ve added syntax support for parallel stages. In earlier versions of\nDeclarative Pipeline, the only way to run chunks of Pipeline code in parallel\nwas to use the parallel step inside the steps block for a stage, like this:\n\n/* .. snip .. */\nstage('run-parallel-branches') {\n  steps {\n    parallel(\n      a: {\n        echo \"This is branch a\"\n      },\n      b: {\n        echo \"This is branch b\"\n      }\n    )\n  }\n}\n/* .. snip .. */\n\nWhile this works, it doesn’t integrate well with the rest of the Declarative\nPipeline syntax. For example, to run each parallel branch on a different agent,\nyou need to use a node step, and if you do that, the output of the parallel\nbranch won’t be available for post directives (at a stage or pipeline\nlevel). Basically the old parallel step required you to use Scripted Pipeline\nwithin a Declarative Pipeline.\n\nBut now with Declarative Pipeline 1.2, we’ve introduced a true Declarative\nsyntax for running stages in parallel:\n\nJenkinsfile\n\npipeline {\n    agent none\n    stages {\n        stage('Run Tests') {\n            parallel {\n                stage('Test On Windows') {\n                    agent {\n                        label \"windows\"\n                    }\n                    steps {\n                        bat \"run-tests.bat\"\n                    }\n                    post {\n                        always {\n                            junit \"**/TEST-*.xml\"\n                        }\n                    }\n                }\n                stage('Test On Linux') {\n                    agent {\n                        label \"linux\"\n                    }\n                    steps {\n                        sh \"run-tests.sh\"\n                    }\n                    post {\n                        always {\n                            junit \"**/TEST-*.xml\"\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nYou can now specify either steps or parallel for a stage, and within\nparallel, you can specify a list of stage directives to run in parallel,\nwith all the configuration you’re used to for a stage in Declarative\nPipeline. We think this will be really useful for cross-platform builds and\ntesting, as an example. Support for parallel stages will be in the\nsoon-to-be-released Blue Ocean Pipeline Editor 1.3 as well.\n\nYou can find more documentation on parallel stages in the\nUser Handbook.\n\nDefining Declarative Pipelines in Shared Libraries\n\nUntil the 1.2 release, Declarative Pipelines did not officially support\ndefining your pipeline blocks in a shared library. Some of you may have tried\nthat out and found that it could work in some cases, but since it was never an\nofficially supported feature, it was vulnerable to breaking due to necessary\nchanges for the supported use cases of Declarative. But with 1.2, we’ve added\nofficial support for defining pipeline blocks in src/.groovy files in your\nshared libraries. Within your src/.groovy file’s call method, you can\ncall pipeline { …​ }, or possibly different pipeline { …​ } blocks\ndepending on if conditions and the like. Note that only one pipeline { …​ }\nblock can actually be executed per run - you’ll get an error if a second one\ntries to execute!\n\nMajor Improvements to Parsing and Environment Variables\n\nHopefully, you’ll never actually care about this change, but we’re very happy\nabout it nonetheless. The original approach used for actually taking the\npipeline { …​ } block and executing its contents was designed almost two\nyears ago, and wasn’t very well suited to how you all are actually using\nDeclarative Pipelines. In our attempts to work around some of those limitations,\nwe made the parsing logic even more complicated and fragile, resulting in an\nimpressive\nnumber of bugs, mainly relating to inconsistencies and bad behavior with\nenvironment variables.\n\nIn Declarative 1.2, we’ve replaced the runtime parsing logic completely with a\nfar more robust system, which also happens to fix most of those bugs at the\nsame time! While not every issue has been resolved, you may find that you can\nuse environment variables in more places, escaping is more consistent,\nWindows paths are no longer handled incorrectly, and a lot more. Again, we’re\nhoping you’ve never had the misfortune to run into any of these bugs, but if\nyou have, well, they’re fixed now, and it’s going to be a lot easier for us to\nfix any future issues that may arise relating to environment variables, when\nexpressions, and more. Also, the parsing at the very beginning of your build\nmay be about 0.5 seconds faster. =)\n\nMore to Come!\n\nWhile we don’t have any concrete plans for what will be going into Declarative\nPipelines 1.3, rest assured that we’ve got some great new features in mind, as\nwell as our continuing dedication to fixing the bugs you encounter and report.\nSo please do keep opening tickets for\nissues and feature requests. Thanks!","title":"Parallel stages with Declarative Pipeline 1.2","tags":["pipeline","declarative"],"authors":[{"avatar":null,"blog":null,"github":"abayer","html":"<div class=\"paragraph\">\n<p>Andrew was a core committer to Hudson and the author of numerous plugins.</p>\n</div>","id":"abayer","irc":null,"linkedin":null,"name":"Andrew Bayer","slug":"/blog/authors/abayer","twitter":"abayer"}]}},{"node":{"date":"2017-09-08T00:00:00.000Z","id":"529abe58-b0d2-5d12-931f-675712ff69a6","slug":"/blog/2017/09/08/enumerators-in-pipeline/","strippedHtml":"While at Jenkins World, Kohsuke Kawaguchi\npresented two long-time Jenkins contributors with a\n\" Small Matter of Programming\"\naward: Andrew Bayer and\nJesse Glick. \"Small Matter of Programming\"\nbeing:\n\na phrase used to ironically indicate that a suggested feature or design change\nwould in fact require a great deal of effort; it often implies that the person\nproposing the feature underestimates its cost.\n\n— Wikipedia\n\nIn this context the \"Small Matter\" relates to Jenkins\nPipeline and a very simple snippet of Scripted Pipeline:\n\n[1, 2, 3].each { println it }\n\nFor a long time in Scripted Pipeline, this simply did not work as users would\nexpect it. Originally filed as\nJENKINS-26481 in 2015,\nit became one of the most voted for, and watched, tickets in the entire issue\ntracker until it was ultimately fixed earlier this year.\n\nAt least some closures are executed only once inside of Groovy CPS DSL scripts\nmanaged by the workflow plugin.\n\n— Original bug description by Daniel Tschan\n\nAt a high level, what has been confusing for many users is that Scripted\nPipeline looks like a Groovy, it quacks like a Groovy, but it’s not exactly\nGroovy. Rather, there’s an custom Groovy interpreter\n( CPS) that executes the\nScripted Pipeline in a manner which provides the durability/resumability that\ndefines Jenkins Pipeline.\n\nWithout diving into too much detail, refer to the pull requests linked to\nJENKINS-26481 for that, the code snippet above was particularly challenging to\nrectify inside the Pipeline execution layer. As one of the chief architects for\nJenkins Pipeline, Jesse made a number of changes around the problem in 2016,\nbut it wasn’t until early 2017 when Andrew, working on Declarative Pipeline,\nstarted to identify a number of areas of improvement in CPS and provided\nmultiple patches and test cases.\n\nAs luck would have it, combining two of the sharpest minds in the Jenkins\nproject resulted in the \"Small Matter of Programming\" being finished, and\nreleased in May of this year with Pipeline: Groovy 2.33.\n\nPlease join me in congratulating, and thanking, Andrew and Jesse for their\ndiligent and hard work smashing one of the most despised bugs in Jenkins\nhistory :).","title":"Closure on enumerators in Pipeline","tags":["pipeline","jenkinsworld"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2017-08-18T00:00:00.000Z","id":"e2e86b67-d732-51e3-9127-44de5e51de0e","slug":"/blog/2017/08/18/declarative-pipelines-at-jenkinsworld/","strippedHtml":"This is a guest post by Andrew Bayer, who is\none of the authors of the\nDeclarative Pipeline plugin,\nand is a software engineer on the Pipeline team at\nCloudBees, Inc.\n\nA year ago at Jenkins World 2016, we unveiled Declarative Pipeline, a\nstructured way to define your Pipeline. It’s been a great year for Declarative\nand Pipeline in general, with the release of Declarative Pipeline 1.0 in\nFebruary, multiple releases since then, the introduction of\ndocumentation on Pipeline at jenkins.io,\nwith a focus on Declarative, and more. Given everything that’s happened over\nthe last year, we thought it’d be good to let you all know what you can expect\nto see and hear about Declarative Pipeline at this year’s Jenkins World.\n\nFirst, on Thursday, August 31, I’ll be giving a talk on Declarative Pipeline\nwith Robert Sandell, one of my coworkers\nhere at CloudBees and another author of Declarative Pipeline. We’ll be\ncovering what’s happened with Declarative over the last year, new features\nadded since the 1.0 release, such as the libraries directive and more when\nconditions, what’s planned for the upcoming 1.2 release (which is planned for\nshortly after Jenkins World!), including parallel stage s, and what’s on the\nroadmap for the future. In addition, we’ll be demoing some of the features in\n1.2, and providing some pointers on best practices for writing your Declarative\nPipeline.\n\nAlso on Thursday, Stephen Donner from Mozilla\nwill be giving a demo showing Mozilla’s usage of Declarative Pipeline and\nshared libraries at the Community Booth - Mozilla has been doing great work\nwith Declarative, and I’m excited to see their usage in more detail and hear\nStephen talk about their experience!\n\nIn addition, Robert, Stephen, and myself will all be at Jenkins World both days\nof the main sessions, and Robert and myself will also be at the\nContributor Summit\non Tuesday. We’d love to hear your thoughts on Declarative and will be happy to\nanswer any questions that we can. Looking forward to seeing you all!\n\nAndrew Bayer and Robert Sandell will be talking about the latest on\nDeclarative Pipeline in Jenkins\nat Jenkins World in August,\nregister with the code JWFOSS for a 30% discount off your pass.","title":"Declarative Pipeline at Jenkins World","tags":["plugins","pipeline"],"authors":[{"avatar":null,"blog":null,"github":"abayer","html":"<div class=\"paragraph\">\n<p>Andrew was a core committer to Hudson and the author of numerous plugins.</p>\n</div>","id":"abayer","irc":null,"linkedin":null,"name":"Andrew Bayer","slug":"/blog/authors/abayer","twitter":"abayer"}]}},{"node":{"date":"2017-08-10T00:00:00.000Z","id":"7a11360a-42a4-5d64-8401-89a9e4f35904","slug":"/blog/2017/08/10/kubernetes-with-pipeline-acs/","strippedHtml":"This is a guest post by Pui Chee Chen,\nProduct Manager at Microsoft working on\nAzure\nDevOps open source integrations.\n\nRecently, we improved the Azure Credential plugin by\nadding a custom binding for Azure Credentials which allows you to use an\nAzure\nservice principal (the analog to a service or system account) via  the\nCredentials Binding plugin. This means it’s now trivial to run Azure CLI\ncommands from a Jenkins Pipeline. We also recently published the first version\nof the Azure App Service plugin which makes it very\neasy to deploy\nAzure Web\nApps directly from Jenkins Pipeline. While we’ll have\nmuch more to discuss in our Jenkins World presentation on\nAzure\nDevOps open source integrations, in this blog post I wanted to share some good\nsnippets of what is possible today with Jenkins Pipeline and Azure.\n\nFirst, a simple example using the Azure CLI to list resources in the\nsubscription:\n\n// Scripted //\nnode {\n    /* .. snip .. */\n    stage('Deploy') {\n        withCredentials([azureServicePrincipal('principal-credentials-id')]) {\n            sh 'az login --service-principal -u $AZURE_CLIENT_ID -p $AZURE_CLIENT_SECRET -t $AZURE_TENANT_ID'\n            sh 'az account set -s $AZURE_SUBSCRIPTION_ID'\n            sh 'az resource list'\n        }\n    }\n}\n// Declarative //\n\nazureServicePrincipal() cannot be used in Declarative Pipeline until\nJENKINS-46103 is\nresolved.\n\nOnce a Pipeline can interact with Azure, there are countless ways one could\nimplement continuous delivery with Jenkins and Azure. From a deploying a simple\nwebapp with the\nAzure\nApp Service plugin and the azureWebAppPublish step, or a more advanced\ncontainer-based delivery pipeline to deliver new containers to\nKubernetes\nvia Azure Container Service.\n\nWith the Docker Pipeline plugin and a little bit of\nextra scripting, a Jenkins Pipeline can also build and publish a Docker\ncontainer to an\nAzure\nContainer Registry :\n\n// Scripted //\nimport groovy.json.JsonSlurper\n\nnode {\n    def container\n    def acrSettings\n\n    withCredentials([azureServicePrincipal('principal-credentials-id')]) {\n        stage('Prepare Environment') {\n            sh 'az login --service-principal -u $AZURE_CLIENT_ID -p $AZURE_CLIENT_SECRET -t $AZURE_TENANT_ID'\n            sh 'az account set -s $AZURE_SUBSCRIPTION_ID'\n            acrSettings = new JsonSlurper().parseText(\n                                            sh(script: \"az acs show -o json -n my-acr\", returnStdout: true))\n        }\n\n        stage('Build') {\n            container = docker.build(\"${acrSettings.loginServer}/my-app:${env.BUILD_ID}\")\n        }\n\n        stage('Publish') {\n            /* https://issues.jenkins.io/browse/JENKINS-46108 */\n            sh \"docker login -u ${AZURE_CLIENT_ID} -p ${AZURE_CLIENT_SECRET} ${acrSettings.loginServer}\"\n            container.push()\n        }\n\n        stage('Deploy') {\n            echo 'Orchestrating a new deployment with kubectl is a simple exercise left to the reader ;)'\n        }\n    }\n}\n// Declarative //\n\nIf you have been following our\nAzure Blog, you may\nhave noticed we have shipped a lot of updates to provide better support for\nAzure on Jenkins, and vice versa, such as:\n\nHosted Jenkins. New\nSolution\nTemplate in Azure Marketplace lets you spin up a\nJenkins Controller on Azure in minutes. Not only is it easy and fast, the solution\ntemplate gives you option to scale up by selecting the VM disk type and size.\nAnd guess what? You can even select the Jenkins release type you want to use -\nLTS, weekly build or Azure verified - all under your control.\n\nContinuous integration experience. In the latest version of our\nAzure VM Agents plugin, we improved the user\nexperience and added the option to let you to select Managed Disk for disk\ntype (which is currently used extensively on\nci.jenknis.io. You no longer need to worry about\nexceeding the number of VMs on your subscription.\n\nContinuous deployment experience. Now, if\nAzure CLI is not your cup of tea, we released our first plugin to provide\ncontinuous deployment support to Azure App Service. The plugin supports all\nlanguages Azure App Service supports. We even have a walkthrough\nhere in the\nbrand new Jenkins Hub where you can find all Jenkins on Azure resources.\n\nPipeline readiness. Also, all Azure plugins are and will be pipeline ready.\nHave you been leveraging our\nAzure Storage plugin in your Pipeline?\n\nSo, what’s next? We have a big surprise in store at Jenkins World! :)\n\nWe are serious about supporting open source and the open source community.\nBe sure to catch our talk on\nAzure\nDevOps open source integrations.\nSee you at\nJenkins World 2017!\n\nJoin the Azure DevOps team at\nJenkins World in August,\nregister with the code JWFOSS for a 30% discount off your pass.","title":"CI/CD with Jenkins Pipeline and Azure","tags":["plugins","kubernetes","pipeline"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2017-08-08T00:00:00.000Z","id":"2e385886-1033-5881-ab06-b060d5568108","slug":"/blog/2017/08/08/introducing-jenkins-minute/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nThere are less than three weeks left until\nJenkins World 2017.\nLike last year, I’ll be at the\n\" Ask the Experts\"\nbooth to answer questions about all things Jenkins.\nIn preparation, I’ve started a continuing series of quick tutorial videos that answer\nsome of the most common questions I’ve seen asked in the community forums.\nThese  are by no means exhaustive - they’re basic answers, which we can build upon.\nEach video give a takes a simple example, shows how to create a working solution,\nand includes links in the description to related Jenkins documentation pages.\n\nI hope you find them useful.  Look for more of them coming soon!\n\nLiam will be at the\n\" Ask the Experts\"\nbooth at\nJenkins World in August.\nRegister with the code JWFOSS for a 30% discount off your pass.\n\nCreating Your First Pipeline in Blue Ocean\n\nUsing a Dockerfile with Jenkins Pipeline\n\nAdding Parameters to Jenkins Pipeline\n\nRecording Test Results and Archiving Artifacts","title":"Introducing the Jenkins Minute video series","tags":["blueocean","docker","jenkins-minute","pipeline"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}}]}},"pageContext":{"tag":"pipeline","limit":8,"skip":32,"numPages":13,"currentPage":5}},
    "staticQueryHashes": ["3649515864"]}