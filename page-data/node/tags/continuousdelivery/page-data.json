{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/continuousdelivery",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2016-12-20T00:00:00.000Z","id":"07094990-1362-5018-bed6-12c7ddfa53ee","slug":"/blog/2016/12/20/jenkins-puppet-enterprise-plugin/","strippedHtml":"This is a guest post by Carl Caum,\nwho works at Puppet and created the\nPuppet Enterprise Pipeline plugin.\n\nDuring PuppetConf 2016, myself and Brian Dawson from CloudBees announced the\nplugin:puppet-enterprise-pipeline[Puppet Enterprise\nplugin for Jenkins Pipeline].\nLet’s take a look at how the plugin makes it trivial to use Puppet to perform\nsome or all of the deployment tasks in continuous delivery pipelines.\n\nJenkins Pipeline introduced an amazing world where the definition for a\npipeline is managed from the same version control repository as the code\ndelivered by the pipeline. This is a powerful idea, and one I felt complemented\nPuppet’s automation strengths. I wanted to make it trivial to control Puppet\nEnterprise’s orchestration and infrastructure code management capabilities, as\nwell as set hierarchical configuration data and use Puppet’s inventory data\nsystem as a source of truth – all from a Pipeline script. The result was the\nPuppet Enterprise plugin, which fully buys into the Pipeline ideals by\nproviding methods to control the different capabilities in Puppet Enterprise.\nThe methods provide ways to query\nPuppetDB, set\nHiera key/value pairs, deploy\nPuppet code environments with\nCode Management, and kick off orchestrated Puppet runs with the\nOrchestrator.\n\nThe Puppet Enterprise for Jenkins Pipeline plugin\n\nThe Puppet Enterprise for Jenkins Pipeline plugin itself has zero system\ndependencies. You need only to install the plugin from the update center. The\nplugin uses APIs available in Puppet Enterprise to do its work. Since the\nPuppetDB query, Code Management, and Orchestrator APIs are all\nbacked by Puppet Enterprise’s role-based access control (RBAC) system, it’s\neasy to restrict what pipelines are allowed to control in Puppet Enterprise. To\nlearn more about RBAC in Puppet Enterprise,\nread the docs here.\n\nConfiguring\n\nConfiguring the plugin is fairly straight forward. It takes three simple steps:\n\nSet the address of the Puppet server\n\nCreate a Jenkins credential with a Pupppet Enterprise RBAC authentication token\n\nConfigure the Hiera backend\n\nSet the Puppet Enterprise Server Address\n\nGo to Jenkins > Manage Jenkins > Puppet Enterprise page. Put the DNS address of\nthe Puppet server in the Puppet Master Address text field. Click the Test\nConnection button to verify the server is reachable, the Puppet CA certificate\nis retrievable, and HTTPS connections are successful. Once the test succeeds,\nClick Save.\n\nCreate a Jenkins Credentials Entry\n\nThe plugin uses the Jenkins built-in credentials system (the plain-credentials\nplugin) to store and refer RBAC tokens to Puppet Enterprise for authentication\nand authorization. First, generate an RBAC token in Puppet Enterprise by\nfollowing\nthe\ninstructions on the docs site. Next, create a new Jenkins Credentials item\nwith Kind Secret text and the Secret value the Puppet Enterprise RBAC\ntoken. It’s highly recommended to give the credential an ID value that’s\ndescriptive and identifiable. You’ll use it in your Pipeline scripts.\n\nIn your Jenkinsfile, use the puppet.credentials method to set all future Puppet\nmethods to use the RBAC token. For example:\n\npuppet.credentials 'pe-team-token'\n\nConfigure the Hiera Backend\n\nThe plugin exposes an HTTP API for performing Hiera data lookups for key/value\npairs managed by Pipeline jobs. To configure Hiera on the Puppet compile\nmaster(s) to query the Jenkins Hiera data store backend, use the\nhiera-http backend. On the\nPuppet Enterprise compile master(s), run the following commands:\n\n/opt/puppetlabs/puppet/bin/gem install hiera-http\n/opt/puppetlabs/bin/puppetserver gem install hiera-http\n\nNow you can configure the /etc/puppetlabs/puppet/hiera.yaml file. The following\nconfiguration instructs Hiera to first look to the Hiera yaml files in the\nPuppet code’s environment, then fall back to the http backend. The http backend\nwill first query the Hiera data store API looking for the key in the scope with\nthe same name as the node. If nothing’s found, look for the key in the node’s\nenvironments. You can use any Facter fact to match scope names.\n\n:backends:\n  - yaml\n  - http\n\n:http:\n  :host: jenkins.example.com\n  :port: 8080\n  :output: json\n  :use_auth: true\n  :auth_user:\n:auth_pass:\n:cache_timeout: 10\n  :failure: graceful\n  :paths:\n    - /hiera/lookup?path=%{clientcert}&key=%{key}\n    - /hiera/lookup?path=%{environment}&key=%{key}\n\nFinally, restart the pe-puppetserver process to pick up the new configs:\n\n/opt/puppetlabs/bin/puppet resource service pe-puppetserver ensure=stopped\n/opt/puppetlabs/bin/puppet resource service pe-puppetserver ensure=running\n\nHiera HTTP Authentication\n\nIf Jenkins' Global Security is configured to allow unauthenticated read-only\naccess, the 'use_auth', 'auth_pass', and 'auth_user' parameters are\nunnecessary. Otherwise, create a local Jenkins user that has permissions to\nview the Hiera Data Lookup page and use that user’s credentials for the\nhiera.yaml configuration.\n\nQuerying the infrastructure\n\nPuppetDB is an extensive data store that holds every bit of information Puppet\ngenerates and collects across every system Puppet is installed on. PuppetDB\nprovides a sweet query language called\nPQL. With PQL,\nyou can ask complex questions of your infrastructure such as \"How many\nproduction Red Hat systems are there with the openssl package installed?\" or\n\"What us-west-2c nodes with the MyApp role that were created in the last 24\nhours?\"\n\nThis can be a powerful tool for parts of your pipeline where you need to\nperform specific operations on subsets of the infrastructure like draining a\nloadbalancer.\n\nHere’s an example using the puppet.query method:\n\nresults = puppet.query '''\n  inventory[certname] {\n    facts.os.name = \"RedHat\" and\n    facts.ec2_metadata.placement.availability-zone = \"us-west-2c\" and\n    facts.uptime_hours < 24\n  }'''\n\nThe query returns an array of matching items. The results can be\niterated on, and even passed to a series of puppet.job calls. For example, the\nfollowing code will query all nodes in production that experienced a failure on\nthe last Puppet run.\n\nresults = puppet.query 'nodes { latest_report_status = \"failed\" and catalog_environment = \"production\"}'\n\nNote that once you can use closures in Pipeline scripts, doing the above\nexample will be much simpler.\n\nCreating an orchestrator job\n\nThe orchestration service in Puppet Enterprise is a tool to perform\norchestrated Puppet runs across as broad or as targeted an infrastructure as\nyou need at different parts of a pipeline. You can use the orchestrator to\nupdate applications in an environment, or update a specific list of nodes, or\nupdate nodes across a set of nodes that match certain criteria. In each\nscenario, Puppet will always push distributed changes in the correct order by\nrespecting the cross-node dependencies.\n\nTo create a job in the Puppet orchestrator from a Jenkins pipeline, use the\npuppet.job method. The puppet.job method will create a new orchestrator job,\nmonitor the job for completion, and determine if any Puppet runs failed. If\nthere were failures, the pipeline will fail.\n\nThe following are just some examples of how to run Puppet orchestration jobs against the infrastructure you need to target.\n\nTarget an entire environment:\n\npuppet.job 'production'\n\nTarget instances of an application in production:\n\npuppet.job 'production', application: 'Myapp'\n\nTarget a specific list of nodes:\n\npuppet.job 'production', nodes: ['db.example.com','appserver01.example.com','appserver02.example.com']\n\nTarget nodes matching a complex set if criteria:\n\npuppet.job 'production', query: 'inventory[certname] { facts.os.name = \"RedHat\" and facts.ec2_metadata.placement.availability-zone = \"us-west-2c\" and uptime_hours < 24 }'\n\nAs you can see, the puppet.job command means you can be as broad or as targeted\nas you need to be for different parts of your pipeline. There are many other\noptions you can add to the puppet.job method call, such as setting the Puppet\nruns to noop, or giving the orchestrator a maximum concurrency limit.\nLearn\nmore about the orchestrator here.\n\nUpdating Puppet code\n\nIf you’re using Code Management in Puppet Enterprise (and you should), you can\nensure that all the modules, site manifests, Hiera data, and roles and profiles\nare staged, synced, and ready across all your Puppet masters, direct from your\nJenkins pipeline.\n\nTo update Puppet code across all Puppet masters, use the puppet.codeDeploy method:\n\npuppet.codeDeploy 'staging'\n\nLearn more Code Management in Puppet Enterprise here.\n\nSetting Hiera values\n\nThe plugin includes an experimental feature to set Hiera key/value pairs. There\nare many cases where you need to promote information through a pipeline, such\nas a build version or artifact location. Doing so is very difficult in Puppet,\nsince data promotion almost always involves changing Hiera files and committing\nto version control.\n\nThe plugin exposes an HTTP API endpoint that Hiera can query using the\nhiera-http backend. With the backend configured on the Puppet master(s),\nkey/value pairs can be set to scopes. A scope is arbitrary and can be anything\nyou like, such as a Puppet environment, a node’s certname, or the name of a\nFacter fact like operatingsystem or domain.\n\nTo set a Hiera value from a pipeline, use the puppet.hiera method.\n\npuppet.hiera scope: 'staging', key: 'build-version', value: env.BUILD_ID\n\nNow you can set the same key with the same value to the production scope later\nin the pipeline, followed by a call to puppet.job to push the change out.\n\nExamples\n\nThe\nplugin’s\nGithub repository contains a set of example Pipeline scripts. Feel free to\nissue pull requests to add your own scripts!\n\nWhat’s next\n\nI’m pretty excited to see how this is going to help simplify continuous\ndelivery pipelines. I encourage everyone to get started with continuous\ndelivery today, even if it’s just a simple pipeline. As your practices evolve,\nyou can begin to add automated tests, automate away manual checkpoints, start\nto incorporate InfoSec tests, and include phases for practices like patch\nmanagement that require lots of manual approvals, verifications and rollouts.\nYou’ll be glad you did.","title":"Continuous Delivery with Jenkins and Puppet Enterprise","tags":["continuousdelivery","puppet","pipeline","puppetenterprise"],"authors":[{"avatar":null,"blog":null,"github":"ccaum","html":"","id":"ccaum","irc":null,"linkedin":null,"name":"Carl Caum","slug":"/blog/authors/ccaum","twitter":"ccaum"}]}},{"node":{"date":"2016-08-10T00:00:00.000Z","id":"4f2eb2b7-1bb1-501e-8b2c-1b8c4ac3e6e3","slug":"/blog/2016/08/10/rails-cd-with-pipeline/","strippedHtml":"This is a guest post by R. Tyler Croy, who is a\nlong-time contributor to Jenkins and the primary contact for Jenkins project\ninfrastructure. He is also a Jenkins Evangelist at\nCloudBees, Inc.\n\nWhen the Ruby on Rails framework debuted it\nchanged the industry in two noteworthy ways: it created a trend of opinionated web\napplication frameworks ( Django,\nPlay, Grails) and it\nalso strongly encouraged thousands of developers to embrace test-driven\ndevelopment along with many other modern best practices (source control, dependency\nmanagement, etc). Because Ruby, the language underneath Rails, is interpreted\ninstead of compiled there isn’t a \"build\" per se but rather tens, if not\nhundreds, of tests, linters and scans which are run to ensure the application’s\nquality. With the rise in popularity of Rails, the popularity of application\nhosting services with easy-to-use deployment tools like Heroku or\nEngine Yard rose too.\n\nThis combination of good test coverage and easily automated deployments\nmakes Rails easy to continuously deliver with Jenkins. In this post we’ll cover\ntesting non-trivial Rails applications with Jenkins\nPipeline and, as an added bonus, we will add security scanning via\nBrakeman and the\nBrakeman\nplugin.\n\nTopics\n\nPreparing the app\n\nPreparing Jenkins\n\nWriting the Pipeline\n\nSecurity scanning\n\nDeploying the good stuff\n\nWrap up\n\nFor this demonstration, I used Ruby Central 's\ncfp-app :\n\nA Ruby on Rails application that lets you manage your conference’s call for\nproposal (CFP), program and schedule. It was written by Ruby Central to run the\nCFPs for RailsConf and RubyConf.\n\nI chose this Rails app, not only because it’s a sizable application with lots\nof tests, but it’s actually the application we used to collect talk proposals\nfor the \" Community Tracks\" at this\nyear’s Jenkins World. For the most part,\ncfp-app is a standard Rails application. It uses\nPostgreSQL for its database,\nRSpec for its tests and\nRuby 2.3.x as its runtime.\n\nIf you prefer to just to look at the code, skip straight to the\nJenkinsfile.\n\nPreparing the app\n\nFor most Rails applications there are few, if any, changes needed to enable\ncontinuous delivery with Jenkins. In the case of\ncfp-app, I added two gems to get\nthe most optimal integration into Jenkins:\n\nci_reporter, for test report\nintegration\n\nbrakeman, for security scanning.\n\nAdding these was simple, I just needed to update the Gemfile and the\nRakefile in the root of the repository to contain:\n\nGemfile\n\n# .. snip ..\ngroup :test do\n  # RSpec, etc\n  gem 'ci_reporter'\n  gem 'ci_reporter_rspec'\n  gem \"brakeman\", :require => false\nend\n\nRakefile\n\n# .. snip ..\nrequire 'ci/reporter/rake/rspec'\n# Make sure we setup ci_reporter before executing our RSpec examples\ntask :spec => 'ci:setup:rspec'\n\nPreparing Jenkins\n\nWith the cfp-app project set up, next on the list is to ensure that Jenkins itself\nis ready. Generally I suggest running the latest LTS of\nJenkins; for this demonstration I used Jenkins 2.7.1 with the following\nplugins:\n\nPipeline plugin\n\nBrakeman plugin\n\nCloudBees\nDocker Pipeline plugin\n\nI also used the\nGitHub\nOrganization Folder plugin to automatically create pipeline items in my\nJenkins instance; that isn’t required for the demo, but it’s pretty cool to see\nrepositories and branches with a Jenkinsfile automatically show up in\nJenkins, so I recommend it!\n\nIn addition to the plugins listed above, I also needed at least one\nJenkins agent with the Docker daemon installed and\nrunning on it. I label these agents with \"docker\" to make it easier to assign\nDocker-based workloads to them in the future.\n\nAny Linux-based machine with Docker installed will work, in my case I was\nprovisioning on-demand agents with the\nAzure\nplugin which, like the\nEC2 plugin,\nhelps keep my test costs down.\n\nIf you’re using Amazon Web Services, you might also be interested in\nthis blog post from\nearlier this year unveiling the\nEC2\nFleet plugin for working with EC2 Spot Fleets.\n\nWriting the Pipeline\n\nTo make sense of the various things that the Jenkinsfile needs to do, I find\nit easier to start by simply defining the stages of my pipeline. This helps me\nthink of, in broad terms, what order of operations my pipeline should have.\nFor example:\n\n/* Assign our work to an agent labelled 'docker' */\nnode('docker') {\n    stage 'Prepare Container'\n    stage 'Install Gems'\n    stage 'Prepare Database'\n    stage 'Invoke Rake'\n    stage 'Security scan'\n    stage 'Deploy'\n}\n\nAs mentioned previously, this Jenkinsfile is going to rely heavily on the\nCloudBees\nDocker Pipeline plugin. The plugin provides two very important features:\n\nAbility to execute steps inside of a running Docker container\n\nAbility to run a container in the \"background.\"\n\nLike most Rails applications, one can effectively test the application with two\ncommands: bundle install followed by bundle exec rake. I already had some\nDocker images prepared with RVM and Ruby 2.3.0 installed,\nwhich ensures a common and consistent starting point:\n\nnode('docker') {\n    // .. 'stage' steps removed\n    docker.image('rtyler/rvm:2.3.0').inside { (1)\nrvm 'bundle install' (2)\nrvm 'bundle exec rake'\n    } (3)\n}\n\n1\nRun the named container. The inside method can take optional additional flags for the docker run command.\n\n2\nExecute our shell commands using our tiny sh step wrapper\nrvm . This ensures that the shell code is executed in the correct RVM environment.\n\n3\nWhen the closure completes, the container will be destroyed.\n\nUnfortunately, with this application, the bundle exec rake command will fail\nif PostgreSQL isn’t available when the process starts. This is where the\nsecond important feature of the CloudBees Docker Pipeline plugin comes\ninto effect: the ability to run a container in the \"background.\"\n\nnode('docker') {\n    // .. 'stage' steps removed\n    /* Pull the latest `postgres` container and run it in the background */\n    docker.image('postgres').withRun { container -> (1)\necho \"PostgreSQL running in container ${container.id}\" (2)\n} (3)\n}\n\n1\nRun the container, effectively docker run postgres\n\n2\nAny number of steps can go inside the closure\n\n3\nWhen the closure completes, the container will be destroyed.\n\nRunning the tests\n\nCombining these two snippets of Jenkins Pipeline is, in my opinion, where the\npower of the DSL\nshines:\n\nnode('docker') {\n    docker.image('postgres').withRun { container ->\n        docker.image('rtyler/rvm:2.3.0').inside(\"--link=${container.id}:postgres\") { (1)\nstage 'Install Gems'\n            rvm \"bundle install\"\n\n            stage 'Invoke Rake'\n            withEnv(['DATABASE_URL=postgres://postgres@postgres:5432/']) { (2)\nrvm \"bundle exec rake\"\n            }\n            junit 'spec/reports/*.xml' (3)\n}\n    }\n}\n\n1\nBy passing the --link argument, the Docker daemon will allow the RVM container to talk to the PostgreSQL container under the host name 'postgres'.\n\n2\nUse the withEnv step to set environment variables for everything that is in the closure. In this case, the cfp-app DB scaffolding will look for the DATABASE_URL variable to override the DB host/user/dbname defaults.\n\n3\nArchive the test reports generated by ci_reporter so that Jenkins can display test reports and trend analysis.\n\nWith this done, the basics are in place to consistently run the tests for\ncfp-app in fresh Docker containers for each execution of the pipeline.\n\nSecurity scanning\n\nUsing Brakeman, the security scanner for Ruby\non Rails, is almost trivially easy inside of Jenkins Pipeline, thanks to the\nBrakeman\nplugin which implements the publishBrakeman step.\n\nBuilding off our example above, we can implement the \"Security scan\" stage:\n\nnode('docker') {\n    /* --8 (1)\npublishBrakeman 'brakeman-output.tabs' (2)\n/* --8\n\n1\nRun the Brakeman security scanner for Rails and store the output for later in brakeman-output.tabs\n\n2\nArchive the reports generated by Brakeman so that Jenkins can display detailed reports with trend analysis.\n\nAs of this writing, there is work in progress\n( JENKINS-31202) to\nrender trend graphs from plugins like Brakeman on a pipeline project’s main\npage.\n\nDeploying the good stuff\n\nOnce the tests and security scanning are all working properly, we can start to\nset up the deployment stage. Jenkins Pipeline provides the variable\ncurrentBuild which we can use to determine whether our pipeline has been\nsuccessful thus far or not. This allows us to add the logic to only deploy when\neverything is passing, as we would expect:\n\nnode('docker') {\n    /* --8 (1)\nsh './deploy.sh' (2)\n}\n    else {\n        mail subject: \"Something is wrong with ${env.JOB_NAME} ${env.BUILD_ID}\",\n                  to: 'nobody@example.com',\n                body: 'You should fix it'\n    }\n    /* --8\n\n1\ncurrentBuild has the result property which would be 'SUCCESS', 'FAILED', 'UNSTABLE', 'ABORTED'\n\n2\nOnly if currentBuild.result is successful should we bother invoking our deployment script (e.g. git push heroku master)\n\nWrap up\n\nI have gratuitously commented the full\nJenkinsfile\nwhich I hope is a useful summation of the work outlined above. Having worked\non a number of Rails applications in the past, the consistency provided by\nDocker and Jenkins Pipeline above would have definitely improved those\nprojects' delivery times. There is still room for improvement however, which\nis left as an exercise for the reader. Such as: preparing new containers with\nall their\ndependencies\nbuilt-in instead of installing them at run-time. Or utilizing the parallel\nstep for executing RSpec across multiple Jenkins agents simultaneously.\n\nThe beautiful thing about defining your continuous delivery, and continuous\nsecurity, pipeline in code is that you can continue to iterate on it!","title":"Continuous Security for Rails apps with Pipeline and Brakeman","tags":["tutorial","ruby","pipeline","rails","brakeman","continuousdelivery"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-04-07T00:00:00.000Z","id":"dc2d5625-7a35-5883-9c50-152fda1c584a","slug":"/blog/2016/04/07/jenkins-community-survey-results-blog/","strippedHtml":"This is a guest post by Brian\nDawson at CloudBees, where he works as a DevOps Evangelist responsible for\ndeveloping and sharing continuous delivery and DevOps best practices. He also\nserves as the CloudBees Product Marketing Manager for Jenkins.\n\nLast fall CloudBees asked attendees at the Jenkins User Conference – US West\n(JUC), and other in the Jenkins community to take a survey.  Almost 250 people\ndid – and thanks to their input, we have results which provided interesting\ninsights into how Jenkins is being used.\n\nBack in 2012, at the time of the last community survey, 83% of respondents felt\nthat Jenkins was mission-critical. By 2015, the percentage saying that\nJenkins was mission-critical was 92%. Additionally, echoing the\nimportance of Jenkins, 89% of respondents said their use of Jenkins had\nincreased over the last year, while 11% said it had stayed the same. 0%\nsaid that it had decreased.\n\nThe trend in the industry over the last couple of years has been to adopt\ncontinuous delivery (CD), thus pushing automation further down the pipeline –\nfrom development all the way into production.  Jenkins being an automation\nengine applicable to any phase of the software delivery lifecycle, is readily\nsupporting this trend. Jenkins' extensible architecture and unparalleled plugin\necosystem enables integration with and orchestration of practically any tool in\nany phase of software delivery.\n\nThe trend towards adoption of CD is clearly reflected amongst the community: 59%\nof respondents are using Jenkins for continuous integration (CI), but an\nadditional 30% have extended CI into CD and are manually deploying code to\nproduction.  Finally, 11% are practicing continuous deployment – they have\nextended CI to CD and are deploying code automatically into production.\n\nAnother trend tied to the adoption of CD and DevOps is the frequent deployment\nof incremental releases to production. 26% of those respondents using continuous\ndelivery practices are deploying code at least once per day.  Another 37% are\ndeploying code at least once per week.\n\nIn keeping with the move to CD, 30% of survey takers are already using the\nrelatively new Pipeline plugin to automate their\nsoftware delivery pipelines.  Of those not using the Pipeline plugin, 79% plan\nto adopt it in the next 12 months.\n\nSurvey respondents are also using Jenkins for many different activities.  97% of\nsurvey takers use it for \"build\" – no surprise, since that is where Jenkins got\nits start - but 58% now also use it for their deployment.\n\nWhen the 2012 community survey was conducted, container technology was not as\nwell understood as it is today,  and many didn’t know what a “Docker” was. A\nshort four years later, 96% of survey respondents who use Linux containers are\nusing Docker.  Container technology has seen impressive adoption and arguably is\nrevolutionizing the way application infrastructure is delivered.  When coupled\nwith Jenkins as an automation engine, containers help accelerate software\ndelivery by providing rapid access to lightweight environments.  The Jenkins\ncommunity has recognized and embraced the power of containers by\nproviding plugins for Docker and Kubernetes.\n\nThe Jenkins improvements which survey respondents desired the most were\nquality/timely bug fixes, a better UI and more documentation/examples.\nInterestingly, Jenkins 2.0 - which is just about to officially launch,\nprovides UI improvements and the new Jenkins.io website\nprovides improved, centralized documentation.\n\nFinally, the respondents favorite Star Wars character was R2-D2, followed by\nObi-Wan and Darth Vader. Yoda and Han Solo also got a fair amount of votes. The\nvotes for Jar-Jar Binks and Jabba the Hutt left us puzzled. Notably, BB-8 had a\nwrite-in vote despite the fact the new Star Wars movie hadn’t been released yet.\n\nAs to where the community is headed, our prediction is that by the next Jenkins Community Survey:\n\nMore Jenkins users will have transitioned from just continuous\nintegration to continuous delivery with some evening practicing continuous\ndeployment\n\nPipeline plugin adoption and improvements will continue, leading to\npipeline-as-code becoming an essential solution for automating the software\n(and infrastructure) delivery process\n\nThere will be a significant increase in use of the Docker plugin to support\nelastic Jenkins infrastructure and continuous delivery of containers using\nsoftware development best practices\n\nBB-8 will be the next favorite Star Wars character! <3</p>\n\nSee you at Jenkins World, September 13-15, in Santa Clara, California!\nRegister now for the largest Jenkins event on the planet in 2016 – and get the Early Bird discount. The Call for Papers is still open – so submit a talk and share your knowledge with the community about Jenkins.\n\n2015 Community Survey Results (PDF)\n\nState of Jenkins Infographic (PDF)","title":"Jenkins Community Survey Results","tags":["continuousdelivery","pipeline","docker"],"authors":[{"avatar":null,"blog":null,"github":"bvdawson","html":"<div class=\"paragraph\">\n<p>DevOps dude at CloudBees.\nJenkins Marketing Manager.\nTools geek.</p>\n</div>","id":"bvdawson","irc":null,"linkedin":null,"name":"Brian Dawson","slug":"/blog/authors/bvdawson","twitter":"brianvdawson"}]}}]}},"pageContext":{"tag":"continuousdelivery","limit":8,"skip":0,"numPages":1,"currentPage":1}},
    "staticQueryHashes": ["3649515864"]}