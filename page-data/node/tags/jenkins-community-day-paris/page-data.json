{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/jenkins-community-day-paris",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2017-07-21T00:00:00.000Z","id":"215b578b-be74-5ce1-b9f3-38b9bfc35f5c","slug":"/blog/2017/07/21/scaling-jenkins-with-kubernetes-on-google-container-engine/","strippedHtml":"This is a guest post by Guillaume Laforge,\nDeveloper Advocate for Google Cloud\n\nLast week, I had the pleasure to speak at the\nJenkins Community Day conference, in Paris,\norganized by my friends from JFrog,\nprovider of awesome tools for software management and distribution.\nI covered how to scale Jenkins with Kubernetes on\nGoogle Container Engine.\n\nFor the impatient, here are the slides of the presentation I’ve given:\n\nBut let’s step back a little. In this article, I’d like to share with you why you would want to run Jenkins in the cloud,\nas well as give you some pointers to interesting resources on the topic.\n\nWhy running Jenkins in the cloud?\n\nSo why running Jenkins in the cloud? First of all, imagine your small team, working on a single project.\nYou have your own little server, running under a desk somewhere, happily building your application on each commit,\na few times a day. So far so good, your build machine running Jenkins isn’t too busy, and stays idle most of the day.\n\nLet’s do some bottom of the napkin calculations. Let’s say you have a team of 3 developers,\ncommitting roughly 4 times a day, on one single project, and the build takes roughly 10 minutes to go.\n\n3 developers * 4 commits / day / developer * 10 minutes build time * 1 project = 1 hour 20 minutes\n\nSo far so good, your server indeed stays idle most of the day. Usually, at most,\nyour developers will wait just 10 minutes to see the result of their work.\n\nBut your team is growing to 10 persons, the team is still as productive, but the project becoming bigger,\nthe build time goes up to 15 minutes:\n\n10 developers * 4 commits / day / developer * 15 minutes build time * 1 project = 10 hours\n\nYou’re already at 10 hours build time, so your server is busy the whole day, and at times,\nyou might have several build going on at the same time, using several CPU cores in parallel.\nAnd instead of building in 15 minutes, sometimes, the build might take longer, or your build might be queued.\nSo in theory, it might be 15 minutes, but in practice, it could be half an hour because of the length of the queue\nor the longer time to build parallel projects.\n\nNow, the company is successful, and has two projects instead of one (think a backend and a mobile app).\nYour teams grow further up to 20 developers per project. The developers are a little less productive\nbecause of the size of the codebase and project, so they only commit 3 times a day.\nThe build takes more time too, at 20 minutes (in ideal time). Let’s do some math again:\n\n20 developers * 3 commits / day / developer * 20 minutes build time * 2 projects = 40 hours\n\nWoh, that’s already 40 hours of total build time, if all the builds are run serially.\nFortunately, our server is multi-core, but still, there are certainly already many builds that are enqueued,\nand many of them, perhaps up to 2-3 or perhaps even 4 could be run in parallel.\nBut as we said, the build queue increases further, the real effective time of build is certainly longer than 30 minutes.\nPerhaps at times, developers won’t see the result of their developments before at least an hour, if not more.\n\nOne last calculation? With team sizes of 30 developers, decreased productivity of 2 commits, 25 build time,\nand 3 projects? And you’ll get 75 hours total build time. You may start creating a little build farm,\nwith a controller and several build agents. But you also increase the burden of server management.\nAlso, if you move towards a full Continuous Delivery or Continuous Deployment approach,\nyou may further increase your build times to go up to deployment, make more but smaller commits, etc.\nYou could think of running builds less often, or even on a nightly basis, to cope with the demand, but then,\nyour company is less agile, and the time-to-market for fixes of new features might increase,\nand your developers may also become more frustrated because they are developing in the blind,\nnot knowing before the next day if their work was successful or not.\n\nWith my calculations, you might think that it makes more sense for big companies, with tons of projects and developers.\nThis is quite true, but when you’re a startup, you also want to avoid taking care of local server management,\nprovisioning, etc. You want to be agile, and use only compute resources you need for the time you need them.\nSo even if you’re a small startup, a small team, it might still make sense to take advantage of the cloud.\nYou pay only for the actual time taken by your builds as the build agent containers are automatically provisioned\nand decommissioned. The builds can scale up via Kubernetes, as you need more (or less) CPU time for building everything.\n\nAnd this is why I was happy to dive into scaling Jenkins in the cloud. For that purpose,\nI decided to go with building with containers, with Kubernetes, as my app was also containerized as well.\nGoogle Cloud offers Container Engine, which is basically just Kubernetes in the cloud.\n\nUseful pointers\n\nI based my presentation and demo on some great solutions that are published on the Google Cloud documentation portal.\nLet me give you some pointers.\n\nOverview of Jenkins on Container Engine\n\nSetting up Jenkins on Container Engine\n\nConfiguring Jenkins for Container Engine\n\nContinuous Deployment to Container Engine using Jenkins\n\nLab: Build a Continuous Deployment Pipeline with Jenkins and Kubernetes\n\nThe latter one is the tutorial I actually followed for the demo that I presented during the conference.\nIt’s a simple Go application, with a frontend and backend.\nIt’s continuously build, on each commit (well, every minute to check if there’s a new commit),\nand deployed automatically in different environments: dev, canary, production.\nThe sources of the project are stored in Cloud Source Repository (it can be mirrored from Github, for example).\nThe containers are stored in Cloud Container Registry.\nAnd both the Jenkins controller and agents, as well as the application are running inside Kubernetes clusters in Container Engine.\n\nSummary and perspective\n\nDon’t bother with managing servers! Quickly, you’ll run out of CPU cycles,\nand you’ll have happier developers with builds that are super snappy!\n\nAnd for the record, at Google, dev teams are also running Jenkins!\nThere was a presentation ( video and\nslides\navailable) given last year by David Hoover at Jenkins World\ntalking about how developers inside Google are running hundreds of build agents to build projects on various platforms.","title":"Scaling Jenkins with Kubernetes on Google Container Engine","tags":["jenkins","kubernetes","jenkins-community-day-paris"],"authors":[]}},{"node":{"date":"2017-07-07T00:00:00.000Z","id":"7541af12-10e1-5d79-bcc7-41f0ae0d3010","slug":"/blog/2017/07/07/jenkins-conan/","strippedHtml":"This is a guest post by Luis Martínez de Bartolomé,\nConan Co-Founder\n\nC and C++ are present in very important industries today, including Operating Systems, embedded systems, finances, research, automotive, robotics, gaming, and many more. The main reason for this is performance, which is critical to many of these industries, and cannot be compared to any other technology.\nAs a counterpart, the C/C++ ecosystem has a few important challenges to face:\n\nHuge projects - With millions of lines of code, it’s very hard to manage your projects without using modern tools.\n\nApplication Binary Interface (ABI) incompatibility - To guarantee the compatibility of a library with other libraries and your application,  different configurations (such as the operating system, architecture, and compiler) need to be under control.\n\nSlow compilation times - Due to header inclusion and pre-processor bloat, together with the challenges mentioned above, it requires special attention to optimize the process and rebuild only the libraries that need to be rebuilt.\n\nCode linkage and inlining - A static C/C++ library can embed headers from a dependent library. Also, a shared library can embed a static library. In both cases, you need to manage the rebuild of your library when any of its dependencies change.\n\nVaried ecosystem - There are many different compilers and build systems, for different platforms, targets and purposes.\n\nThis post will show how to implement DevOps best practices for C/C++ development, using Jenkins CI, Conan C/C++ package manager, and JFrog Artifactory the universal artifact repository.\n\nConan, The C/C++ Package Manager\n\nConan was born to mitigate these pains.\n\nConan uses python recipes, describing how to build a library by explicitly calling any build system, and also describing the needed information for the consumers (include directories, library names etc.).\nTo manage the different configurations and the ABI compatibility, Conan uses \"settings\" (os, architecture, compiler…). When a setting is changed, Conan generates a different binary version for the same library:\n\nThe built binaries can be uploaded to JFrog Artifactory or Bintray, to be shared with your team or the whole community. The developers in your team won’t need to rebuild the libraries again, Conan will fetch only the needed Binary packages matching the user’s configuration from the configured remotes (distributed model).\nBut there are still some more challenges to solve:\n\nHow to manage the development and release process of your C/C++ projects?\n\nHow to distribute your C/C++ libraries?\n\nHow to test your C/C++ project?\n\nHow to generate multiple packages for different configurations?\n*How to manage the rebuild of the libraries when one of them changes?\n\nConan Ecosystem\n\nThe Conan ecosystem is growing fast, and DevOps with C/C++ is now a reality:\n\nJFrog Artifactory manages the full development and releasing cycles.\n\nJFrog Bintray is the universal distribution hub.\n\nJenkins automates the project testing, generates different binary configurations of your Conan packages, and automates the rebuilt libraries.\n\nJenkins Artifactory plugin\n\nProvides a Conan DSL, a very generic but powerful way to call Conan from a Jenkins Pipeline script.\n\nManages the remote configuration with your Artifactory instance, hiding the authentication details.\n\nCollects from any Conan operation (installing/uploading packages) all the involved artifacts to generate and publish the buildInfo to Artifactory. The buildInfo object is very useful, for example, to promote the created Conan packages to a different repository and to have full traceability of the Jenkins build:\n\nHere’s an example of the Conan DSL with the Artifactory plugin.  First we configure the Artifactory repository, then retrieve the dependencies and finally build it:\n\ndef artifactory_name = \"artifactory\"\ndef artifactory_repo = \"conan-local\"\ndef repo_url = 'https://github.com/memsharded/example-boost-poco.git'\ndef repo_branch = 'master'\n\nnode {\n   def server\n   def client\n   def serverName\n\nstage(\"Get project\"){\n    git branch: repo_branch, url: repo_url\n}\n\nstage(\"Configure Artifactory/Conan\"){\n    server = Artifactory.server artifactory_name\n    client = Artifactory.newConanClient()\n    serverName = client.remote.add server: server, repo: artifactory_repo\n}\n\nstage(\"Get dependencies and publish build info\"){\n    sh \"mkdir -p build\"\n    dir ('build') {\n      def b = client.run(command: \"install ..\")\n      server.publishBuildInfo b\n    }\n}\n\nstage(\"Build/Test project\"){\n        dir ('build') {\n          sh \"cmake ../ && cmake --build .\"\n        }\n    }\n}\n\nYou can see in the above example that the Conan DSL is very explicit. It helps a lot with common operations, but also allows powerful and custom integrations. This is very important for C/C++ projects, because every company has a very specific project structure, custom integrations etc.\n\nComplex Jenkins Pipeline operations: Managed and parallelized libraries building\n\nAs we saw at the beginning of this blog post, it’s crucial to save time when building a C/C++ project. Here are several ways to optimize the process:\n\nOnly re-build the libraries that need to be rebuilt. These are the libraries that  have been affected by a dependant library that has changed.\n\nBuild in parallel, if possible. When there is no relation between two or more libraries in the project graph, you can build them in parallel.\n\nBuild different configurations (os, compiler, etc) in parallel. Use different agents if needed.\n\nLet’s see an example using Jenkins Pipeline feature\n\nThe above graph represents our project P and its dependencies (A-G). We want to distribute the project for two different architectures, x86 and x86_64.\n\nWhat happens if we change library A?\n\nIf we bump the version to A(v1) there is no problem, we can update the B requirement and also bump its version to B(v1) and so on. The complete flow would be as follows:\n\nPush A(v1) version to Git, Jenkins will build the x86 and x86_64 binaries. Jenkins will upload all the packages to Artifactory.\n\nManually change B to v1, now depending on A1, push to Git, Jenkins will build the B(v1) for x86 and x86_64 using the retrieved new A1 from Artifactory.\n\nRepeat the same process for C, D, F, G and finally our project.\n\nBut if we are developing our libraries in a development repository, we probably depend on the latest A version or will override A (v0) packages on every git push, and we want to automatically rebuild the affected libraries in this case B, D, F, G and P.\n\nHow we can do this with Jenkins Pipelines?\n\nFirst we need to know which libraries need to be rebuilt. The \"conan info --build_order\" command identifies the libraries that were changed in our project, and also tells us which can be rebuilt in parallel.\n\nSo, we created two Jenkins pipelines tasks:\n\nThe\"SimpleBuild\" task which builds every single library. Similar to the first example using Conan DSL with the Jenkins Artifactory plugin. It’s a parameterized task that receives the libraries that need to built.\n\nThe\"MultiBuild\" task which coordinates and launches the \" SimpleBuild\" tasks, in parallel when possible.\n\nWe also have a repository with a configuration yml. The Jenkins tasks will use it to know where the recipe of each library is, and the different profiles to be used. In this case they are x86 and x86_64.\n\nleaves:\n  PROJECT:\n    profiles:\n       - ./profiles/osx_64\n       - ./profiles/osx_32\n\nartifactory:\n  name: artifactory\n  repo: conan-local\n\nrepos:\n LIB_A/1.0:\n   url: https://github.com/lasote/skynet_example.git\n   branch: master\n   dir: ./recipes/A\n\nLIB_B/1.0:\n url: https://github.com/lasote/skynet_example.git\n branch: master\n dir: ./recipes/b\n\n…\n\nPROJECT:\n url: https://github.com/lasote/skynet_example.git\n branch: master\n dir: ./recipes/PROJECT\n\nIf we change and push library A to the repository, the \" MultiBuild\" task will be triggered. It will start by checking which libraries need to be rebuilt, using the \"conan info\" command.\nConan will return something like this:\n[B, [D, F], G]\n\nThis means that we need to start building B, then we can build D and F in parallel, and finally build G. Note that library C does not need to be rebuilt, because it’s not affected by a change in library A.\n\nThe \" MultiBuild\" Jenkins pipeline script will create closures with the parallelized calls to the \" SimpleBuild\" task, and finally launch the groups in parallel.\n\n//for each group\n      tasks = [:]\n      // for each dep in group\n         tasks[label] = { -> build(job: \"SimpleBuild\",\n                            parameters: [\n                               string(name: \"build_label\", value: label),\n                               string(name: \"channel\", value: a_build[\"channel\"]),\n                               string(name: \"name_version\", value: a_build[\"name_version\"]),\n                               string(name: \"conf_repo_url\", value: conf_repo_url),\n                               string(name: \"conf_repo_branch\", value: conf_repo_branch),\n                               string(name: \"profile\", value: a_build[\"profile\"])\n                            ]\n                     )\n          }\n     parallel(tasks)\n\nEventually, this is what will happen:\n\nTwo SimpleBuild tasks will be  triggered, both for building library B, one for x86 and another for x86_64 architectures\n\nOnce \"A\" and \"B\" are built, \"F\" and \"D\" will be triggered, 4 workers will run the \"SimpleBuild\" task in parallel, (x86, x86_64)\n\nFinally \"G\" will be built. So 2 workers will run in parallel.\n\nThe Jenkins Stage View for the will looks similar to the figures below:\n\nMultiBuild\n\nSimpleBuild\n\nWe can configure the \" SimpleBuild\" task within different nodes (Windows, OSX, Linux…), and control the number of executors available in our Jenkins configuration.\n\nConclusions\n\nEmbracing DevOps for C/C++ is still marked as a to-do for many companies. It requires a big investment of time but can save huge amounts of time in the development and releasing life cycle for the long run. Moreover it increases the quality and the reliability of the C/C++ products. Very soon, adoption of DevOps for C/C++ companies will be a must!\n\nThe Jenkins example shown above that demonstrating how to control the library building in parallel is just Groovy code and a custom convenient yml file. The great thing about it is not the example or the code itself. The great thing is the possibility of defining your own pipeline scripts to adapt to your specific workflows, thanks to Jenkins Pipeline, Conan and JFrog Artifactory.\n\nMore on this topic will be presented at Jenkins Community Day Paris on\nJuly 11, and Jenkins User Conference Israel on July 13.","title":"Continuous Integration for C/C++ Projects with Jenkins and Conan","tags":["event","jenkins-user-conference","jenkins-community-day-paris"],"authors":[]}}]}},"pageContext":{"tag":"jenkins-community-day-paris","limit":8,"skip":0,"numPages":1,"currentPage":1}},
    "staticQueryHashes": ["3649515864"]}