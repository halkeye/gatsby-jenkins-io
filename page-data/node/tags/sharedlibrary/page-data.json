{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/sharedlibrary",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2020-10-21T00:00:00.000Z","id":"7c9b1586-7a87-5fd5-8543-1a27aa94a68f","slug":"/blog/2020/10/21/a-sustainable-pattern-with-shared-library/","strippedHtml":"Table of Contents\n\nContext\nThe Problems\nThe Solution\n\nShared Library\nDuplication\nDocumentation\nScalability\nInstallation Agnostic\nFeature Toggling\n\nThis post will describe how I use a shared library in Jenkins. Typically when using multibranch pipeline.\n\nIf possible (if not forced to) I implement the pipelines without multibranch. I previously wrote about how I do that with my Generic Webhook Trigger Plugin in a previous post. But this will be my second choice, If I am not allowed to remove the Jenkinsfile :s from the repositories entirely.\n\nContext\n\nWithin an organization, you typically have a few different kinds of repositories. Each repository versioning one application. You may use different techniques for different kinds of applications. The Jenkins organization on GitHub is an example with 2300 repositories.\n\nThe Problems\n\nLarge Jenkinsfiles in every repository containing duplicated code. It seems common that the Jenkinsfile :s in every repository contains much more than just the things that are unique for that repository. The shared libraries feature may not be used, or it is used but not with an optimal pattern.\n\nInstallation specific Jenkinsfile:s that only work with one specific Jenkins installation. Sometimes I see multiple Jenkinsfile :s, one for each purpose or Jenkins installation.\n\nNo documentation and/or no natural place to write documentation.\n\nDevelopment is slow. Adding new features to repositories is a time consuming task. I want to be able to push features to 1000+ repositories without having to update their Jenkinsfile :s.\n\nNo flexible way of doing feature toggling. When maintaining a large number of repositories it is sometimes nice to introduce a feature to a subset of those repositories. If that works well, the feature is introduced to all repositories.\n\nThe Solution\n\nMy solution is a pattern that is inspired by how the Jenkins organization on GitHub does it with its buildPlugin(). But it is not exactly the same.\n\nShared Library\n\nHere is how I organize my shared libraries.\n\nJenkinsfile\n\nI put this in the Jenkinsfile :s:\n\nbuildRepo()\n\nDefault Configuration\n\nI provide a default configuration that any repository will get, if no other configuration is given in buildRepo().\n\nI create a vars/getConfig.groovy with:\n\ndef call(givenConfig = [:]) {\n  def defaultConfig = [\n    /**\n      * The Jenkins node, or label, that will be allocated for this build.\n      */\n    \"jenkinsNode\": \"BUILD\",\n    /**\n      * All config specific to NPM repo type.\n      */\n    \"npm\": [\n      /**\n        * Whether or not to run Cypress tests, if there are any.\n        */\n      \"cypress\": true\n    ],\n    \"maven\": [\n      /**\n        * Whether or not to run integration tests, if there are any.\n        */\n      \"integTest\": true\n    ]\n  ]\n  // https://e.printstacktrace.blog/how-to-merge-two-maps-in-groovy/\n  def effectiveConfig merge(defaultConfig, givenConfig)\n  println \"Configuration is documented here: https://whereverYouHos/getConfig.groovy\"\n  println \"Default config: \" + defaultConfig\n  println \"Given config: \" + givenConfig\n  println \"Effective config: \" + effectiveConfig\n  return effectiveConfig\n}\n\nBuild Plan\n\nI construct a build plan as early as possible. Taking decisions on what will be done in this build. So that the rest of the code becomes more streamlined.\n\nI try to rely as much as possible on conventions. I may provide configuration that lets users turn off features, but they are otherwise turned on if they are detected.\n\nI create a vars/getBuildPlan.groovy with:\n\ndef call(effectiveConfig = [:]) {\n  def derivedBuildPlan = [\n    \"repoType\": \"NOT DETECTED\"\n    \"npm\": [],\n    \"maven\": []\n  ]\n\n  node {\n    deleteDir()\n    checkout([$class: 'GitSCM',\n      branches: [[name: '*/branchName']],\n      extensions: [\n          [$class: 'SparseCheckoutPaths',\n            sparseCheckoutPaths:\n            [[$class:'SparseCheckoutPath', path:'package.json,pom.xml']]\n          ]\n      ],\n      userRemoteConfigs: [[credentialsId: 'someID',\n      url: 'git@link.git']]\n    ])\n\n    if (fileExists('package.json')) {\n      def packageJSON = readJSON file: 'package.json'\n      derivedBuildPlan.repoType = \"NPM\"\n      derivedBuildPlan.npm.cypress = effectiveConfig.npm.cypress && packageJSON.devDependencies.cypress\n      derivedBuildPlan.npm.eslint = packageJSON.devDependencies.eslint\n      derivedBuildPlan.npm.tslint = packageJSON.devDependencies.tslint\n    } else if (fileExists('pom.xml')) {\n      derivedBuildPlan.repoType = \"MAVEN\"\n      derivedBuildPlan.maven.integTest = effectiveConfig.maven.integTest && fileExists('src/integtest')\n    } else {\n      throw RuntimeException('Unable to detect repoType')\n    }\n\n    println \"Build plan: \" + derivedBuildPlan\n    deleteDir()\n  }\n  return derivedBuildPlan\n}\n\nPublic API\n\nThis is the public API, this is what I want the users of this library to actually invoke.\n\nI implement a buildRepo() method that will use that default configuration. It can also be called with a subset of the default configuration to tweak it.\n\nI create a vars/buildRepo.groovy with:\n\ndef call(givenConfig = [:]) {\n  def effectiveConfig = getConfig(givenConfig)\n  def buildPlan = getBuildPlan(effectiveConfig)\n\n  if (effectiveConfig.repoType == 'MAVEN')\n    buildRepoMaven(buildPlan);\n  } else if (effectiveConfig.repoType == 'NPM')\n    buildRepoNpm(buildPlan);\n  }\n}\n\nA user can get all the default behavior with:\n\nbuildRepo()\n\nA user can also choose not to run Cypress, even if it exists in the repository:\n\nbuildRepo([\n  \"npm\": [\n    \"cypress\": false\n  ]\n])\n\nSupporting Methods\n\nThis is usually much more complex, but I put some code here just to have a complete implementation.\n\nI create a vars/buildRepoNpm.groovy with:\n\ndef call(buildPlan = [:]) {\n  node(buildPlan.jenkinsNode) {\n    stage(\"Install\") {\n      sh \"npm install\"\n    }\n    stage(\"Build\") {\n      sh \"npm run build\"\n    }\n    if (buildPlan.npm.tslint) {\n      stage(\"TSlint\") {\n        sh \"npm run tslint\"\n      }\n    }\n    if (buildPlan.npm.eslint) {\n      stage(\"ESlint\") {\n        sh \"npm run eslint\"\n      }\n    }\n    if (buildPlan.npm.cypress) {\n      stage(\"Cypress\") {\n        sh \"npm run e2e:cypress\"\n      }\n    }\n  }\n}\n\nI create a vars/buildRepoMaven.groovy with:\n\ndef call(buildPlan = [:]) {\n  node(buildPlan.jenkinsNode) {\n    if (buildPlan.maven.integTest) {\n      stage(\"Verify\") {\n        sh \"mvn verify\"\n      }\n    } else {\n      stage(\"Package\") {\n        sh \"mvn package\"\n      }\n    }\n  }\n}\n\nDuplication\n\nThe Jenkinsfile :s are kept extremely small. It is only when they, for some reason, diverge from the default config that they need to be changed.\n\nDocumentation\n\nThere is one single point where documentation is written, the getConfig.groovy -file. It can be referred to whenever someone asks for documentation.\n\nScalability\n\nThis is a highly scalable pattern. Both with regards to performance and maintainability in code.\n\nIt scales in performance because the Jenkinsfile :s can be used by any Jenkins installation. So that you can scale by adding several completely separate Jenkins installations, not only nodes.\n\nIt scales in code because it adds just a tiny Jenkinsfile to repositories. It relies on conventions instead, like the existence of attributes in package.json and location of integration tests in src/integtest.\n\nInstallation Agnostic\n\nThe Jenkinsfile :s does not point at any implementation of this API. It just invokes it and it is up to the Jenkins installation to implement it, with a shared libraries.\n\nIt can even be used by something that is not Jenkins. Perhaps you decide to do something in a Docker container, you can still parse the Jenkinsfile with Groovy or (with some magic) with any language.\n\nFeature Toggling\n\nThe shared library can do feature toggling by:\n\nLetting some feature be enabled by default for every repository with name starting with x.\n\nOr, adding some default config saying\"feature-x-enabled\": false, while some repos change their Jenkinsfile :s to buildRepo([\"feature-x-enabled\": true]).\n\nWhenever the feature feels stable, it can be enabled for everyone by changing only the shared library.","title":"A sustainable pattern with shared library","tags":["pipeline","scalability","sharedlibrary","infrastructure"],"authors":[{"avatar":{"childImageSharp":null},"blog":"https://bjurr.com/","github":"tomasbjerre","html":"<div class=\"paragraph\">\n<p>Tomas Bjerre is an experienced fullstack software developer. Been working full time since 2010 after graduating with a masters degree in computer science from Lund University (Faculty of Engineering, LTH). Is currently working full time and maintaining a bunch of Jenkins plugins on his spare time.</p>\n</div>","id":"tomasbjerre","irc":null,"linkedin":"tomasbjerre","name":"Tomas Bjerre","slug":"/blog/author/tomasbjerre","twitter":null}]}},{"node":{"date":"2019-01-08T00:00:00.000Z","id":"058588ed-0d18-5d03-bd6b-e2e62be7093b","slug":"/blog/2019/01/08/mpl-modular-pipeline-library/","strippedHtml":"Despite speeding up development with deployment automation, one of our clients\nwas experiencing slow time-to-market due to a lack of collaboration in DevOps.\nWhile they had invested in DevOps, every production pipeline was set up\nindividually, forcing teams to remake the wheel for each project. Making matters\nworse, there was no cross-team collaboration, so any bug in the platform was\npresent in each new pipeline. Many of our clients have similar issues, so we\ndecided that we should develop a common tool which would both help current\nclients, and be adaptable for use in the future. While the most obvious option\nwas standardizing the CI/CD platform with a common framework, this led to a\nmonolithic structure, which was inflexible and ultimately unworkable. Since each\nteam needed to work on their own pipelines, we developed a solution that would\nstore each reusable part of the DevOps pipeline for later use: a Jenkins-powered\nmodular pipeline library.\n\nSolution: a modular pipeline library\n\nThe modular pipeline library ( MPL) we\ncreated is a highly-flexible shared library for a Jenkins Pipeline that enables\neasy sharing of best practices across the entire company. It has a clear modular\nstructure, an advanced testing framework, multi-level nesting, a pipeline\nconfiguration system, improved error handling, and many other useful components.\n\nWe will take a look under the hood and explain how our solution works in several\nparts:\n\nExplore the technologies and tools we used to build the MPL\n\nReview the MPL, and illustrate why it’s effective\n\nFollow a step-by-step guide to operate the MPL on a sample pipeline\n\nDive into some of the more important components of the solution, such as the test framework and nested libraries\n\nSo now let’s jump right into an explanation of the crucial features we used to\nbuild our solution.\n\nBuilding the MPL with shared libraries and Jenkins pipelines\n\nJenkins, our main automation platform, recently received some updates to\nJenkins Pipeline. These updates allow us to\ncreate one Jenkinsfile that\ndescribes the entire pipeline, and the steps that need to be executed with a\nseries of self-explanatory scripts. This increases the visibility of CI/CD\nautomation processes for end users, and improves supportability by DevOps teams.\n\nHowever, there’s a large issue with Pipeline: it’s hard to support multiple\nJenkinsfiles (and therefore multiple projects) with unique pipelines. We need to\nstore the common logic somewhere, which is where\nJenkins Shared Libraries\ncome in. They are included in the Jenkinsfile, and allow the use of prepared\ninterfaces to simplify automation and store common pieces.\n\nWhile shared libraries allow you to store logic and manipulate Jenkins, they\ndon’t provide a good way to utilize all the common information. Therefore, the\nMPL optimizes the pipeline and shared libraries by allowing users to create\neasy-to-follow descriptions for processes, which are then stored for later use\nby other teams.\n\nThe MPL works to create collaborative DevOps processes across teams\n\nWith the MPL, we are now able to collaborate and share our DevOps practices\nacross teams, easily adopt existing pipelines for specific projects, and debug\nand test features before we actually integrate them into the library. Each team\ncan create a nested library, add a number of pipelines and modules inside, and\nuse it with pipeline automation to create great visibility of the processes for\nthe end user. The MPL can also work on any project to prepare a Jenkinsfile, and\nmanage it as flexibly as the project team wants.\n\nAt its core, the MPL provides a simple way to:\n\nSeparate pipelines and steps by introducing modules\n\nDescribe steps in the modules with an easy configuration interface\n\nTest the described modules and share the results with other pipelines and projects\n\nThere are a lot of other features in the MPL, but it’s essentially a platform to\nsolve general DevOps collaboration issues. To simplify development and manual\ntesting, the MPL provides modules overriding and an inheritance model, allowing\nusers to test specific fixes in the project without affecting anything else. In\nJenkins, a module is a file with scripted steps and logic to reach a simple goal\n(build an artifact, run tests, create an image, etc.). These modules are\ncombined in the pipeline stages, and are easily readable for anyone who knows\nthe Jenkins Pipeline syntax.\n\nThe MPL allows users to use the core features of the library (structure,\nmodules, pipelines) and create nested libraries for specific DevOps team needs.\nA DevOps team can prepare complete pipelines with any custom logic and use it\nfor their projects. They can also override and inherit the core MPL modules in a\nnumber of ways, or prepare custom modules which are easy to share with other\nteams. Check out the infographic below to see how modules fit in:\n\nYou can also specify certain pipeline required poststeps in a module. For\nexample, a dynamic deployment module creates the test environment, which needs\nto be destroyed when the pipeline ends. To take a closer look at the MPL calling\nprocess, check out the infographic below:\n\nThis infographic shows how calls are executed in the MPL. First, you need a job\non your Jenkins, which will call a Jenkinsfile (for example, when the source\ncode is changed), after which the Jenkinsfile will call a pipeline. The pipeline\ncould be described on the MPL side, in the pipeline script in the job, in the\nnested library, or in the project Jenkinsfile. Finally, the stages of the\npipeline will call the modules, and these modules will use features, which could\nbe groovy logic, pipeline steps, or steps in the shared libraries.\n\nNow that we’ve done an overview of the solution, let’s take a look at a simple\npipeline execution to see how the MPL works in action.\n\nAn example of a pipeline execution in the MPL\n\nFor example, let’s say you have a common Java Maven project. You are creating a\nJenkinsfile in the repo, and want to use the default pipeline prepared by your\nDevOps team. The MPL already has a simple pipeline: the core MPLPipeline. It’s\na really simple pipeline, but it’s a good start for anyone who wants to try the\nMPL. Let’s look at a simple Jenkinsfile:\n\n@Library('mpl') _\nMPLPipeline {}\n\nThis Jenkinsfile contains a single line to load the MPL, and another line to run\nthe pipeline. Most of the shared libraries implement an interface like this,\ncalling one step and providing some parameters. MPLPipeline is merely a custom\nPipeline step, as it lies in the vars directory, and its structure is very\nsimple, following these steps:\n\nInitialize the MPL\nThe MPL uses the MPLManager singleton object to control the pipeline\n\nMerge configuration with default and store it\nA default configuration needed to specify stages and predefine some useful configs\n\nDefine a declarative pipeline with 4 stages and poststeps:\n\nCheckout - Getting the project sources\n\nBuild - Compiling, validation of static, unit tests\n\nDeploy - Uploading artifacts to the dynamic environment and running the app\n\nTest - Checking integration with other components\n\nPoststeps - Cleaning dynamic environment, sending notifications, etc.\n\nRunning the defined pipeline\nThis is where the MPL starts to work its magic and actually runs\n\nStages of the main MPL usually have just one step, the MPLModule .\nThis step contains the core functionality of the MPL: executing the modules\nwhich contain the pipeline logic. You can find default modules in the MPL\nrepository, which are placed in resources/com/griddynamics/devops/mpl/modules.\nSome of the folders include: Checkout, Build, Deploy, and Test, and in each of\nthem we can find Groovy files with the actual logic for the stages. This\ninfographic is a good example of a simplified MPL repository\nstructure:\n\nWhen the Checkout stage starts, MPLModule loads the module by name (by default\na stage name), and runs the Checkout/Checkout.groovy\nlogic:\n\nif( CFG.'git.url' )\n  MPLModule('Git Checkout', CFG)\nelse\n  MPLModule('Default Checkout', CFG)\n\nIf the configuration contains the git.url option, it will load a Git Checkout\nmodule; otherwise, it will run the Default Checkout module. All the called\nmodules use the same configuration as the parent module, which is why CFG was\npassed to the MPLModule call. In this case, we have no specific configuration,\nso it will run the\nCheckout/DefaultCheckout.groovy\nlogic. The space in the name is a separator to place the module into a specific\nfolder.\n\nIn the Default Checkout module, there is just one line with checkout scm\nexecution, which clones the repository specified in the Jenkins job. That’s all\nthe Checkout stage does, as the MPL functionality is excessive for such a small\nstage, and we only need to talk about it here to show how the MPL works in\nmodules.\n\nThe same process applies to the Build stage, as the pipeline runs the\nMaven Build\nmodule:\n\nwithEnv([\"PATH+MAVEN=${tool(CFG.'maven.tool_version' ?: 'Maven 3')}/bin\"]) {\n  def settings = CFG.'maven.settings_path' ? \"-s '${CFG.'maven.settings_path'}'\" : ''\n  sh \"\"\"mvn -B ${settings} -DargLine='-Xmx1024m -XX:MaxPermSize=1024m' clean install\"\"\"\n}\n\nThis stage is a little bit more complicated, but the action is simple: we take\nthe tool with the default name Maven 3, and use it to run mvn clean install.\nThe modules are scripted pipelines, so you can do the same steps usually\navailable in the Jenkins Pipeline. The files don’t need any specific and\ncomplicated syntax, just a plain file with steps and CFG as a predefined\nvariable with a stage configuration. The MPL modules inherited the sandbox from\nthe parent, so your scripts will be safe and survive the Jenkins restart, just\nlike a plain Jenkins pipeline.\n\nIn the Deploy folder, we find the sample structure of the Openshift Deploy\nmodule. Its main purpose here is to show how to use poststep definitions in the\nmodules:\n\nMPLPostStep('always') {\n  echo \"OpenShift Deploy Decommission poststep\"\n}\necho 'Executing Openshift Deploy process'\n\nFirst, we define the always poststep. It is stored in the MPLManager, and is\ncalled when poststeps are executed. We can call MPLPostStep with always as\nmany times as we want: all the poststeps will be stored and executed in FILO\norder. Therefore, we can store poststep logic for actions that need to be done,\nand then undone, in the same module, such as the decommission of the dynamic\nenvironment. This ensures that the actions will be executed when the pipeline\nis complete.\n\nAfter the deploy stage, the pipeline executes the Test stage, but nothing too\ninteresting happens there. However, there is an aspect of testing which is very\nimportant, and that’s the testing framework of the MPL itself.\n\nTesting of the MPL\n\nThe testing framework of the MPL is based on the\nJenkinsPipelineUnit\nfrom LesFurets, with the one small difference being its ability to test the MPL\nmodules. Testing the whole pipeline doesn’t work, as pipelines can be really\ncomplicated, and writing tests for such monsters is a Sisyphean task. It is much\neasier to test a black box with a small amount of steps, ensuring that this\nparticular task is working correctly.\n\nIn the MPL, you can find Build module testing examples: all the tests are\nstored in the\ntest/groovy/com/griddynamics/devops/mpl/modules\ndirectory, and you can find the\nBuild/BuildTest.groovy\nfile with a number of test cases there. Tests are executed during the MPL build\nprocess, allowing users to see traces like this:\n\nLoading shared library mpl with version snapshot\n  MPLModule.call(Build, {maven={tool_version=Maven 2}})\n    Build.run()\n      Build.MPLModule(Maven Build, {maven.tool_version=Maven 2})\n        MavenBuild.run()\n          MavenBuild.tool(Maven 2)\n          MavenBuild.withEnv([PATH+MAVEN=Maven 2_HOME/bin], groovy.lang.Closure)\n            MavenBuild.sh(mvn -B  -DargLine='-Xmx1024m -XX:MaxPermSize=1024m' clean install)\n      Build.fileExists(openshift)\n\nThe test runs the MPLModule with custom configuration and mocked steps to\ncheck that, during execution, the tool was changed to Maven 2 according to the\nprovided configuration. We cover all test cases with such tests, ensuring that\nthe modules are working as expected, and that the pipeline will work properly.\nYou can test the whole pipeline if you want, but testing by modules is just an\nadditional way to simplify the testing process.\n\nNow that we’ve looked at how to test the MPL modules, it’s time to look at one\nof the key features of the MPL, which is nested libraries.\n\nThe benefits of nested libraries\n\nWhen working with a large company, supporting one big library makes no sense.\nEach department requires multiple configuration options and tuning for a\nsomewhat standard pipeline, which creates extra work. The MPL solves such\nproblems by introducing nested libraries. This infographic displays how a nested\nlibrary compares to just using the main library:\n\nA nested library is the same as a shared library that imports the MPL and uses\nits functionality, modules, and pipelines. Also, it allows the separation of\nsome team-related logic from the company common logic. Here is the structure of\nthe MPL with nested libraries:\n\nYou can import the MPL in the overridden pipeline, specify the path of some\nadditional modules, override module logic, and use Jenkins power moves: there\nare no limitations. When another team needs your unique module, you can just\ncreate a change request to the basic company MPL repo, and share your functional\nmodule with the others.\n\nWith nested libraries, it’s possible to debug and modify MPL-provided steps\n( MPLModule for example) and pipelines. This is because nested libraries can\noverride low-level functionalities of the MPL or the Jenkins Pipeline. There are\nno limitations to what you can or can’t change, as these overrides only affect\nyour own pipeline. This enables experimentation to be done, and then discussed\nwith other teams to see if it will work in other nested libraries as well.\n\nThere are also no limits to the number of nesting levels created, but we\nrecommend using just two (MPL and nested), because additional levels make\nconfiguration and testing of the nested libraries on lower levels very\ncomplicated.\n\nThe power of module overriding\n\nFurther into the nested libraries or project-side modules, it’s possible to\nstore a module with the same name as one in the upper-level library. This is a\ngood way to override the logic - you can just replace Build/Build.groovy with\nyour own - as the functional module will be executed instead of the upper-level\nmodule. For example, this infographic shows module overriding:\n\nEven better, one of the strengths of the MPL is that you still can use the\nupper-level module! The MPL has mechanisms to prevent loops, so the same module\ncan’t be executed in the same executing branch again. However, you can easily\ncall the original module a name from another module to use the upper-level\nlogic.\n\nThe Petclinic-Selenium example above uses the default MPLPipeline (you can\nfind it on the MPL Wiki-page), and\ncontains project-side modules in a.jenkins directory. These modules will be\ncalled before the library modules. For example, the Checkout module is not\nplaced on the project side, so it will be called from the MPL, but the Build\nmodule exists in a.jenkins directory on the project side, and it will be\ncalled:\n\nMPLPostStep('always') {\n  junit 'target/surefire-reports/*.xml'\n}\n\nMPLModule('Build', CFG)\n\nif( fileExists('Dockerfile') ) {\n  MPLModule('Docker Build', CFG)\n}\n\nAs you can see, the Build module from the project registers the poststep,\ncalls the original Build module from the MPL, and then calls the additional\nDocker Build module. The following stages of the pipeline are more\ncomplicated, but all module overriding essentially works like this. Some\nprojects can be tricky, and need some small tunings for the existing modules.\nHowever, you can easily implement those changes on the project level, and think\nabout how to move the functionality to the nested library or MPL later.\n\nConclusion: what the MPL brings to DevOps\n\nMany DevOps teams and companies work with bloated, restrictive, and buggy CI/CD\nautomation platforms. These increase the learning curve for users, cause teams\nto work slower, and raise production costs. DevOps teams frequently run into\nsimilar issues on different projects, but a lack of collaboration means that\nthey have to be individually fixed each time.\n\nHowever, with the MPL, DevOps teams have a shared, simple, and flexible CI/CD\nplatform to improve user support, collaboration, and overall project source code\nto the production process. By utilizing the MPL, your company can find an\nautomation consensus, reach cross-company collaboration goals, and reuse the\nbest practices from a large community, all with open source tools. If you’re\ninterested in building an MPL, please contact us to learn more!\n\nAdditional resources\n\nJenkins Pipeline Engine\n\nJenkins Shared Libraries\n\nMPL GitHub repository\n\nOverview & demo videos:\n\nIntroduction\n\nOverview\n\nDemo of the MPL Build\n\nDemo of the Nested Library\n\nDemo of the Petclinic Pipeline","title":"MPL - Modular Pipeline Library","tags":["jenkinsfile","pipeline","sharedlibrary"],"authors":[{"avatar":{"childImageSharp":null},"blog":"https://www.state-of-the-art.io/","github":"sparshev","html":"<div class=\"paragraph\">\n<p>Sergei is a DevOps engineer and using Jenkins as a main automation tool since 2011.\nWants to automate everything to make sure that there no more room for boring tasks.</p>\n</div>","id":"sparshev","irc":null,"linkedin":null,"name":"Sergei Parshev","slug":"/blog/author/sparshev","twitter":null}]}},{"node":{"date":"2018-04-25T00:00:00.000Z","id":"e34fdb6b-09c7-5772-a251-10cec0ccef26","slug":"/blog/2018/04/25/configuring-jenkins-pipeline-with-yaml-file/","strippedHtml":"A few years ago our CTO wrote about building a\nContinuous Integration server for Ruby On Rails using Jenkins and docker.\nThe solution has been our CI pipeline for the past years until we recently decided to\nmake an upgrade. Why?\n\nJenkins version was way out of date and it was getting difficult to\nupgrade\n\nWolox has grown significantly over the past years\nand we’ve been experiencing scaling issues\n\nVery few people knew how to fix any issues with the server\n\nConfiguring jobs was not an easy task and that made our project\nkickoff process slower\n\nMaking changes to the commands that each job runs was not easy and not\nmany people had permissions to do so. Wolox has a wide range of\nprojects, with a wide variety of languages which made this problem even\nbigger.\n\nTaking into account these problems, we started digging into the newest\nversion of Jenkins to see how we could improve our CI. We needed to\nbuild a new CI that could, at least, address the following:\n\nProjects must be built using Docker. Our projects depend on one or\nmultiple docker images to run (app, database, redis, etc)\n\nEasy to configure and replicate if necessary\n\nEasy to add a new project\n\nEasy to change the building steps. Everyone working on the project\nshould be able to change if they want to run npm install or yarn\ninstall.\n\nInstalling Jenkins and Docker\n\nInstalling Jenkins is straightforward. You can visit\nJenkins Installation page and choose the\noption that best suits your needs.\n\nHere are the steps we followed to install Jenkins in AWS:\n\nsudo rpm — import https://pkg.jenkins.io/debian/jenkins.io.key\nsudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat/jenkins.repo\nsudo yum install java-1.8.0 -y\nsudo yum remove java-1.7.0-openjdk -y\nsudo yum install jenkins -y\nsudo yum update -y\nsudo yum install -y docker\n\nAutomatically adding projects from Github\n\nAdding projects automatically from Github can be achieved using the\nGitHub Branch Source Plugin.\nIt allows Jenkins to scan a GitHub organization\nfor projects that match certain rules and add them to Jenkins\nautomatically. The only constraint that all branches must meet in order\nto be added is that they contain a Jenkinsfile that explains how to\nbuild the project.\n\nEasy to change configuration\n\nNot so easy to change configuration\n\nOne of the biggest pains we had with our previous Jenkins was the\ndifficulty of changing the steps necessary to build the project. If you\nlooked at a project’s build steps, you would find something like this:\n\n#!/bin/bash +x\nset -e\n\n# Remove unnecessary files\necho -e \"\\033[34mRemoving unnecessary files...\\033[0m\"\nrm -f log/*.log &> /dev/null || true &> /dev/null\nrm -rf public/uploads/* &> /dev/null || true &> /dev/null\n\n# Build Project\necho -e \"\\033[34mBuilding Project...\\033[0m\"\ndocker-compose --project-name=${JOB_NAME} build\n\n# Prepare test database\nCOMMAND=\"bundle exec rake db:drop db:create db:migrate\"\necho -e \"\\033[34mRunning: $COMMAND\\033[0m\"\ndocker-compose --project-name=${JOB_NAME} run  \\\n\t-e RAILS_ENV=test web $COMMAND\n\n# Run tests\nCOMMAND=\"bundle exec rspec spec\"\necho -e \"\\033[34mRunning: $COMMAND\\033[0m\"\nunbuffer docker-compose --project-name=${JOB_NAME} run web $COMMAND\n\n# Run rubocop lint\nCOMMAND=\"bundle exec rubocop app spec -R --format simple\"\necho -e \"\\033[34mRunning: $COMMAND\\033[0m\"\nunbuffer docker-compose --project-name=${JOB_NAME} run -e RUBYOPT=\"-Ku\" web $COMMAND\n\nAnd some post build steps that cleaned up the docker:\n\n#!/bin/bash +x\ndocker-compose --project-name=${JOB_NAME} stop &> /dev/null || true &> /dev/null\ndocker-compose --project-name=${JOB_NAME} rm --force &> /dev/null || true &> /dev/null\ndocker stop `docker ps -a -q -f status=exited` &> /dev/null || true &> /dev/null\ndocker rm -v `docker ps -a -q -f status=exited` &> /dev/null || true &> /dev/null\ndocker rmi `docker images --filter 'dangling=true' -q --no-trunc` &> /dev/null || true &> /dev/null\n\nAlthough these commands are not complex, changing any of them required\nsomeone with permissions to modify the job and an understanding ofwhat\nneeded to be done.\n\nJenkinsfile to the rescue…​ or not\n\nWith the current Jenkins version, we can take advantage of\nJenkins Pipeline and model our build\nflow in a file. This file is checked into the repository and, therefore,\nanyone with access to it can change the build steps. Yay!\n\nJenkins Pipeline even has support for:\n\nDocker and\nmultiple\nimages can be used for a build!\n\nSetting environment variables with withEnv and many other built -in\nfunctions that can be found\nhere.\n\nThis makes a perfect case for Wolox. We can have\nour build configuration in a file that’s checked into the repository and\ncan be changed by anyone with write access to it. However, a Jenkinsfile\nfor a simple rails project would look something like this:\n\n# sample Jenkinsfile. Might not compile\nnode {\n    checkout scm\n    withEnv(['MYTOOL_HOME=/usr/local/mytool']) {\n        docker.image(\"postgres:9.2\").withRun() { db ->\n            withEnv(['DB_USERNAME=postgres', 'DB_PASSWORD=', \"DB_HOST=db\", \"DB_PORT=5432\"]) {\n                docker.image(\"redis:X\").withRun() { redis ->\n                    withEnv([\"REDIS_URL=redis://redis\"]) {\n                        docker.build(imageName, \"--file .woloxci/Dockerfile .\").inside(\"--link ${db.id}:postgres --link ${redis.id}:redis\") {\n                            sh \"rake db:create\"\n                            sh \"rake db:migrate\"\n                            sh \"bundle exec rspec spec\"\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nThis file is not only difficult to read, but also difficult to change.\nIt’s quite easy to break things if you’re not familiar with Groovy and\neven easier if you know nothing about how Jenkins’ pipeline works.\nChanging or adding a new Docker image isn’t straightforward and might\nlead to confusion.\n\nConfiguring Jenkins Pipeline via YAML\n\nPersonally, I’ve always envied simple configuration files for CIs and\nthis time it was our chance to build CI that could be configured using a\nYAML file. After some analysis we concluded that a YAML like this one\nwould suffice:\n\nconfig:\n  dockerfile: .woloxci/Dockerfile\n  project_name: some-project-name\n\nservices:\n  - postgresql\n  - redis\n\nsteps:\n  analysis:\n    - bundle exec rubocop -R app spec --format simple\n    - bundle exec rubycritic --path ./analysis --minimum-score 80 --no-browser\n  setup_db:\n    - bundle exec rails db:create\n    - bundle exec rails db:schema:load\n  test:\n    - bundle exec rspec\n  security:\n    - bundle exec brakeman --exit-on-error\n  audit:\n    - bundle audit check --update\n\n\nenvironment:\n  RAILS_ENV: test\n  GIT_COMMITTER_NAME: a\n  GIT_COMMITTER_EMAIL: b\n  LANG: C.UTF-8\n\nIt outlines some basic configuration for the project, environment\nvariables that need to be present during the run, dependentservices, and\nour build steps.\n\nJenkinsfile + Shared Libraries = WoloxCI\n\nAfter investigating for a while about Jenkins and the pipeline, we found\nthat we could extend it with\nshared libraries.\nShared libraries are written in groovy and can be imported\ninto the pipeline and executed when necessary.\n\nIf you look carefully at this Jenkinsfile,\nwe see that the code is a chain of methods calls that receive a\nclosure, where we execute another method passing a new closure to it.\n\n# sample Jenkinsfile. Might not compile\nnode {\n    checkout scm\n    withEnv(['MYTOOL_HOME=/usr/local/mytool']) {\n        docker.image(\"postgres:9.2\").withRun() { db ->\n            withEnv(['DB_USERNAME=postgres', 'DB_PASSWORD=', \"DB_HOST=db\", \"DB_PORT=5432\"]) {\n                docker.image(\"redis:X\").withRun() { redis ->\n                    withEnv([\"REDIS_URL=redis://redis\"]) {\n                        docker.build(imageName, \"--file .woloxci/Dockerfile .\").inside(\"--link ${db.id}:postgres --link ${redis.id}:redis\") {\n                            sh \"rake db:create\"\n                            sh \"rake db:migrate\"\n                            sh \"bundle exec rspec spec\"\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nGroovy is flexible enough to allow this same declarative code to be\ncreated at runtime, making our dream of using a YAML to configure our\njob come true!\n\nIntroducing Wolox-CI\n\nThat’s how wolox-ci was born- our\nshared library for Jenkins!\n\nWith wolox-ci, our Jenkinsfile is now\nreduced to:\n\n@Library('wolox-ci') _\n\nnode {\n\n  checkout scm\n\n  woloxCi('.woloxci/config.yml');\n}\n\nNow it simply checks out the code and then calls wolox-ci. The library\nreads yaml file like this one\n\nconfig:\n  dockerfile: .woloxci/Dockerfile\n  project_name: some-project-name\n\nservices:\n  - postgresql\n  - redis\n\nsteps:\n  analysis:\n    - bundle exec rubocop -R app spec --format simple\n    - bundle exec rubycritic --path ./analysis --minimum-score 80 --no-browser\n  setup_db:\n    - bundle exec rails db:create\n    - bundle exec rails db:schema:load\n  test:\n    - bundle exec rspec\n  security:\n    - bundle exec brakeman --exit-on-error\n  audit:\n    - bundle audit check --update\n\n\nenvironment:\n  RAILS_ENV: test\n  GIT_COMMITTER_NAME: a\n  GIT_COMMITTER_EMAIL: b\n  LANG: C.UTF-8\n\nand builds the Jenkinsfile to get your job running on the fly.\n\nThe nice part about having a shared library is that we can extend and\nfix our library in a centralized way. Once we add new code, the library\nis automatically updated in Jenkins which will notify all of our jobs\nwith the update.\n\nSince we have projects in different languages we use Docker to build the\ntesting environment. WoloxCI assumes there is a Dockerfile to build and\nwill run all the specified commands inside the container.\n\nWoloxci config.yml\n\nConfig\n\nThe first part of the config.yml file specifies some basic\nconfiguration: project’s name and Dockerfile location. The Dockerfile is\nused to build the image where the commands will be run.\n\nServices\n\nThis section describes which services will be exposed to the container.\nOut of the box, WoloxCI has support for postgresql, mssql and\nredis. You can also specify the docker image version you want! It is\nnot hard to add a new service. You just need to add the corresponding\nfile at\n\nhttps://github.com/Wolox/wolox-ci/tree/development/vars\n\nand modify how the services are parsed\n\nhttps://github.com/Wolox/wolox-ci/blob/development/src/com/wolox/parser/ConfigParser.groovy#L76\n\nSteps\n\nThe listed commands in this section will run inside the Docker\ncontainer. As a result, you’ll see each of the steps on the Jenkins UI.\n\nEnvironment\n\nIf you need some environment variables during your build, you can\nspecify them here. Whatever variable you set will be available inside\nthe Docker container when your commands listed in the steps section\ndescribed above.\n\nWrapping up\n\nWoloxCI is still being tested with a not-so-small sample of our\nprojects. The possibility of changing the build steps through a YAML\nfile makes it accessible for everyone and that is a great improvement in\nour CI workflow.\n\nDocker gives us the possibility of easily changing the programming\nlanguage without making any changes to our Jenkins installation and\nJenkins’ Github Organization feature automatically adds new projects\nwhen a new repository with a Jenkinsfile is detected.\n\nAll of these improvements have reduced the time we spend maintaining\nJenkins significantly and give us the possibility of easily scaling\nwithout any extra configuration.\n\nThis library is working in our CI but it still can be improved.\nIf you would like to add features, feel free to\ncontribute!","title":"Configuring a Jenkins Pipeline using a YAML file","tags":["jenkins","pipelines","yaml","sharedlibrary"],"authors":[{"avatar":null,"blog":null,"github":"mdesanti","html":"<div class=\"paragraph\">\n<p>Head of Infrastructure &amp; Cloud at <a href=\"https://www.wolox.com.ar\">Wolox</a></p>\n</div>","id":"mdesanti","irc":null,"linkedin":null,"name":"Matias De Santi","slug":"/blog/author/mdesanti","twitter":"mdsanti"}]}}]}},"pageContext":{"tag":"sharedlibrary","limit":8,"skip":0,"numPages":1,"currentPage":1}},
    "staticQueryHashes": ["3649515864"]}