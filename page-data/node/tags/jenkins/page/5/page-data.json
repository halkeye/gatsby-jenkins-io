{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/jenkins/page/5",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2017-04-18T00:00:00.000Z","id":"714ef576-2af6-5d69-a880-a3280836f4e4","slug":"/blog/2017/04/18/continuousdelivery-devops-sonarqube/","strippedHtml":"This is a guest post by Michael Hüttermann. Michael is an expert\nin Continuous Delivery, DevOps and SCM/ALM. More information about him at huettermann.net, or\nfollow him on Twitter: @huettermann.\n\nContinuous Delivery and DevOps are well known and widely spread practices nowadays. It is commonly accepted that it\nis crucial to form great teams and define shared goals first and then choose and integrate the tools fitting best to\ngiven tasks. Often it is a mashup of lightweight tools, which are integrated to build up Continuous Delivery pipelines\nand underpin DevOps initiatives. In this blog post, we zoom in to an important part of the overall pipeline, that is the discipline\noften called Continuous Inspection, which comprises inspecting code and injecting a quality gate on that, and show how artifacts can\nbe uploaded after the quality gate was met. DevOps enabler tools covered are Jenkins, SonarQube, and Artifactory.\n\nThe Use Case\n\nYou already know that quality cannot be injected after the fact, rather it should be part of the process and product from the very beginning.\nAs a commonly used good practice, it is strongly recommended to inspect the code and make findings visible, as soon as possible.\nFor that SonarQube is a great choice. But SonarQube is not just running on any isolated\nisland, it is integrated in a Delivery Pipeline. As part of the pipeline, the code is inspected, and only if the code is fine according to defined\nrequirements, in other words: it meets the quality gates, the built artifacts are uploaded to the binary repository manager.\n\nLet’s consider the following scenario. One of the busy developers has to fix code, and checks in changes to the central\nversion control system. The day was long and the night short, and against all team commitments the developer\ndid not check the quality of the code in the local sandbox. Luckily, there is the build engine Jenkins\nwhich serves as a single point of truth, implementing the Delivery Pipeline with its native pipeline features, and as a handy coincidence\nSonarQube has support for Jenkins pipeline.\n\nThe change triggers a new run of the pipeline. Oh no! The build pipeline broke, and the change is not further processed.\nIn the following image you see that a defined quality gate was missed. The visualizing is done with Jenkins Blue Ocean.\n\nSonarQube inspection\n\nWhat is the underlying issue? We can open the SonarQube web application and drill down to the finding. In the Java code, obviously a string literal is not placed on the right side.\n\nDuring a team meeting it was decided to define this to be a Blocker, and SonarQube was configured accordingly. Furthermore, a SonarQube quality gate was created to break any build, if a blocker was identified. Let’s now quickly look into the code.\nYes, SonarQube is right, there is the issue with the following code snippet.\n\nWe do not want to discuss in detail all used tools, and also covering the complete Jenkins build job would be out of scope.\nBut the interesting extract here in regard of the inspection is the following stage defined in Jenkins pipeline DSL:\n\nconfig.xml: SonarQube inspection\n\nstage('SonarQube analysis') { (1)\nwithSonarQubeEnv('Sonar') { (2)\nsh 'mvn org.sonarsource.scanner.maven:sonar-maven-plugin:3.3.0.603:sonar ' + (3)\n'-f all/pom.xml ' +\n          '-Dsonar.projectKey=com.huettermann:all:master ' +\n          '-Dsonar.login=$SONAR_UN ' +\n          '-Dsonar.password=$SONAR_PW ' +\n          '-Dsonar.language=java ' +\n          '-Dsonar.sources=. ' +\n          '-Dsonar.tests=. ' +\n          '-Dsonar.test.inclusions=**/*Test*/** ' +\n          '-Dsonar.exclusions=**/*Test*/**'\n        }\n    }\n\n1\nThe dedicated stage for running the SonarQube analysis.\n\n2\nAllow to select the SonarQube server you want to interact with.\n\n3\nRunning and configuring the scanner, many options available, check the docs.\n\nMany options are available to integrate and configure SonarQube. Please consult the documentation for alternatives. Same applies to the other covered tools.\n\nSonarQube Quality Gate\n\nAs part of a Jenkins pipeline stage, SonarQube is configured to run and inspect the code. But this is just the first part,\nbecause we now also want to add the quality gate in order to break the build. The next stage is covering exactly that, see\nnext snippet. The pipeline is paused until the quality gate is computed, specifically the waitForQualityGate step will pause the\npipeline until SonarQube analysis is completed and returns the quality gate status. In case a quality gate was missed, the build breaks.\n\nconfig.xml: SonarQube Quality Gate\n\nstage(\"SonarQube Quality Gate\") { (1)\ntimeout(time: 1, unit: 'HOURS') { (2)\ndef qg = waitForQualityGate() (3)\nif (qg.status != 'OK') {\n             error \"Pipeline aborted due to quality gate failure: ${qg.status}\"\n           }\n        }\n    }\n\n1\nThe defined quality gate stage.\n\n2\nA timeout to define when to proceed without waiting for any results for ever.\n\n3\nHere we wait for the OK. Underlying implementation is done with SonarQube’s webhooks feature.\n\nThis blog post is an appetizer, and scripts are excerpts. For more information, please consult the respective documentation, or a good book, or the great community, or ask your local expert.\n\nSince they all work in a wonderful Agile team, the next available colleague just promptly fixes the issue. After checking in\nthe fixed code, the build pipeline runs again.\n\nThe pipeline was processed successfully, including the SonarQube quality gate, and as the final step, the packaged and tested artifact was\ndeployed to Artifactory. There are a couple of different flexible ways how to upload the artifacts,\nthe one we use here is using an upload spec to actually collect and upload the artifact which was built at the very beginning of the pipeline.\nAlso meta information are published to Artifactory, since it is the context which matters and thus we can add valuable labels to the artifact for further processing.\n\nconfig.xml: Upload to Artifactory\n\nstage ('Distribute binaries') { (1)\ndef SERVER_ID = '4711' (2)\ndef server = Artifactory.server SERVER_ID\n    def uploadSpec = (3)\"\"\"\n    {\n    \"files\": [\n        {\n            \"pattern\": \"all/target/all-(*).war\",\n            \"target\": \"libs-snapshots-local/com/huettermann/web/{1}/\"\n        }\n      ]\n    }\n    \"\"\"\n    def buildInfo = Artifactory.newBuildInfo() (4)\nbuildInfo.env.capture = true (5)\nbuildInfo=server.upload(uploadSpec) (6)\nserver.publishBuildInfo(buildInfo) (7)\n}\n\n1\nThe stage responsible for uploading the binary.\n\n2\nThe server can be defined Jenkins wide, or as part of the build step, as done here.\n\n3\nIn the upload spec, in JSON format, we define what to deploy to which target, in a fine-grained way.\n\n4\nThe build info contains meta information attached to the artifact.\n\n5\nWe want to capture environmental data.\n\n6\nUpload of artifact, according to upload spec.\n\n7\nBuild info are published as well.\n\nNow let’s see check that the binary was deployed to Artifactory, successfully. As part of the context information, also a reference to the\nproducing Jenkins build job is available for better traceability.\n\nSummary\n\nIn this blog post, we’ve discovered tips and tricks to integrate Jenkins with SonarQube, how to define\nJenkins stages with the Jenkins pipeline DSL, how those stages are visualized with Jenkins Blue Ocean, and how the artifact\nwas deployed to our binary repository manager Artifactory.\nNow I wish you a lot of further fun with your great tools of choice to implement your Continuous Delivery pipelines.\n\nReferences\n\nJenkins 2\n\nSonarqube\n\nSonarqube Jenkins plugin\n\nArtifactory\n\nJenkins Artifactory plugin\n\n'DevOps for Developers', Apress, 2012\n\n'Agile ALM', Manning, 2011","title":"Delivery Pipelines, with Jenkins 2, SonarQube, and Artifactory","tags":["quality","sonarqube","jenkins","artifactory"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/c09ea/michaelhuettermann.jpg","srcSet":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/534e5/michaelhuettermann.jpg 32w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/99887/michaelhuettermann.jpg 64w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/c09ea/michaelhuettermann.jpg 128w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/76fd4/michaelhuettermann.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/59a6b/michaelhuettermann.webp 32w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/cbb78/michaelhuettermann.webp 64w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/96250/michaelhuettermann.webp 128w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/50511/michaelhuettermann.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":171}}},"blog":"http://huettermann.net","github":"michaelhuettermann","html":"<div class=\"paragraph\">\n<p>Michael is expert in Continuous Delivery, DevOps and SCM/ALM supporting enterprises in implementing DevOps.\nMichael is Jenkins Ambassador.</p>\n</div>","id":"michaelhuettermann","irc":null,"linkedin":null,"name":"Michael Hüttermann","slug":"/blog/authors/michaelhuettermann","twitter":"huettermann"}]}},{"node":{"date":"2016-09-09T00:00:00.000Z","id":"ebc932b1-1d9b-5ee9-80be-9c7081c78a03","slug":"/blog/2016/09/09/take-the-2016-jenkins-survey-blog/","strippedHtml":"This is a guest post by Brian\nDawson on behalf of CloudBees, where he works as a DevOps Evangelist\nresponsible for developing and sharing continuous delivery and DevOps best\npractices. He also serves as the CloudBees Product Marketing Manager for\nJenkins.\n\nOnce again it’s that time of year when CloudBees sponsors the\nJenkins Community Survey to\nassist the community with gathering objective insights into how jenkins is\nbeing used and what users would like to see in the Jenkins project.\n\nYour personal information (name, email address and company) will NOT be used by CloudBees for\nsales or marketing.\n\nAs an added incentive to take the survey, CloudBees will enter participants\ninto a drawing for a free pass to Jenkins World 2017 (1st prize) and a $100\nAmazon Gift Card (2nd prize). The survey will close at the end of September, so\nclick the link at the end of the blog post to get started!\n\nAll participants will be able to access reports summarizing survey results. If\nyou’re curious about what insights your input will provide, see the results of\nlast year’s 2015 survey:\n\n2015 Community Survey Results (PDF)\n\nState of Jenkins Infographic (PDF)\n\nYour feedback helps capture a bigger picture of\ncommunity trends and needs. There are laws that govern prize giveaways and\neligibility; CloudBees has compiled all those fancy\nterms and conditions here.\n\nPlease take the survey and let your voice be heard - it will take less than 10\nminutes.\n\nTake me to the survey","title":"Take the 2016 Jenkins Survey!","tags":["jenkins"],"authors":[{"avatar":null,"blog":null,"github":"bvdawson","html":"<div class=\"paragraph\">\n<p>DevOps dude at CloudBees.\nJenkins Marketing Manager.\nTools geek.</p>\n</div>","id":"bvdawson","irc":null,"linkedin":null,"name":"Brian Dawson","slug":"/blog/authors/bvdawson","twitter":"brianvdawson"}]}},{"node":{"date":"2016-06-15T00:00:00.000Z","id":"92e6efae-588c-5a80-886a-8fe7822dcea3","slug":"/blog/2016/06/15/jenkins-pipeline-scalability/","strippedHtml":"This is a guest post by Damien\nCoraboeuf, Jenkins project contributor and Continuous Delivery consultant.\n\nImplementing a CI/CD solution based on Jenkins has become very easy. Dealing\nwith hundreds of jobs? Not so much. Having to scale to thousands of jobs?\nNow this is a real challenge.\n\nThis is the story of a journey to get out of the jungle of jobs…​\n\nStart of the journey\n\nAt the beginning of the journey there were several projects using roughly the same\ntechnologies. Those projects had several\nbranches, for maintenance of releases, for new features.\n\nIn turn, each of those branches had to be carefully built, deployed on different\nplatforms and versions, promoted so they could be tested for functionalities,\nperformances and security, and then promoted again for actual delivery.\n\nAdditionally, we had to offer the test teams the means to deploy any version of\ntheir choice on any supported platform in order to carry out some manual tests.\n\nThis represented, for each branch, around 20 jobs. Multiply this by the number of\nbranches and projects, and there you are: more than two years after the start\nof the story, we had more than 3500 jobs.\n\n3500 jobs. Half a dozen people to manage them all…​\n\nPreparing the journey\n\nHow did we deal with this load?\n\nWe were lucky enough to have several assets:\n\ntime - we had time to design a solution before the scaling went really out of\ncontrol\n\nforecast - we knew that the scaling would occur and we were not taken by\nsurprise\n\ntooling - the Jenkins Job DSL\nwas available, efficient and well documented\n\nWe also knew that, in order to scale, we’d have to provide a solution with the\nfollowing characteristics:\n\nself-service - we could not have a team of 6 people become a bottleneck for\nenabling CI/CD in projects\n\nsecurity - the solution had to be secure enough in order for it to be used by\nremote developers we never met and didn’t know\n\nsimplicity - enabling CI/CD had to be simple so that people having\nnever heard of it could still use it\n\nextensibility - no solution is a one-size-fits-all and must be flexible\nenough to allow for corner cases\n\nAll the mechanisms described in this article are available through the\nJenkins Seed plugin.\n\nCreating pipelines using the Job DSL and embedding the scripts in the code was\nsimple enough. But what about branching? We needed a mechanism to allow the\ncreation of pipelines per branch, by downloading the associated DSL and to\nrun it in a dedicated folder.\n\nBut then, all those projects, all those branches, they were mostly using the\nsame pipelines, give or take a few configurable items. Going this way would\nhave lead to a terrible duplication of code, transforming a job maintenance\nnightmare into a code maintenance nightmare.\n\nPipeline as configuration\n\nOur trick was to transform this vision of \"pipeline as code\" into a \"pipeline\nas configuration\":\n\nby maintaining well documented and tested \"pipeline libraries\"\n\nby asking projects to describe their pipeline not as code, but as property\nfiles which would:\n\ndefine the name and version of the DSL pipeline library to use\n\nuse the rest of the property file to configure the pipeline library, using\nas many sensible default values as possible\n\nPiloting the pipeline from the SCM\n\nOnce this was done, the only remaining trick was to automate the creation,\nupdate, start and deletion of the pipelines using SCM events. By enabling SCM\nhooks (in GitHub, BitBucket or even in Subversion), we could:\n\nautomatically create a pipeline for a new branch\n\nregenerate a pipeline when the branch’s pipeline description was modified\n\nstart the pipeline on any other commit on the branch\n\nremove the pipeline when the branch was deleted\n\nOnce a project wants to go in our ecosystem, the Jenkins team \"seeds\" the\nproject into Jenkins, by running a job and giving a few parameters.\n\nIt will create a folder for the project and grant proper authorisations, using\nActive Directory group names based on the project name.\n\nThe hook for the project must be registered into the SCM and you’re up and\nrunning.\n\nConfiguration and code\n\nMixing the use of strong pipeline libraries configured by properties and the\ndirect use of the Jenkins Job DSL is still possible. The Seed plugin\nsupports all kinds of combinations:\n\nuse of pipeline libraries only - this can even be enforced\n\nuse a DSL script which can in turn use some classes and methods defined in\na pipeline library\n\nuse of a Job DSL script only\n\nUsually, we tried to have a maximum reuse, through only pipeline libraries, for\nmost of our projects, but in other circumstances, we were less strict and\nallowed some teams to develop their own pipeline script.\n\nEnd of the journey\n\nIn the end, what did we achieve?\n\nSelf service ✔︎\n\nPipeline automation from SCM - no intervention from the Jenkins team but for\nthe initial bootstrapping\n\nGetting a project on board of this system can be done in a few minutes only\n\nSecurity ✔︎\n\nProject level authorisations\n\nNo code execution on the controller\n\nSimplicity ✔︎\n\nProperty files\n\nExtensibility ✔︎\n\nPipeline libraries\n\nDirect job DSL still possible\n\nSeed and Pipeline plugin\n\nNow, what about the Pipeline plugin? Both\nthis plugin and the Seed plugin have common functionalities:\n\nWhat we have found in our journey is that having a \"pipeline as configuration\"\nwas the easiest and most secure way to get a lot of projects on board, with\ndevelopers not knowing Jenkins and even less the DSL.\n\nThe outcome of the two plugins is different:\n\none pipeline job for the Pipeline plugin\n\na list of orchestrated jobs for the Seed plugin\n\nIf time allows, it would be probably a good idea to find a way to integrate the\nfunctionalities of the Seed plugin into the pipeline framework, and to keep\nwhat makes the strength of the Seed plugin:\n\npipeline as configuration\n\nreuseable pipeline libraries, versioned and tested\n\nLinks\n\nYou can find additional information about the Seed plugin and its usage at the\nfollowing links:\n\nthe Seed plugin itself\n\nJUC London, June 2015\n\nBruJUG Brussels, March 2016","title":"Jenkins Pipeline Scalability in the Enterprise","tags":["jenkins","scalability","dsl"],"authors":[{"avatar":null,"blog":null,"github":"dcoraboeuf","html":"<div class=\"paragraph\">\n<p>I&#8217;ve started many years ago in the Java development before switching\nprogressively toward continuous delivery aspects.  I&#8217;m now a consultant\nimplementing CD solutions based on Jenkins. Implementation of the Pipeline\nas Code principles have allowed one of my clients to be able to manage more\nthan 3000 jobs, using a self service approach based on the Seed plugin.</p>\n</div>\n<div class=\"paragraph\">\n<p>I&#8217;m also a contributor for some Jenkins plugins and the author of the\nOntrack application, which allows the monitoring of continuous delivery\npipelines.</p>\n</div>","id":"dcoraboeuf","irc":null,"linkedin":null,"name":"Damien Coraboeuf","slug":"/blog/authors/dcoraboeuf","twitter":"DamienCoraboeuf"}]}},{"node":{"date":"2016-04-21T00:00:00.000Z","id":"d9ce8340-eedf-5dba-874d-c9eba3f8e717","slug":"/blog/2016/04/21/dsl-plugins/","strippedHtml":"In this post I will show how you can make your own DSL extensions and distribute\nthem as a plugin, using Pipeline Script.\n\nA quick refresher\n\nPipeline has a well kept secret: the ability to add your own DSL\nelements. Pipeline is itself a DSL, but you can extend it.\n\nThere are 2 main reasons I can think you may want to do this:\n\nYou want to reduce boilerplate by encapsulating common snippets/things you do\nin one DSL statement.\n\nYou want to provide a DSL that provides a prescriptive way that your builds\nwork - uniform across your organisations Jenkinsfiles.\n\nA DSL could look as simple as\n\nacmeBuild {\n    script = \"./bin/ci\"\n    environment = \"nginx\"\n    team = \"evil-devs\"\n    deployBranch = \"production\"\n}\n\nThis could be the entirety of your Jenkinsfile!\n\nIn this \"simple\" example, it could actually be doing a multi stage build with\nretries, in a specified docker container, that deploys only from the production\nbranch.  Detailed notifications are sent to the right team on important events\n(as defined by your org).\n\nTraditionally this is done via the\nglobal\nlibrary.  You take a snippet of DSL you want to want to make into a DSL, and\ndrop it in the git repo that is baked into Jenkins.\n\nA great trivial\nexample\nis this:\n\njenkinsPlugin {\n    name = 'git'\n}\n\nWhich is enabled by git pushing the following into vars/jenkinsPlugin.groovy\n\nThe name of the file is the name of the DSL expression you use in the Jenkinsfile\n\ndef call(body) {\n    def config = [:]\n    body.resolveStrategy = Closure.DELEGATE_FIRST\n    body.delegate = config\n    body()\n\n    // This is where the magic happens - put your pipeline snippets in here, get variables from config.\n    node {\n        git url: \"https://github.com/jenkinsci/${config.name}-plugin.git\"\n        sh \"mvn install\"\n        mail to: \"...\", subject: \"${config.name} plugin build\", body: \"...\"\n    }\n}\n\nYou can imagine many more pipelines, or even archetypes/templates of pipelines\nyou could do in this way, providing a really easy Jenkinsfile syntax for your\nusers.\n\nMaking it a plugin\n\nUsing the global DSL library is a handy thing if you have a single Jenkins, or\nwant to keep the DSLs local to a Jenkins instance.  But what if you want to\ndistribute it around your org, or, perhaps it is general purpose enough you want\nto share it with the world?\n\nWell this is possible, by wrapping it in a plugin. You use the same pipeline\nsnippet tricks you use in the global lib, but put it in the dsl directory of a\nplugin.\n\nMy simple\nbuild plugin shows how it is done.  To make your own plugin:\n\nCreate a new plugin project, either fork the simple build one, or add a\ndependency to it in your pom.xml / build.gradle file\n\nPut your dsl in the resources directory in a similar fashion to\nthis\n(note the \"package dsl\" declaration at the top)\n\nCreate the equivalent extension that just points to the DSL by name like\nthis\nThis is mostly \"boiler plate\" but it tells Jenkins there is a GlobalVariable extension available when Pipelines run.\n\nDeploy it to an Jenkins Update Center to share with your org, or everyone!\n\nThe advantage of delivering this DSL as a plugin is that it has a version (you\ncan also put tests in there), and distributable just like any other plugin.\n\nFor the more advanced, Andrew Bayer has a Simple\nTravis Runner plugin that\ninterprets and runs\ntravis.yml files which is also implemented in pipeline.\n\nSo, approximately, you can build plugins for pipeline that extend pipeline, in\npipeline script (with a teeny bit of boiler plate).\n\nEnjoy!","title":"Making your own DSL with plugins, written in Pipeline script","tags":["jenkins","dsl","pipeline","plugins"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/19e71/michaelneale.jpg","srcSet":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/77b35/michaelneale.jpg 32w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/d4a57/michaelneale.jpg 64w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/19e71/michaelneale.jpg 128w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/ef6ff/michaelneale.webp 32w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/8257c/michaelneale.webp 64w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/6766a/michaelneale.webp 128w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"michaelneale","html":"<div class=\"paragraph\">\n<p>Michael is a CD enthusiast with a interest in User Experience.\nHe is a co-founder of CloudBees and a long time OSS developer, and can often be found\nlurking around the jenkins-dev mailing list or #jenkins on irc (same nick as twitter name).\nBefore CloudBees he worked at Red Hat.</p>\n</div>","id":"michaelneale","irc":null,"linkedin":null,"name":"Michael Neale","slug":"/blog/authors/michaelneale","twitter":"michaelneale"}]}},{"node":{"date":"2016-01-27T00:00:00.000Z","id":"71059d3b-342c-5d7f-bb17-4f49e709e88c","slug":"/blog/2016/01/27/jenkins-world-call-for-papers/","strippedHtml":"This is a guest post by Alyssa Tong.\nAlyssa works for CloudBees, helping to organize\nJenkins community events around the\nworld.\n\nPlanning is underway for Jenkins World, a major Jenkins event for developers,\nrelease engineers and others interested in automation. The conference will be\nheld from September 13th to 15th in Santa Clara, California and is being\norganized and sponsored in part by CloudBees.\nJust like the \"Jenkins User Conferences\" before it, this year’s event will\nfeature many experts from the Jenkins community that help make Jenkins\nthe most popular open source automation server on the planet. We’ve found that\nwe outgrew the popular multi-city one-day Jenkins User Conferences, so unlike\nprevious years Jenkins World will be a three-day event in one place with an\nincredible amount of great content.\n\nThe goal of the event is to bring Jenkins contributors and users of all levels\ntogether, from around the world, to discuss, share and learn from one another.\nStarting today we’re opening the\ncall for\nproposals . As a global event, users from all over the world are encouraged to\nsubmit a talk between now and May 1st, 2016 (11:59pm PST).\n\nWe look forward to receiving your amazing submission, and seeing you in Santa\nClara this fall.\n\nSubmit a\nproposal today!","title":"Jenkins World 2016: Call For Papers Is Open!","tags":["jenkins world","event","jenkins"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#281818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg","srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/8d248/alyssat.jpg 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/c004c/alyssat.jpg 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/9e67b/alyssat.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/22924/alyssat.webp 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/89767/alyssat.webp 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/40d97/alyssat.webp 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/5028e/alyssat.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":166}}},"blog":null,"github":"alyssat","html":"<div class=\"paragraph\">\n<p>Member of the <a href=\"/sigs/advocacy-and-outreach/\">Jenkins Advocacy and Outreach SIG</a>.\nAlyssa drives and manages Jenkins participation in community events and conferences like <a href=\"https://fosdem.org/\">FOSDEM</a>, <a href=\"https://www.socallinuxexpo.org/\">SCaLE</a>, <a href=\"https://events.linuxfoundation.org/cdcon/\">cdCON</a>, and <a href=\"https://events19.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2019/\">KubeCon</a>.\nShe is also responsible for Marketing &amp; Community Programs at <a href=\"https://cloudbees.com\">CloudBees, Inc.</a></p>\n</div>","id":"alyssat","irc":null,"linkedin":null,"name":"Alyssa Tong","slug":"/blog/authors/alyssat","twitter":null}]}}]}},"pageContext":{"tag":"jenkins","limit":8,"skip":32,"numPages":5,"currentPage":5}},
    "staticQueryHashes": ["3649515864"]}