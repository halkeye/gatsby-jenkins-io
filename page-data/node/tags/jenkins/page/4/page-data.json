{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/jenkins/page/4",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2018-09-12T00:00:00.000Z","id":"4f66e646-7f00-5fcb-825e-a751b21abc62","slug":"/blog/2018/09/12/2018-community-survey/","strippedHtml":"Take the 5th Annual DevOps and Jenkins Community Survey\n\nWith DevOps World | Jenkins World San Francisco right around the corner, CloudBees is excited to sponsor the 2018 DevOps and Jenkins Community Survey. We want to capture the details of your DevOps experience in order to provide valuable insights to the Jenkins Community and beyond. Our community is stronger together - and this look at our collective experience will reveal the big picture and shine a light on key trends. This year, as the Jenkins project continues to evolve with Jenkins X , Configuration as Code and more, your input is more critical than ever.\n\nLet’s look at what we learned in 2016 and 2017:\n\nIn 2016 we found that:\n\nJenkins continued to hold the position as a company standard orchestration solution.\n\n29% of respondents companies use Jenkins on more than 50 projects\n\nIn regards to SCM tools, Git continued the march to dominance:\n\nGit usage increased to 85%\n\nSubversion usage decreased to 35%\n\nWhen it comes to practices, Agile and CI seemed to be the standard, and CD adoption still had a ways to go:\n\n85% practiced Agile\n\n82% practiced CI\n\n61% practiced DevOps\n\n46% practiced CD\n\nIn 2017 respondents reported that:\n\nJenkins Pipeline gained widespread adoption with 89% of survey takers used pipeline or planned to use in 6 months or less.\n\nContainer technology was cemented as a key part of the CD/DevOps ecosystem, yet Kubernetes usage was just starting gain momentum at 20.15%:\n\nJenkins, CD, and DevOps are getting more attention from Architects with 39% of respondents identified as Architects, nearly double the previous year\n\nGit was the clear SCM of choice at 90%, increasing nearly 5% over last the previous year\n\nWhat will you and the community tell us this year?  Are more people practicing DevOps?  Is Kubernetes the leader in container orchestration?  Is pipeline the standard for creating workflows?  Take the survey and let’s find out!\n\nAs always, your personal information (name, email address and company) will NOT be used by CloudBees for sales or marketing and the survey results will be made publicly available to the Jenkins Community. We will also be publishing a blog series analyzing trends over the last 5 years and offering  predictions on the evolution of DevOps. If you’re curious about what insights your input will provide, see the results of last year’s 2017 survey.\n\nAs an added incentive to participate, CloudBees will enter participants into a drawing for a free pass to DevOps World | Jenkins World 2019 (1st prize, $1,199.00 value) or a $100 Amazon gift card (2nd prize)!\n\nThe survey will close at the end of October so grab a cup of coffee get started. We promise the survey will be done before your latte is.\n\nTake me to the survey.\n\nThere are laws that govern prize giveaways and eligibility; CloudBees has compiled all those fancy terms and conditions here.","title":"2018 DevOps|Jenkins Community Survey Now Open","tags":["jenkins","devops","jenkinsworld2018"],"authors":[{"avatar":null,"blog":null,"github":"bvdawson","html":"<div class=\"paragraph\">\n<p>DevOps dude at CloudBees.\nJenkins Marketing Manager.\nTools geek.</p>\n</div>","id":"bvdawson","irc":null,"linkedin":null,"name":"Brian Dawson","slug":"/blog/author/bvdawson","twitter":"brianvdawson"}]}},{"node":{"date":"2018-08-29T00:00:00.000Z","id":"edd1abcd-b413-5696-b610-a81a6a7fdd9f","slug":"/blog/2018/08/29/day-of-jenkins-and-other-chances-to-meet-jcasc/","strippedHtml":"The Jenkins Configuration as Code plugin is reaching a stage when it is almost ready to be used in a production environment.\nAs a matter of fact, I know some living-on-the-edge users are already doing that.\nThe first release candidates are out and the official 1.0 is just around the corner.\n\nI’d like to use this chance to invite you to meet us and contribute to the plugin.\nThere will be plenty of opportunities this autumn.\n\nJenkins Configuration as Code (also called \"JCasC\") is a Jenkins plugin that allows you to store and maintain all your Jenkins configuration in yaml file.\nIt’s like Pipeline or Job DSL but for managing Jenkins.\n\nIn one of my blogposts,\nJenkins Configuration as Code - Automating an automation server,\nI provide a longer explanation of the plugin, and answer questions like\n“why did we decided to develop it?” and “why you may want to use it?”.\nI recommend you to read that one if you’re not familiar with the project yet.\n\nThe plugin has been presented at a number of meetups - by me but also other contributors.\nThis is the first open source project that I’ve actively participated in and I’m quite shocked - positively - to see how many people decided to join the effort and actively develop the plugin with us.\nNow it’s time to take it to the bigger stage and broader audience.\nSo together with Nicolas de Loof I’m gonna present the plugin at DevOps World | Jenkins World in San Francisco (19th of September)  and in Nice (24th of October) - yes, Jenkins World is coming to Europe.\n\nBut that’s not all!\nPraqma - the company I work for -\nhas organised a number of “Day of Jenkins” events around Scandinavia in past years.\nThis October they have decided to bring the events back with a theme: Day of Jenkins 2018 is\nDay of Jenkins [as code] .\nIt’s a two track one day event with presentations and hands-on sessions for users and a hackathon for contributors - in that specific case Configuration as Code Plugin’s contributors.\n\nDetailed agenda is available on the\nevent page -\nJenkins X, Jenkins Evergreen, Jenkins Configuration as Code and more waiting for you!\n\nI really can’t wait to hear what Kohsuke has to say and to introduce you to the plugin during the hands-on session I’ll run.\n\nHope to see you at least at one of those events!\n\nCome meet the Configuration as Code contributors, Nicolas de Loof and Ewelina Wilkosz at\nJenkins World on September 16-19th,\nregister with the code JWFOSS for a 30% discount off your pass.","title":"Day of Jenkins, and other chances to meet JCasC","tags":["jenkins","jcasc","configuration as code"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#c8c8c8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/62cac5d7353c59176523b1ca3cb1166c/19e71/ewelinawilkosz.jpg","srcSet":"/gatsby-jenkins-io/static/62cac5d7353c59176523b1ca3cb1166c/77b35/ewelinawilkosz.jpg 32w,\n/gatsby-jenkins-io/static/62cac5d7353c59176523b1ca3cb1166c/d4a57/ewelinawilkosz.jpg 64w,\n/gatsby-jenkins-io/static/62cac5d7353c59176523b1ca3cb1166c/19e71/ewelinawilkosz.jpg 128w,\n/gatsby-jenkins-io/static/62cac5d7353c59176523b1ca3cb1166c/68974/ewelinawilkosz.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/62cac5d7353c59176523b1ca3cb1166c/ef6ff/ewelinawilkosz.webp 32w,\n/gatsby-jenkins-io/static/62cac5d7353c59176523b1ca3cb1166c/8257c/ewelinawilkosz.webp 64w,\n/gatsby-jenkins-io/static/62cac5d7353c59176523b1ca3cb1166c/6766a/ewelinawilkosz.webp 128w,\n/gatsby-jenkins-io/static/62cac5d7353c59176523b1ca3cb1166c/22bfc/ewelinawilkosz.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"ewelinawilkosz","html":"<div class=\"paragraph\">\n<p>Jenkins Contributor since 2017, when she got involved in Jenkins Configuration as Code Plugin development.\nVoted Most Valuable Contributor in 2018.\nShe has 14 years of experience in IT, currently working as a CI/CD consultant at Verifa.\nIn that role she’s trying to solve numerous issues Jenkins users are facing daily - as developers, administrators, maintainers.\nJenkins Governance Board member.</p>\n</div>","id":"ewelinawilkosz","irc":null,"linkedin":null,"name":"Ewelina Wilkosz","slug":"/blog/author/ewelinawilkosz","twitter":"WilkoszEwelina"}]}},{"node":{"date":"2018-04-25T00:00:00.000Z","id":"e34fdb6b-09c7-5772-a251-10cec0ccef26","slug":"/blog/2018/04/25/configuring-jenkins-pipeline-with-yaml-file/","strippedHtml":"A few years ago our CTO wrote about building a\nContinuous Integration server for Ruby On Rails using Jenkins and docker.\nThe solution has been our CI pipeline for the past years until we recently decided to\nmake an upgrade. Why?\n\nJenkins version was way out of date and it was getting difficult to\nupgrade\n\nWolox has grown significantly over the past years\nand we’ve been experiencing scaling issues\n\nVery few people knew how to fix any issues with the server\n\nConfiguring jobs was not an easy task and that made our project\nkickoff process slower\n\nMaking changes to the commands that each job runs was not easy and not\nmany people had permissions to do so. Wolox has a wide range of\nprojects, with a wide variety of languages which made this problem even\nbigger.\n\nTaking into account these problems, we started digging into the newest\nversion of Jenkins to see how we could improve our CI. We needed to\nbuild a new CI that could, at least, address the following:\n\nProjects must be built using Docker. Our projects depend on one or\nmultiple docker images to run (app, database, redis, etc)\n\nEasy to configure and replicate if necessary\n\nEasy to add a new project\n\nEasy to change the building steps. Everyone working on the project\nshould be able to change if they want to run npm install or yarn\ninstall.\n\nInstalling Jenkins and Docker\n\nInstalling Jenkins is straightforward. You can visit\nJenkins Installation page and choose the\noption that best suits your needs.\n\nHere are the steps we followed to install Jenkins in AWS:\n\nsudo rpm — import https://pkg.jenkins.io/debian/jenkins.io.key\nsudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat/jenkins.repo\nsudo yum install java-1.8.0 -y\nsudo yum remove java-1.7.0-openjdk -y\nsudo yum install jenkins -y\nsudo yum update -y\nsudo yum install -y docker\n\nAutomatically adding projects from Github\n\nAdding projects automatically from Github can be achieved using the\nGitHub Branch Source Plugin.\nIt allows Jenkins to scan a GitHub organization\nfor projects that match certain rules and add them to Jenkins\nautomatically. The only constraint that all branches must meet in order\nto be added is that they contain a Jenkinsfile that explains how to\nbuild the project.\n\nEasy to change configuration\n\nNot so easy to change configuration\n\nOne of the biggest pains we had with our previous Jenkins was the\ndifficulty of changing the steps necessary to build the project. If you\nlooked at a project’s build steps, you would find something like this:\n\n#!/bin/bash +x\nset -e\n\n# Remove unnecessary files\necho -e \"\\033[34mRemoving unnecessary files...\\033[0m\"\nrm -f log/*.log &> /dev/null || true &> /dev/null\nrm -rf public/uploads/* &> /dev/null || true &> /dev/null\n\n# Build Project\necho -e \"\\033[34mBuilding Project...\\033[0m\"\ndocker-compose --project-name=${JOB_NAME} build\n\n# Prepare test database\nCOMMAND=\"bundle exec rake db:drop db:create db:migrate\"\necho -e \"\\033[34mRunning: $COMMAND\\033[0m\"\ndocker-compose --project-name=${JOB_NAME} run  \\\n\t-e RAILS_ENV=test web $COMMAND\n\n# Run tests\nCOMMAND=\"bundle exec rspec spec\"\necho -e \"\\033[34mRunning: $COMMAND\\033[0m\"\nunbuffer docker-compose --project-name=${JOB_NAME} run web $COMMAND\n\n# Run rubocop lint\nCOMMAND=\"bundle exec rubocop app spec -R --format simple\"\necho -e \"\\033[34mRunning: $COMMAND\\033[0m\"\nunbuffer docker-compose --project-name=${JOB_NAME} run -e RUBYOPT=\"-Ku\" web $COMMAND\n\nAnd some post build steps that cleaned up the docker:\n\n#!/bin/bash +x\ndocker-compose --project-name=${JOB_NAME} stop &> /dev/null || true &> /dev/null\ndocker-compose --project-name=${JOB_NAME} rm --force &> /dev/null || true &> /dev/null\ndocker stop `docker ps -a -q -f status=exited` &> /dev/null || true &> /dev/null\ndocker rm -v `docker ps -a -q -f status=exited` &> /dev/null || true &> /dev/null\ndocker rmi `docker images --filter 'dangling=true' -q --no-trunc` &> /dev/null || true &> /dev/null\n\nAlthough these commands are not complex, changing any of them required\nsomeone with permissions to modify the job and an understanding ofwhat\nneeded to be done.\n\nJenkinsfile to the rescue…​ or not\n\nWith the current Jenkins version, we can take advantage of\nJenkins Pipeline and model our build\nflow in a file. This file is checked into the repository and, therefore,\nanyone with access to it can change the build steps. Yay!\n\nJenkins Pipeline even has support for:\n\nDocker and\nmultiple\nimages can be used for a build!\n\nSetting environment variables with withEnv and many other built -in\nfunctions that can be found\nhere.\n\nThis makes a perfect case for Wolox. We can have\nour build configuration in a file that’s checked into the repository and\ncan be changed by anyone with write access to it. However, a Jenkinsfile\nfor a simple rails project would look something like this:\n\n# sample Jenkinsfile. Might not compile\nnode {\n    checkout scm\n    withEnv(['MYTOOL_HOME=/usr/local/mytool']) {\n        docker.image(\"postgres:9.2\").withRun() { db ->\n            withEnv(['DB_USERNAME=postgres', 'DB_PASSWORD=', \"DB_HOST=db\", \"DB_PORT=5432\"]) {\n                docker.image(\"redis:X\").withRun() { redis ->\n                    withEnv([\"REDIS_URL=redis://redis\"]) {\n                        docker.build(imageName, \"--file .woloxci/Dockerfile .\").inside(\"--link ${db.id}:postgres --link ${redis.id}:redis\") {\n                            sh \"rake db:create\"\n                            sh \"rake db:migrate\"\n                            sh \"bundle exec rspec spec\"\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nThis file is not only difficult to read, but also difficult to change.\nIt’s quite easy to break things if you’re not familiar with Groovy and\neven easier if you know nothing about how Jenkins’ pipeline works.\nChanging or adding a new Docker image isn’t straightforward and might\nlead to confusion.\n\nConfiguring Jenkins Pipeline via YAML\n\nPersonally, I’ve always envied simple configuration files for CIs and\nthis time it was our chance to build CI that could be configured using a\nYAML file. After some analysis we concluded that a YAML like this one\nwould suffice:\n\nconfig:\n  dockerfile: .woloxci/Dockerfile\n  project_name: some-project-name\n\nservices:\n  - postgresql\n  - redis\n\nsteps:\n  analysis:\n    - bundle exec rubocop -R app spec --format simple\n    - bundle exec rubycritic --path ./analysis --minimum-score 80 --no-browser\n  setup_db:\n    - bundle exec rails db:create\n    - bundle exec rails db:schema:load\n  test:\n    - bundle exec rspec\n  security:\n    - bundle exec brakeman --exit-on-error\n  audit:\n    - bundle audit check --update\n\n\nenvironment:\n  RAILS_ENV: test\n  GIT_COMMITTER_NAME: a\n  GIT_COMMITTER_EMAIL: b\n  LANG: C.UTF-8\n\nIt outlines some basic configuration for the project, environment\nvariables that need to be present during the run, dependentservices, and\nour build steps.\n\nJenkinsfile + Shared Libraries = WoloxCI\n\nAfter investigating for a while about Jenkins and the pipeline, we found\nthat we could extend it with\nshared libraries.\nShared libraries are written in groovy and can be imported\ninto the pipeline and executed when necessary.\n\nIf you look carefully at this Jenkinsfile,\nwe see that the code is a chain of methods calls that receive a\nclosure, where we execute another method passing a new closure to it.\n\n# sample Jenkinsfile. Might not compile\nnode {\n    checkout scm\n    withEnv(['MYTOOL_HOME=/usr/local/mytool']) {\n        docker.image(\"postgres:9.2\").withRun() { db ->\n            withEnv(['DB_USERNAME=postgres', 'DB_PASSWORD=', \"DB_HOST=db\", \"DB_PORT=5432\"]) {\n                docker.image(\"redis:X\").withRun() { redis ->\n                    withEnv([\"REDIS_URL=redis://redis\"]) {\n                        docker.build(imageName, \"--file .woloxci/Dockerfile .\").inside(\"--link ${db.id}:postgres --link ${redis.id}:redis\") {\n                            sh \"rake db:create\"\n                            sh \"rake db:migrate\"\n                            sh \"bundle exec rspec spec\"\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nGroovy is flexible enough to allow this same declarative code to be\ncreated at runtime, making our dream of using a YAML to configure our\njob come true!\n\nIntroducing Wolox-CI\n\nThat’s how wolox-ci was born- our\nshared library for Jenkins!\n\nWith wolox-ci, our Jenkinsfile is now\nreduced to:\n\n@Library('wolox-ci') _\n\nnode {\n\n  checkout scm\n\n  woloxCi('.woloxci/config.yml');\n}\n\nNow it simply checks out the code and then calls wolox-ci. The library\nreads yaml file like this one\n\nconfig:\n  dockerfile: .woloxci/Dockerfile\n  project_name: some-project-name\n\nservices:\n  - postgresql\n  - redis\n\nsteps:\n  analysis:\n    - bundle exec rubocop -R app spec --format simple\n    - bundle exec rubycritic --path ./analysis --minimum-score 80 --no-browser\n  setup_db:\n    - bundle exec rails db:create\n    - bundle exec rails db:schema:load\n  test:\n    - bundle exec rspec\n  security:\n    - bundle exec brakeman --exit-on-error\n  audit:\n    - bundle audit check --update\n\n\nenvironment:\n  RAILS_ENV: test\n  GIT_COMMITTER_NAME: a\n  GIT_COMMITTER_EMAIL: b\n  LANG: C.UTF-8\n\nand builds the Jenkinsfile to get your job running on the fly.\n\nThe nice part about having a shared library is that we can extend and\nfix our library in a centralized way. Once we add new code, the library\nis automatically updated in Jenkins which will notify all of our jobs\nwith the update.\n\nSince we have projects in different languages we use Docker to build the\ntesting environment. WoloxCI assumes there is a Dockerfile to build and\nwill run all the specified commands inside the container.\n\nWoloxci config.yml\n\nConfig\n\nThe first part of the config.yml file specifies some basic\nconfiguration: project’s name and Dockerfile location. The Dockerfile is\nused to build the image where the commands will be run.\n\nServices\n\nThis section describes which services will be exposed to the container.\nOut of the box, WoloxCI has support for postgresql, mssql and\nredis. You can also specify the docker image version you want! It is\nnot hard to add a new service. You just need to add the corresponding\nfile at\n\nhttps://github.com/Wolox/wolox-ci/tree/development/vars\n\nand modify how the services are parsed\n\nhttps://github.com/Wolox/wolox-ci/blob/development/src/com/wolox/parser/ConfigParser.groovy#L76\n\nSteps\n\nThe listed commands in this section will run inside the Docker\ncontainer. As a result, you’ll see each of the steps on the Jenkins UI.\n\nEnvironment\n\nIf you need some environment variables during your build, you can\nspecify them here. Whatever variable you set will be available inside\nthe Docker container when your commands listed in the steps section\ndescribed above.\n\nWrapping up\n\nWoloxCI is still being tested with a not-so-small sample of our\nprojects. The possibility of changing the build steps through a YAML\nfile makes it accessible for everyone and that is a great improvement in\nour CI workflow.\n\nDocker gives us the possibility of easily changing the programming\nlanguage without making any changes to our Jenkins installation and\nJenkins’ Github Organization feature automatically adds new projects\nwhen a new repository with a Jenkinsfile is detected.\n\nAll of these improvements have reduced the time we spend maintaining\nJenkins significantly and give us the possibility of easily scaling\nwithout any extra configuration.\n\nThis library is working in our CI but it still can be improved.\nIf you would like to add features, feel free to\ncontribute!","title":"Configuring a Jenkins Pipeline using a YAML file","tags":["jenkins","pipelines","yaml","sharedlibrary"],"authors":[{"avatar":null,"blog":null,"github":"mdesanti","html":"<div class=\"paragraph\">\n<p>Head of Infrastructure &amp; Cloud at <a href=\"https://www.wolox.com.ar\">Wolox</a></p>\n</div>","id":"mdesanti","irc":null,"linkedin":null,"name":"Matias De Santi","slug":"/blog/author/mdesanti","twitter":"mdsanti"}]}},{"node":{"date":"2018-01-21T00:00:00.000Z","id":"4b83704c-93fe-50dd-86b2-f971dac01a43","slug":"/blog/2018/01/21/overhaul-of-manage-jenkins-page/","strippedHtml":"Overview\n\nRecently some UI improvements around the Manage Jenkins page have been introduced. The visual changes are very subtle but behind them, there are interesting benefits.\n\nSome of the goals that we have tried to achieve:\n\nApplying a semantic HTML\n\nRemoving the tag usage for implementing layouts and content structures. Read this article if you want to know reasons and/or arguments.\n\nSmall re-styling focused on spacing, margins, composition, etc..\n\nAccessibility\n\nIn order to provide a quick overview of the visual changes, let’s take a look at these screenshots.\n\nSystem tray with administrative messages (before)\n\nSystem tray with administrative messages (after)\n\nManage Jenkins page (before)\n\nManage Jenkins page (after)\n\nInformation about how this change can affect the current implementations of Administrative Monitors can be found in the following section\n\nFor core developers\n\nLet’s use a real example for showing how this proposal works.\n\nThis is the original UI implementation of HudsonHomeDiskUsageMonitor.java :\n\n${%blurb(app.rootDir)}\n\nAnd this is the proposed change:\n\n${%blurb(app.rootDir)}\n\nSome highlights:\n\nNo more ad hoc UI compositions\n\nNo more custom CSS classes when Jenkins project is already using Bootstrap for many different things\n\nBased on Bootstrap Alert\n\nAll administrative monitors defined in Jenkins core have been adapted as part of this proposal.\n\nFor plugin developers\n\nNo changes are really needed, but we do recommend you to adapt your plugins to this proposal so Jenkins users have a better user experience.\n\nTaking into account that you want to keep backward compatibility, you will need some changes.\n\nIn your implementation of Administrative Monitor, add this helper method:\n\n/**\n * This method can be removed when the baseline is updated to 2.103\n *\n * @return If this version of the plugin is running on a Jenkins version where JENKINS-43786 is included.\n */\n @Restricted(DoNotUse.class)\n public boolean isTheNewDesignAvailable() {\n    if (Jenkins.getVersion().isNewerThan(new VersionNumber(\"2.103\"))) {\n        return true;\n    }\n    return false;\n}\n\nIn your view (a.k.a. Jelly file or Groovy file):\n\nSSH Host Key Verifiers are not configured for all SSH agents on this Jenkins instance. This could leave these agents open to man-in-the-middle attacks. Update your agent configuration to resolve this.\n\nSSH Host Key Verifiers are not configured for all SSH agents on this Jenkins instance. This could leave these agents open to man-in-the-middle attacks. Update your agent configuration to resolve this.\n\nIf you don’t want to keep a strict backward compatibility, the impact is minimal. In fact, you can see an example on GitHub Plugin.\n\nSome helpful references:\n\nJIRA issue where the proposal was tracked\n\nPull Request with the change in Jenkins core. You can find several screenshots\n\nPull Request for adapting SSH Agent Plugin\n\nDo not hesitate to ping me if you decide to adapt your Administrative Monitors.","title":"Overhaul of Manage Jenkins page","tags":["jenkins","ui","restyling","upgrade"],"authors":[{"avatar":null,"blog":null,"github":"recena","html":"","id":"recena","irc":null,"linkedin":null,"name":"Manuel Recena","slug":"/blog/author/recena","twitter":null}]}},{"node":{"date":"2017-08-29T00:00:00.000Z","id":"76ba3f00-38ff-54a3-ba40-edf86486b934","slug":"/blog/2017/08/29/2017-community-survey/","strippedHtml":"This is a guest post by Brian\nDawson on behalf of CloudBees, where he works as a DevOps Evangelist\nresponsible for developing and sharing continuous delivery and DevOps best\npractices. He also serves as the CloudBees Product Marketing Manager for\nJenkins.\n\nOnce again it’s that time of year when CloudBees sponsors the\nJenkins Community Survey to\nassist the community with gathering objective insights into how jenkins is\nbeing used and what users would like to see in the Jenkins project.\n\nYour personal information (name, email address and company) will NOT be used by CloudBees for\nsales or marketing.\n\nAs an added incentive to take the survey, CloudBees will enter participants\ninto a drawing for a free pass to Jenkins World 2018 (1st prize) and a $100\nAmazon Gift Card (2nd prize). The survey will close at the end of September, so\nclick the link at the end of the blog post to get started!\n\nAll participants will be able to access reports summarizing survey results. If\nyou’re curious about what insights your input will provide, see the results of\nlast year’s 2016 survey:\n\n2016 Community Survey Results\n\nYour feedback helps capture a bigger picture of\ncommunity trends and needs. There are laws that govern prize giveaways and\neligibility; CloudBees has compiled all those fancy\nterms and conditions here.\n\nPlease take the survey and let your voice be heard - it will take less than 10\nminutes.\n\nTake me to the survey","title":"Take the 2017 Jenkins Survey!","tags":["jenkins"],"authors":[{"avatar":null,"blog":null,"github":"bvdawson","html":"<div class=\"paragraph\">\n<p>DevOps dude at CloudBees.\nJenkins Marketing Manager.\nTools geek.</p>\n</div>","id":"bvdawson","irc":null,"linkedin":null,"name":"Brian Dawson","slug":"/blog/author/bvdawson","twitter":"brianvdawson"}]}},{"node":{"date":"2017-07-21T00:00:00.000Z","id":"215b578b-be74-5ce1-b9f3-38b9bfc35f5c","slug":"/blog/2017/07/21/scaling-jenkins-with-kubernetes-on-google-container-engine/","strippedHtml":"This is a guest post by Guillaume Laforge,\nDeveloper Advocate for Google Cloud\n\nLast week, I had the pleasure to speak at the\nJenkins Community Day conference, in Paris,\norganized by my friends from JFrog,\nprovider of awesome tools for software management and distribution.\nI covered how to scale Jenkins with Kubernetes on\nGoogle Container Engine.\n\nFor the impatient, here are the slides of the presentation I’ve given:\n\nBut let’s step back a little. In this article, I’d like to share with you why you would want to run Jenkins in the cloud,\nas well as give you some pointers to interesting resources on the topic.\n\nWhy running Jenkins in the cloud?\n\nSo why running Jenkins in the cloud? First of all, imagine your small team, working on a single project.\nYou have your own little server, running under a desk somewhere, happily building your application on each commit,\na few times a day. So far so good, your build machine running Jenkins isn’t too busy, and stays idle most of the day.\n\nLet’s do some bottom of the napkin calculations. Let’s say you have a team of 3 developers,\ncommitting roughly 4 times a day, on one single project, and the build takes roughly 10 minutes to go.\n\n3 developers * 4 commits / day / developer * 10 minutes build time * 1 project = 1 hour 20 minutes\n\nSo far so good, your server indeed stays idle most of the day. Usually, at most,\nyour developers will wait just 10 minutes to see the result of their work.\n\nBut your team is growing to 10 persons, the team is still as productive, but the project becoming bigger,\nthe build time goes up to 15 minutes:\n\n10 developers * 4 commits / day / developer * 15 minutes build time * 1 project = 10 hours\n\nYou’re already at 10 hours build time, so your server is busy the whole day, and at times,\nyou might have several build going on at the same time, using several CPU cores in parallel.\nAnd instead of building in 15 minutes, sometimes, the build might take longer, or your build might be queued.\nSo in theory, it might be 15 minutes, but in practice, it could be half an hour because of the length of the queue\nor the longer time to build parallel projects.\n\nNow, the company is successful, and has two projects instead of one (think a backend and a mobile app).\nYour teams grow further up to 20 developers per project. The developers are a little less productive\nbecause of the size of the codebase and project, so they only commit 3 times a day.\nThe build takes more time too, at 20 minutes (in ideal time). Let’s do some math again:\n\n20 developers * 3 commits / day / developer * 20 minutes build time * 2 projects = 40 hours\n\nWoh, that’s already 40 hours of total build time, if all the builds are run serially.\nFortunately, our server is multi-core, but still, there are certainly already many builds that are enqueued,\nand many of them, perhaps up to 2-3 or perhaps even 4 could be run in parallel.\nBut as we said, the build queue increases further, the real effective time of build is certainly longer than 30 minutes.\nPerhaps at times, developers won’t see the result of their developments before at least an hour, if not more.\n\nOne last calculation? With team sizes of 30 developers, decreased productivity of 2 commits, 25 build time,\nand 3 projects? And you’ll get 75 hours total build time. You may start creating a little build farm,\nwith a controller and several build agents. But you also increase the burden of server management.\nAlso, if you move towards a full Continuous Delivery or Continuous Deployment approach,\nyou may further increase your build times to go up to deployment, make more but smaller commits, etc.\nYou could think of running builds less often, or even on a nightly basis, to cope with the demand, but then,\nyour company is less agile, and the time-to-market for fixes of new features might increase,\nand your developers may also become more frustrated because they are developing in the blind,\nnot knowing before the next day if their work was successful or not.\n\nWith my calculations, you might think that it makes more sense for big companies, with tons of projects and developers.\nThis is quite true, but when you’re a startup, you also want to avoid taking care of local server management,\nprovisioning, etc. You want to be agile, and use only compute resources you need for the time you need them.\nSo even if you’re a small startup, a small team, it might still make sense to take advantage of the cloud.\nYou pay only for the actual time taken by your builds as the build agent containers are automatically provisioned\nand decommissioned. The builds can scale up via Kubernetes, as you need more (or less) CPU time for building everything.\n\nAnd this is why I was happy to dive into scaling Jenkins in the cloud. For that purpose,\nI decided to go with building with containers, with Kubernetes, as my app was also containerized as well.\nGoogle Cloud offers Container Engine, which is basically just Kubernetes in the cloud.\n\nUseful pointers\n\nI based my presentation and demo on some great solutions that are published on the Google Cloud documentation portal.\nLet me give you some pointers.\n\nOverview of Jenkins on Container Engine\n\nSetting up Jenkins on Container Engine\n\nConfiguring Jenkins for Container Engine\n\nContinuous Deployment to Container Engine using Jenkins\n\nLab: Build a Continuous Deployment Pipeline with Jenkins and Kubernetes\n\nThe latter one is the tutorial I actually followed for the demo that I presented during the conference.\nIt’s a simple Go application, with a frontend and backend.\nIt’s continuously build, on each commit (well, every minute to check if there’s a new commit),\nand deployed automatically in different environments: dev, canary, production.\nThe sources of the project are stored in Cloud Source Repository (it can be mirrored from Github, for example).\nThe containers are stored in Cloud Container Registry.\nAnd both the Jenkins controller and agents, as well as the application are running inside Kubernetes clusters in Container Engine.\n\nSummary and perspective\n\nDon’t bother with managing servers! Quickly, you’ll run out of CPU cycles,\nand you’ll have happier developers with builds that are super snappy!\n\nAnd for the record, at Google, dev teams are also running Jenkins!\nThere was a presentation ( video and\nslides\navailable) given last year by David Hoover at Jenkins World\ntalking about how developers inside Google are running hundreds of build agents to build projects on various platforms.","title":"Scaling Jenkins with Kubernetes on Google Container Engine","tags":["jenkins","kubernetes","jenkins-community-day-paris"],"authors":[{"avatar":null,"blog":"https://glaforge.appspot.com/","github":"glaforge","html":"","id":"glaforge","irc":null,"linkedin":null,"name":"Guillaume Laforge","slug":"/blog/author/glaforge","twitter":"glaforge"}]}},{"node":{"date":"2017-07-05T00:00:00.000Z","id":"fd2d8b89-b05f-5caf-a6b1-923d33960065","slug":"/blog/2017/07/05/continuousdelivery-devops-artifactory/","strippedHtml":"This is a guest post by Michael Hüttermann. Michael is an expert\nin Continuous Delivery, DevOps and SCM/ALM. More information about him at huettermann.net, or\nfollow him on Twitter: @huettermann.\n\nIn a past blog post Delivery Pipelines,\nwith Jenkins 2, SonarQube, and Artifactory, we talked about pipelines which result in binaries for development versions. Now, in this blog post, I zoom in to different parts of the\nholistic pipeline and cover the handling of possible downstream steps once you have the binaries of development versions, in our example a Java EE WAR and a Docker image (which contains the WAR).\nWe discuss basic concept of staging software, including further information about quality gates, and show example toolchains. This contribution particularly examines the staging from binaries from\ndev versions to release candidate versions and from release candidate versions to final releases from the perspective of the automation server Jenkins, integrating with the binary\nrepository manager JFrog Artifactory and the distribution management platform JFrog Bintray, and ecosystem.\n\nStaging software\n\nStaging (also often called promoting) software is the process of completely and consistently transferring a release with all its configuration items\nfrom one environment to another. This is even more true with DevOps, where you want to accelerate the cycle time (see Michael Hüttermann, DevOps for Developers (Apress, 2012), 38ff).\nFor accelerating the cycle time, meaning to bring software to production, fast and in good quality, it is crucial to have fine processes and integrated tools to streamline the\ndelivery of software. The process of staging releases consists of deploying software to different staging levels, especially different test environments.\nStaging also involves configuring the software for various environments without needing to recompile or rebuild the software. Staging is necessary\nto transport the software to production systems in high quality. Many Agile projects make great experience with implementing a staging ladder in\norder to optimize the cycle time between development software and the point when the end user is able to use the software in production.\n\nCommonly, the staging ladder is illustrated on its side, with the higher rungs being the boxes further to the right. It’s good practice not to skip any rungs during staging.\nThe central development environment packages and integrates all respective configuration items and is the base for releasing. Software is staged over different environments by\nconfiguration, without rebuilding. All changes go through the entire staging process, although defined exception routines may be in place,\nfor details see Michael Hüttermann, Agile ALM (Manning, 2012).\n\nTo make concepts clearer, this blog post covers sample tools. Please note, that there are also alternative tools available. As one example: Sonatype Nexus is also able to host the covered binaries and also offers scripting functionality.\n\nWe nowadays often talk about delivery pipelines. A pipeline is just a set of stages and transition rules between those stages. From a DevOps perspective, a pipeline bridges multiple\nfunctions in organizations, above all development and operations. A pipeline is a staging ladder. A change enters the pipeline at the beginning and leaves it at the end. The processing\ncan be triggered automatically (typical for delivery pipelines) or by a human actor (typical for special steps at overall pipelines, e.g. pulling and thus cherry-picking specific\nversions to promote them to be release candidates are final releases).\n\nPipelines often look different, because they strongly depend on requirements and basic conditions, and can contain further sub pipelines. In our scenario, we have two sub pipelines to\nmanage the promotion of continuous dev versions to release candidates and the promotion of release candidates to final release. A change typically waits at a stage for further processing\naccording to the transition rules, aligned with defined requirements to meet, which are the Quality Gates, explored next.\n\nQuality Gates\n\nQuality gates allow the software to pass through stages only if it meets their defined requirements. The next illustration shows a staging ladder with quality gates injected. You and\nother engaged developers commit code to the version control system (please, use VCS as an abbreviation, not SCM, because the latter is much more) in order to update the central test\nenvironment only if the code satisfies the defined quality requirements; for instance, the local build may need to run successfully and have all tests pass locally. Build, test, and\nmetrics should pass out of the central development environment, and then automated and manual acceptance tests are needed to pass the system test. In our case, the last quality gate\nto pass is the one from the  production mirror to production. Here, for example, specific production tests are done or relevant documents must be filled in and signed.\n\nIt’s mandatory to define the quality requirements in advance and to resist customizing them after the fact, when the software has failed. Quality gates are different at lower and\nhigher stages; the latter normally consist of a more severe or broader set of quality requirements, and they often include the requirements of the lower gates. The binary repository\nmanager must underpin corresponding quality gates, while managing the binaries, what we cover next.\n\nThis blog post illustrates typical concepts and sample toolchains. For more information, please consult the respective documentation, good books or attend top notch conferences, e.g.\nJenkins World, powered by CloudBees.\n\nBinary repository manager\n\nA central backbone of the staging ladder is the binary repository manager, e.g. JFrog Artifactory. The binary repository manager manages all binaries including the self-produced\nones (producing view) and the 3rd party ones (consuming view), across all artifact types, in our case a Java EE WAR file and a Docker image. Basic idea here is that the repo manager serves\nas a proxy, thus all developers access the repo manager, and not remote binary pools directly, e.g. Maven Central. The binary repository manager offers cross-cutting services,\ne.g. role-based access control on specific logical repositories, which may correspond to specific stages of the staging ladder.\n\nLogical repositories can be generic ones (meaning they are agnostic regarding any tools and platforms, thus you can also just upload the menu of your local canteen) or repos\nspecific to tools and platforms. In our case, we need a repository for managing the Java EE WAR files and for the Docker images. This can be achieved by\n\na generic repository (preferred for higher stages) or a repo which is aligned with the layout of the Maven build tool, and\n\na repository for managing Docker images, which serves as a Docker registry.\n\nIn our scenario, preparing the staging of artifacts includes the following ramp-up activities\n\nCreating two sets of logical repositories, inside JFrog Artifactory, where each set has a repo for the WAR file and a repo for the Docker image, and one set is for managing dev\nversions and one set is for release candidate versions.\n\nDefining and implementing processes to promote the binaries from the one set of repositories (which is for dev versions) to the other set of repositories (which is for RC versions).\nPart of the process is defining roles, and JFrog Artifactory helps you to implement role-based access control.\n\nSetting up procedures or scripts to bring binaries from one set of repositories to the other set of repositories, reproducibly. Adding meta data to binaries is important if the degree of maturity\nof the binary cannot be easily derived from the context.\n\nThe following illustration shows a JFrog Artifactory instance with the involved logical repos in place. In our simplified example, the repo promotions are supposed to go from\ndocker-local to docker-prod-local, and from libs-release-local to libs-releases-staging-local. In our use case, we promote the software in version 1.0.0.\n\nAnother type of binary repository manager is JFrog Bintray, which serves as a universal distribution platform for many technologies. JFrog Bintray can be an interesting choice\nif you have strong requirements for scalability and worldwide coverage including IP restrictions and handy features around statistics. Most of the concepts and ramp up activities\n are similar compared to JFrog Artifactory, thus I do not want to repeat them here. Bintray is used by lot of projects e.g. by Groovy, to host their deliverables in the public.\n But keep in mind that you can of course also host your release binaries in JFrog Artifactory.\n In this blog post, I’d like to introduce different options, thus we promote our release candidates to JFrog Artifactory and our releases to JFrog Bintray.\n Bintray has the concept of products, packages and versions. A product can have multiple packages and has different versions. In our example, the product has two packages, namely the Java EE WAR and\n the Docker image, and the concrete version that will be processed is 1.0.0.\n\nSome tool features covered in this blog post are available as part of commercial offerings of tool vendors. Examples include the Docker support of JFrog Artifactory or the Firehose Event API of JFrog Bintray.\nPlease consult the respective documentation for more information.\n\nNow it is time to have a deeper look at the pipelines.\n\nImplementing Pipelines\n\nOur example pipelines are implemented with Jenkins, including its Blue Ocean and declarative pipelines facilities, JFrog Artifactory and JFrog Bintray. To derive your personal\npipelines, please check your individual requirements and basic conditions to come up with the best solution for your target architecture, and consult the respective documentation for\n more information, e.g. about scripting the tools.\n\nIn case your development versions are built with Maven, and have SNAPSHOT character, you need to either rebuild the software after setting the release version, as part of\nyour pipeline, or you solely use Maven releases from the very beginning. Many projects make great experience with morphing Maven snapshot versions into\nrelease versions, as part of the pipeline, by using a dedicated Maven plugin, and externalizing it into a Jenkins shared library. This can look like the following:\n\nsl.groovy (excerpt): A Jenkins shared library, to include in Jenkins pipelines.\n\n#!/usr/bin/groovy\n    def call(args) { (1)\necho \"Calling shared library, with ${args}.\"\n       sh \"mvn com.huettermann:versionfetcher:1.0.0:release versions:set -DgenerateBackupPoms=false -f ${args}\" (2)\n}\n\n1\nWe provide a global variable/function to include it in our pipelines.\n\n2\nThe library calls a Maven plugin, which dynamically morphs the snapshot version of a Maven project to a release version.\n\nAnd including it into the pipeline is then also very straight forward:\n\npipeline.groovy (excerpt): A stage calling a Jenkins shared library.\n\nstage('Produce RC') { (1)\nreleaseVersion 'all/pom.xml' (2)\n}\n\n1\nThis stage is part of a scripted pipeline and is dedicated to morphing a Maven snapshot version into a release version, dynamically.\n\n2\nWe call the Jenkins shared library, with a parameter pointing to the Maven POM file, which can be a parent POM.\n\nYou can find the code of the underlying Maven plugin here.\n\nLet’s now discuss how to proceed for the release candidates.\n\nRelease Candidate (RC)\n\nThe pipeline to promote a dev version to a RC version does contain a couple of different stages, including stages to certify the binaries (meaning labeling it or adding context information) and stages to process the concrete promotion.\nThe following illustration shows the successful run of the promotion, for software version 1.0.0.\n\nWe utilize Jenkins Blue Ocean that is a new user experience for Jenkins based on a personalizable, modern design that allows users to graphically create, visualize and diagnose\ndelivery pipelines. Besides the new approach in general, single Blue Ocean features help to boost productivity dramatically, e.g. to provide log information at your fingertips\nand the ability to search pipelines. The stages to perform the promote are as follows starting with the  Jenkins pipeline stage for promoting the WAR file. Keep in mind that all\nscripts are parameterized, including variables for versions and Artifactory domain names, which are either injected to the pipeline run by user input or set system wide in the Jenkins admin panel,\nand the underlying call is using the JFrog command line interface, CLI in short. JFrog Artifactory\nas well as JFrog Bintray can be used and managed by scripts, based on a REST API. The JFrog CLI\nis an abstraction on top of the JFrog REST API, and we show sample usages of both.\n\npipeline.groovy (excerpt): Staging WAR file to different logical repository\n\nstage('Promote WAR') { (1)\nsteps { (2)\nsh 'jfrog rt cp --url=https://$ARTI3 --apikey=$artifactory_key --flat=true libs-release-local/com/huettermann/web/$version/ ' + (3)\n'libs-releases-staging-local/com/huettermann/web/$version/'\n       }\n    }\n\n1\nThe dedicated stage for running the promotion of the WAR file.\n\n2\nHere we have the steps which make up the stage, based on Jenkins declarative pipeline syntax.\n\n3\nCopying the WAR file, with JFrog CLI, using variables, e.g. the domain name of the Artifactory installation. Many options available, check the docs.\n\nThe second stage to explore more is the promotion of the Docker image. Here, I want to show you a different way how to achieve the goal, thus in this use case we utilize the JFrog REST API.\n\npipeline.grovvy (excerpt): Promote Docker image\n\nstage('Promote Docker Image') {\n          sh '''curl -H \"X-JFrog-Art-Api:$artifactory_key\" -X POST https://$ARTI3/api/docker/docker-local/v2/promote ''' + (1)\n'''-H \"Content-Type:application/json\" ''' + (2)\n'''-d \\'{\"targetRepo\" : \"docker-prod-local\", \"dockerRepository\" : \"michaelhuettermann/tomcat7\", \"tag\": \"\\'$version\\'\", \"copy\": true }\\' (3)\n'''\n    }\n\n1\nThe shell script to perform the staging of Docker image is based on JFrog REST API.\n\n2\nPart of parameters are sent in JSON format.\n\n3\nThe payload tells the REST API endpoint what to to, i.e. gives information about target repo and tag.\n\nOnce the binaries are promoted (and hopefully deployed and tested on respective environments before), we can promote them to become final releases, which I like to call GA.\n\nGeneral Availability (GA)\n\nIn our scenario, JFrog Bintray serves as the distribution platform to manage and provide binaries for further usage. Bintray can also serve as a Docker registry, or can just\nprovide binaries for scripted or manual download. There are again different ways how to promote binaries, in this case from the RC repos inside JFrog Artifactory to the GA storage in JFrog Bintray, and I summarize one of those possible ways. First, let’s look at the Jenkins pipeline, showed in the next illustration. The processing is on its way, currently, and we again have a list of linked stages.\n\nZooming in now to the key stages, we see that promoting the WAR file is a set of steps that utilize JFrog REST API. We download the binary from JFrog Artifactory, parameterized,\nand upload it to JFrog Bintray.\n\npipeline.groovy (excerpt): Promote WAR to Bintray\n\nstage('Promote WAR to Bintray') {\n       steps {\n          sh '''\n             curl -u michaelhuettermann:${bintray_key} -X DELETE https://api.bintray.com/packages/huettermann/meow/cat/versions/$version (1)\ncurl -u michaelhuettermann:${bintray_key} -H \"Content-Type: application/json\" -X POST https://api.bintray.com/packages/huettermann/meow/cat/$version --data \"\"\"{ \"name\": \"$version\", \"desc\": \"desc\" }\"\"\" (2)\ncurl -T \"$WORKSPACE/all-$version-GA.war\" -u michaelhuettermann:${bintray_key} -H \"X-Bintray-Package:cat\" -H \"X-Bintray-Version:$version\" https://api.bintray.com/content/huettermann/meow/ (3)\ncurl -u michaelhuettermann:${bintray_key} -H \"Content-Type: application/json\" -X POST https://api.bintray.com/content/huettermann/meow/cat/$version/publish --data '{ \"discard\": \"false\" }' (4)\n'''\n       }\n    }\n\n1\nFor testing and demo purposes, we remove the existing release version.\n\n2\nNext we create the version in Bintray, in our case the created version is 1.0.0. The value was insert by user while triggering the pipeline.\n\n3\nThe upload of the WAR file.\n\n4\nBintray needs a dedicated publish step to make the binary publicly available.\n\nProcessing the Docker image is as easy as processing the WAR. In this case, we just push the Docker image to the Docker registry, which is served by JFrog Bintray.\n\npipeline.groovy (excerpt): Promote Docker image to Bintray\n\nstage('Promote Docker Image to Bintray') { (1)\nsteps {\n          sh 'docker push $BINTRAYREGISTRY/michaelhuettermann/tomcat7:$version' (2)\n}\n    }\n\n1\nThe stage for promoting the Docker image. Please note, depending on your setup, you may add further stages, e.g. to login to your Docker registry.\n\n2\nThe Docker push of the specific version. Note, that also here all variables are parameterized.\n\nWe now have promoted the binaries and uploaded them to JFrog Bintray. The overview page of our product lists two packages: the WAR file and the Docker image. Both can be downloaded\nnow and used, the Docker image can be pulled from the JFrog Bintray Docker registry with native Docker commands.\n\nAs part of its graphical visualization capabilitites, Bintray is able to show the single layers of the uploaded Docker images.\n\nBintray can also display usage statistics, e.g. download details. Now guess where I’m sitting right now while downloading the binary?\n\nBesides providing own statistics, Bintray provides the JFrog Firehose Event API. This API streams live usage data, which in turn can be integrated or aggregated with your ecosystem.\nIn our case, we visualize the data, particularly download, upload, and delete statistics, with the ELK stack, as part of a functional monitoring initiative.\n\nCrisp, isn’t it?\n\nSummary\n\nThis closes are quick ride through the world of staging binaries, based on Jenkins. We’ve discussed concepts and example DevOps enabler tools, which can help to implement\n the concepts. Along the way, we discussed some more options how to integrate with ecosystem, e.g. releasing Maven snapshots and functional monitoring with dedicated tools.\n After this appetizer you may want to now consider to double-check your staging processes and toolchains, and maybe you find some room for further adjustments.\n\nReferences\n\n'Agile ALM', Manning, 2011\n\nBinary Repository Manager Feature Matrix\n\n'DevOps for Developers', Apress, 2012\n\nDocker\n\nELK\n\nJFrog Artifactory\n\nJFrog Bintray\n\nJFrog CLI\n\nJFrog REST API\n\nSonatype Nexus","title":"Delivery pipelines, with Jenkins 2: how to promote Java EE and Docker binaries toward production.","tags":["devops","jenkins","artifactory","bintray"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/c09ea/michaelhuettermann.jpg","srcSet":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/534e5/michaelhuettermann.jpg 32w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/99887/michaelhuettermann.jpg 64w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/c09ea/michaelhuettermann.jpg 128w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/76fd4/michaelhuettermann.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/59a6b/michaelhuettermann.webp 32w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/cbb78/michaelhuettermann.webp 64w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/96250/michaelhuettermann.webp 128w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/50511/michaelhuettermann.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":171}}},"blog":"http://huettermann.net","github":"michaelhuettermann","html":"<div class=\"paragraph\">\n<p>Michael is expert in Continuous Delivery, DevOps and SCM/ALM supporting enterprises in implementing DevOps.\nMichael is Jenkins Ambassador.</p>\n</div>","id":"michaelhuettermann","irc":null,"linkedin":null,"name":"Michael Hüttermann","slug":"/blog/author/michaelhuettermann","twitter":"huettermann"}]}},{"node":{"date":"2017-04-18T00:00:00.000Z","id":"714ef576-2af6-5d69-a880-a3280836f4e4","slug":"/blog/2017/04/18/continuousdelivery-devops-sonarqube/","strippedHtml":"This is a guest post by Michael Hüttermann. Michael is an expert\nin Continuous Delivery, DevOps and SCM/ALM. More information about him at huettermann.net, or\nfollow him on Twitter: @huettermann.\n\nContinuous Delivery and DevOps are well known and widely spread practices nowadays. It is commonly accepted that it\nis crucial to form great teams and define shared goals first and then choose and integrate the tools fitting best to\ngiven tasks. Often it is a mashup of lightweight tools, which are integrated to build up Continuous Delivery pipelines\nand underpin DevOps initiatives. In this blog post, we zoom in to an important part of the overall pipeline, that is the discipline\noften called Continuous Inspection, which comprises inspecting code and injecting a quality gate on that, and show how artifacts can\nbe uploaded after the quality gate was met. DevOps enabler tools covered are Jenkins, SonarQube, and Artifactory.\n\nThe Use Case\n\nYou already know that quality cannot be injected after the fact, rather it should be part of the process and product from the very beginning.\nAs a commonly used good practice, it is strongly recommended to inspect the code and make findings visible, as soon as possible.\nFor that SonarQube is a great choice. But SonarQube is not just running on any isolated\nisland, it is integrated in a Delivery Pipeline. As part of the pipeline, the code is inspected, and only if the code is fine according to defined\nrequirements, in other words: it meets the quality gates, the built artifacts are uploaded to the binary repository manager.\n\nLet’s consider the following scenario. One of the busy developers has to fix code, and checks in changes to the central\nversion control system. The day was long and the night short, and against all team commitments the developer\ndid not check the quality of the code in the local sandbox. Luckily, there is the build engine Jenkins\nwhich serves as a single point of truth, implementing the Delivery Pipeline with its native pipeline features, and as a handy coincidence\nSonarQube has support for Jenkins pipeline.\n\nThe change triggers a new run of the pipeline. Oh no! The build pipeline broke, and the change is not further processed.\nIn the following image you see that a defined quality gate was missed. The visualizing is done with Jenkins Blue Ocean.\n\nSonarQube inspection\n\nWhat is the underlying issue? We can open the SonarQube web application and drill down to the finding. In the Java code, obviously a string literal is not placed on the right side.\n\nDuring a team meeting it was decided to define this to be a Blocker, and SonarQube was configured accordingly. Furthermore, a SonarQube quality gate was created to break any build, if a blocker was identified. Let’s now quickly look into the code.\nYes, SonarQube is right, there is the issue with the following code snippet.\n\nWe do not want to discuss in detail all used tools, and also covering the complete Jenkins build job would be out of scope.\nBut the interesting extract here in regard of the inspection is the following stage defined in Jenkins pipeline DSL:\n\nconfig.xml: SonarQube inspection\n\nstage('SonarQube analysis') { (1)\nwithSonarQubeEnv('Sonar') { (2)\nsh 'mvn org.sonarsource.scanner.maven:sonar-maven-plugin:3.3.0.603:sonar ' + (3)\n'-f all/pom.xml ' +\n          '-Dsonar.projectKey=com.huettermann:all:master ' +\n          '-Dsonar.login=$SONAR_UN ' +\n          '-Dsonar.password=$SONAR_PW ' +\n          '-Dsonar.language=java ' +\n          '-Dsonar.sources=. ' +\n          '-Dsonar.tests=. ' +\n          '-Dsonar.test.inclusions=**/*Test*/** ' +\n          '-Dsonar.exclusions=**/*Test*/**'\n        }\n    }\n\n1\nThe dedicated stage for running the SonarQube analysis.\n\n2\nAllow to select the SonarQube server you want to interact with.\n\n3\nRunning and configuring the scanner, many options available, check the docs.\n\nMany options are available to integrate and configure SonarQube. Please consult the documentation for alternatives. Same applies to the other covered tools.\n\nSonarQube Quality Gate\n\nAs part of a Jenkins pipeline stage, SonarQube is configured to run and inspect the code. But this is just the first part,\nbecause we now also want to add the quality gate in order to break the build. The next stage is covering exactly that, see\nnext snippet. The pipeline is paused until the quality gate is computed, specifically the waitForQualityGate step will pause the\npipeline until SonarQube analysis is completed and returns the quality gate status. In case a quality gate was missed, the build breaks.\n\nconfig.xml: SonarQube Quality Gate\n\nstage(\"SonarQube Quality Gate\") { (1)\ntimeout(time: 1, unit: 'HOURS') { (2)\ndef qg = waitForQualityGate() (3)\nif (qg.status != 'OK') {\n             error \"Pipeline aborted due to quality gate failure: ${qg.status}\"\n           }\n        }\n    }\n\n1\nThe defined quality gate stage.\n\n2\nA timeout to define when to proceed without waiting for any results for ever.\n\n3\nHere we wait for the OK. Underlying implementation is done with SonarQube’s webhooks feature.\n\nThis blog post is an appetizer, and scripts are excerpts. For more information, please consult the respective documentation, or a good book, or the great community, or ask your local expert.\n\nSince they all work in a wonderful Agile team, the next available colleague just promptly fixes the issue. After checking in\nthe fixed code, the build pipeline runs again.\n\nThe pipeline was processed successfully, including the SonarQube quality gate, and as the final step, the packaged and tested artifact was\ndeployed to Artifactory. There are a couple of different flexible ways how to upload the artifacts,\nthe one we use here is using an upload spec to actually collect and upload the artifact which was built at the very beginning of the pipeline.\nAlso meta information are published to Artifactory, since it is the context which matters and thus we can add valuable labels to the artifact for further processing.\n\nconfig.xml: Upload to Artifactory\n\nstage ('Distribute binaries') { (1)\ndef SERVER_ID = '4711' (2)\ndef server = Artifactory.server SERVER_ID\n    def uploadSpec = (3)\"\"\"\n    {\n    \"files\": [\n        {\n            \"pattern\": \"all/target/all-(*).war\",\n            \"target\": \"libs-snapshots-local/com/huettermann/web/{1}/\"\n        }\n      ]\n    }\n    \"\"\"\n    def buildInfo = Artifactory.newBuildInfo() (4)\nbuildInfo.env.capture = true (5)\nbuildInfo=server.upload(uploadSpec) (6)\nserver.publishBuildInfo(buildInfo) (7)\n}\n\n1\nThe stage responsible for uploading the binary.\n\n2\nThe server can be defined Jenkins wide, or as part of the build step, as done here.\n\n3\nIn the upload spec, in JSON format, we define what to deploy to which target, in a fine-grained way.\n\n4\nThe build info contains meta information attached to the artifact.\n\n5\nWe want to capture environmental data.\n\n6\nUpload of artifact, according to upload spec.\n\n7\nBuild info are published as well.\n\nNow let’s see check that the binary was deployed to Artifactory, successfully. As part of the context information, also a reference to the\nproducing Jenkins build job is available for better traceability.\n\nSummary\n\nIn this blog post, we’ve discovered tips and tricks to integrate Jenkins with SonarQube, how to define\nJenkins stages with the Jenkins pipeline DSL, how those stages are visualized with Jenkins Blue Ocean, and how the artifact\nwas deployed to our binary repository manager Artifactory.\nNow I wish you a lot of further fun with your great tools of choice to implement your Continuous Delivery pipelines.\n\nReferences\n\nJenkins 2\n\nSonarqube\n\nSonarqube Jenkins plugin\n\nArtifactory\n\nJenkins Artifactory plugin\n\n'DevOps for Developers', Apress, 2012\n\n'Agile ALM', Manning, 2011","title":"Delivery Pipelines, with Jenkins 2, SonarQube, and Artifactory","tags":["quality","sonarqube","jenkins","artifactory"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/c09ea/michaelhuettermann.jpg","srcSet":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/534e5/michaelhuettermann.jpg 32w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/99887/michaelhuettermann.jpg 64w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/c09ea/michaelhuettermann.jpg 128w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/76fd4/michaelhuettermann.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/59a6b/michaelhuettermann.webp 32w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/cbb78/michaelhuettermann.webp 64w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/96250/michaelhuettermann.webp 128w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/50511/michaelhuettermann.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":171}}},"blog":"http://huettermann.net","github":"michaelhuettermann","html":"<div class=\"paragraph\">\n<p>Michael is expert in Continuous Delivery, DevOps and SCM/ALM supporting enterprises in implementing DevOps.\nMichael is Jenkins Ambassador.</p>\n</div>","id":"michaelhuettermann","irc":null,"linkedin":null,"name":"Michael Hüttermann","slug":"/blog/author/michaelhuettermann","twitter":"huettermann"}]}}]}},"pageContext":{"tag":"jenkins","limit":8,"skip":24,"numPages":5,"currentPage":4}},
    "staticQueryHashes": ["3649515864"]}