{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/puppet",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2017-05-15T00:00:00.000Z","id":"355e4b5a-0c27-5f01-94a5-7a981139c2fb","slug":"/blog/2017/05/15/kubernetes-journey-on-azure/","strippedHtml":"With the\nongoing migration to Azure,\nI would like to share my thoughts regarding one of the biggest challenges we\nhave faced thus far: orchestrating container infrastructure. Many of the\nJenkins project’s applications are run as Docker containers, making Kubernetes\na logical choice as far as running our containers, but it presents its own set\nof challenges. For example, what would the workflow from development to\nproduction look like?\n\nBefore going deeper into the challenges, let’s review the requirements we\nstarted with:\n\nGit\n\nWe found it mandatory to keep track of all the infrastructure changes in Git\nrepositories, including secrets, in order to facilitate reviewing,\nvalidation, rollback, etc of all infra changes.\n\nTests\n\nInfrastructure contributors are geographically distributed and in different\ntimezones.  Getting feedback can take time, so we heavily rely on a lot of\ntests before any changes can be merged.\n\nAutomation\n\nThe change submitter is not necessarily the person who will deploy it.\nRepetitive tasks are error prone and a waste of time.\nFor these reasons, all steps must be automated and stay as simple as possible.\n\nA high level overview of our \"infrastructure as code\" workflow would look like:\n\nInfrastructure as Code Workflow\n\n__________       _________       ______________\n  |         |      |        |      |             |\n  | Changes | ---->|  Test  |----->| Deployment  |\n  |_________|      |________|  ^   |_____________|\n                               |\n                        ______________\n                       |             |\n                       | Validation  |\n                       |_____________|\n\nWe identified two possible approaches for implementing our container\norchestration with Kubernetes:\n\nThe Jenkins Way: Jenkins is triggered by a Git commit, runs the tests, and\nafter validation, Jenkins deploys changes into production.\n\nThe Puppet Way: Jenkins is triggered by a Git commit, runs the tests, and\nafter validation, it triggers Puppet to deploy into production.\n\nLet’s discuss these two approaches in detail.\n\nThe Jenkins Way\n\nWorkflow\n\n_________________       ____________________       ______________\n  |                |      |                   |      |             |\n  |    Github:     |      |     Jenkins:      |      |   Jenkins:  |\n  | Commit trigger | ---->| Test & Validation | ---->|  Deployment |\n  |________________|      |___________________|      |_____________|\n\nIn this approach, Jenkins is used to test, validate, and deploy our Kubernetes\nconfiguration files. kubectl can be run on a directory and is idempotent.\nThis means that we can run it as often as we want: the result will not change.\nTheoretically, this is the simplest way. The only thing needed is to run\nkubectl command each time Jenkins detects changes.\n\nThe following Jenkinsfile gives an example of this workflow.\n\nJenkinsfile\n\npipeline {\n    agent any\n    stages {\n      stage('Init'){\n        steps {\n          sh 'curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl'\n        }\n      }\n      stage('Test'){\n        steps {\n          sh 'Run tests'\n        }\n      }\n      stage('Deploy'){\n        steps {\n          sh './kubectl apply -R true -f my_project'\n        }\n      }\n    }\n  }\n\nThe devil is in the details of course, and it was not as easy as it looked at\nfirst sight.\n\nOrder matters\n\nSome resources needed to be deployed before others. A workaround was to use\nnumbers as file names. But this added extra logic at file name level, for\nexample:\n\nproject/00-nginx-ingress\nproject/09-www.jenkins.io\n\nPortability\n\nThe deployment environments needed to be the same across development machines\nand the Jenkins host. Although this a well-known problem, it was not easy to\nsolve.  The more the project grew, the more our scripts needed additional tools\n( make, bats, jq gpg, etc).  The more tools we used, the more issues\nappeared because of the different versions used.\n\nAnother challenge that emerged when dealing with different environments was:\nhow should we manage environment-specific configurations (dev, prod, etc)?\nWould it be better to define different configuration files per environment?\nPerhaps, but this means code duplication, or using file templates which would require\nmore tools ( sed, jinja2, erb), and more work.\n\nThere wasn’t a golden rule we discovered, and the answer is probably somewhere in between.\n\nIn any case, the good news is that a Jenkinsfile provides an easy way to\nexecute tasks from a Docker image, and an image can contain all the necessary\ntools in our environment. We can even use different Docker images for each\nstage along the way.\n\nIn the following example, I use the my_env Docker image. It contains all the\ntools needed to test, validate, and deploy changes.\n\nJenkinsfile\n\npipeline{\n  agent {\n    docker{\n      image 'my_env:1.0'\n    }\n  }\n  options{\n    buildDiscarder(logRotator(numToKeepStr: '10'))\n    disableConcurrentBuilds()\n    timeout(time: 1, unit: 'HOURS')\n  }\n  triggers{\n    pollSCM('* * * * *')\n  }\n  stages{\n    stage('Init'){\n      steps{\n        // Init everything required to deploy our infra\n        sh 'make init'\n      }\n    }\n    stage('Test'){\n      steps{\n       // Run tests to validate changes\n       sh 'make test'\n      }\n    }\n    stage('Deploy'){\n      steps{\n       // Deploy changes in production\n       sh 'make deploy'\n      }\n    }\n  }\n  post{\n    always {\n      sh 'make notify'\n    }\n  }\n}\n\nSecret credentials\n\nManaging secrets is a big subject and brings with it many different\nrequirements which are very hard to fulfill.  For obvious reasons, we couldn’t\npublish the credentials used within the infra project.  On the other hand, we\nneeded to keep track and share them, particularly for the Jenkins node that\ndeploys our cluster.  This means that we needed a way to encrypt or decrypt\nthose credentials depending on permissions, environments, etc.  We analyzed two\ndifferent approaches to handle this:\n\nStoring secrets in a key management tool like Key Vault or Vault and use them like a Kubernetes \"secret\" type of resource.\n→ Unfortunately, these tools are not yet integrated in Kubernetes. But we may come back to this option later.\nKubernetes issue: 10439\n\nPublishing and encrypting using a public GPG key.\nThis means that everybody can encrypt credentials for the infrastructure project but only the owner of the private key can decrypt credentials.\nThis solution implies:\n\nScripting: as secrets need to be decrypted at deployment time.\n\nTemplates: as secret values will change depending on the environment.\n→ Each Jenkins node should only have the private key to decrypt secrets associated to its environment.\n\nScripting\n\nFinally, the system we had built was hard to work with.  Our initial\nJenkinsfile which only ran one kubectl command slowly become a bunch of\nscripts to accommodate for:\n\nResources needing to be updated only in some situations.\n\nSecrets needing to be encrypted/decrypted.\n\nTests needing to be run.\n\nIn the end, the amount of scripts required to deploy the Kubernetes resources\nstarted to become unwieldy and we began asking ourselves: \"aren’t we\nre-inventing the wheel?\"\n\nThe Puppet Way\n\nThe Jenkins project already uses Puppet, so we decided to look at using Puppet\nto orchestrate our container deployment with Kubernetes.\n\nWorkflow\n\n_________________       ____________________       _____________\n  |                |      |                   |      |            |\n  |    Github:     |      |     Jenkins:      |      | Puppet:    |\n  | Commit trigger | ---->| Test & Validation | ---->| Deployment |\n  |________________|      |___________________|      |____________|\n\nIn this workflow, Puppet is used to template and deploy all Kubernetes\nconfigurations files needed to orchestrate our cluster.\nPuppet is also used to automate basic kubectl operations such as 'apply' or\n'remove' for various resources based on file changes.\n\nPuppet workflow\n\n______________________\n|                     |\n|  Puppet Code:       |\n|    .                |\n|    ├── apply.pp     |\n|    ├── kubectl.pp   |\n|    ├── params.pp    |\n|    └── resources    |\n|        ├── lego.pp  |\n|        └── nginx.pp |\n|_____________________|\n          |                                        _________________________________\n          |                                       |                                |\n          |                                       |  Host: Prod orchestrator       |\n          |                                       |    /home/k8s/                  |\n          |                                       |    .                           |\n          |                                       |    └── resources               |\n          | Puppet generate workspace             |        ├── lego                |\n          └-------------------------------------->|        │   ├── configmap.yaml  |\n            Puppet apply workspaces' resources on |        │   ├── deployment.yaml |\n          ----------------------------------------|        │   └── namespace.yaml  |\n          |                                       |        └── nginx               |\n          v                                       |            ├── deployment.yaml |\n ______________                                   |            ├── namespace.yaml  |\n |     Azure:  |                                  |            └── service.yaml    |\n | K8s Cluster |                                  |________________________________|\n |_____________|\n\nThe main benefit of this approach is letting Puppet manage the environment and run\ncommon tasks. In the following example, we define a Puppet class for Datadog.\n\nPuppet class for resource Datadog\n\n# Deploy datadog resources on kubernetes cluster\n#   Class: profile::kubernetes::resources::datadog\n#\n#   This class deploy a datadog agent on each kubernetes node\n#\n#   Parameters:\n#     $apiKey:\n#       Contain datadog api key.\n#       Used in secret template\nclass profile::kubernetes::resources::datadog (\n    $apiKey = base64('encode', $::datadog_agent::api_key, 'strict')\n  ){\n  include ::stdlib\n  include profile::kubernetes::params\n  require profile::kubernetes::kubectl\n\n  file { \"${profile::kubernetes::params::resources}/datadog\":\n    ensure => 'directory',\n    owner  => $profile::kubernetes::params::user,\n  }\n\n  profile::kubernetes::apply { 'datadog/secret.yaml':\n    parameters => {\n        'apiKey' => $apiKey\n    },\n  }\n  profile::kubernetes::apply { 'datadog/daemonset.yaml':}\n  profile::kubernetes::apply { 'datadog/deployment.yaml':}\n\n  # As secrets change do not trigger pods update,\n  # we must reload pods 'manually' in order to use updated secrets.\n  # If we delete a pod defined by a daemonset,\n  # this daemonset will recreate pods automatically.\n  exec { 'Reload datadog pods':\n    path        => [\"${profile::kubernetes::params::bin}/\"],\n    command     => 'kubectl delete pods -l app=datadog',\n    refreshonly => true,\n    environment => [\"KUBECONFIG=${profile::kubernetes::params::home}/.kube/config\"] ,\n    logoutput   => true,\n    subscribe   => [\n      Exec['apply datadog/secret.yaml'],\n      Exec['apply datadog/daemonset.yaml'],\n    ],\n  }\n}\n\n→\nMore \"resources\" examples\n\nLet’s compare the Puppet way with the challenges discovered with the Jenkins\nway.\n\nOrder Matters\n\nWith Puppet, it becomes easier to define priorities as\nPuppet provides relationship meta parameters and the function 'require' (see\nalso:\nPuppet\nrelationships).\n\nIn our Datadog example, we can be sure that deployment will respect the following order:\n\ndatadog/secret.yaml -> datadog/daemonset.yaml -> datadog/deployment.yaml\n\nCurrently, our Puppet code only applies configuration when it detects file\nchanges.  It would be better to compare local files with the cluster\nconfiguration in order to trigger the required updates, but we haven’t found a\ngood way to implement this yet.\n\nPortability\n\nAs Puppet is used to configure working environments, it becomes easier to be\nsure that all tools are present and correctly configured.  It’s also easier to\nreplicate environments and run tests on them with tools like\nRSpec-puppet, Serverspec or\nVagrant.\n\nIn our Datadog example, we can also easily change the Datadog API key depending\non the environment with Hiera.\n\nSecret credentials\n\nAs we were already using Hiera GPG\nwith Puppet, we decided to continue to use it, making managing secrets for\ncontainers very simple.\n\nScripting\n\nOf course the Puppet DSL is used, and even if it seems harder at the beginning,\nPuppet simplifies a lot the management of Kubernetes configuration files.\n\nConclusion\n\nIt was much easier to bootstrap the project with a full CI workflow within\nJenkins as long as the Kubernetes project itself stays basic. But as soon as\nthe project grew, and we started deploying different applications with\ndifferent configurations per environment, it became easier to delegate\nKubernetes management to Puppet.\n\nIf you have any comments feel free to send a message to\nJenkins Infra mailing list.\n\nThanks\n\nThanks to Lindsay Vanheyste, Jean Marc Meessen, and Damien Duportal for their feedback.","title":"A journey to Kubernetes on Azure","tags":["puppet","kubernetes","docker","azure"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/bf8e1/olblak.png","srcSet":"/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/914ee/olblak.png 32w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/1c9ce/olblak.png 64w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/bf8e1/olblak.png 128w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/acb7c/olblak.png 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/ef6ff/olblak.webp 32w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/8257c/olblak.webp 64w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/6766a/olblak.webp 128w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/22bfc/olblak.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"olblak","html":"<div class=\"paragraph\">\n<p>Olivier is the Jenkins infrastructure officer and Senior Operations Engineer at CloudBees.\nAs a regular contributor to the Jenkins infrastructure projects, he works on a wide range of tasks from services reliability to applications maintenance.</p>\n</div>","id":"olblak","irc":"olblak","linkedin":null,"name":"Olivier Vernin","slug":"/blog/author/olblak","twitter":"0lblak"}]}},{"node":{"date":"2016-12-20T00:00:00.000Z","id":"07094990-1362-5018-bed6-12c7ddfa53ee","slug":"/blog/2016/12/20/jenkins-puppet-enterprise-plugin/","strippedHtml":"This is a guest post by Carl Caum,\nwho works at Puppet and created the\nPuppet Enterprise Pipeline plugin.\n\nDuring PuppetConf 2016, myself and Brian Dawson from CloudBees announced the\nplugin:puppet-enterprise-pipeline[Puppet Enterprise\nplugin for Jenkins Pipeline].\nLet’s take a look at how the plugin makes it trivial to use Puppet to perform\nsome or all of the deployment tasks in continuous delivery pipelines.\n\nJenkins Pipeline introduced an amazing world where the definition for a\npipeline is managed from the same version control repository as the code\ndelivered by the pipeline. This is a powerful idea, and one I felt complemented\nPuppet’s automation strengths. I wanted to make it trivial to control Puppet\nEnterprise’s orchestration and infrastructure code management capabilities, as\nwell as set hierarchical configuration data and use Puppet’s inventory data\nsystem as a source of truth – all from a Pipeline script. The result was the\nPuppet Enterprise plugin, which fully buys into the Pipeline ideals by\nproviding methods to control the different capabilities in Puppet Enterprise.\nThe methods provide ways to query\nPuppetDB, set\nHiera key/value pairs, deploy\nPuppet code environments with\nCode Management, and kick off orchestrated Puppet runs with the\nOrchestrator.\n\nThe Puppet Enterprise for Jenkins Pipeline plugin\n\nThe Puppet Enterprise for Jenkins Pipeline plugin itself has zero system\ndependencies. You need only to install the plugin from the update center. The\nplugin uses APIs available in Puppet Enterprise to do its work. Since the\nPuppetDB query, Code Management, and Orchestrator APIs are all\nbacked by Puppet Enterprise’s role-based access control (RBAC) system, it’s\neasy to restrict what pipelines are allowed to control in Puppet Enterprise. To\nlearn more about RBAC in Puppet Enterprise,\nread the docs here.\n\nConfiguring\n\nConfiguring the plugin is fairly straight forward. It takes three simple steps:\n\nSet the address of the Puppet server\n\nCreate a Jenkins credential with a Pupppet Enterprise RBAC authentication token\n\nConfigure the Hiera backend\n\nSet the Puppet Enterprise Server Address\n\nGo to Jenkins > Manage Jenkins > Puppet Enterprise page. Put the DNS address of\nthe Puppet server in the Puppet Master Address text field. Click the Test\nConnection button to verify the server is reachable, the Puppet CA certificate\nis retrievable, and HTTPS connections are successful. Once the test succeeds,\nClick Save.\n\nCreate a Jenkins Credentials Entry\n\nThe plugin uses the Jenkins built-in credentials system (the plain-credentials\nplugin) to store and refer RBAC tokens to Puppet Enterprise for authentication\nand authorization. First, generate an RBAC token in Puppet Enterprise by\nfollowing\nthe\ninstructions on the docs site. Next, create a new Jenkins Credentials item\nwith Kind Secret text and the Secret value the Puppet Enterprise RBAC\ntoken. It’s highly recommended to give the credential an ID value that’s\ndescriptive and identifiable. You’ll use it in your Pipeline scripts.\n\nIn your Jenkinsfile, use the puppet.credentials method to set all future Puppet\nmethods to use the RBAC token. For example:\n\npuppet.credentials 'pe-team-token'\n\nConfigure the Hiera Backend\n\nThe plugin exposes an HTTP API for performing Hiera data lookups for key/value\npairs managed by Pipeline jobs. To configure Hiera on the Puppet compile\nmaster(s) to query the Jenkins Hiera data store backend, use the\nhiera-http backend. On the\nPuppet Enterprise compile master(s), run the following commands:\n\n/opt/puppetlabs/puppet/bin/gem install hiera-http\n/opt/puppetlabs/bin/puppetserver gem install hiera-http\n\nNow you can configure the /etc/puppetlabs/puppet/hiera.yaml file. The following\nconfiguration instructs Hiera to first look to the Hiera yaml files in the\nPuppet code’s environment, then fall back to the http backend. The http backend\nwill first query the Hiera data store API looking for the key in the scope with\nthe same name as the node. If nothing’s found, look for the key in the node’s\nenvironments. You can use any Facter fact to match scope names.\n\n:backends:\n  - yaml\n  - http\n\n:http:\n  :host: jenkins.example.com\n  :port: 8080\n  :output: json\n  :use_auth: true\n  :auth_user:\n:auth_pass:\n:cache_timeout: 10\n  :failure: graceful\n  :paths:\n    - /hiera/lookup?path=%{clientcert}&key=%{key}\n    - /hiera/lookup?path=%{environment}&key=%{key}\n\nFinally, restart the pe-puppetserver process to pick up the new configs:\n\n/opt/puppetlabs/bin/puppet resource service pe-puppetserver ensure=stopped\n/opt/puppetlabs/bin/puppet resource service pe-puppetserver ensure=running\n\nHiera HTTP Authentication\n\nIf Jenkins' Global Security is configured to allow unauthenticated read-only\naccess, the 'use_auth', 'auth_pass', and 'auth_user' parameters are\nunnecessary. Otherwise, create a local Jenkins user that has permissions to\nview the Hiera Data Lookup page and use that user’s credentials for the\nhiera.yaml configuration.\n\nQuerying the infrastructure\n\nPuppetDB is an extensive data store that holds every bit of information Puppet\ngenerates and collects across every system Puppet is installed on. PuppetDB\nprovides a sweet query language called\nPQL. With PQL,\nyou can ask complex questions of your infrastructure such as \"How many\nproduction Red Hat systems are there with the openssl package installed?\" or\n\"What us-west-2c nodes with the MyApp role that were created in the last 24\nhours?\"\n\nThis can be a powerful tool for parts of your pipeline where you need to\nperform specific operations on subsets of the infrastructure like draining a\nloadbalancer.\n\nHere’s an example using the puppet.query method:\n\nresults = puppet.query '''\n  inventory[certname] {\n    facts.os.name = \"RedHat\" and\n    facts.ec2_metadata.placement.availability-zone = \"us-west-2c\" and\n    facts.uptime_hours < 24\n  }'''\n\nThe query returns an array of matching items. The results can be\niterated on, and even passed to a series of puppet.job calls. For example, the\nfollowing code will query all nodes in production that experienced a failure on\nthe last Puppet run.\n\nresults = puppet.query 'nodes { latest_report_status = \"failed\" and catalog_environment = \"production\"}'\n\nNote that once you can use closures in Pipeline scripts, doing the above\nexample will be much simpler.\n\nCreating an orchestrator job\n\nThe orchestration service in Puppet Enterprise is a tool to perform\norchestrated Puppet runs across as broad or as targeted an infrastructure as\nyou need at different parts of a pipeline. You can use the orchestrator to\nupdate applications in an environment, or update a specific list of nodes, or\nupdate nodes across a set of nodes that match certain criteria. In each\nscenario, Puppet will always push distributed changes in the correct order by\nrespecting the cross-node dependencies.\n\nTo create a job in the Puppet orchestrator from a Jenkins pipeline, use the\npuppet.job method. The puppet.job method will create a new orchestrator job,\nmonitor the job for completion, and determine if any Puppet runs failed. If\nthere were failures, the pipeline will fail.\n\nThe following are just some examples of how to run Puppet orchestration jobs against the infrastructure you need to target.\n\nTarget an entire environment:\n\npuppet.job 'production'\n\nTarget instances of an application in production:\n\npuppet.job 'production', application: 'Myapp'\n\nTarget a specific list of nodes:\n\npuppet.job 'production', nodes: ['db.example.com','appserver01.example.com','appserver02.example.com']\n\nTarget nodes matching a complex set if criteria:\n\npuppet.job 'production', query: 'inventory[certname] { facts.os.name = \"RedHat\" and facts.ec2_metadata.placement.availability-zone = \"us-west-2c\" and uptime_hours < 24 }'\n\nAs you can see, the puppet.job command means you can be as broad or as targeted\nas you need to be for different parts of your pipeline. There are many other\noptions you can add to the puppet.job method call, such as setting the Puppet\nruns to noop, or giving the orchestrator a maximum concurrency limit.\nLearn\nmore about the orchestrator here.\n\nUpdating Puppet code\n\nIf you’re using Code Management in Puppet Enterprise (and you should), you can\nensure that all the modules, site manifests, Hiera data, and roles and profiles\nare staged, synced, and ready across all your Puppet masters, direct from your\nJenkins pipeline.\n\nTo update Puppet code across all Puppet masters, use the puppet.codeDeploy method:\n\npuppet.codeDeploy 'staging'\n\nLearn more Code Management in Puppet Enterprise here.\n\nSetting Hiera values\n\nThe plugin includes an experimental feature to set Hiera key/value pairs. There\nare many cases where you need to promote information through a pipeline, such\nas a build version or artifact location. Doing so is very difficult in Puppet,\nsince data promotion almost always involves changing Hiera files and committing\nto version control.\n\nThe plugin exposes an HTTP API endpoint that Hiera can query using the\nhiera-http backend. With the backend configured on the Puppet master(s),\nkey/value pairs can be set to scopes. A scope is arbitrary and can be anything\nyou like, such as a Puppet environment, a node’s certname, or the name of a\nFacter fact like operatingsystem or domain.\n\nTo set a Hiera value from a pipeline, use the puppet.hiera method.\n\npuppet.hiera scope: 'staging', key: 'build-version', value: env.BUILD_ID\n\nNow you can set the same key with the same value to the production scope later\nin the pipeline, followed by a call to puppet.job to push the change out.\n\nExamples\n\nThe\nplugin’s\nGithub repository contains a set of example Pipeline scripts. Feel free to\nissue pull requests to add your own scripts!\n\nWhat’s next\n\nI’m pretty excited to see how this is going to help simplify continuous\ndelivery pipelines. I encourage everyone to get started with continuous\ndelivery today, even if it’s just a simple pipeline. As your practices evolve,\nyou can begin to add automated tests, automate away manual checkpoints, start\nto incorporate InfoSec tests, and include phases for practices like patch\nmanagement that require lots of manual approvals, verifications and rollouts.\nYou’ll be glad you did.","title":"Continuous Delivery with Jenkins and Puppet Enterprise","tags":["continuousdelivery","puppet","pipeline","puppetenterprise"],"authors":[{"avatar":null,"blog":null,"github":"ccaum","html":"","id":"ccaum","irc":null,"linkedin":null,"name":"Carl Caum","slug":"/blog/author/ccaum","twitter":"ccaum"}]}}]}},"pageContext":{"tag":"puppet","limit":8,"skip":0,"numPages":1,"currentPage":1}},
    "staticQueryHashes": ["3649515864"]}