{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/cloud-native/page/2",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2018-10-16T00:00:00.000Z","id":"2abe9d0a-5a91-57f9-b708-13a0a890774b","slug":"/blog/2018/10/16/custom-war-packager/","strippedHtml":"I would like to introduce Custom WAR Packager -\na new tool for Jenkins administrators and developers.\nThis tool allows packaging custom Jenkins distributions as WAR files,\nDocker images\nand Jenkinsfile Runner bundles.\nThis tool allows packaging Jenkins, plugins, and configurations in a ready-to-fly distribution.\nCustom WAR packager is a part of the Ephemeral Jenkins controller toolchain\nwhich we presented in our A Cloud Native Jenkins blogpost.\nThis toolchain is already used in Jenkins X to package serverless images.\n\nIn this blogpost I will show some common use-cases for Custom WAR Packager.\n\nHistory\n\nAs with Jenkins itself, Custom WAR Packager started as a small development tool.\nFor a long time it was a problem to run integration testing in Jenkins.\nWe have 3 main frameworks for it:\nJenkins Test Harness,\nAcceptance Test Harness,\n    and Plugin Compatibility Tester.\nAll these frameworks require a Jenkins WAR file to be passed to them to run tests.\nWhat if you want to run Jenkins tests in a custom environment like AWS?\nOr what if you want to reuse existing Jenkins Pipeline tests and to run them against\nPluggable Storage to ensure there are no regressions?\n\nAnd it was not just an idle question.\nThere were major activities happening in the Jenkins project: Cloud-Native Jenkins, Jenkins Evergreen, and Jenkins X.\nAll these activities required a lot of integration testing  to enable Continuous Delivery flows.\nIn order to do this in existing test frameworks, we needed to package a self-configuring WAR file so that it would be possible to run integration tests in existing frameworks.\nThat is why Custom WAR Packager was created in April 2018.\nLater it got support for packaging Docker images,\nand in September 2018 it also got support for Jenkinsfile Runner\nwhich was created by Kohsuke Kawaguchi\nand then improved by Nicolas de Loof.\n\nWhat’s inside?\n\nCustom WAR packager is a tool which is available as CLI Executable, Maven Plugin, or Docker package.\nThis tool takes input definitions and packages them as requested by the user.\nEverything is managed by a YAML configuration file:\n\nThe tool supports various types of inputs.\nThe list of plugins can be passed via YAML itself, pom.xml, or a BOM file from jep:309[].\nCustom WAR Packager supports not only released versions,\nbut also builds deployed to the Incremental repository (CD flow for Jenkins core and plugins - jep:305[]) and\neven direct builds by Git or directory path specifications.\nIt allows building packages from any source, without waiting for official releases.\nThe builds are also pretty fast, because the plugin does caching in the local Maven repository by using commit IDs.\n\nCustom WAR packager also supports the following self-configuration options:\n\nYAML files for Jenkins Configuration as Code\n\nGroovy Hooks (e.g. init hooks for pre-configuration)\n\nSystem properties\n\nWAR Packaging\n\nWAR packaging happens by default every time the repo is built.\nGenerally Custom WAR Packager repackages all inputs into a single WAR file by following conventions defined in the Jenkins core and the JCasC plugin.\n\nSample configuration:\n\nbundle:\n  groupId: \"io.jenkins.tools.war-packager.demo\"\n  artifactId: \"blogpost-demo\"\n  vendor: \"Jenkins project\"\n  description: \"Just a demo for the blogpost\"\nwar:\n  groupId: \"org.jenkins-ci.main\"\n  artifactId: \"jenkins-war\"\n  source:\n    version: 2.138.2\nplugins:\n  - groupId: \"io.jenkins\"\n    artifactId: \"configuration-as-code\"\n    source:\n      # Common release\n      version: 1.0-rc2\n  - groupId: \"io.jenkins\"\n    artifactId: \"artifact-manager-s3\"\n    source:\n      # Incrementals\n      version: 1.2-rc259.c9d60bf2f88c\n  - groupId: \"org.jenkins-ci.plugins.workflow\"\n    artifactId: \"workflow-job\"\n    source:\n      # Git\n      git: https://github.com/jglick/workflow-job-plugin.git\n      commit: 18d78f305a4526af9cdf3a7b68eb9caf97c7cfbc\n  # etc.\nsystemProperties:\n    jenkins.model.Jenkins.slaveAgentPort: \"9000\"\n    jenkins.model.Jenkins.slaveAgentPortEnforce: \"true\"\ngroovyHooks:\n  - type: \"init\"\n    id: \"initScripts\"\n    source:\n      dir: src/main/groovy\ncasc:\n  - id: \"jcasc\"\n    source:\n      dir: casc.yml\n\nDocker packaging\n\nIn order to do the Docker packaging, Custom WAR Packager uses the official\njenkins/jenkins\nDocker images or other images using the same format.\nDuring the build the WAR file just gets replaced by the one built by the tool.\nIt means that ALL image features are available for such custom builds: plugins.txt, Java options, Groovy hooks, etc., etc.\n\n## ...\n## WAR configuration from above\n## ...\n\nbuildSettings:\n  docker:\n    build: true\n    # Base image\n    base: \"jenkins/jenkins:2.138.2\"\n    # Tag to set for the produced image\n    tag: \"jenkins/custom-war-packager-casc-demo\"\n\nFor example, this demo\nshows packaging of a Docker image with External Build Logging to Elasticsearch.\nAlthough the implementations have been improved as a part of jep:207[] and jep:210[],\nyou can check out this demo to see how the Docker image does self-configuration, connects to a Elasicsearch, and then starts externally storing logs without changes in build log UIs.\nA Docker Compose file for running the entire cluster is included.\n\nJenkinsfile Runner packaging\n\nThis is probably the most tricky mode of Jenkinsfile Runner.\nIn March a new Jenkinsfile Runner project\nwas announced in the developer mailing list.\nThe main idea is to support running Jenkins Pipeline in a single-shot controller mode when the instance just executes a single run and prints outputs to the console.\nJenkinsfile Runner runs as CLI or as a Docker image.\nCustom WAR Packager is able to produce both, though only Docker run mode is recommended.\nWith Jenkinsfile Runner you can run Pipelines simply as…​\n\ndocker run --rm -v $PWD/Jenkinsfile:/workspace/Jenkinsfile acmeorg/jenkinsfile-runner\n\nWhen we started working on Ephemeral (aka \"single-shot\") controllers in the Cloud Native SIG,\nthere was an idea to use Custom WAR Packager and other existing tools (Jenkinsfile Runner, Jenkins Configuration as Code, etc.) to implement it.\nIt would be possible to just replace Jenkins core JAR and add plugins to Jenkinsfile Runner, but it is not enough.\nTo be efficient, Jenkinsfile Runner images should start up FAST, really fast.\nIn the build flow implementation we used some experimental options available in Jenkins and Jenkinsfile Runner, including classloader precaching, plugin unarchiving, etc, etc.\nWith such patches Jenkins starts up in few seconds with configuration-as-code and dozens of bundled plugins.\n\nSo, how to build custom Jenkinsfile Runner images?\nAlthough there is no release so far, it is not something which can stop us as you see above.\n\n##...\n## WAR Configuration from above\n##...\n\nbuildSettings:\n  jenkinsfileRunner:\n    source:\n      groupId: \"io.jenkins\"\n      artifactId: \"jenkinsfile-runner\"\n      build:\n        noCache: true\n      source:\n        git: https://github.com/jenkinsci/jenkinsfile-runner.git\n        commit: 8ff9b1e9a097e629c5fbffca9a3d69750097ecc4\n    docker:\n      base: \"jenkins/jenkins:2.138.2\"\n      tag: \"onenashev/cwp-jenkinsfile-runner-demo\"\n      build: true\n\nYou can find a Demo of Jenkinsfile Runner packaging with Custom WAR Packager\nhere.\n\nMore info\n\nThere are many other features which are not described in this blogpost.\nFor example, it is possible to alter Maven build settings or to add/replace libraries within the Jenkins core (e.g. Remoting).\nPlease see the Custom WAR Packager documentation for more information.\nThere are a number of demos available in the repository.\n\nIf you are interested to contribute to the repository,\nplease create pull requests and CC @oleg-nenashev\nand Raul Arabaolaza who is the second maintainer now working on Jenkins test automation flows.\n\nWhat’s next?\n\nThere are still many improvements that could be made to the tool to make it more efficient:\n\nAdd upper bounds checks for transitive plugin dependencies so that the conflicts are discovered during the build\n\nAllow passing all kinds of system properties and Java options via configuration YAML\n\nImprove Jenkinsfile Runner to improve performance\n\nIntegrate the tool into Jenkins Integration test flows\n(see essentialsTest()\nin the Jenkins Pipeline library)\n\nMany other tasks could be implemented in Custom WAR Packager,\nbut even now it is available to all Jenkins users so that they can build their own Jenkins bundles with it.\n\nWant to know more?\n\nIf you are going to DevOps World - Jenkins World in Nice on Oct 22-25,\nI will be presenting Custom WAR Packager at the Community Booth during the lunch demo sessions.\nWe will be also repeating our A Cloud Native Jenkins talk together with Carlos Sanchez where we will show how Ephemeral Jenkins works with Pluggable Storage.\nJenkins X team is also going to present their project using Custom WAR Packager.\n\nCome meet Oleg and other Cloud Native SIG members at\nDevOps World - Jenkins World on October 22-25 in Nice.\nregister with the code JWFOSS for a 30% discount off your pass.","title":"Build your own Jenkins! Introducing Custom WAR/Docker Packager","tags":["tools","docker","jenkins-x","cloud-native"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8c8e8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/bf8e1/oleg_nenashev.png","srcSet":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/914ee/oleg_nenashev.png 32w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/1c9ce/oleg_nenashev.png 64w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/bf8e1/oleg_nenashev.png 128w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/acb7c/oleg_nenashev.png 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/ef6ff/oleg_nenashev.webp 32w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/8257c/oleg_nenashev.webp 64w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/6766a/oleg_nenashev.webp 128w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/22bfc/oleg_nenashev.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"https://oleg-nenashev.github.io/","github":"oleg-nenashev","html":"<div class=\"paragraph\">\n<p>Jenkins core maintainer and board member.\nOleg started using Hudson for Hardware/Embedded projects in 2008 and became an active Jenkins contributor in 2012.\nNowadays he leads several Jenkins <a href=\"/sigs\">SIGs</a>, outreach programs (<a href=\"/projects/gsoc\">Google Summer of Code</a>, <a href=\"/events/hacktoberfest\">Hacktoberfest</a>) and <a href=\"/projects/jam/\">Jenkins meetups</a> in Switzerland and Russia.\nOleg works for <a href=\"https://www.cloudbees.com/\">CloudBees</a> and focuses on key projects in the community.</p>\n</div>","id":"oleg_nenashev","irc":"oleg_nenashev","linkedin":"onenashev","name":"Oleg Nenashev","slug":"/blog/author/oleg_nenashev","twitter":"oleg_nenashev"}]}},{"node":{"date":"2018-09-14T00:00:00.000Z","id":"3853be51-02ed-55b0-8b3f-edbfcfe998c7","slug":"/blog/2018/09/14/kubernetes-and-secret-agents/","strippedHtml":"At long last, the way we build and deploy software is finally changing and significantly so.\nThe days of the persnickety, prima donna build machine where monolithic applications were built, tested, and deployed are numbered.\nAnd that is a \"Good Thing (tm)\" - a consequence of how we will meet the transformation goals of our businesses.\nModern applications consist of distributed services, often with multiple microservices that are developed and deployed independent of other services.\nHowever, the only way to build these services with their own dependencies and schedules is to bake in continuous integration and delivery from the beginning.\nAnd as usual, your Jenkins platform is your friend.\n\nBut let’s take a moment and think about that in the context of microservices, especially if you’ve only used Jenkins for monolithic applications.\nYou’ll be creating a greater number of individual Jenkins jobs that each run multiple times a day.\nThis is a significant process change, and it’s important to acknowledge this and change our approach to managing Jenkins to accommodate these changes.\nIt’s well within Jenkins’ capabilities, but you will need to think a little differently, and invest to close those last-mile deployment gaps.\n\nEvolution of my Jenkins Environment\n\nOne of the biggest challenges I’ve faced as a DevOps practitioner is a long and evolving set of options to manage my Jenkins agent infrastructure.\nWith only a few large jobs you don’t really need to worry too much about your agents.\nBut when you’re orchestrating the CI/CD pipelines for dozens or even hundreds of services, optimizing efficiency and minimizing cost becomes important.\nAnd that journey has allowed me to consider and test many different Jenkins build agent architectures over the years.\nThis journey may be familiar to you as well.\n\nThese are the types of Jenkins environments I’ve run over the years.\n\nExecute all the builds on the controller.\nConcentrate all the moving parts on one instance.\n(I call this Hello Jenkins)\n\nCreate a Jenkins EC2 agent with all the required tools for building every service, and then clone it if I need to “scale” Jenkins.\n(I call this the Monster Agent.)\n\nCreate an individual Jenkins EC2 agent for each service I need to build.\n(I call this the Snowflake Agent.)\n\nRun build steps in containers.\nFor example, launching agents in containers using the\nDocker Plugin or using multi-stage Dockerfiles to encapsulate all the logic for building, testing and packaging an application.\nThey are both good first steps in container abstraction and allow you to easily copy artifacts from one container to another.\nOf course, access to a Docker engine is required for either approach, and I’ve managed my Docker host(s) for running Jenkins agents several different ways:\n\nRun the Docker engine inside my Jenkins controller container - Docker in Docker (DinD)\n\nMount the Docker socket of the host on which my Jenkins controller container runs, allowing agents to run as sibling or sidecar containers - Docker outside of Docker (DooD)\n\nConfigure a single external EC2 Docker host for the Jenkins controller to use for launching builds in containers\n\nDynamically launch agents using the EC2 plugin with an AMI that contains the Docker Engine and then run all the steps in a multi-stage Dockerfile\n\nAll these approaches were attempts to get out of the business of curating and managing Jenkins agents and infrastructure, each with their own benefits and drawbacks.\nBut recently I begin working in a new Jenkins environment - Jenkins on Kubernetes.\n\nOnce you’ve come to view Jenkins, build agents and jobs as containerized services, migrating platforms becomes much more straightforward.\nAnd total disclaimer here - I had never used Kubernetes in my life, not even for side projects - when I set out to do this.\nThat said, it was surprisingly simple to create a Kubernetes cluster in Google Cloud Platform’s (GCP) GKE, launch a Jenkins controller using a\nHelm chart and begin running build steps in Jenkins agents running in containers on my new Kubernetes cluster.\n\nLaunch agents in Kubernetes from your pipeline scripts\n\nThe focus of this post and my Jenkins World talk for 2018, is to show you how to configure Jenkins to launch agents in Kubernetes from your pipeline scripts.\nMy examples assume you are launching your agents in the same Kubernetes cluster where your Jenkins controller is running, but there are other options.\nYou’ll begin by installing the\nKubernetes plugin.\nAs a bonus, when I installed Jenkins using the latest stable chart in the default Helm repository, the Kubernetes plugin was automatically installed for me.\n\nOnce you get the Jenkins controller running on your Kubernetes cluster, there are only a few configuration steps required and then you can begin launching ephemeral build agents on Kubernetes.\n\nConfigure the Jenkins controller\n\nYou’ll first need to create a credentials set for the Jenkins controller to access the Kubernetes cluster.\nTo do this, perform the following steps:\n\nIn the Jenkins UI, click the Credentials link in the left-hand navigation pane\n\nClick the arrow next to (global) in the Stores scoped to Jenkins table (you have to hover next to the link to see the arrow)\n\nClick Add Credentials\n\nUnder Kind, specify Kubernetes Service Account\n\nLeave the scope set to Global\n\nClick OK.\n\nThat’s it! This configuration allows the Jenkins controller to use a Kubernetes service account to access the Kubernetes API.\n\nCreate a Cloud Configuration on the Jenkins controller\n\nThe next step is to create a cloud configuration for your K8s cluster.\n(When I use K8s instead of Kubernetes it’s because it is quicker to type, not just for coolness.)\n\nIn the Jenkins UI, go to Manage Jenkins → Configure System\n\nScroll down until you see Cloud settings and click the Add a new cloud box and select kubernetes\n\nThe following parameters must be set:\n\nName : - This defaults to kubernetes\n\nKubernetes URL : https://kubernetes.default - This was automatically configured from the service account.\n\nKubernetes Namespace : default - Unless you are running your controller in another namespace\n\nCredentials :  Select the Kubernetes Service Account credentials you created in the previous step\n\nJenkins URL : http:// :8080\n\nJenkins tunnel : :5555 - This is the port that is used to communicate with an agent\n\nThese were the only parameters I had to set to launch an agent in my K8s cluster.\nYou can certainly modify other parameters to tweak your environment.\n\nNow that you’ve configured your Jenkins controller so that it can access your K8s cluster, it’s time to define some pods.\nA pod is the basic building block of Kubernetes and consists of one or more containers with shared network and storage.\nEach Jenkins agent is launched as a Kubernetes pod.\nIt will always contain the default JNLP container that runs the Jenkins agent jar and any other containers you specify in the pod definition.\nThere are at least two ways to configure pod templates – in the Jenkins UI and in your pipeline script.\n\nConfigure a Pod Template in the Jenkins UI\n\nIn the Jenkins UI, go to Manage Jenkins → Configure Systems\n\nScroll down to the cloud settings you configured in the previous step\n\nClick the Add Pod Template button and select Kubernetes Pod Template\n\nEnter values for the following parameters:\n\nName :\n\nNamespace : default - unless you configured a different namespace in the previous step\n\nLabels : - this will be used to identify the agent pod from your Jenkinsfiles\n\nUsage : Select \" Use this node as much as possible\" if you would like for this pod to be your default node when no node is specified.\nSelect \" Only build jobs with label matching expressions matching this node\" to use this pod only when its label is specified in the pipeline script\n\nThe name of the pod template to inherit from - you can leave this blank.\nIt will be useful once you gain experience with this configuration, but don’t worry about it for now.\n\nContainers : The containers you want to launch inside this pod.\nThis is described in detail below.\n\nEnvVars : The environment variables you would like to inject into your pod at runtime.\nThis is described in detail below.\n\nVolumes :  Any volumes you want to mount inside your pod.\nThis is described further below.\n\nRemember that a pod consists of one or more containers that live and die together.\nThe pod must always include a JNLP container, which is configured by default if you installed the controller using the Helm Chart.\nHowever, you will want to add containers with the tool chains required to build your application.\n\nAdd Your Own Container Template\n\nIn the Jenkins UI, return to the pod template you created in the last step\n\nClick the Add Container button and select Container Template\n\nEnter values in the following fields:\n\nName :\n\nDocker image : any Docker image you’d like\nFor example, if you are building an application written in Go, you can enter 'golang:1.11-alpine3.8'\n\nLabel : Enter any label strings you’d like to use to refer to this container template in your pipeline scripts\n\nAlways pull image : - Select this option if you want the plugin to pull the image each time a pod is created.\n\nYou can leave the default values for the other parameters, but you can see that the plugin gives you fine-grained control over your pod and the individual containers that run within it.\nAny values you might set in your Kubernetes pod configuration can be set via this plugin as well.\nYou can also inject your configuration data by entering raw YAML.\nI encourage you not to get distracted by the sheer number of options you can configure in this plugin.\nYou only have to configure a small subset of them to get a working environment.\n\nYou can click the Add Environment Variable button in the container template to inject environment variables into a specific container.\nYou can click the Add Environment Variable button in the pod template to inject environment variables into all containers in the pod.\nThe following environment variables are automatically injected into the default JNLP container to allow it to connect automatically to the Jenkins controller:\n\nJENKINS_URL : Jenkins web interface url\n\nJENKINS_JNLP_URL : url for the jnlp definition of the specific agent\n\nJENKINS_SECRET : the secret key for authentication\n\nJENKINS_NAME : the name of the Jenkins agent\n\nIf you click the Add Volume button in the pod template, you’ll see several options for adding volumes to your pod.\nI use the Host Path Volume option to mount the docker socket inside the pod.\nI can then run a container with the Docker client installed and use the host Docker socket to build and push Docker images.\n\nAt this point, we’ve created a cloud configuration for our Kubernetes cluster and defined a pod consisting of one or more containers.\nNow, how do we use this to run Jenkins jobs? We simply refer to the pod and containers by label in our Jenkins pipeline script.\nWe use the label we gave to the pod in the node block and the label for the container we wish to use in the container block.\nThe examples in this post use scripted pipeline, but you can achieve the same outcome using the declarative pipeline syntax:\n\nnode('test-pod') {\n    stage('Checkout') {\n        checkout scm\n    }\n    stage('Build'){\n        container('go-agent') {\n            // This is where we build our code.\n        }\n    }\n}\n\nDefining the Pod in the Jenkinsfile\n\nConfiguring a plugin through the UI is perfectly fine in a proof of concept.\nHowever, it does not result in a software-defined infrastructure that can be versioned and stored right alongside your source code.\nLuckily, you can create the entire pod definition directly in your Jenkinsfile.\nIs there anything you can’t do in a Jenkinsfile???\n\nAny of the configuration parameters available in the UI or in the YAML definition can be added to the podTemplate and containerTemplate sections.\nIn the example below, I’ve defined a pod with two container templates.\nThe pod label is used in the node block to signify that we want to spin up an instance of this pod.\nAny steps defined directly inside the node block but not in a container block with be run in the default JNLP container.\n\nThe container block is used to signify that the steps inside the block should be run inside the container with the given label.\nI’ve defined a container template with the label 'golang', which I will use to build the Go executable that I will eventually package into a Docker image.\nIn the volumes definition, I have indicated that I want to mount the Docker socket of the host, but I still need the Docker client to interact with it using the Docker API.\nTherefore, I’ve defined a container template with the label 'docker' which uses an image with the Docker client installed.\n\npodTemplate(\n    name: 'test-pod',\n    label: 'test-pod',\n    containers: [\n        containerTemplate(name: 'golang', image: 'golang:1.9.4-alpine3.7'),\n        containerTemplate(name: 'docker', image:'trion/jenkins-docker-client'),\n    ],\n    volumes: [\n        hostPathVolume(mountPath: '/var/run/docker.sock'),\n        hostPath: '/var/run/docker.sock',\n    ],\n    {\n        //node = the pod label\n        node('test-pod'){\n            //container = the container label\n            stage('Build'){\n                container('golang'){\n                    // This is where we build our code.\n                }\n            }\n            stage('Build Docker Image'){\n                container(‘docker’){\n                    // This is where we build the Docker image\n                }\n            }\n        }\n    })\n\nIn my Docker-based pipeline scripts, I was building Docker images and pushing them to a Docker registry, and it was important to me to replicate that exactly with my new Kubernetes setup.\nOnce I accomplished that, I was ready to build my image using gcloud, the Google Cloud SDK, and push that image to the Google Container Registry in anticipation of deploying to my K8s cluster.\n\nTo do this, I specified a container template using a gcloud image and changed my docker command to a gcloud command.\nIt’s that simple!\n\npodTemplate(\n    name: 'test-pod',\n    label: 'test-pod',\n    containers: [\n        containerTemplate(name: 'golang', image: 'golang:1.9.4-alpine3.7'),\n        containerTemplate(name: 'gcloud', image:'gcr.io/cloud-builders/gcloud'),\n    ],\n    {\n        //node = the pod label\n        node('test-pod'){\n            //container = the container label\n            stage('Build'){\n                container('golang'){\n                    // This is where we build our code.\n                }\n            }\n            stage('Build Docker Image'){\n                container(‘gcloud’){\n                    //This is where we build and push our Docker image.\n                }\n            }\n        }\n    })\n\nStanding up a Jenkins controller on Kubernetes, running ephemeral agents, and building and deploying a sample application only took me a couple of hours.\nI spent another weekend really digging in to better understand the platform.\nYou can be up and running in a matter of days if you are a quick study.\nThere are a wealth of resources available on running Jenkins on Kubernetes, and I hope this blog post helps to further that knowledge.\nEven better, come to\nmy session at Jenkins World and let’s talk in person.\n\nSo, what else do you want to know?\nHit me up on Twitter.\nI might even add your questions to my Jenkins World session.\nI suppose next up is Mesos?\n\nCome meet Mandy and other Jenkins and Kubernetes experts at\nJenkins World on September 16-19th,\nregister with the code JWFOSS for a 30% discount off your pass.","title":"Jenkins and Kubernetes - Secret Agents in the Clouds","tags":["jenkinsworld","jenkinsworld2018","cloud-native","kubernetes"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#485858","images":{"fallback":{"src":"/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/19e71/devmandy.jpg","srcSet":"/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/77b35/devmandy.jpg 32w,\n/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/d4a57/devmandy.jpg 64w,\n/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/19e71/devmandy.jpg 128w,\n/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/68974/devmandy.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/ef6ff/devmandy.webp 32w,\n/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/8257c/devmandy.webp 64w,\n/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/6766a/devmandy.webp 128w,\n/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/22bfc/devmandy.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"DevMandy","html":"<div class=\"paragraph\">\n<p>Mandy Hubbard has almost 20 years of professional QA experience,\nmost of which has been spent in fast-paced startup environments driving product quality.\nShe is passionate about ensuring quality through process improvements, test automation, following CI/CD best practices and all things DevOps.\nShe is currently a software engineer/QA architect at CS Disco, an innovative startup delivering a cloud-based eDiscovery platform.</p>\n</div>","id":"devmandy","irc":null,"linkedin":null,"name":"Mandy Hubbard","slug":"/blog/author/devmandy","twitter":"DevMandy"}]}},{"node":{"date":"2018-09-12T00:00:00.000Z","id":"25b519ed-5da4-5f9a-a173-6ea9a343b90a","slug":"/blog/2018/09/12/speaker-blog-a-cloud-native-jenkins/","strippedHtml":"A few months ago I published a\nblog post about\nCloud Native Special Interest Group (SIG)\nand ongoing projects related to Cloud Native Jenkins.\nNext week we will be presenting at DevOps World | Jenkins World together with Carlos Sanchez and Jesse Glick,\nso I would like to provide a heads up for\nour talk: “A Cloud Native Jenkins”.\n\nIn our talk, we will focus on the following topics: Pluggable Storage,\nour ephemeral Jenkins controllers experiments,\nand tools which may be used to implement single-shot controllers.\n\nPluggable Storage\n\nPluggable storage is one of the major areas we have been working on over the last few months.\nThere are a number of parallel stories which are summarized on\nthis page.\nThere has been significant progress in the areas of artifact storage, build logging and configuration storage.\nA number of Jenkins Enhancement Proposals were submitted and accepted,\nand there are plugin releases and prototypes for these stories.\n\nDuring our talk we will discuss the current status of these stories and future plans.\nIn particular, we will cover the following areas and reference implementations:\n\nStoring all your artifacts transparently, e.g. in a cloud service blob store like AWS S3.\n\nArtifact Manager for S3 Plugin is an implementation we have recently released\n\nProviding credentials from an external location.\n\nKubernetes Credentials Provider is one of the existing implementations for Kubernetes secrets\n\nSending and retrieving the build logs from a cloud service.\n\nWe are working on reference implementations for AWS CloudWatch Logs and\nElasticsearch\n\nStoring configuration data in external storage like Kubernetes Resources and SQL database\n\nStoring test results externally, e.g. in an SQL database or a specialized Test Management System\n\nThere are existing plugins for the areas above, but there is a difference in approach we have taken.\nInstead of creating new custom steps we extend Jenkins architecture in a way that the storage becomes transparent to users.\nFor example, with Artifact Manager for S3 Plugin common Archive Artifacts steps\nwork transparently with Remote storage, as well as Jenkins Pipeline’s stash() / unstash() steps.\n\nThe reference implementations intentionally use different technologies so that we cover more scenarios.\nWe regularly discuss the implementations in the Cloud Native SIG,\nand we would appreciate your feedback.\n\nEphemeral Jenkins controllers research\n\nWant something new?\nSeveral days ago Kohsuke Kawaguchi, the creator of Jenkins, posted the\nJenkins: Shifting Gears article to summarize the plan for Jenkins evolution.\nCloud Native Jenkins is a critical part of this plan, and it is not “just Jenkins X”.\nThere are various architectural changes in Jenkins required to make this vision happen,\nand we plan to work on these changes in the Cloud Native SIG.\n\nIn our presentation, we will talk about our experiment with ephemeral Jenkins and single-shot controllers.\nIn this story we are creating a headless single-shot controller which starts in a container,\nexecutes a Pipeline build and pushes all the results to remote storage so that the container can just be deleted after completion.\nSuch a controller bundles plugins and self-configuration logic using “Configuration as Code”,\nso that it can start executing Pipelines in just a few seconds.\nOnce packaged, it can be invoked from CLI as simply as…​\n\ndocker run --rm -v $PWD/demo/Jenkinsfile:/workspace/Jenkinsfile onenashev/cwp-jenkinsfile-runner-demo\n\nor, in Kubernetes:\n\nkubectl create configmap jenkinsfile --from-file=demo/Jenkinsfile\nkubectl create -f demo/kubernetes.yaml\n\nSuch a single-shot controller could also be made a part of a Cloud Native Jenkins system.\nStandard event handlers like Prow can invoke the builds on webhooks and report results back,\nso that the single-shot controller can be used to build pull requests or to run Continuous Delivery flows.\nExtra agents could also be connected to the controller on-demand, using the Kubernetes plugin or sidecar containers.\n\nTools\n\nIn order to make this experiment possible, we used a toolchain based on\nDocker,\nJenkinsfile Runner,\nConfiguration as Code Plugin (JCasC), and a\nCustom WAR Packager tool which glues all the things together.\n\nCustom WAR Packager is a new tool which takes various configurations (YAML specification defining core version, list of plugins, system properties, Groovy Hooks, JCasC YAMLs)…​\nand then bundles everything as a ready-to-fly WAR file or Docker image.\nStarting from version 1.2, Custom WAR Packager also supports packaging Jenkinsfile Runner images as an experimental feature.\nI will do a separate blogpost about this new tool later,\nbut there is already some documentation a number of demos in the project’s repo.\n\nOur demo\n\nYes, we will have a demo! We will show a single-shot controller running with Pluggable storage implementations for AWS environments (Amazon S3, AWS CloudWatch, EKS, etc.),\nwhich executes Jenkins Pipelines for Maven projects and provisions agents in Kubernetes on-demand.\n\nThe demo has to be published yes, but you can already find a more simple Jenkinsfile Runner demo\nhere.\n\nWant to know more?\n\nThe upcoming DevOps World | Jenkins World conferences\nare heavily packed with talks related to Cloud Native Jenkins,\nincluding war stories and presentations on projects like Jenkins X and Jenkins Evergreen.\nIt is a great chance to get more information about using Jenkins in cloud environments.\n\nIf you are a Jenkins contributor or just want to become a contributor,\nalso join the Contributor Summit (Sep 17 in US and Oct 23 in Nice) or visit the Jenkins community booth in the Exhibition hall.\nAt the Contributor Summit on Sep 17 we will also have a face-to-face Cloud Native SIG meeting.\nFeel free to contribute to the agenda here.\n\nCome meet Carlos, Jesse, Oleg, and other Cloud Native SIG members at\nJenkins World on September 16-19th in San Francisco and on October 22-25 in Nice.\nregister with the code JWFOSS for a 30% discount off your pass.","title":"Speaker blogpost: A Cloud Native Jenkins","tags":["jenkinsworld","jenkinsworld2018","cloud-native","pluggable-storage","jenkinsfile-runner"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8c8e8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/bf8e1/oleg_nenashev.png","srcSet":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/914ee/oleg_nenashev.png 32w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/1c9ce/oleg_nenashev.png 64w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/bf8e1/oleg_nenashev.png 128w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/acb7c/oleg_nenashev.png 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/ef6ff/oleg_nenashev.webp 32w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/8257c/oleg_nenashev.webp 64w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/6766a/oleg_nenashev.webp 128w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/22bfc/oleg_nenashev.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"https://oleg-nenashev.github.io/","github":"oleg-nenashev","html":"<div class=\"paragraph\">\n<p>Jenkins core maintainer and board member.\nOleg started using Hudson for Hardware/Embedded projects in 2008 and became an active Jenkins contributor in 2012.\nNowadays he leads several Jenkins <a href=\"/sigs\">SIGs</a>, outreach programs (<a href=\"/projects/gsoc\">Google Summer of Code</a>, <a href=\"/events/hacktoberfest\">Hacktoberfest</a>) and <a href=\"/projects/jam/\">Jenkins meetups</a> in Switzerland and Russia.\nOleg works for <a href=\"https://www.cloudbees.com/\">CloudBees</a> and focuses on key projects in the community.</p>\n</div>","id":"oleg_nenashev","irc":"oleg_nenashev","linkedin":"onenashev","name":"Oleg Nenashev","slug":"/blog/author/oleg_nenashev","twitter":"oleg_nenashev"}]}},{"node":{"date":"2018-09-10T00:00:00.000Z","id":"3cfe2a34-ab7c-5a8e-baa2-c46178e12dd1","slug":"/blog/2018/09/10/scaling-network-connections/","strippedHtml":"Oleg Nenashev and I will be speaking at DevOps World | Jenkins World in San Francisco this year about\nScaling Network Connections from the Jenkins Controller.\nOver the years there have been many efforts to analyze, optimize, and fortify the “Remoting channel”\nthat allows a controller to orchestrate agent activity and receive build results.\nTechniques such as tuning the agent launcher can improve service,\nbut qualitative change can only come from fundamentally reworking what gets transmitted and how.\n\nIn March, jira:27035[] introduced a framework for inspecting the traffic on a Remoting channel at a high level.\nPreviously, developers could only use generic low-level tools such as Wireshark,\nwhich cannot identify the precise piece of Jenkins code responsible for traffic.\n\nOver the past few months, the\nCloud Native SIG\nhas been making progress in addressing root causes.\nThe\nArtifact Manager on S3 plugin\nhas been released and integrated with Jenkins Evergreen,\nallowing upload and download of large artifacts to happen entirely between the agent and Amazon servers.\nPrototype plugins allow all build log content generated by an agent (such as in sh steps)\nto be streamed directly to external storage services such as AWS CloudWatch Logs.\nWork has also begun on uploading JUnit-format test results, which can sometimes get big,\ndirectly from an agent to database storage.\nAll these efforts can reduce the load on the Jenkins controller and local network\nwithout requiring developers to touch their Pipeline scripts.\n\nOther approaches are on the horizon.\nWhile “one-shot” agents run in fresh VMs or containers greatly improve reproducibility,\nthey suffer from the need to transmit megabytes of Java code for every build,\nso Jenkins features will need to be built to precache most or all of it.\nWork is underway to use Apache Kafka to make channels more robust against network failures.\nMost dramatically, the proposed\nCloud Native Jenkins MVP\nwould eliminate the bottleneck of a single Jenkins controller service handling hundreds of builds.\n\nCome meet Jesse, Oleg, and other Cloud Native SIG members at\nJenkins World on September 16-19th,\nregister with the code JWFOSS for a 30% discount off your pass.","title":"Scaling Network Connections from the Jenkins Controller","tags":["jenkinsworld","jenkinsworld2018","cloud-native","performance","scalability","remoting"],"authors":[{"avatar":null,"blog":null,"github":"jglick","html":"<div class=\"paragraph\">\n<p>Jesse has been developing Jenkins core and plugins for years.\nHe is the coauthor with Kohsuke of the core infrastructure of the Pipeline system.</p>\n</div>","id":"jglick","irc":null,"linkedin":null,"name":"Jesse Glick","slug":"/blog/author/jglick","twitter":"tyvole"}]}},{"node":{"date":"2018-07-30T00:00:00.000Z","id":"3f5d0df0-bf7b-55f9-bbfe-fd861133015d","slug":"/blog/2018/07/30/introducing-cloud-native-sig/","strippedHtml":"On large-scale Jenkins instances controller disk and network I/O become bottlenecks in particular cases.\nBuild logging and artifact storage were one for the most intensive I/O consumers,\nhence it would be great to somehow redirect them to an external storage.\nBack in 2016 there were active discussions about such Pluggable Storage for Jenkins.\nAt that point we created several prototypes, but then other work took precedence.\nThere was still a high demand in Pluggable Storage for large-scale instances,\nand these stories also become a major obstacle for cloud native Jenkins setups.\n\nI am happy to say that the Pluggable Storage discussions are back online.\nYou may have seen changes in the Core for Artifact Storage\n( JEP-202)\nand a new Artifact Manager for S3 plugin.\nWe have also created a number of JEPs for External Logging\nand created a new Cloud Native Special Interest Group (SIG)\nto offer a venue for discussing changes and to keep them as open as possible.\n\nTomorrow Jesse Glick and I will be\npresenting the current External Logging designs at the\nCloud Native SIG online meeting,\nyou can find more info about the meeting here.\nI decided that it is a good time to write about the new SIG.\nIn this blogpost I will try to provide my vision of the SIG and its purpose.\nI will also summarize the current status of the activities in the group.\n\nWhat are Special Interest Groups?\n\nIf you follow the developer mailing list,\nyou may have seen the discussion about introducing SIGs\nin the Jenkins project.\nThe SIG model has been proposed by\nR. Tyler Croy,\nand it largely follows the successful\nKubernetes SIG model.\nThe objective of these SIGs is to make the community more transparent to contributors\nand to offer venues for specific discussions.\nThe idea of SIGs and how to create them is documented in\nJEP-4.\nJEP-4 is still in Draft state, but a few SIGs have been already created using that process:\nPlatform SIG, GSoC SIG and, finally,\nCloud Native SIG.\n\nSIGs are a big opportunity to the Jenkins project,\noffering a new way to onboard contributors who are interested only in particular aspects of Jenkins.\nWith SIGs they can subscribe to particular topics without\nfollowing the entire Developer mailing list which can become pretty buzzy nowadays.\nIt also offers company contributors a clear way how to join community and participate in specific areas.\nThis is great for larger projects which cannot be done by a single contributor.\nLike JEPs, SIGs help focus and coordinate efforts.\n\nAnd, back to major efforts…​\nLack of resources among core contributors was one of the reasons\nwhy we did not deliver on Pluggable Storage stories back in 2016.\nI believe that SIGs can help fix that in Jenkins,\nmaking it easier to find groups with the same interests and\nreach out to them in order to organize activity.\nRegular meetings are also helpful to get such efforts moving.\n\nPoints above are the main reasons why I joined the Cloud Native SIG.\nSimilarly, that’s why I decided to create a Platform SIG\nto deliver on major efforts like Java 10+ support in Jenkins.\nI hope that more SIGs get created soon so that contributors could focus on areas of their interest.\n\nCloud Native SIG\n\nIn the original proposal Carlos Sanchez,\nthe Cloud Native SIG chair, has described the purpose of the SIG well.\nThere has been great progress this year in cloud-native-minded projects like Jenkins X and Jenkins Evergreen,\nbut the current Jenkins architecture does not offer particular\nfeatures which could be utilized there:\nPluggable Storage, High Availability, etc.\nThere are ways to achieve it using Jenkins plugins and some infrastructure tweaks,\nbut it is far from the out-of-the-box experience.\nIt complicates Jenkins management and slows down development of new cloud-native solutions for Jenkins.\n\nSo, what do I expect from the SIG?\n\nDefine roadmap towards Cloud-Native Jenkins architecture\nwhich will help the project to stay relevant for Cloud Native installations\n\nProvide a venue for discussion of critical Jenkins architecture changes\n\nAct as a steering committee for Jenkins Enhancement Proposals in the area of\nCloud-Native solutions\n\nFinally, coordinate efforts between contributors and get new\ncontributors onboard\n\nWhat’s next in the SIG?\n\nThe SIG agenda is largely defined by the SIG participants.\nIf you are interested to discuss particular topics,\njust propose them in the SIG mailing list.\nAs the current SIG page describes,\nthere are several areas defined as initial topics:\nArtifact Storage,\nLog Storage,\nConfiguration Storage\n\nAll these topics are related to the Pluggable Storage Area,\nand the end goal for them is to ensure that all data is externalized\nso that replication becomes possible.\nIn addition to the mentioned data types,\ndiscussed at the Jenkins World 2016 summit,\nwe will need to externalize other data types:\nItem and Run storage,\nFingerprints,\nTest and coverage results,\netc.\nThere is some foundation work being done for that.\nFor example, Shenyu Zheng is working on a\nCode Coverage API plugin\nwhich would allow to unify the code coverage storage formats in Jenkins.\n\nOnce the Pluggable Storage stories are done the next steps are true High Availability, rolling or canary upgrades and zero downtime.\nAt that point other foundation stories like Remoting over Kafka\nby Pham Vu Tuan\nmight be integrated into the Cloud Native architecture to make Jenkins more robust against outages within the cluster.\nIt will take some time to get to this state, but it can be done incrementally.\n\nLet me briefly summarize current state of the 3 focuses listed in the Cloud Native SIG.\n\nArtifact Storage\n\nThere are many existing plugins allowing to upload and download artifacts from external storage\n(e.g. S3, Artifactory, Publish over SFTP, etc., etc.),\nbut there are no plugins which can do it transparently without using\nnew steps.\nIn many cases the artifacts also get uploaded through the controller,\nand it increases load on the system.\nIt would be great if there was a layer which would allow storing artifacts externally\nwhen using common steps like Archive Artifacts.\n\nArtifact storage work was started this spring by Jesse Glick, Carlos Sanchez and\nIvan Fernandez Calvo\nbefore the Cloud Native SIG was actually founded.\nCurrent state:\n\nJEP-202 \"External Artifact Storage\"\nhas been proposed in the Jenkins community.\nThis JEP defines API changes in the Jenkins core which are needed to\nsupport External artifact managers\n\nJenkins Pipeline has been updated to support external artifact storages\nfor archive / unarchive and stash / unstash\n\nNew Artifact Manager for S3 plugin\nreference implementation of the new API.\nThe plugin is available in main Jenkins update centers\n\nA number of plugins has been updated in order to support\nexternal artifact storage\n\nThe Artifact Manager API is available in Jenkins LTS starting from 2.121.1,\nso it is possible to create new implementations using the provided API and\nexisting implementations.\nThis new feature is fully backward compatible with the default Filesystem-based storage,\nbut there are known issues for plugins explicitly relying on artifact locations in JENKINS_HOME\n(you can find a list of such plugins\nhere).\nIt will take a while to get all plugins supported,\nbut the new API in the core should allow migrating plugins.\n\nI hope we will revisit the External Artifact Storage at the SIG meetings at some point.\nIt would be a good opportunity to do a retrospective and to understand how to improve the process\nin SIG.\n\nLog storage\n\nLog storage is a separate big story.\nBack in 2016 External logging was one of the key Pluggable Storage stories we defined at the contributor summit.\nWe created an EPIC for the story ( JENKINS-38313)\nand after that created a number of prototypes together with\nXing Yan and Jesse Glick.\nOne of these prototypes for Pipeline has recently been updated and published\nhere.\n\nJesse Glick and Carlos Sanchez\nare returning to this story and plan to discuss it within the Cloud Native SIG.\nThere are a number of Jenkins Enhancement proposals which have been submitted recently:\n\njep:207[] -\nExternal Build Logging support in the Jenkins Core\n\njep:210[] -\nExternal log storage for Pipeline\n\njep:212[] -\nExternal Logging API Plugin\n\njep:206[] -\nUse UTF-8 for Pipeline build logs\n\nIn the linked documents you can find references to current reference implementations.\nSo far we have a working prototype for the new design.\nThere are still many bits to fix before the final release,\nbut the designs are ready for review and feedback.\n\nThis Tuesday (Jul 31) we are going to have a SIG meeting in order to present the current state and to discuss the proposed designs and JEPs.\nThe meeting will happen at 3PM UTC.\nYou can watch the broadcast using this link.\nParticipant link will be posted in the SIGs Gitter channel 10 minutes before the meeting.\n\nConfiguration storage\n\nThis is one of the future stories we would like to consider.\nAlthough configurations are not big, externalizing them is a critical task\nfor getting highly-available or disposable Jenkins controllers.\nThere are many ways to store configurations in Jenkins,\nbut 95% of cases are covered by the XmlFile layer which\nserializes objects to disk and reads them using the XStream library.\nExternalizing these XmlFile s would be a great step forward.\n\nThere are several prototypes for externalizing configurations,\ne.g. in DotCI.\nThere are also other implementations which could be upstreamed to the Jenkins core:\n\nAlex Nordlund has recently proposed a\npull request\nto Jenkins Core, which should make the XML Storage pluggable\n\nJames Strachan has implemented similar engine\nfor Kubernetes in the kubeify prototype\n\nI also did some experiments with externalizing XML Storages back in 2016\n\nThe next steps for this story would be to aggregate implementations into a single JEP.\nI have it in my queue, and I hope to write up a design once we get more clarity on the External logging stories.\n\nConclusions\n\nSpecial Interest Groups are a new format for collaboration and disucssion in the Jenkins community.\nAlthough we have had some work groups before (Infrastructure, Configuration-as-Code, etc.),\nintroduction of SIGs sets a new bar in terms of the project transparency and consistency.\nMajor architecture changes in Jenkins are needed to ensure its future in the new environments,\nand SIGs will help to boost visibility and participation around these changes.\n\nIf you want to know more about the Cloud Native SIG,\nall resources are listed on the SIG’s page on jenkins.io.\nIf you want to participate in the SIG’s activities, just do the following:\n\nSubscribe to the mailing list\n\nJoin our Gitter channel\n\nJoin our public meetings\n\nI am also working on organizing a face-to-face Cloud Native SIG meeting at the\nJenkins Contributor Summit,\nwhich will happen on September 17 during\nDevOps World | Jenkins World in San Francisco.\nIf you come to DevOps World | Jenkins World,\nplease feel free to join us at the contributor summit or to meet us at the community booth.\nTogether with Jesse and Carlos we are also going to present some bits of our work at the\nA Cloud Native Jenkins talk.\n\nStay tuned for more updates and demos on the Cloud-Native Jenkins fronts!","title":"Introducing Jenkins Cloud Native SIG","tags":["community","sig","cloud-native","cloud-native-sig"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8c8e8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/bf8e1/oleg_nenashev.png","srcSet":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/914ee/oleg_nenashev.png 32w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/1c9ce/oleg_nenashev.png 64w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/bf8e1/oleg_nenashev.png 128w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/acb7c/oleg_nenashev.png 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/ef6ff/oleg_nenashev.webp 32w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/8257c/oleg_nenashev.webp 64w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/6766a/oleg_nenashev.webp 128w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/22bfc/oleg_nenashev.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"https://oleg-nenashev.github.io/","github":"oleg-nenashev","html":"<div class=\"paragraph\">\n<p>Jenkins core maintainer and board member.\nOleg started using Hudson for Hardware/Embedded projects in 2008 and became an active Jenkins contributor in 2012.\nNowadays he leads several Jenkins <a href=\"/sigs\">SIGs</a>, outreach programs (<a href=\"/projects/gsoc\">Google Summer of Code</a>, <a href=\"/events/hacktoberfest\">Hacktoberfest</a>) and <a href=\"/projects/jam/\">Jenkins meetups</a> in Switzerland and Russia.\nOleg works for <a href=\"https://www.cloudbees.com/\">CloudBees</a> and focuses on key projects in the community.</p>\n</div>","id":"oleg_nenashev","irc":"oleg_nenashev","linkedin":"onenashev","name":"Oleg Nenashev","slug":"/blog/author/oleg_nenashev","twitter":"oleg_nenashev"}]}}]}},"pageContext":{"tag":"cloud-native","limit":8,"skip":8,"numPages":2,"currentPage":2}},
    "staticQueryHashes": ["3649515864"]}