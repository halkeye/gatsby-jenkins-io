{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/plugins/page/10",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2016-05-25T00:00:00.000Z","id":"3bdee89e-566b-52d9-a3cc-f41507becadc","slug":"/blog/2016/05/25/update-plugin-for-pipeline/","strippedHtml":"This is a guest post by Chris Price.\nChris is a software engineer at Puppet, and has been\nspending some time lately on automating performance testing using the latest\nJenkins features.\n\nIn this blog post, I’m going to attempt to provide some step-by-step notes on\nhow to refactor an existing Jenkins plugin to make it compatible with the new\nJenkins Pipeline jobs.  Before we get to the fun stuff, though, a little\nbackground.\n\nHow’d I end up here?\n\nRecently, I started working on a project to automate some performance tests for\nmy company’s products.  We use the awesome Gatling load\ntesting tool for these tests, but we’ve largely been handling the testing very\nmanually to date, due to a lack of bandwidth to get them automated in a clean,\nmaintainable, extensible way.  We have a years-old Jenkins server where we use\nthe gatling jenkins\nplugin to track the\nhistory of certain tests over time, but the setup of the Jenkins instance was\nvery delicate and not easy to reproduce, so it had fallen into a state of\ndisrepair.\n\nOver the last few days I’ve been putting some effort into getting things more\nautomated and repeatable so that we can really maximize the value that we’re\ngetting out of the performance tests.  With some encouragement from the fine\nfolks in the #jenkins IRC channel, I ended up exploring\nthe JobDSL\nplugin and the new Pipeline jobs.  Combining those two\nthings with some Puppet code to provision a Jenkins server via the\njenkins puppet module gave me\na really nice way to completely automate my Jenkins setup and get a seed job in\nplace that would create my perf testing jobs.  And the Pipeline job format is\njust an awesome fit for what I wanted to do in terms of being able to easily\nmonitor the stages of my performance tests, and to make the job definitions\nmodular so that it would be really easy to create new performance testing jobs\nwith slight variations.\n\nSo everything’s going GREAT up to this point.  I’m really happy with how it’s\nall shaping up.  But then…​ (you knew there was a \"but\" coming, right?) I\nstarted trying to figure out how to add the\nGatling Jenkins\nplugin to the Pipeline jobs, and kind of ran into a wall.\n\nAs best as I could tell from my Googling, the plugin was probably going to\nrequire some modifications in order to be able to be used with Pipeline jobs.\nHowever, I wasn’t able to find any really cohesive documentation that\ndefinitively confirmed that or explained how everything fits together.\n\nEventually, I got it all sorted out.  So, in hopes of saving the next person a\nlittle time, and encouraging plugin authors to invest the time to get their\nplugins working with Pipeline, here are some notes about what I learned.\n\nSpoiler: if you’re just interested in looking at the individual git commits that\nI made on may way to getting the plugin working with Pipeline, have a look at\nthis github\nbranch.\n\nCreating a pipeline step\n\nThe main task that the Gatling plugin performs is to archive Gatling reports\nafter a run.  I figured that the end game for this exercise was that I was going\nto end up with a Pipeline \"step\" that I could include in my Pipeline scripts, to\ntrigger the archiving of the reports.  So my first thought was to look for an\nexisting plugin / Pipeline \"step\" that was doing something roughly similar, so\nthat I could use it as a model.  The Pipeline \"Snippet Generator\" feature\n(create a pipeline job, scroll down to the \"Definition\" section of its\nconfiguration, and check the \"Snippet Generator\" checkbox) is really helpful for\nfiguring out stuff like this; it is automatically populated with all of the\nsteps that are valid on your server (based on which plugins you have installed),\nso you can use it to verify whether or not your custom \"step\" is recognized, and\nalso to look at examples of existing steps.\n\nLooking through the list of existing steps, I figured that the archive step\nwas pretty likely to be similar to what I needed for the gatling plugin:\n\nSo, I started poking around to see what magic it was that made that archive\nstep show up there.  There are some mentions of this in the\npipeline-plugin\nDEVGUIDE.md and the\nworkflow-step-api-plugin\nREADME.md, but the real breakthrough for me was finding the definition of the\narchive step in the workflow-basic-steps-plugin source\ncode.\n\nWith that as an example, I was able to start poking at getting a\ngatlingArchive step to show up in the Snippet Generator.  The first thing that\nI needed to do was to update the gatling-plugin project’s pom.xml to depend\non a recent enough version of Jenkins, as well as specify dependencies on the\nappropriate pipeline\nplugins\n\nOnce that was out of the way, I noticed that the archive step had some tests\nwritten for it, using what looks to be a pretty awesome test API for pipeline\njobs and plugins.  Based on those archive\ntests,\nI added\na\nskeleton for a test for the gatlingArchive step that I was about to write.\n\nThen, I moved on to\nactually\ncreating the step.  The meat of the code was this:\n\npublic class GatlingArchiverStep extends AbstractStepImpl {\n    @DataBoundConstructor\n    public GatlingArchiverStep() {}\n\n    @Extension\n    public static class DescriptorImpl extends AbstractStepDescriptorImpl {\n        public DescriptorImpl() { super(GatlingArchiverStepExecution.class); }\n\n        @Override\n        public String getFunctionName() {\n            return \"gatlingArchive\";\n        }\n\n        @NonNull\n        @Override\n        public String getDisplayName() {\n            return \"Archive Gatling reports\";\n        }\n    }\n}\n\nNote that in that commit I also added a config.jelly file.  This is how you\ndefine the UI for your step, which will show up in the Snippet Generator.  In\nthe case of this Gatling step there’s really not much to configure, so my\nconfig.jelly is basically empty.\n\nWith that (and the rest of the code from that commit) in place, I was able to\nfire up the development Jenkins server (via mvn hpi:run, and note that you\nneed to go into the \"Manage Plugins\" screen on your development server and\ninstall the Pipeline plugin once before any of this will work) and visit the\nSnippet Generator to see if my step showed up in the dropdown:\n\nGREAT SUCCESS!\n\nThis step doesn’t actually do anything yet, but it’s recognized by Jenkins and\ncan be included in your pipeline scripts at that point, so, we’re on our way!\n\nThe step metastep\n\nThe step that we created above is a first-class DSL addition that can be used in\nPipeline scripts.  There’s another way to make your plugin work usable from a\nPipeline job, without making it a first-class build step.  This is by use of the\nstep\"metastep\", mentioned in the pipeline-plugin\nDEVGUIDE.\nWhen using this approach, you simply refactor your Builder or Publisher to\nextend SimpleBuildStep, and then you can reference the build step from the\nPipeline DSL using the step method.\n\nIn the Jenkins GUI, go to the config screen for a Pipeline job and click on the\nSnippet Generator checkbox.  Select 'step: General Build Step' from the\ndropdown, and then have a look at the options that appear in the 'Build Step'\ndropdown.  To compare with our previous work, let’s see what \"Archive the\nartifacts\" looks like:\n\nFrom the snippet generator we can see that it’s possible to trigger an Archive\naction with syntax like:\n\nstep([$class: 'ArtifactArchiver', artifacts: 'foo*', excludes: null])\n\nThis is the \"metastep\".  It’s a way to trigger any build action that implements\nSimpleBuildStep, without having to actually implement a real \"step\" that\nextends the Pipeline DSL like we did above.  In many cases, it might only make\nsense to do one or the other in your plugin; you probably don’t really need\nboth.\n\nFor the purposes of this tutorial, we’re going to do both.  For a couple of reasons:\n\nWhy the heck not?  :)  It’s a good demonstration of how the metastep stuff\nworks.\n\nBecause implementing the \"for realz\" step will be a lot easier if the Gatling\naction that we’re trying to call from our gatlingArchive() syntax is using the\nnewer Jenkins APIs that are required for subclasses of SimpleBuildStep.\n\nGatlingPublisher is the main build action that we’re interested in using in\nPipeline jobs.  So, with all of that in mind, here’s our next goal: get\nstep([$class: 'GatlingPublisher', …​) showing up in the Snippet Generator.\n\nThe javadocs for the SimpleBuildStep\nclass\nhave some notes on what you need to do when porting an existing Builder or\nPublisher over to implement the SimpleBuildStep interface.  In all\nlikelihood, most of what you’re going to end up doing is to replace occurrences\nof AbstractBuild with references to the Run class, and replace occurrences\nof AbstractProject with references to the Job class.  The APIs are pretty\nsimilar, so it’s not too hard to do once you understand that that’s the game.\nThere is some discussion of this in the pipeline-plugin\nDEVGUIDE.\n\nFor the Gatling plugin, my\ninitial\nefforts to port the GatlingPublisher over to implement SimpleBuildStep only\nrequired the AbstractBuild → Run refactor.\n\nAfter making these changes, I fired up the development Jenkins server, and, voila!\n\nSo, now, we can add a line like this to a Pipeline build script:\n\nstep([$class: 'GatlingPublisher', enabled: true])\n\nAnd it’ll effectively be the same as if we’d added the Gatling \"Post-Build\nAction\" to an old-school Freestyle project.\n\nWell…​ mostly.\n\nBuild Actions vs. Project Actions\n\nAt this point our modified Gatling plugin should work the same way as it always\ndid in a Freestyle build, but in a Pipeline build, it only partially works.\nSpecifically, the Gatling plugin implements two different \"Actions\" to surface\nthings in the Jenkins GUI: a \"Build\" action, which adds the Gatling icon to the\nleft sidebar in the GUI when you’re viewing an individual build in the build\nhistory of a job, and a \"Project\" action, which adds that same icon to the left\nsidebar of the GUI of the main page for a job.  The \"Project\" action also adds a\n\"floating panel\" on the main job page, which shows a graph of the historical\ndata for the Gatling runs.\n\nIn a Pipeline job, though, assuming we’ve added a call to the metastep, we’re\nonly seeing the \"Build\" actions.  Part of this is because, in the last round of\nchanges that I linked, we only modified the \"Build\" action, and not the\n\"Project\" action.  Running the metastep in a Pipeline job has no visible effect\nat all on the project/job page at this point.  So that’s what we’ll tackle next.\n\nThe key thing to know about getting \"Project\" actions working in a Pipeline job\nis that, with a Pipeline job, there is no way for Jenkins to know up front what\nsteps or actions are going to be involved in a job.  It’s only after the job\nruns once that Jenkins has a chance to introspect what all the steps were.  As\nsuch, there’s no list of Builders or Publishers that it knows about up front to\ncall getProjectAction on, like it would with a Freestyle job.\n\nThis is where\nSimpleBuildStep.LastBuildAction\ncomes into play.  This is an interface that you can add to your Build actions,\nwhich give them their own getProjectActions method that Jenkins recognizes and\nwill call when rendering the project page after the job has been run at least\nonce.\n\nSo, effectively, what we need to do is to\nget\nrid of the getProjectAction method on our Publisher class, modify the Build\naction to implement SimpleBuildStep.LastBuildAction, and encapsulate our\nProject action instances in the Build action.\n\nThe build action class now constructs an instance of the Project action and\nmakes it accessible via getProjectActions (which comes from the\nLastBuildAction interface):\n\npublic class GatlingBuildAction implements Action, SimpleBuildStep.LastBuildAction {\n    public GatlingBuildAction(Run build, List sims) {\n        this.build = build;\n        this.simulations = sims;\n\n        List projectActions = new ArrayList<>();\n        projectActions.add(new GatlingProjectAction(build.getParent()));\n        this.projectActions = projectActions;\n    }\n\n    @Override\n    public Collection getProjectActions() {\n        return this.projectActions;\n    }\n}\n\nAfter making these changes, if we run the development Jenkins server, we can see\nthat after the first successful run of the Pipeline job that calls the\nGatlingPublisher metastep, the Gatling icon indeed shows up in the sidebar on\nthe main project page, and the floating box with the graph shows up as well:\n\nMaking our DSL step do something\n\nSo at this point we’ve got the metastep syntax working from end-to-end, and\nwe’ve got a valid Pipeline DSL step ( gatlingArchive()) that we can use in our\nPipeline scripts without breaking anything…​ but our custom step doesn’t\nactually do anything.  Here’s the part where we tie it all together…​ and it’s\npretty easy!  All we need to do is to make our step \"Execution\" class\ninstantiate a Publisher and call perform on\nit.\n\nAs per the\nnotes\nin the pipeline-plugin DEVGUIDE, we can use the @StepContextParameter\nannotation to inject in the objects that we need to pass to the Publisher’s\nperform method:\n\npublic class GatlingArchiverStepExecution extends AbstractSynchronousNonBlockingStepExecution {\n\n    @StepContextParameter\n    private transient TaskListener listener;\n\n    @StepContextParameter\n    private transient FilePath ws;\n\n    @StepContextParameter\n    private transient Run build;\n\n    @StepContextParameter\n    private transient Launcher launcher;\n\n    @Override\n    protected Void run() throws Exception {\n        listener.getLogger().println(\"Running Gatling archiver step.\");\n\n        GatlingPublisher publisher = new GatlingPublisher(true);\n        publisher.perform(build, ws, launcher, listener);\n\n        return null;\n    }\n}\n\nAfter these changes, we can fire up the development Jenkins server, and hack up\nour Pipeline script to call gatlingArchive() instead of the metastep\nstep([$class: 'GatlingPublisher', enabled: true]) syntax.  One of these is\nnicer to type and read than the other, but I’ll leave that as an exercise for\nthe reader.\n\nFin\n\nWith that, our plugin now works just as well in the brave new Pipeline world as\nit did in the olden days of Freestyle builds.  I hope these notes save someone\nelse a little bit of time and googling on your way to writing (or porting) an\nawesome plugin for Jenkins Pipeline jobs!\n\nLinks\n\nJenkins Pipeline Overview\n\nPipeline Plugin Developer Guide\n\nJenkins Source Code\n\nWorkflow Step API Plugin\n\nWorkflow Basic Steps Plugin","title":"Refactoring a Jenkins plugin for compatibility with Pipeline jobs","tags":["core","pipeline","plugins"],"authors":[{"avatar":null,"blog":null,"github":"cprice404","html":"<div class=\"paragraph\">\n<p>Chris is a software engineer at Puppet, who mostly works on backend services\nfor Puppet itself, but occasionally gets to spend some time improving CI\nand automation using Jenkins.</p>\n</div>","id":"cprice404","irc":null,"linkedin":null,"name":"Chris Price","slug":"/blog/author/cprice404","twitter":"cprice404"}]}},{"node":{"date":"2016-05-23T00:00:00.000Z","id":"85257451-d116-575c-b893-3c51d7386caa","slug":"/blog/2016/05/23/external-workspace-manager-plugin/","strippedHtml":"About myself\n\nMy name is Alexandru Somai.\nI’m following a major in Software Engineering at the Babes-Bolyai University of Cluj-Napoca, Romania.\nI have more than two years hands-on experience working in Software Development.\n\nI enjoy writing code in Java, Groovy and JavaScript.\nThe technologies and frameworks that I’m most familiar with are: Spring Framework, Spring Security, Hibernate,\nJMS, Web Services, JUnit, TestNG, Mockito.\nAs build tools and continuous integration, I’m using Maven and Jenkins.\nI’m a passionate software developer who is always learning, always looking for new challenges.\nI want to start contributing to the open source community and Google Summer of Code is a starting point for me.\n\nProject summary\n\nCurrently, Jenkins’ build workspace may become very large in size due to the fact that some compilers generate\nvery large volumes of data.\nThe existing plugins that share the workspace across builds are able to do this by copying the files from\none workspace to another, process which is inefficient.\nA solution is to have a Jenkins plugin that is able to manage and reuse the same workspace between multiple builds.\n\nAs part of the Google Summer of Code 2016 I will be working on\nthe External Workspace Manager plugin.\nMy mentors for this project are Oleg Nenashev\nand Martin d’Anjou.\nThis plugin aims to provide an external workspace management system.\nIt should facilitate workspace share and reuse across multiple Jenkins jobs.\nIt should eliminate the need to copy, archive or move files.\nThe plugin will be written for Pipeline jobs.\n\nUsage\n\nPrerequisites\n\nMultiple physical disks accessible from controller.\n\nThe same physical disks must be accessible from Jenkins Nodes (renamed to Agents in Jenkins 2.0).\n\nIn the Jenkins global configuration, define a disk pool (or many) that will contain the physical disks.\n\nIn each Node configuration, define the mounting point from the current node to each physical disk.\n\nThe following diagram gives you an overview of how an External Workspace Manager configuration may look like:\n\nExample one\n\nLet’s assume that we have one Jenkins job. In this job, we want to use the same workspace on multiple Jenkins nodes.\nOur pipeline code may look like this:\n\nstage ('Stage 1. Allocate workspace')\ndef extWorkspace = exwsAllocate id: 'diskpool1'\n\nnode ('linux') {\n    exws (extWorkspace) {\n        stage('Stage 2. Build on the build server')\n        git url: '...'\n        sh 'mvn clean install'\n    }\n}\n\nnode ('test') {\n    exws (extWorkspace) {\n        stage('Stage 3. Run tests on a test machine')\n        sh 'mvn test'\n    }\n}\n\nNote: The stage() steps are optional from the External Workspace Manager plugin perspective.\n\nStage 1. Allocate workspace\n\nThe exwsAllocate step selects a disk from diskpool1\n(default behavior: the disk with the most available size).\nOn that disk, let’s say disk1, it allocates a directory.\nThe computed directory path is: /physicalPathOnDisk/$JOB_NAME/$BUILD_NUMBER.\n\nFor example, Let’s assume that the $JOB_NAME is integration and the $BUILD_NUMBER is 14.\nThen, the resulting path is: /jenkins-project/disk1/integration/14.\n\nStage 2. Build on the build server\n\nAll the nodes labeled linux must have access to the disks defined in the disk pool.\nIn the Jenkins Node configurations we have defined the local paths that are the mounting points to each disk.\n\nThe exws step concatenates the node’s local path with the path returned by the exwsAllocate step.\nIn our case, the node labeled linux has its local path to disk1 defined as: /linux-node/disk1/.\nSo, the complete workspace path is: /linux-node/disk1/jenkins-project/disk1/integration/14.\n\nStage 3. Run tests on a test machine\n\nFurther, we want to run our tests on a different node, but we want to reuse the previously created workspace.\n\nIn the node labeled test we have defined the local path to disk1 as: /test-node/disk1/.\nBy applying the exws step, our tests will be able to run in the same workspace as the build.\nTherefore, the path is: /test-node/disk1/jenkins-project/disk1/integration/14.\n\nExample two\n\nLet’s assume that we have two Jenkins jobs, one called upstream and the other one called downstream.\nIn the upstream job, we clone the repository and build the project, and in the downstream job we run the tests.\nIn the downstream job we don’t want to clone and re-build the project, we need to use the same\nworkspace created in the upstream job.\nWe have to be able to do so without copying the workspace content from one location to another.\n\nThe pipeline code in the upstream job is the following:\n\nstage ('Stage 1. Allocate workspace in the upstream job')\ndef extWorkspace = exwsAllocate id: 'diskpool1'\n\nnode ('linux') {\n    exws (extWorkspace) {\n        stage('Stage 2. Build in the upstream job')\n           git url: '...'\n           sh 'mvn clean install'\n    }\n}\n\nAnd the downstream 's pipeline code is:\n\nstage ('Stage 3. Allocate workspace in the downstream job')\ndef extWorkspace = exwsAllocate id: 'diskpool1', upstream: 'upstream'\n\nnode ('test') {\n    exws (extWorkspace) {\n        stage('Stage 4. Run tests in the downstream job')\n        sh 'mvn test'\n    }\n}\n\nStage 1. Allocate workspace in the upstream job\n\nThe functionality is the same as in example one - stage 1.\nIn our case, the allocated directory on the physical disk is: /jenkins-project/disk1/upstream/14.\n\nStage 2. Build in the upstream job\n\nSame functionality as example one - stage 2.\nThe final workspace path is: /linux-node/disk1/jenkins-project/disk1/upstream/14.\n\nStage 3. Allocate workspace in the downstream job\n\nBy passing the upstream parameter to the exwsAllocate step,\nit selects the most recent stable upstream workspace (default behavior).\nThe workspace path pattern is like this: /physicalPathOnDisk/$UPSTREAM_NAME/$MOST_RECENT_STABLE_BUILD.\nLet’s assume that the last stable build number is 12, then the resulting path is:\n/jenkins-project/disk1/upstream/12.\n\nStage 4. Run tests in the downstream job\n\nThe exws step concatenates the node’s local path with the path returned by the exwsAllocate step in stage 3.\nIn this scenario, the complete path for running tests is: /test-node/disk1/jenkins-project/disk1/upstream/12.\nIt will reuse the workspace defined in the upstream job.\n\nAdditional details\n\nYou may find the complete project proposal, along with the design details, features, more examples and use cases,\nimplementation ideas and milestones in the design document.\nThe plugin repository will be available on GitHub.\n\nA prototype version of the plugin should be available in late June and the releasable version in late August.\nI will be holding plugin functionality demos within the community.\n\nI do appreciate any feedback.\nYou may add comments in the design document.\nIf you are interested to have a verbal conversation, feel free to join our regular meetings on Mondays at\n12:00 PM UTC\non the Jenkins hangout.\nI will be posting updates from time to time about the plugin status on the\nJenkins developers mailing list.\n\nLinks\n\nDesign document\n\nGSoC program\n\nJenkins GSoC Page\n\nProject repository","title":"GSoC Project Intro: External Workspace Manager Plugin","tags":["pipeline","plugins","gsoc"],"authors":[{"avatar":null,"blog":null,"github":"alexsomai","html":"","id":"alexsomai","irc":null,"linkedin":null,"name":"Alexandru Somai","slug":"/blog/author/alexsomai","twitter":"alex_somai"}]}},{"node":{"date":"2016-04-21T00:00:00.000Z","id":"d9ce8340-eedf-5dba-874d-c9eba3f8e717","slug":"/blog/2016/04/21/dsl-plugins/","strippedHtml":"In this post I will show how you can make your own DSL extensions and distribute\nthem as a plugin, using Pipeline Script.\n\nA quick refresher\n\nPipeline has a well kept secret: the ability to add your own DSL\nelements. Pipeline is itself a DSL, but you can extend it.\n\nThere are 2 main reasons I can think you may want to do this:\n\nYou want to reduce boilerplate by encapsulating common snippets/things you do\nin one DSL statement.\n\nYou want to provide a DSL that provides a prescriptive way that your builds\nwork - uniform across your organisations Jenkinsfiles.\n\nA DSL could look as simple as\n\nacmeBuild {\n    script = \"./bin/ci\"\n    environment = \"nginx\"\n    team = \"evil-devs\"\n    deployBranch = \"production\"\n}\n\nThis could be the entirety of your Jenkinsfile!\n\nIn this \"simple\" example, it could actually be doing a multi stage build with\nretries, in a specified docker container, that deploys only from the production\nbranch.  Detailed notifications are sent to the right team on important events\n(as defined by your org).\n\nTraditionally this is done via the\nglobal\nlibrary.  You take a snippet of DSL you want to want to make into a DSL, and\ndrop it in the git repo that is baked into Jenkins.\n\nA great trivial\nexample\nis this:\n\njenkinsPlugin {\n    name = 'git'\n}\n\nWhich is enabled by git pushing the following into vars/jenkinsPlugin.groovy\n\nThe name of the file is the name of the DSL expression you use in the Jenkinsfile\n\ndef call(body) {\n    def config = [:]\n    body.resolveStrategy = Closure.DELEGATE_FIRST\n    body.delegate = config\n    body()\n\n    // This is where the magic happens - put your pipeline snippets in here, get variables from config.\n    node {\n        git url: \"https://github.com/jenkinsci/${config.name}-plugin.git\"\n        sh \"mvn install\"\n        mail to: \"...\", subject: \"${config.name} plugin build\", body: \"...\"\n    }\n}\n\nYou can imagine many more pipelines, or even archetypes/templates of pipelines\nyou could do in this way, providing a really easy Jenkinsfile syntax for your\nusers.\n\nMaking it a plugin\n\nUsing the global DSL library is a handy thing if you have a single Jenkins, or\nwant to keep the DSLs local to a Jenkins instance.  But what if you want to\ndistribute it around your org, or, perhaps it is general purpose enough you want\nto share it with the world?\n\nWell this is possible, by wrapping it in a plugin. You use the same pipeline\nsnippet tricks you use in the global lib, but put it in the dsl directory of a\nplugin.\n\nMy simple\nbuild plugin shows how it is done.  To make your own plugin:\n\nCreate a new plugin project, either fork the simple build one, or add a\ndependency to it in your pom.xml / build.gradle file\n\nPut your dsl in the resources directory in a similar fashion to\nthis\n(note the \"package dsl\" declaration at the top)\n\nCreate the equivalent extension that just points to the DSL by name like\nthis\nThis is mostly \"boiler plate\" but it tells Jenkins there is a GlobalVariable extension available when Pipelines run.\n\nDeploy it to an Jenkins Update Center to share with your org, or everyone!\n\nThe advantage of delivering this DSL as a plugin is that it has a version (you\ncan also put tests in there), and distributable just like any other plugin.\n\nFor the more advanced, Andrew Bayer has a Simple\nTravis Runner plugin that\ninterprets and runs\ntravis.yml files which is also implemented in pipeline.\n\nSo, approximately, you can build plugins for pipeline that extend pipeline, in\npipeline script (with a teeny bit of boiler plate).\n\nEnjoy!","title":"Making your own DSL with plugins, written in Pipeline script","tags":["jenkins","dsl","pipeline","plugins"],"authors":[{"avatar":{"childImageSharp":null},"blog":null,"github":"michaelneale","html":"<div class=\"paragraph\">\n<p>Michael is a CD enthusiast with a interest in User Experience.\nHe is a co-founder of CloudBees and a long time OSS developer, and can often be found\nlurking around the jenkins-dev mailing list or #jenkins on irc (same nick as twitter name).\nBefore CloudBees he worked at Red Hat.</p>\n</div>","id":"michaelneale","irc":null,"linkedin":null,"name":"Michael Neale","slug":"/blog/author/michaelneale","twitter":"michaelneale"}]}},{"node":{"date":"2016-04-11T00:00:00.000Z","id":"e9d4f87e-38f5-5fa5-8f99-7062f5a29099","slug":"/blog/2016/04/11/jenkins-plugins-security-update/","strippedHtml":"The Script Security Plugin and the Extra Columns Plugin were updated today to fix medium-severity security vulnerabilities. For detailed information about the security content of these updates, see the security advisory.\n\nSubscribe to the jenkinsci-advisories mailing list to receive important notifications related to Jenkins security.","title":"Security fixes in Script Security Plugin and Extra Columns Plugin","tags":["plugins","security"],"authors":[{"avatar":null,"blog":null,"github":"daniel-beck","html":"<div class=\"paragraph\">\n<p>Daniel is a Jenkins core maintainer and, as security officer, leads the <a href=\"/security/#team\">Jenkins security team</a>.\nHe sometimes contributes to developer documentation and project infrastructure.</p>\n</div>","id":"daniel-beck","irc":null,"linkedin":null,"name":"Daniel Beck","slug":"/blog/author/daniel-beck","twitter":null}]}},{"node":{"date":"2015-11-01T00:00:00.000Z","id":"5c931f6f-0e08-5295-b4bb-ee8fc4aaf3b6","slug":"/blog/2015/11/01/adopt-a-plugin/","strippedHtml":"With more than a thousand public plugins in the Jenkins community now, it should come as no surprise that some of them are no longer actively maintained. Plugin authors move on when they change jobs, or lose interest in the plugin, and that’s fine. Plugins are hosted on the Jenkins project infrastructure after all, and when a maintainer moves on, others can continue their work.\n\nThe major problem of course is that it’s often difficult to tell whether a plugin is still maintained (and there’s just not a lot going on), or whether its maintainer has lost interest. Most plugins don’t get released every few weeks, or even every few months, and still do their job just fine.\n\nTo connect plugins that aren’t actively maintained with potential maintainers, we recently implemented the \"Adopt-a-plugin\" initiative: We built a list of plugins that are up for \"adoption\", and display a prominent message on the plugins' wiki pages. Anyone interested in taking over as a plugin maintainer can then contact us, and we’ll set you up.\n\nAre you interested in becoming a plugin maintainer? Maybe one of your favorite plugins isn’t actively maintained right now. Check out the Adopt a Plugin page for more details on this program, and a list of plugins that would benefit from your help.","title":"Adopt a plugin!","tags":["development","meta","plugins"],"authors":[{"avatar":null,"blog":null,"github":"daniel-beck","html":"<div class=\"paragraph\">\n<p>Daniel is a Jenkins core maintainer and, as security officer, leads the <a href=\"/security/#team\">Jenkins security team</a>.\nHe sometimes contributes to developer documentation and project infrastructure.</p>\n</div>","id":"daniel-beck","irc":null,"linkedin":null,"name":"Daniel Beck","slug":"/blog/author/daniel-beck","twitter":null}]}},{"node":{"date":"2015-10-02T00:00:00.000Z","id":"e12d694f-12f6-5a91-8fb8-5f594f21f900","slug":"/blog/2015/10/02/winners-of-docker-global-hack-day-3-are/","strippedHtml":"+\nimage:https://jenkins-ci.org/sites/default/files/images/docker-hack-day_0.preview.jpg[image,width=320] +\n\n+\n+\n\nOver 2,000 members of the Docker community attended Docker Hack Day events around the world. One of the forty-two Docker Hacks has some familiar names attached…​\n\n+\n+\n\nNicolas De Loof and Yoann Dubreuil from Docker Rennes, who are also active in our community, waved the Jenkins flag in this event and produced Jenkins docker agents plugin.\n\n+\n+\n\n+\nThis plugin lets you run builds inside containers, and in that sense it's similar to https://wiki.jenkins.io/display/JENKINS/Docker+Plugin[the Docker plugin] and https://wiki.jenkins.io/display/JENKINS/CloudBees+Docker+Custom+Build+Environment+Plugin[the Docker custom build environment plugin]. But internally it uses a quite interesting approach. +\n\n+\n+\n\n+\nThis fresh new implementation relies on a set of docker containers (aka ‘pod’) to setup a build executor, letting development team customize the build environment for their need without any constraint or prerequisite, and relying on docker containers to host test resources.\n\n+\n+\n\n+\nThis project https://blog.docker.com/2015/09/docker-global-hack-day-3-winners/[won the 3rd place in the Freestyle category of Docker Hack Day]. Congratulations to Nicolas and Yoann on their win! Jenkins + Docker is a winning pair and this plugin will make a huge difference in your projects.","title":"Winners of Docker Global Hack Day #3 are...","tags":["general","plugins","jenkinsci","docker"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"","id":"hinman","irc":null,"linkedin":null,"name":"Hannah Inman","slug":"/blog/author/hinman","twitter":null}]}},{"node":{"date":"2015-08-31T00:00:00.000Z","id":"6428552e-8c9c-54a0-ac04-d0926fae9b71","slug":"/blog/2015/08/31/plugin-spotlight-version-column-plugin/","strippedHtml":"Most Jenkins controllers with a distributed build configuration will leverage nodes that run a agent.jar to start an agent. Regardless of whether the agent.jar is launched through a Java Web Start or SSH launcher, the jar will be copied from https://yourserver:port/jnlpJars/agent.jar to the build node. Keeping this jar up to date ensures that it picks up the newest features in a more recent release, such as the self-restart feature to keep agent JVMs “clean” and to automatically reconnect to their controller. Additionally, newer versions of this component may fix bugs or implement newer protocol versions with various improvements.\n\nWhat is the Version Column Plugin?\n\nLaunch methods designed to pull the latest agent.jar are not always reliable and some launch methods don’t even try to update the agent.jar. Therefore it can be useful to see what agent.jar version is running on a given build node and take offline any nodes which fails to update to the latest version of the jar.\n\nThe Version Column Plugin allows Jenkins controllers to do just this, adding a new column to the “Manage Nodes” view and a new option for version enforcement on the node configuration screen.\n\nGetting started\n\nAfter installing the Version Column Plugin, navigate to the list of nodes in your Jenkins instance by clicking Build Executor Status in the executors widget below the side panel on the Jenkins home page.\n\nIf the plugin installed successfully, you will see a new column simply called “Version”. This column displays the version of the agent.jar that each build node is using.\n\nThis column is simply displaying the versions, so enforcement of agent.jar versions will need to be configured elsewhere. To activate this, click on the “Configure” link in the node manager’s left-hand menu.\n\nYou will then see a set of options for agents. To activate version enforcement, check the “Version” box and apply your changes.\n\nWhen you update Jenkins, there’s a chance it’ll come with a new version of agent.jar. Now if the agent.jar on a particular agent doesn’t get updated automatically, the controller will take it offline and show a warning next to the out-of-date agent version number:\n\nThe Version Column Plugin is available for download in the Jenkins plugin manager or from its wiki page.","title":"Plugin Spotlight: Version Column Plugin","tags":["general","plugins"],"authors":[{"avatar":null,"blog":null,"github":"daniel-beck","html":"<div class=\"paragraph\">\n<p>Daniel is a Jenkins core maintainer and, as security officer, leads the <a href=\"/security/#team\">Jenkins security team</a>.\nHe sometimes contributes to developer documentation and project infrastructure.</p>\n</div>","id":"daniel-beck","irc":null,"linkedin":null,"name":"Daniel Beck","slug":"/blog/author/daniel-beck","twitter":null}]}},{"node":{"date":"2015-08-20T00:00:00.000Z","id":"d1860023-e5b8-5452-a7f4-ba23d5ca66a3","slug":"/blog/2015/08/20/upcoming-office-hour-on-kubernetes/","strippedHtml":"Nicolas De Loof will host an office hour next Wednesday 11 AM PDT on integrating Kubernetes with Jenkins. Kubernetes is an open-source project by Google that provides a platform for managing Docker containers as a cluster.\n\nDuring this session, Nicolas will introduce Kubernetes, explain how it can benefit Jenkins and demonstrate the Kubernetes Plugin.\nThen he will discuss the design of the Kubernetes plugin and plans he has for future improvements.\n\nParticipate in the Hangout on Air or watch live on YouTube.","title":"Upcoming office hour on Kubernetes","tags":["general","plugins","screencast","tutorial","docker"],"authors":[{"avatar":null,"blog":null,"github":"daniel-beck","html":"<div class=\"paragraph\">\n<p>Daniel is a Jenkins core maintainer and, as security officer, leads the <a href=\"/security/#team\">Jenkins security team</a>.\nHe sometimes contributes to developer documentation and project infrastructure.</p>\n</div>","id":"daniel-beck","irc":null,"linkedin":null,"name":"Daniel Beck","slug":"/blog/author/daniel-beck","twitter":null}]}}]}},"pageContext":{"tag":"plugins","limit":8,"skip":72,"numPages":14,"currentPage":10}},
    "staticQueryHashes": ["3649515864"]}