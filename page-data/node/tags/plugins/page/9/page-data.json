{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/plugins/page/9",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2016-08-09T00:00:00.000Z","id":"e7eb5984-a870-5548-985b-25b9210fb2a0","slug":"/blog/2016/08/09/ewm-beta-version/","strippedHtml":"This blog post is a continuation of the External Workspace Manager Plugin related posts, starting with\nthe introductory blog post, and followed by\nthe alpha version release announcement.\n\nAs the title suggests, the beta version of the External Workspace Manager Plugin was launched!\nThis means that it’s available only in the\nExperimental Plugins Update Center.\n\nTake care when installing plugins from the Experimental Update Center, since they may change in\nbackward-incompatible ways.\nIt’s advisable not to use it for Jenkins production environments.\n\nThe plugin’s repository is on GitHub.\nThe complete plugin’s documentation can be accessed\nhere.\n\nWhat’s new\n\nBellow is a summary of the features added so far, since the alpha version.\n\nMultiple upstream run selection strategies\n\nIt has support for the\nRun Selector Plugin (which is still in beta),\nso you can provide different run selection strategies when allocating a disk from the upstream job.\n\nLet’s suppose that we have an upstream job that clones the repository and builds the project:\n\ndef extWorkspace = exwsAllocate 'diskpool1'\n\nnode ('linux') {\n    exws (extWorkspace) {\n        checkout scm\n        sh 'mvn clean install -DskipTests'\n    }\n}\n\nIn the downstream job, we run the tests on a different node, but we reuse the same workspace as the previous job:\n\ndef run = selectRun 'upstream'\ndef extWorkspace = exwsAllocate selectedRun: run\n\nnode ('test') {\n    exws (extWorkspace) {\n        sh 'mvn test'\n    }\n}\n\nThe selectRun in this example selects the last stable build from the upstream job.\nBut, we can be more explicit, and select a specific build number from the upstream job.\n\ndef run = selectRun 'upstream',\n selector: [$class: 'SpecificRunSelector', buildNumber: UPSTREAM_BUILD_NUMBER]\ndef extWorkspace = exwsAllocate selectedRun: run\n// ...\n\nWhen the selectedRun parameter is given to the exwsAllocate step, it will allocate the same workspace that was\nused by that run.\n\nThe Run Selector Plugin has several run selection strategies that are briefly explained\nhere.\n\nAutomatic workspace cleanup\n\nProvides an automatic workspace cleanup by integrating the\nWorkspace Cleanup Plugin.\nFor example, if we need to delete the workspace only if the build has failed, we can do the following:\n\ndef extWorkspace = exwsAllocate diskPoolId: 'diskpool1'\n\nnode ('linux') {\n    exws (extWorkspace) {\n        try {\n            checkout scm\n            sh 'mvn clean install'\n        } catch (e) {\n            currentBuild.result = 'FAILURE'\n            throw e\n        } finally {\n            step ([$class: 'WsCleanup', cleanWhenFailure: false])\n        }\n    }\n}\n\nMore workspace cleanup examples can be found at this\nlink.\n\nCustom workspace path\n\nAllows the user to specify a custom workspace path to be used when allocating workspace on the disk.\nThe plugin offers two alternatives for doing this:\n\nby defining a global workspace template for each Disk Pool\n\nThis can be defined in the Jenkins global config, External Workspace Definitions section.\n\nby defining a custom workspace path in the Pipeline script\n\nWe can use the Pipeline DSL to compute the workspace path.\nThen we pass this path as input parameter to the exwsAllocate step.\n\ndef customPath = \"${env.JOB_NAME}/${PULL_REQUEST_NUMBER}/${env.BUILD_NUMBER}\"\ndef extWorkspace = exwsAllocate diskPoolId: 'diskpool1', path: customPath\n// ...\n\nFor more details see the afferent\ndocumentation page.\n\nDisk Pool restrictions\n\nThe plugin comes with Disk Pool restriction strategies.\nIt does this by using the restriction capabilities provided by the\nJob Restrictions Plugin.\n\nFor example, we can restrict a Disk Pool to be allocated only if the Jenkins job in which it’s allocated was triggered\nby a specific user:\n\nOr, we can restrict the Disk Pool to be allocated only for those jobs whose name matches a well defined pattern:\n\nWhat’s next\n\nCurrently there is ongoing work for providing flexible disk allocation strategies.\nThe user will be able to define a default disk allocation strategy in the Jenkins global config.\nSo for example, we want to select the disk with the most usable space as default allocation strategy:\n\nIf needed, this allocation strategy may be overridden in the Pipeline code.\nLet’s suppose that for a specific job, we want to allocate the disk with the highest read speed.\n\ndef extWorkspace = exwsAllocate diskPoolId: 'diskpool1', strategy: fastestRead()\n// ...\n\nWhen this feature is completed, the plugin will enter a final testing phase.\nIf all goes to plan, a stable version should be released in about two weeks.\n\nIf you have any issues in setting up or using the plugin, please feel free to ask me on the plugin’s Gitter\nchat.\nAny feedback is welcome, and you may provide it either on the Gitter chat, or on\nJira by using the external-workspace-manager-plugin component.\n\nLinks\n\nProject repository\n\nProject intro blog post\n\nAlpha version announcement\n\nGSoC page\n\nJenkins GSoC Page","title":"GSoC: External Workspace Manager for Pipeline. Beta release is available","tags":["pipeline","plugins","external-workspace-manager","gsoc"],"authors":[]}},{"node":{"date":"2016-08-08T00:00:00.000Z","id":"649a2e8e-4f2f-56eb-99f1-8897de882a49","slug":"/blog/2016/08/08/docker-pipeline-environments/","strippedHtml":"This is a guest post by Michael Neale, long time open\nsource developer and contributor to the Blue Ocean\nproject.\n\nIf you are running parts of your pipeline on Linux, possibly the easiest way to\nget a clean reusable environment is to use:\nCloudBees\nDocker Pipeline plugin.\n\nIn this short post I wanted to show how you can avoid installing stuff on the agents, and have per project, or even per branch, customized build environments.\nYour environment, as well as your pipeline is defined and versioned alongside your code.\n\nI wanted to use the Blue Ocean project as an\nexample of a\nproject that uses the CloudBees Docker Pipeline plugin.\n\nEnvironment and Pipeline for JavaScript components\n\nThe Blue Ocean project has a few moving parts, one of\nwhich is called the \"Jenkins Design Language\".  This is a grab bag of re-usable\nCSS, HTML, style rules, icons and JavaScript components (using React.js) that\nprovide the look and feel for Blue Ocean.\n\nJavaScript and Web Development being what it is in 2016, many utilities are\nneed to assemble a web app.  This includes npm and all that it needs, less.js\nto convert Less to CSS, Babel to \"transpile\" versions of JavaScript to other\ntypes of JavaScript (don’t ask) and more.\n\nWe could spend time installling nodejs/npm on the agents, but why not just use\nthe official off the shelf docker image\nfrom Docker Hub?\n\nThe only thing that has to be installed and run on the build agents is the Jenkins agent, and a docker daemon.\n\nA simple pipeline using this approach would be:\n\nnode {\n        stage \"Prepare environment\"\n          checkout scm\n          docker.image('node').inside {\n            stage \"Checkout and build deps\"\n                sh \"npm install\"\n\n            stage \"Test and validate\"\n                sh \"npm install gulp-cli && ./node_modules/.bin/gulp\"\n          }\n}\n\nThis uses the stock \"official\" Node.js image from the Docker Hub, but doesn’t let us customize much about the environment.\n\nCustomising the environment, without installing bits on the agent\n\nBeing the forward looking and lazy person that I am, I didn’t want to have to\ngo and fish around for a Docker image every time a developer wanted something\nspecial installed.\n\nInstead, I put a Dockerfile in the root of the repo, alongside the Jenkinsfile :\n\nThe contents of the Dockerfile can then define the exact environment needed\nto build the project.  Sure enough, shortly after this, someone came along\nsaying they wanted to use Flow from Facebook (A\ntypechecker for JavaScript).  This required an additional native component to\nwork (via apt-get install).\n\nThis was achieved via a\npull\nrequest to both the Jenkinsfile and the Dockerfile at the same time.\n\nSo now our environment is defined by a Dockerfile with the following contents:\n\n# Lets not just use any old version but pick one\nFROM node:5.11.1\n\n# This is needed for flow, and the weirdos that built it in ocaml:\nRUN apt-get update && apt-get install -y libelf1\n\nRUN useradd jenkins --shell /bin/bash --create-home\nUSER jenkins\n\nThe Jenkinsfile pipeline now has the following contents:\n\nnode {\n    stage \"Prepare environment\"\n        checkout scm\n        def environment  = docker.build 'cloudbees-node'\n\n        environment.inside {\n            stage \"Checkout and build deps\"\n                sh \"npm install\"\n\n            stage \"Validate types\"\n                sh \"./node_modules/.bin/flow\"\n\n            stage \"Test and validate\"\n                sh \"npm install gulp-cli && ./node_modules/.bin/gulp\"\n                junit 'reports/**/*.xml'\n        }\n\n    stage \"Cleanup\"\n        deleteDir()\n}\n\nEven hip JavaScript tools can emit that weird XML format that test\nreporters can use, e.g. the junit result archiver.\n\nThe main change is that we have docker.build being called to produce the\nenvironment which is then used.  Running docker build is essentially a\n\"no-op\" if the image has already been built on the agent before.\n\nWhat’s it like to drive?\n\nWell, using Blue Ocean, to build Blue Ocean, yields a pipeline that visually\nlooks like this (a recent run I screen capped):\n\nThis creates a pipeline that developers can tweak on a pull-request basis,\nalong with any changes to the environment needed to support it, without having\nto install any packages on the agent.\n\nWhy not use docker commands directly?\n\nYou could of course just use shell commands to do things with Docker directly,\nhowever, Jenkins Pipeline keeps track of Docker images used in a Dockerfile\nvia the \"Docker Fingerprints\" link (which is good, should that image need to\nchange due to a security patch).\n\nLinks\n\nThe project used as as an example is here\n\nThe pipeline is defined by the Jenkinsfile\n\nThe environment is defined by the Dockerfile\n\nRead more on Docker Pipeline","title":"Don't install software, define your environment with Docker and Pipeline","tags":["pipeline","plugins","blueocean","ux","javascript","nodejs"],"authors":[]}},{"node":{"date":"2016-07-18T00:00:00.000Z","id":"523da562-6777-53bd-978f-45ab4cb9092c","slug":"/blog/2016/07/18/pipeline-notifications/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nRather than sitting and watching Jenkins for job status, I want Jenkins to send\nnotifications when events occur.  There are Jenkins plugins for\nSlack,\nHipChat,\nor even email\namong others.\n\nNote: Something is happening!\n\nI think we can all agree getting notified when events occur is preferable to\nhaving to constantly monitor them just in case.  I’m going to continue from\nwhere I left off in my\nprevious post with the\nhermann project.  I added a Jenkins\nPipeline with an HTML publisher for code coverage. This week, I’d like to make\nJenkins to notify me when builds start and when they succeed or fail.\n\nSetup and Configuration\n\nFirst, I select targets for my notifications. For this blog post, I’ll use sample\ntargets that I control.  I’ve created Slack and HipChat organizations called\n\"bitwiseman\", each with one member - me.  And for email I’m running a Ruby SMTP server called\nmailcatcher, that is perfect for local testing\nsuch as this.  Aside for these concessions, configuration would be much the\nsame in a non-demo situation.\n\nNext, I install and add server-wide configuration for the\nSlack,\nHipChat,\nand Email-ext\nplugins.  Slack and HipChat use API tokens - both products have integration\npoints on their side that generate tokens which I copy into my Jenkins\nconfiguration. Mailcatcher SMTP runs locally. I just point Jenkins\nat it.\n\nHere’s what the Jenkins configuration section for each of these looks like:\n\nOriginal Pipeline\n\nNow I can start adding notification steps. The same as\nlast week, I’ll use the\nJenkins Pipeline Snippet Generator\nto explore the step syntax for the notification plugins.\n\nHere’s the base pipeline before I start making changes:\n\nstage 'Build'\n\nnode {\n  // Checkout\n  checkout scm\n\n  // install required bundles\n  sh 'bundle install'\n\n  // build and run tests with coverage\n  sh 'bundle exec rake build spec'\n\n  // Archive the built artifacts\n  archive (includes: 'pkg/*.gem')\n\n  // publish html\n  // snippet generator doesn't include \"target:\"\n  // https://issues.jenkins.io/browse/JENKINS-29711.\n  publishHTML (target: [\n      allowMissing: false,\n      alwaysLinkToLastBuild: false,\n      keepAll: true,\n      reportDir: 'coverage',\n      reportFiles: 'index.html',\n      reportName: \"RCov Report\"\n    ])\n}\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit 'https://github.com/reiseburo/hermann.git'.\n\nJob Started Notification\n\nFor the first change, I decide to add a \"Job Started\" notification.  The\nsnippet generator and then reformatting makes this straightforward:\n\nnode {\n\n  notifyStarted()\n\n  /* ... existing build steps ... */\n}\n\ndef notifyStarted() {\n  // send to Slack\n  slackSend (color: '#FFFF00', message: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\")\n\n  // send to HipChat\n  hipchatSend (color: 'YELLOW', notify: true,\n      message: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\"\n    )\n\n  // send to email\n  emailext (\n      subject: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\",\n      body: \"\"\" STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':\nCheck console output at \" ${env.JOB_NAME} [${env.BUILD_NUMBER}]\"\"\"\",\n      recipientProviders: [[$class: 'DevelopersRecipientProvider']]\n    )\n}\n\nSince Pipeline is a Groovy-based DSL, I can use\nstring interpolation\nand variables to add exactly the details I want in my notification messages. When\nI run this I get the following notifications:\n\nJob Successful Notification\n\nThe next logical choice is to get notifications when a job succeeds.  I’ll\ncopy and paste based on the notifyStarted method for now and do some refactoring\nlater.\n\nnode {\n\n  notifyStarted()\n\n  /* ... existing build steps ... */\n\n  notifySuccessful()\n}\n\ndef notifyStarted() { /* .. */ }\n\ndef notifySuccessful() {\n  slackSend (color: '#00FF00', message: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\")\n\n  hipchatSend (color: 'GREEN', notify: true,\n      message: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\"\n    )\n\n  emailext (\n      subject: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\",\n      body: \"\"\" SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':\nCheck console output at \" ${env.JOB_NAME} [${env.BUILD_NUMBER}]\"\"\"\",\n      recipientProviders: [[$class: 'DevelopersRecipientProvider']]\n    )\n}\n\nAgain, I get notifications, as expected.  This build is fast enough,\nsome of them are even on the screen at the same time:\n\nJob Failed Notification\n\nNext I want to add failure notification.  Here’s where we really start to see the power\nand expressiveness of Jenkins pipeline.  A Pipeline is a Groovy script, so as we’d\nexpect in any Groovy script, we can handle errors using try-catch blocks.\n\nnode {\n  try {\n    notifyStarted()\n\n    /* ... existing build steps ... */\n\n    notifySuccessful()\n  } catch (e) {\n    currentBuild.result = \"FAILED\"\n    notifyFailed()\n    throw e\n  }\n}\n\ndef notifyStarted() { /* .. */ }\n\ndef notifySuccessful() { /* .. */ }\n\ndef notifyFailed() {\n  slackSend (color: '#FF0000', message: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\")\n\n  hipchatSend (color: 'RED', notify: true,\n      message: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\"\n    )\n\n  emailext (\n      subject: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\",\n      body: \"\"\" FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':\nCheck console output at \" ${env.JOB_NAME} [${env.BUILD_NUMBER}]\"\"\"\",\n      recipientProviders: [[$class: 'DevelopersRecipientProvider']]\n    )\n}\n\nCode Cleanup\n\nLastly, now that I have it all working, I’ll do some refactoring. I’ll unify\nall the notifications in one method and move the final success/failure notification\ninto a finally block.\n\nstage 'Build'\n\nnode {\n  try {\n    notifyBuild('STARTED')\n\n    /* ... existing build steps ... */\n\n  } catch (e) {\n    // If there was an exception thrown, the build failed\n    currentBuild.result = \"FAILED\"\n    throw e\n  } finally {\n    // Success or failure, always send notifications\n    notifyBuild(currentBuild.result)\n  }\n}\n\ndef notifyBuild(String buildStatus = 'STARTED') {\n  // build status of null means successful\n  buildStatus = buildStatus ?: 'SUCCESS'\n\n  // Default values\n  def colorName = 'RED'\n  def colorCode = '#FF0000'\n  def subject = \"${buildStatus}: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\"\n  def summary = \"${subject} (${env.BUILD_URL})\"\n  def details = \"\"\" STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':\nCheck console output at \" ${env.JOB_NAME} [${env.BUILD_NUMBER}]\"\"\"\"\n\n  // Override default values based on build status\n  if (buildStatus == 'STARTED') {\n    color = 'YELLOW'\n    colorCode = '#FFFF00'\n  } else if (buildStatus == 'SUCCESS') {\n    color = 'GREEN'\n    colorCode = '#00FF00'\n  } else {\n    color = 'RED'\n    colorCode = '#FF0000'\n  }\n\n  // Send notifications\n  slackSend (color: colorCode, message: summary)\n\n  hipchatSend (color: color, notify: true, message: summary)\n\n  emailext (\n      subject: subject,\n      body: details,\n      recipientProviders: [[$class: 'DevelopersRecipientProvider']]\n    )\n}\n\nYou have been notified!\n\nI now get notified twice per build on three different channels.  I’m not sure I\nneed to get notified this much for such a short build.  However, for a longer\nor complex CD pipeline, I might want exactly that.  If needed, I could even\nimprove this to handle other status strings and call it as needed throughout\nmy pipeline.\n\nLinks\n\nSlack Plugin\n\nHipChat Plugin\n\nEmail-ext Plugin\n\nJenkins Pipeline Snippet Generator","title":"Sending Notifications in Pipeline","tags":["tutorial","pipeline","plugins","notifications","slack","hipchat","emailext"],"authors":[]}},{"node":{"date":"2016-07-01T00:00:00.000Z","id":"4848db1b-feac-54a0-8b3e-2a0f5e3fbfc6","slug":"/blog/2016/07/01/html-publisher-plugin/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nMost projects need more that just JUnit result reporting.  Rather than writing a\ncustom plugin for each type of report, we can use the\nHTML Publisher Plugin.\n\nLet’s Make This Quick\n\nI’ve found a Ruby project,\nhermann, I’d like to build using Jenkins Pipeline. I’d\nalso like to have the code coverage results published with each build job.  I could\nwrite a plugin to publish this data, but I’m in a bit of hurry and\nthe build already creates an HTML report file using SimpleCov\nwhen the unit tests run.\n\nSimple Build\n\nI’m going to use the\nHTML Publisher Plugin\nto add the HTML-formatted code coverage report to my builds.  Here’s a simple\npipeline for building the hermann\nproject.\n\nstage 'Build'\n\nnode {\n  // Checkout\n  checkout scm\n\n  // install required bundles\n  sh 'bundle install'\n\n  // build and run tests with coverage\n  sh 'bundle exec rake build spec'\n\n  // Archive the built artifacts\n  archive (includes: 'pkg/*.gem')\n}\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit 'https://github.com/reiseburo/hermann.git'.\n\nSimple enough, it builds, runs tests, and archives the package.\n\nNow I just need to add the step to publish the code coverage report.\nI know that rake spec creates an index.html file in the coverage directory.\nI’ve already installed the\nHTML Publisher Plugin.\nHow do I add the HTML publishing step to the pipeline?  The plugin page doesn’t\nsay anything about it.\n\nSnippet Generator to the Rescue\n\nDocumentation is hard to maintain and easy to miss, even more so in a system\nlike Jenkins with hundreds of plugins the each potential have one or more\ngroovy fixtures to add to the Pipeline.  The Pipeline Syntax\"Snippet Generator\" helps users\nnavigate this jungle by providing a way to generate a code snippet for any step using\nprovided inputs.\n\nIt offers a dynamically generated list of steps, based on the installed plugins.\nFrom that list I select the publishHTML step:\n\nThen it shows me a UI similar to the one used in job configuration.  I fill in\nthe fields, click \"generate\", and it shows me snippet of groovy generated from\nthat input.\n\nHTML Published\n\nI can use that snippet directly or as a template for further customization.\nIn this case, I’ll just reformat and copy it in at the end of my\npipeline.  (I ran into a minor bug\nin the snippet generated for this plugin step. Typing\nerror string in my search bar immediately found the bug and a workaround.)\n\n/* ...unchanged... */\n\n  // Archive the built artifacts\n  archive (includes: 'pkg/*.gem')\n\n  // publish html\n  // snippet generator doesn't include \"target:\"\n  // https://issues.jenkins.io/browse/JENKINS-29711.\n  publishHTML (target: [\n      allowMissing: false,\n      alwaysLinkToLastBuild: false,\n      keepAll: true,\n      reportDir: 'coverage',\n      reportFiles: 'index.html',\n      reportName: \"RCov Report\"\n    ])\n\n}\n\nWhen I run this new pipeline I am rewarded with an RCov Report link on left side,\nwhich I can follow to show the HTML report.\n\nI even added the keepAll setting to let I can also go back an look at reports on old jobs as\nmore come in.  As I said to to begin with, this is not as slick as what I\ncould do with a custom plugin, but it is much easier and works with any static\nHTML.\n\nLinks\n\nHTML Publisher Plugin\n\nJenkins Pipeline Snippet Generator","title":"Publishing HTML Reports in Pipeline","tags":["tutorial","pipeline","plugins","ruby"],"authors":[]}},{"node":{"date":"2016-06-30T00:00:00.000Z","id":"6dc0bcbf-e180-50ae-86bf-0a881c810b38","slug":"/blog/2016/06/30/ewm-alpha-version/","strippedHtml":"Currently it’s quite difficult to share and reuse the same workspace between multiple jobs and across nodes.\nThere are some possible workarounds for achieving this, but each of them has its own drawback,\ne.g. stash/unstash pre-made artifacts, Copy Artifacts plugin or advanced job settings.\nA viable solution for this problem is the External Workspace Manager plugin, which facilitates workspace share and\nreuse across multiple Jenkins jobs and nodes.\nIt also eliminates the need to copy, archive or move files.\nYou can learn more about the design and goals of the External Workspace Manager project in\nthis introductory blog post.\n\nI’d like to announce that an alpha version of the External Manager Plugin has been released!\nIt’s now public available for testing.\nTo be able to install this plugin, you must follow the steps from the Experimental Plugins Update Center\nblog post.\n\nPlease be aware that it’s not recommended to use the Experimental Update Center in production installations of\nJenkins, since it may break it.\n\nThe plugin’s wiki page may be accessed\nhere.\nThe documentation that helps you get started with this plugin may be found on the\nREADME page.\nTo get an idea of what this plugin does, which are the features implemented so far and to see a working demo of it,\nyou can watch my mid-term presentation that is available here.\nThe slides for the presentation are shared on\nGoogle Slides.\n\nMy mentors, Martin and Oleg,\nand I have set up public meetings related to this plugin.\nYou are invited to join our discussions if you’d like to get more insight about the project.\nThe meetings are taking place twice a week on the Jenkins hangout,\nevery Monday at\n12 PM UTC\nand every Thursday at\n5 PM UTC.\n\nIf you have any issues in setting up or using the plugin, please feel free to ask me on the plugin’s Gitter\nchat.\nThe plugin is open-source, having the repository on\nGitHub, and you may contribute to it.\nAny feedback is welcome, and you may provide it either on the Gitter chat, or on\nJira by using the external-workspace-manager-plugin component.\n\nLinks\n\nProject repository\n\nPlugin wiki page\n\nMid-term presentation\n\nProject intro blog post\n\nGSoC page\n\nJenkins GSoC Page","title":"GSoC: External Workspace Manager Plugin alpha version","tags":["pipeline","plugins","external-workspace-manager","gsoc"],"authors":[]}},{"node":{"date":"2016-06-16T00:00:00.000Z","id":"9d5b4fb7-1151-5470-985c-045b0dd79455","slug":"/blog/2016/06/16/parallel-test-executor-plugin/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nIn this blog post, I’ll show you how to speed up your pipeline by using the\nParallel Test Executor Plugin.\n\nSo much to do, so little time…​\n\nIn my career, I’ve helped many teams move to continuous integration and delivery. One problem\nwe always encounter is how to run all the tests needed to ensure high-quality\nchanges while still keeping pipeline times reasonable and changes flowing\nsmoothly. More tests mean greater confidence, but also longer wait times.\nBuild systems may or may not support running tests in parallel, but they still only use one\nmachine even while other lab machines sit idle. In these cases, parallelizing\ntest execution across multiple machines is a great way to speed up pipelines.\nThe Parallel Test Executor plugin lets us leverage Jenkins do just that with no\ndisruption to the rest of the build system.\n\nSerial Test Execution\n\nFor this post, I’ll be running a pipeline based on the\nJenkins Git Plugin. I’ve modified\nthe Jenkinsfile from that project to allow us to compare execution times to our\nlater changes, and I’ve truncated the \"mvn\" utility method since it remains\nunchanged.  You can find the original file\nhere.\n\nnode {\n  stage 'Checkout'\n  checkout scm\n\n  stage 'Build'\n\n  /* Call the Maven build without tests. */\n  mvn \"clean install -DskipTests\"\n\n  stage 'Test'\n  runTests()\n\n  /* Save Results. */\n  stage 'Results'\n\n  /* Archive the build artifacts */\n  archive includes: 'target/*.hpi,target/*.jpi'\n}\n\nvoid runTests(def args) {\n  /* Call the Maven build with tests. */\n  mvn \"install -Dmaven.test.failure.ignore=true\"\n\n  /* Archive the test results */\n  junit '**/target/surefire-reports/TEST-*.xml'\n}\n\n/* Run Maven */\nvoid mvn(def args) { /* ... */ }\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit 'https://github.com/jenkinsci/git-plugin.git'.\n\nThis is a Maven project, so the Jenkinsfile is pretty simple.\nI’ve split the Maven build into separate “Build” and “Test”\nstages. Maven doesn’t support this split very well, it wants to run all\nthe steps of the lifecycle in order every time. So, I have to call Maven twice:\nfirst using the “skipTests” property to do only build steps in the first call,\nand then a second time with out that property to run tests.\n\nOn my quad-core machine, executing this pipeline takes about 13 minutes and 30\nseconds.  Of that time, it takes 13 minutes to run about 2.7 thousand tests in\nserial.\n\nParallel Test Execution\n\nThis looks like an ideal project for parallel test execution: a short build\nfollowed by a large number of serially executed tests that consume the most of\nthe pipeline time. There are a number of things I could try to speed this up.\nFor example, I could modify test harness to look for ways to parallelize\nthe test execution on this single machine. Or I could try speed up the tests\nthemselves. Both of those can be time-consuming and both risk destabilizing the\ntests. I’d need to know more about the project to do it well.\n\nI’ll avoid that risk by using Jenkins and the\nParallel Test Executor Plugin to\nparallelize the tests across multiple nodes instead. This will isolate the tests\nfrom each other, while still giving us speed gains from parallel execution.\n\nThe plugin reads the list of tests from the results archived in the previous execution of this\njob and splits that list into a specified number of sublists. I can then use\nthose sublists to execute the tests in parallel, passing a different sublist to\neach node.\n\nLet’s look at how this changes the pipeline:\n\nnode { /* ...unchanged... */ }\n\nvoid runTests(def args) {\n  /* Request the test groupings.  Based on previous test results. */\n  /* see https://wiki.jenkins.io/display/JENKINS/Parallel+Test+Executor+Plugin and demo on github\n  /* Using arbitrary parallelism of 4 and \"generateInclusions\" feature added in v1.8. */\n  def splits = splitTests parallelism: [$class: 'CountDrivenParallelism', size: 4], generateInclusions: true\n\n  /* Create dictionary to hold set of parallel test executions. */\n  def testGroups = [:]\n\n  for (int i = 0; i }. */\n    /*     includes = whether list specifies tests to include (true) or tests to exclude (false). */\n    /*     list = list of tests for inclusion or exclusion. */\n    /* The list of inclusions is constructed based on results gathered from */\n    /* the previous successfully completed job. One additional record will exclude */\n    /* all known tests to run any tests not seen during the previous run.  */\n    testGroups[\"split-${i}\"] = {  // example, \"split3\"\n      node {\n        checkout scm\n\n        /* Clean each test node to start. */\n        mvn 'clean'\n\n        def mavenInstall = 'install -DMaven.test.failure.ignore=true'\n\n        /* Write includesFile or excludesFile for tests.  Split record provided by splitTests. */\n        /* Tell Maven to read the appropriate file. */\n        if (split.includes) {\n          writeFile file: \"target/parallel-test-includes-${i}.txt\", text: split.list.join(\"\\n\")\n          mavenInstall += \" -Dsurefire.includesFile=target/parallel-test-includes-${i}.txt\"\n        } else {\n          writeFile file: \"target/parallel-test-excludes-${i}.txt\", text: split.list.join(\"\\n\")\n          mavenInstall += \" -Dsurefire.excludesFile=target/parallel-test-excludes-${i}.txt\"\n        }\n\n        /* Call the Maven build with tests. */\n        mvn mavenInstall\n\n        /* Archive the test results */\n        junit '**/target/surefire-reports/TEST-*.xml'\n      }\n    }\n  }\n  parallel testGroups\n}\n\n/* Run Maven */\nvoid mvn(def args) { /* ... */ }\n\nThat’s it!  The change is significant but it is all encapsulated in this one\nmethod in the Jenkinsfile.\n\nGreat (ish) Success!\n\nHere’s the results for the new pipeline with parallel test execution:\n\nThe tests ran almost twice as fast, without changes outside pipeline.  Great!\n\nHowever, I used 4 test executors, so why am I not seeing a 4x? improvement.\nA quick review of the logs shows the problem: A small number of tests are taking up\nto 5 minutes each to complete! This is actually good news. It means that I\nshould be able to see further improvement in pipeline throughput just by refactoring\nthose few long running tests into smaller parts.\n\nConclusion\n\nWhile I would like to have seen closer to a 4x improvement to match to number\nof executors, 2x is still perfectly respectable. If I were working on a group of projects\nwith similar pipelines, I’d be completely comfortable reusing these same changes\non my other project and I’d expect to similar improvement without any disruption to\nother tools or processes.\n\nLinks\n\nhttps://wiki.jenkins.io/display/JENKINS/Parallel+Test+Executor+Plugin","title":"Faster Pipelines with the Parallel Test Executor Plugin","tags":["tutorial","pipeline","plugins"],"authors":[]}},{"node":{"date":"2016-06-10T00:00:00.000Z","id":"14ea0208-9b66-5e17-a4c1-d9b43525e9f4","slug":"/blog/2016/06/10/save-costs-with-ec2-spot-fleet/","strippedHtml":"This is a guest post by Aleksei Besogonov, Senior Software Developer at\nAmazon Web Services.\n\nEarlier this year, we published a case study on how\nLyft has used Amazon EC2 Spot instances to save 75% on their continuous delivery\ninfrastructure costs by simply changing four lines of code. Several other EC2 customers like Mozilla have\nalso reduced costs of their\ncontinuous integration, deployment and testing pipelines by up to 90% on Spot instances. You can view\nthe current savings on Spot instances over EC2 On-demand instances using the\nSpot Bid Advisor :\n\nAWS Spot instances are spare EC2 instances that you can bid on. While your Spot instances may be\nterminated when EC2’s spare capacity declines, you can automatically replenish these instances and\nmaintain your target capacity using\nEC2 Spot fleets. As each\ninstance type and Availability Zone provides an alternative capacity pool, you can select multiple\nsuch pools to launch the lowest priced instances currently available by launching a Spot\nfleet on the Amazon EC2 Spot Requests console\nor using the AWS CLI/SDK tools.\n\nIn this walkthrough, we’ll show you how to configure Jenkins to automatically scale a fleet of Spot\ninstances up or down depending on the number jobs to be completed.\n\nRequest an Amazon EC2 Spot fleet\n\nTo get started, login to Amazon EC2 console, and click on Spot Requests\nin the left hand navigation pane. Alternatively, you can directly login to\nAmazon EC2 Spot Requests console. Then click on the\nRequest Spot Instances button at the top of the dashboard.\n\nIn the Spot instance launch wizard, select the Request & Maintain option to request a Spot fleet that automatically\nprovisions the most cost-effective EC2 Spot instances, and replenishes them if interrupted. Enter an initial\ntarget capacity, choose an AMI, and select multiple instance types to automatically provision the lowest priced\ninstances available.\n\nOn the next page, ensure that you have selected a key pair, complete the launch wizard, and note the Spot\nfleet request ID.\n\nAmazon EC2 Spot fleet automates finding the lowest priced instances for you, and enables your Jenkins cluster\nto maintain the required capacity; so, you don’t need any bidding algorithms to provision the optimal Spot\ninstances over time.\n\nConfigure Jenkins\n\nInstall the Plugin\n\nFrom the Jenkins dashboard, select Manage Jenkins, and then click Manage Plugins. On the Available tab,\nsearch for and select the EC2 Fleet Jenkins Plugin. Then click the Install button.\n\nAfter the plugin installation is completed, select Manage Jenkins from the Jenkins dashboard, and\nclick Configure System. In the Cloud section, select Amazon Spot Fleet to add a new Cloud.\n\nConfigure AWS Credentials\n\nNext, we will configure the AWS and agent node credentials. Click the Add button next to AWS Credentials,\nselect Jenkins, and enter your AWS Access Key, secret, and ID.\n\nNext, click the Add button in the Spot fleet launcher to configure your agents with an SSH key.\nSelect Jenkins, and enter the username and private key (from the key pair you configured in your Spot fleet request)\nas shown below.\n\nConfirm that the AWS and SSH credentials you just added are selected. Then choose the region, and the Spot fleet\nrequest ID from the drop-down. You can also enter the maximum idle time before your cluster automatically scales\ndown, and the maximum cluster size that it can scale up to.\n\nSubmit Jobs and View Status\n\nAfter you have finished the previous step, you can view the EC2 Fleet Status in the left hand navigation pane on\nthe Jenkins dashboard. Now, as you submit more jobs, Jenkins will automatically scale your Spot fleet to add more\nnodes. You can view these new nodes executing jobs under the Build Executor Status.\nAfter the jobs are done, if the nodes remain free for the specified idle time (configured in the previous step),\nthen Jenkins releases the nodes, automatically scaling down your Spot fleet nodes.\n\nBuild faster and cheaper\n\nIf you have a story to share about your team or product, or have a question to ask, do leave a comment\nfor us; we’d love to connect with you!","title":"Save up to 90% of CI cost on AWS with Jenkins and EC2 Spot Fleet","tags":["aws","plugins","ec2"],"authors":[]}},{"node":{"date":"2016-06-01T00:00:00.000Z","id":"2ec37d75-53e9-562c-9bbb-de2a06ecbafd","slug":"/blog/2016/06/01/gsoc-automatic-plugin-documentation/","strippedHtml":"About me\n\nI am Cynthia Anyango from Nairobi, Kenya. I am a second year student at Maseno\nUniversity. I am currently specializing on Ruby on Rails and trying to learn\nPython. I recently started contributing to Open source projects.My major\ncontribution was at Mozilla, where I worked with the QA for Cloud services. I did\nmanual and automated tests for various cloud services. I wrote documentation\ntoo. Above that, I am competent and I am always passionate about what I get my\nhands on.\n\nProject summary\n\nCurrently Jenkins plugin documentation is being stored in Confluence. Sometimes\nthe documentation is scattered and outdated. In order to improve the situation we\nwould like to follow the documentation-as-code approach and to put docs to\nplugin repositories and then publish them on the project website using the\nawestruct engine. The project aims an implementation of a documentation\ncontinuous deployment flow powered by Jenkins and Pipeline Plugin.\n\nThe idea is to automatically pull in the README and other docs from GitHub, show\nchangelogs with versions and releases dates. I will be designing file templates\nthat will contain most of the  docs information that will be required from\nplugin developers. Initially the files will be written in\nAsciiDoc. Plugin developers will get a chance to\nreview the templates. The templates will be prototyped by various plugin\ndevelopers.\n\nThe docs that will be automatically pulled from github and will be published on\nJenkins.io under the Documentation section.\n\nMy mentors are R.Tyler and\nBaptiste Mathus\n\nI hope to achieve this by 25th June when we will be having our mid-term\nevaluations.\n\nI will update more on the progress.\n\nLinks\n\nGsoc Page\n\nJenkins Gsoc Page\n\nMailing List discussion on Jenkins-Developers\n\nMy blog on Medium","title":"GSOC Project Intro: Automatic Plugin Documentation","tags":["gsoc","plugins"],"authors":[]}}]}},"pageContext":{"tag":"plugins","limit":8,"skip":64,"numPages":14,"currentPage":9}},
    "staticQueryHashes": ["3649515864"]}