{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/plugins/page/8",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2017-02-08T00:00:00.000Z","id":"fe83f4ec-fd69-5daf-b26d-80c88853da21","slug":"/blog/2017/02/08/jenkins-datadog-plugin/","strippedHtml":"This is a guest post by Emily Chang, Technical Author at Datadog. A modified version of this article was originally posted on the\nDatadog blog.\n\nIf you’re using Jenkins to continuously integrate changes into your projects, it’s helpful to be able to quickly identify build failures and assess their impact on other components of your stack.\n\nDatadog’s plugin helps users monitor and alert on the performance of their Jenkins builds, right alongside the rest of their infrastructure and applications.\n\nAs shown in the out-of-the-box dashboard below, the Datadog plugin provides a bird’s-eye view of job history and trends. You can use Datadog to:\n\nSet alerts for important build failures\n\nIdentify trends in build durations\n\nCorrelate Jenkins events with performance metrics from other parts of your infrastructure in order to identify and resolve issues\n\nTrack Jenkins build status in real-time\n\nOnce you install the Datadog plugin, Jenkins activities (when a build starts, fails, or succeeds) will start appearing in your Datadog event stream. You will also see what percentage of builds failed within the same job, so that you can quickly spot which jobs are experiencing a higher rate of failure than others.\n\nRemember to blacklist any jobs you don’t want to track by indicating them in your plugin configuration.\n\nDatadog’s out-of-the-box Jenkins dashboard includes a status widget that displays the count of all jobs that have run in the past day, grouped by success or failure. To explore further, you can also click on the widget to view the individual jobs that have failed or succeeded in the past day.\n\nThe dashboard also displays the proportion of successful vs. failed builds, along with the total number of job runs completed over the past four hours.\n\nDatadog enables you to correlate Jenkins events with application performance metrics to investigate the root cause of an issue. For example, the screenshot below shows that average CPU on the app servers increased sharply after a Jenkins build was completed and deployed (indicated by the pink bar). Your team can use this information as a starting point to investigate if code changes in the corresponding release may be causing issues.\n\nVisualize job duration metrics\n\nEvery time a build is completed, the plugin collects the build duration as a metric that you can aggregate by job name or any other tag, and graph over time. In the screenshot below, we can view the average job durations in the past four hours, sorted in decreasing order:\n\nYou can also graph and visualize trends in build durations for each job by using Datadog’s robust_trend() linear regression function, as shown in the screenshot below. This graph indicates which jobs' durations are trending longer over time, so that you can investigate if there appears to be a problem. If you’re experimenting with changes to your CI pipeline, consulting this graph can help you track the effects of those changes over time.\n\nUse tags to monitor your Jenkins jobs\n\nTags add custom dimensions to your monitoring, so you can focus on what’s important to you right now.\n\nEvery Jenkins event, metric, and service check is auto-tagged with job, result, and branch (if applicable). You can also enable the optional node tag in the plugin settings.\n\nAs of version 0.5.0, the plugin supports custom tags. This update was developed by one of our open source contributors, Mads Nielsen. Many thanks to Mads for helping us implement this feature!\n\nYou can create custom tags for the name of the application you’re building, your particular team name (e.g. team=licorice), or any other info that matters to you. For example, if you have multiple jobs that perform nightly builds, you might want to create a descriptive tag that distinguishes them from other types of jobs.\n\nAs shown in the configuration settings above, you can add custom tags, formatted as key=value, in two ways:\n\nin a text file (saved in the workspace for the job)\n\nin a list of properties in the text box\n\nSet up the Datadog plugin\n\nThe Datadog plugin requires Jenkins 1.580.1 or newer.\n\nIn Jenkins, navigate to Manage Jenkins > Manage Plugins.\n\nSearch for Datadog Plugin and check the box to install it.\n\nIn Jenkins, go to Manage Jenkins > Configure System.\n\nScroll down to the Datadog Plugin section, and paste your API key in the text box. You can copy this from the API Keys page of your Datadog account. Click Test Key to confirm that the plugin recognizes your API key.\n\nSave your changes, and you’re all set!\n\nGet started\n\nIf you’re already using Datadog, you can start monitoring Jenkins jobs by following the instructions here to download the Datadog plugin. If you’re not using Datadog yet, here’s a 14-day free trial.","title":"Monitor Jenkins jobs with the Datadog plugin","tags":["plugins","monitoring"],"authors":[{"avatar":null,"blog":null,"github":"echang26","html":"","id":"echang26","irc":null,"linkedin":null,"name":"Emily Chang","slug":"/blog/authors/echang26","twitter":null}]}},{"node":{"date":"2017-02-06T00:00:00.000Z","id":"46885786-902f-55cb-b263-cae8df8f0611","slug":"/blog/2017/02/06/scm-api-2-take2/","strippedHtml":"In January we\nannounced the release of SCM API 2.0.\nAfter the original release was published we identified four new high-impact\nissues.  We decided to remove the new versions of the plugins from the update\ncenter until those issues could be resolved. The issues have now been resolved\nand the plugins are now available from the update center.\n\nSummary for busy Jenkins Administrators\n\nUpgrading should make multi-branch projects much better.  When you are ready to\nupgrade you must ensure that you upgrade all the required plugins.  If you miss\nsome, just upgrade them and restart to fix the issue. And of course, it’s\nalways a good idea to take a backup of your JENKINS_HOME before upgrading any\nplugins.\n\nIn the list below, version numbers in bold indicate a change from the\noriginal version in the\noriginal announcement\n\nFolders Plugin\n\n5.17 or newer\n\nSCM API Plugin\n\n2.0.2 or newer\n\nBranch API Plugin\n\n2.0.2 or newer\n\nGit Plugin\n\nThis depends on the exact release line of the Git plugin that you are using.\n\nFollowing the 2.6.x release line: 2.6.4 or newer\n\nFollowing the 3.0.x release line ( recommended): 3.0.4 or newer\n\nMercurial Plugin\n\n1.58 or newer\n\nGitHub Branch Source Plugin\n\n2.0.1 or newer\n\nBitBucket Branch Source Plugin\n\n2.0.2 or newer\n\nGitHub Organization Folders Plugin\n\n1.6\n\nPipeline Multibranch Plugin\n\n2.12 or newer\n\nIf you are using the Blue Ocean plugin\n\nBlue Ocean Plugin\n\n1.0.0-b22 or newer\n\nOther plugins that may require updating:\n\nGitHub API Plugin\n\n1.84 or newer\n\nGitHub Plugin\n\n1.25.0 or newer\n\nIf you upgrade to Branch API 2.0.x and you have either the GitHub Branch Source or the BitBucket Branch Source plugins and you do not upgrade those instances to the 2.0.x line then your Jenkins instance will fail to start-up correctly.\n\nThe solution is just to upgrade the GitHub Branch Source or the BitBucket Branch Source plugin (as appropriate) to the 2.0.x line.\n\nAfter an upgrade you will see the data migration warning (see the screenshot in\nJENKINS-41608 for an\nexample) this is normal and expected.  The unreadable data will be removed by\nthe next scan / index or can be removed manually using the Discard Unreadable\nData button.  The warning will disappear on the next restart after the\nunreadable data has been removed.\n\nPlease update to the versions listed above. If you want to know more about the\nissues and how they were resolved, see the next section.\n\nAnalysis of the issues\n\nThe issues described below are resolved with these plugin releases:\n\nFolders Plugin: 5.17\n\nSCM API Plugin: 2.0.2\n\nBranch API Plugin: 2.0.2\n\nGit Plugin: Either 2.6.4 or 3.0.4\n\nGitHub Branch Source Plugin: 2.0.1\n\nBitBucket Branch Source Plugin: 2.0.2\n\nPipeline Multibranch Plugin: 2.12\n\nJENKINS-41121: GitHub Branch Source upgrade can cause a lot of rebuilds :\n\nMigration of GitHub branches from 1.x to 2.x resulted in a change of the\nimplementation class used to identify branches.  Some other other bugs in\nBranch API had been fixed and the combined effect resulted in a rebuild of all\nGitHub Branches (not PRs) after an upgrade to GitHub Branch Source Plugin\n2.0.0.  This rebuild was referred to as a \"build storm\".\n\nResolution:\n\nThe SCM API plugin was enhanced to add an extension point that allows for a second round of data migration when upgrading.\n\nThe second round of data migration allows plugins implementing the SCM API contract to fix implementation class issues in context.\n\nThe Branch API plugin was enhanced to use this new extension point.\n\nThe GitHub Branch Source plugin was enhanced to provide an implementation of this extension point.\n\nJENKINS-41255: Upgrading from a navigator that did not assign consistent source ids to a version that does assign consistent source ids causes a build storm on first scan :\n\nThe GitHub Branch Source and BitBucket Branch Source plugins in 1.x were not\nassigning consistent IDs to multi-branch projects discovered in an Organization\nFolder.  Both plugins were fixed in 2.0.0 to assign consistent IDs as a change\nof ID would result in a rebuild of all projects.  What was missed is that the\nvery first scan of an Organization Folder after an upgrade will change the\nrandomly assigned ID assigned by the 1.x plugins into the consistent ID\nassigned by the 2.0.0 plugins and consequently trigger a rebuild of all\nbranches. This rebuild was referred to as a \"build storm\".\n\nResolution:\n\nThe Branch API plugin was enhanced to detect the case where a branch source has\nbeen changed but the change is only changing the ID.  When such changes are\nidentified, the downstream references of the ID are all updated which will\nprevent a build storm.\n\nJENKINS-41313: On first index after upgrade to 2.0.0 all open PRs are rebuilt :\n\nThe BitBucket Branch Source 1.x did not store all the information about PRs\nthat is required by the SCM API 2.0.x model.  This could well have resulted in\nsubtle effects when manually triggering a rebuild of a merge PR if the PR’s\ntarget branch has been modified after the PR branch was first detected by\nJenkins. Consequently, as the information is required, BitBucket Branch Source\nplugin 2.0.0 populated the information with dummy values which would force the\ncorrect information to be retrieved.  The side-effect is that all PR branches\nwould be rebuilt.\n\nResolution:\n\nThe changes in SCM API 2.0.2 introduced to resolve JENKINS-41121 provided a path to resolve this issue without causing a rebuild of all PR branches.\n\nThe BitBucket Branch Source plugin was enhanced to provide an implementation of the new SCM API extension point that connects to BitBucket and retrieves the missing information.\n\nJENKINS-41124: Can’t get a human readable job name anymore :\n\nDuring initial testing of the Branch API 2.0.0 release an issue was identified\nwith how Organization Folders handled unusual names.  None of the existing\nimplementations of the SCMNavigator API could generate such unusual names due\nto form validation on GitHub / BitBucket replacing unusual characters with -\nwhen creating a repository.\n\nIt would be irresponsible to rely on external services sanitizing their input\ndata for the correct operation of Organization Folders.  Consequently, in\nBranch API 2.0.0 the names were all transformed into URL safe names, with the\noriginal URLs still resolving to the original projects so that any existing\nsaved links would remain functional.\n\nQuite a number of people objected to this change of URL scheme.\n\nResolution:\n\nThere has been a convention in Jenkins that the on-disk storage structure for\njobs mirrors the URL structure. This is only a convention and there is nothing specific in the code that\nmandates following the convention.\n\nThe Folders Plugin was enhanced to allow for computed folders (where the item\nnames are provided by an external source) to provide a strategy to use when\ngenerating the on-disk storage names as well as the URL component names for\nthe folder’s child items.\n\nThe Branch API plugin was enhanced to use this new strategy for name transformation.\n\nThe net effect of this change is that the URLs remain the same as for 1.x but\nthe on-disk storage uses transformed names that are future proofed against\nany new SCMNavigator implementations where the backing service allows names\nthat are problematic to use as filesystem directory names.\n\nSide-effect:\n\nThe Branch API 2.0.0 approach handled the transformation of names by renaming the items using the Jenkins Item rename API.\n\nThe Branch API 2.0.2 approach does not rename the child items as it is only the on-disk storage location that is moved.\n\nThis means that the Jenkins Item rename API cannot be used.\n\nAt this time, the only known side-effect is in the Job Configuration History plugin.\nThe configuration history of each child item will still be tracked going\nforward after the upgrade.  The pre-upgrade configuration history is also\nretained.  Because the Jenkins Item rename API cannot be used to flag the\nconfiguration file location change, there is no association between the\npre-upgrade history chain and the post-upgrade history chain.","title":"SCM API 2.0 Release Take 2","tags":["development","plugins"],"authors":[{"avatar":null,"blog":null,"github":"stephenc","html":"","id":"stephenc","irc":null,"linkedin":null,"name":"Stephen Connolly","slug":"/blog/authors/stephenc","twitter":"connolly_s"}]}},{"node":{"date":"2017-01-19T00:00:00.000Z","id":"437a3a39-d6ca-5875-b27d-0189cefc4150","slug":"/blog/2017/01/19/converting-conditional-to-pipeline/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nIntroduction\n\nWith all the new developments in\nJenkins Pipeline (and\nDeclarative Pipeline on the horizon),\nit’s easy to forget what we did to create \"pipelines\" before\nPipeline.\nThere are number of plugins, some that have been around since the very beginning,\nthat enable users to create \"pipelines\" in Jenkins.\nFor example, basic job chaining worked well in many cases, and the\nParameterized Trigger plugin\nmade chaining more flexible.\nHowever, creating chained jobs with conditional behavior was\nstill one of the harder things to do in Jenkins.\n\nThe\nConditional BuildStep plugin\nis a powerful tool that has allowed Jenkins users to write Jenkins jobs with complex conditional logic.\nIn this post, we’ll take a look at how we might converting Freestyle jobs that\ninclude conditional build steps to Jenkins Pipeline.\nUnlike Freestyle jobs, implementing conditional operations in Jenkins Pipeline is trivial,\nbut matching the behavior of complex conditional build steps will require a bit more care.\n\nGraphical Programming\n\nThe Conditional BuildStep plugin lets users add conditional logic to Freestyle\njobs from within the Jenkins web UI.  It does this by:\n\nAdding two types of Conditional BuildStep (\"Single\" and \"Multiple\") -\nthese build steps contain one or more other build steps to be run when the configured\ncondition is met\n\nAdding a set of Condition operations -\nthese control whether the Conditional BuildStep execute the contained step(s)\n\nLeveraging the Token Macro facility -\nthese provide values to the Conditions for evaluation\n\nIn the example below, this project will run the shell script step when the value of the\nREQUESTED_ACTION token equals \"greeting\".\n\nHere’s the output when I run this project with REQUESTED_ACTION set to \"greeting\":\n\nRun condition [Strings match] enabling prebuild for step [Execute shell]\nStrings match run condition: string 1=[greeting], string 2=[greeting]\nRun condition [Strings match] enabling perform for step [Execute shell]\n[freestyle-conditional] $ /bin/sh -xe /var/folders/hp/f7yc_mwj2tq1hmbv_5n10v2c0000gn/T/hudson5963233933358491209.sh\n+ echo 'Hello, bitwiseman!'\nHello, bitwiseman!\nFinished: SUCCESS\n\nAnd when I pass the value \"silence\":\n\nRun condition [Strings match] enabling prebuild for step [Execute shell]\nStrings match run condition: string 1=[silence], string 2=[greeting]\nRun condition [Strings match] preventing perform for step [Execute shell]\nFinished: SUCCESS\n\nThis is a simple example but the conditional step can contain any regular build step.\nWhen combined with other plugins, it can control whether to send notifications,\ngather data from other sources, wait for user feedback, or call other projects.\n\nThe Conditional BuildStep plugin does a great job of leveraging strengths of\nthe Jenkins web UI, Freestyle jobs, and UI-based programming,\nbut it is also hampered by their limitations.\nThe Jenkins web UI can be clunky and confusing at times.\nLike the steps in any Freestyle job, these conditional steps are only\nstored and viewable in Jenkins.\nThey are not versioned with other product or build code and can’t be code reviewed.\nLike any number of UI-based programming tools, it has to make trade-offs between clarity\nand flexibility: more options or clearer presentation.\nThere’s only so much space on the screen.\n\nConverting to Pipeline\n\nJenkins Pipeline, on the other hand, enables users to implement their pipeline as code.\nPipeline code can be written directly in the Jenkins Web UI or in any text editor.\nIt is a full-featured programming language,\nwhich gives users access to much broader set of conditional statements\nwithout the restrictions of UI-based programming.\n\nSo, taking the example above, the Pipeline equivalent is:\n\n// Declarative //\npipeline {\n    agent any\n    parameters {\n        choice(\n            choices: ['greeting' , 'silence'],\n            description: '',\n            name: 'REQUESTED_ACTION')\n    }\n\n    stages {\n        stage ('Speak') {\n            when {\n                // Only say hello if a \"greeting\" is requested\n                expression { params.REQUESTED_ACTION == 'greeting' }\n            }\n            steps {\n                echo \"Hello, bitwiseman!\"\n            }\n        }\n    }\n}\n// Script //\nproperties ([\n    parameters ([\n        choice (\n            choices: ['greeting', 'silence'],\n            description: '',\n            name : 'REQUESTED_ACTION')\n    ])\n])\n\nnode {\n    stage ('Speak') {\n        // Only say hello if a \"greeting\" is requested\n        if (params.REQUESTED_ACTION == 'greeting') {\n            echo \"Hello, bitwiseman!\"\n        }\n    }\n}\n\nWhen I run this project with REQUESTED_ACTION set to \"greeting\", here’s the output:\n\n[Pipeline] node\nRunning on osx_mbp in /Users/bitwiseman/jenkins/agents/osx_mbp/workspace/pipeline-conditional\n[Pipeline] {\n[Pipeline] stage\n[Pipeline] { (Speak)\n[Pipeline] echo\nHello, bitwiseman!\n[Pipeline] }\n[Pipeline] // stage\n[Pipeline] }\n[Pipeline] // node\n[Pipeline] End of Pipeline\nFinished: SUCCESS\n\nWhen I pass the value \"silence\", the only change is \"Hello, bitwiseman!\" is not printed.\n\nSome might argue that the Pipeline code is a bit harder to understand on first reading.\nOthers would say the UI is just as confusing if not more so.\nEither way, the Pipeline representation is considerably more compact than the Jenkins UI presentation.\nPipeline also lets us add helpful comments, which we can’t do in the Freestyle UI.\nAnd we can easily put this Pipeline in a Jenkinsfile to be code-reviewed, checked-in, and versioned\nalong with the rest of our code.\n\nConditions\n\nThe previous example showed the \"Strings match\" condition and its Pipeline equivalent.\nLet’s look at couple more interesting conditions and their Jenkins Pipeline equivalents.\n\nBoolean condition\n\nYou might think that a boolean condition would be the simplest condition, but it isn’t.\nSince it works with string values from tokens, the Conditional BuildStep plugin offers\na number of ways to indicate true or false.\nTruth is a case insensitive match of one of the following:\n1 (the number one), Y, YES, T, TRUE, ON or RUN.\n\nPipeline can duplicate these, but depending on the scenario we might consider\nwhether a simpler expression would suffice.\n\nPipeline\n\n// Declarative //\nwhen {\n    // case insensitive regular expression for truthy values\n    expression { return token ==~ /(?i)(Y|YES|T|TRUE|ON|RUN)/ }\n}\nsteps {\n    /* step */\n}\n\n// Script //\n// case insensitive regular expression for truthy values\nif (token ==~ /(?i)(Y|YES|T|TRUE|ON|RUN)/) {\n    /* step */\n}\n\nLogical \"OR\" of conditions\n\nThis condition wraps other conditions.\nIt takes their results as inputs and performs a logical \"or\" of the results.\nThe AND and NOT conditions do the same, performing their respective operations.\n\nPipeline\n\n// Declarative //\nwhen {\n    // A or B\n    expression { return A || B }\n}\nsteps {\n    /* step */\n}\n\n// Script //\n// A or B\nif (A || B) {\n    /* step */\n}\n\nTokens\n\nTokens can be considerably more work than conditions.\nThere are more of them and they cover a much broader range of behaviors.\nThe previous example showed one of the simpler cases, accessing a build parameter,\nwhere the token has a direct equivalent in Pipeline.\nHowever, many tokens don’t have direct equivalents,\nsome take a parameters (adding to their complexity),\nand some provide information that is simply not exposed in Pipeline yet.\nSo, determining how to migrate tokens needs to be done on case-by-case basis.\n\nLet’s look at a few examples.\n\n\"FILE\" token\n\nExpands to the contents of a file. The file path is relative to the build workspace root.\n\n${FILE,path=\"PATH\"}\n\nThis token maps directly to the readFile step.\nThe only difference is the file path for readFile is relative to the\ncurrent working directory on the agent, but that is the workspace root by default.\nNo problem.\n\nPipeline\n\n// Declarative //\nwhen {\n    expression { return readFile('pom.xml').contains('mycomponent') }\n}\nsteps {\n    /* step */\n}\n\n// Script //\nif (readFile('pom.xml').contains('mycomponent')) {\n    /* step */\n}\n\nGIT_BRANCH\n\nExpands to the name of the branch that was built.\n\nParameters (descriptions omitted): all, fullName.\n\nThis information may or may not be exposed in Pipeline.  If you’re using the\nPipeline Multibranch plugin\nenv.BRANCH_NAME will give similar basic information, but doesn’t offer the parameters.\nThere are also\nseveral\nissues\nfiled around GIT_* tokens in Pipeline.\nUntil they are addressed fully, we can follow the pattern shown in\npipeline-examples,\nexecuting a shell to get the information we need.\n\nPipeline\n\nGIT_BRANCH = sh(returnStdout: true, script: 'git rev-parse --abbrev-ref HEAD').trim()\n\nCHANGES_SINCE_LAST_SUCCESS\n\nDisplays the changes since the last successful build.\n\nParameters (descriptions omitted):\nreverse, format, changesFormat, showPaths, pathFormat,\nshowDependencies, dateFormat, regex, replace, default.\n\nNot only is the information provided by this token not exposed in Pipeline,\nthe token has ten optional parameters, including format strings and regular expression\nsearches. There are a number of ways we might get similar information in Pipeline.\nEach have their own particular limitations and ways they differ from the token output.\nThen we’ll need to consider how each of the parameters changes the output.\nIf nothing else, translating this token is clearly beyond the scope of this post.\n\nSlightly More Complex Example\n\nLet’s do one more example that shows some of these conditions and tokens.\nThis time we’ll perform different build steps depending on what branch we’re building.\nWe’ll take two build parameters: BRANCH_PATTERN and FORCE_FULL_BUILD.\nBased on BRANCH_PATTERN, we’ll checkout a repository.\nIf we’re building on the master branch or the user checked FORCE_FULL_BUILD,\nwe’ll call three other builds in parallel\n( full-build-linux, full-build-mac, and full-build-windows),\nwait for them to finish, and report the result.\nIf we’re not building on the master branch and the user did not check FORCE_FULL_BUILD,\nwe’ll print a message saying we skipped the full builds.\n\nFreestyle\n\nHere’s the configuration for Freestyle version.\n(It’s pretty long.  Feel free to skip down to the Pipeline version):\n\nThe Pipeline version of this job determines the GIT_BRANCH branch by\nrunning a shell script that returns the current local branch name.\nThis means that the Pipeline version must checkout to a local branch (not a detached head).\n\nFreestyle version of this job does not require a local branch, GIT_BRANCH is set automatically.\nHowever, to maintain functional parity, the Freestyle version of this job includes\n\"Checkout to Specific Local Branch\" as well.\n\nPipeline\n\nHere’s the equivalent Pipeline:\n\nFreestyle version of this job is not stored in source control.\n\nIn general, the Pipeline version of this job would be stored in source control,\nwould checkout scm, and would run that same repository.\nHowever, to maintain functional parity, the Pipeline version shown does a checkout\nfrom source control but is not stored in that repository.\n\nPipeline\n\n// Script //\nproperties ([\n    parameters ([\n        string (\n            defaultValue: '*',\n            description: '',\n            name : 'BRANCH_PATTERN'),\n        booleanParam (\n            defaultValue: false,\n            description: '',\n            name : 'FORCE_FULL_BUILD')\n    ])\n])\n\nnode {\n    stage ('Prepare') {\n        checkout([$class: 'GitSCM',\n            branches: [[name: \"origin/${BRANCH_PATTERN}\"]],\n            doGenerateSubmoduleConfigurations: false,\n            extensions: [[$class: 'LocalBranch']],\n            submoduleCfg: [],\n            userRemoteConfigs: [[\n                credentialsId: 'bitwiseman_github',\n                url: 'https://github.com/bitwiseman/hermann']]])\n    }\n\n    stage ('Build') {\n        GIT_BRANCH = 'origin/' + sh(returnStdout: true, script: 'git rev-parse --abbrev-ref HEAD').trim()\n        if (GIT_BRANCH == 'origin/master' || params.FORCE_FULL_BUILD) {\n\n            // Freestyle build trigger calls a list of jobs\n            // Pipeline build() step only calls one job\n            // To run all three jobs in parallel, we use \"parallel\" step\n            // https://jenkins.io/doc/pipeline/examples/#jobs-in-parallel\n            parallel (\n                linux: {\n                    build job: 'full-build-linux', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                },\n                mac: {\n                    build job: 'full-build-mac', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                },\n                windows: {\n                    build job: 'full-build-windows', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                },\n                failFast: false)\n\n        } else {\n            echo 'Skipped full build.'\n        }\n    }\n}\n// Declarative //\npipeline {\n    agent any\n    parameters {\n        string (\n            defaultValue: '*',\n            description: '',\n            name : 'BRANCH_PATTERN')\n        booleanParam (\n            defaultValue: false,\n            description: '',\n            name : 'FORCE_FULL_BUILD')\n    }\n\n    stages {\n        stage ('Prepare') {\n            steps {\n                checkout([$class: 'GitSCM',\n                    branches: [[name: \"origin/${BRANCH_PATTERN}\"]],\n                    doGenerateSubmoduleConfigurations: false,\n                    extensions: [[$class: 'LocalBranch']],\n                    submoduleCfg: [],\n                    userRemoteConfigs: [[\n                        credentialsId: 'bitwiseman_github',\n                        url: 'https://github.com/bitwiseman/hermann']]])\n            }\n        }\n\n        stage ('Build') {\n            when {\n                expression {\n                    GIT_BRANCH = 'origin/' + sh(returnStdout: true, script: 'git rev-parse --abbrev-ref HEAD').trim()\n                    return GIT_BRANCH == 'origin/master' || params.FORCE_FULL_BUILD\n                }\n            }\n            steps {\n                // Freestyle build trigger calls a list of jobs\n                // Pipeline build() step only calls one job\n                // To run all three jobs in parallel, we use \"parallel\" step\n                // https://jenkins.io/doc/pipeline/examples/#jobs-in-parallel\n                parallel (\n                    linux: {\n                        build job: 'full-build-linux', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                    },\n                    mac: {\n                        build job: 'full-build-mac', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                    },\n                    windows: {\n                        build job: 'full-build-windows', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                    },\n                    failFast: false)\n            }\n        }\n        stage ('Build Skipped') {\n            when {\n                expression {\n                    GIT_BRANCH = 'origin/' + sh(returnStdout: true, script: 'git rev-parse --abbrev-ref HEAD').trim()\n                    return !(GIT_BRANCH == 'origin/master' || params.FORCE_FULL_BUILD)\n                }\n            }\n            steps {\n                echo 'Skipped full build.'\n            }\n        }\n    }\n}\n\nConclusion\n\nAs I said before, the Conditional BuildStep plugin is great.\nIt provides a clear, easy to understand way to add conditional logic to any Freestyle job.\nBefore Pipeline, it was one of the few plugins to do this and it remains one of the most popular plugins.\nNow that we have Pipeline, we can implement conditional logic directly in code.\n\nThis is blog post discussed how to approach converting conditional build steps to Pipeline\nand showed a couple concrete examples.  Overall, I’m pleased with the results so far.\nI found scenarios which could not easily be migrated to Pipeline, but even those\nare only more difficult, rather than impossible.\n\nThe next thing to do is add a section to the\nJenkins Handbook documenting the Pipeline\nequivalent of all of the Conditions and the most commonly used Tokens.\nLook for it soon!\n\nLinks\n\nConditional BuildStep plugin","title":"Converting Conditional Build Steps to Jenkins Pipeline","tags":["pipeline","freestyle","plugins","conditional-build-step","tutorial"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2017-01-17T00:00:00.000Z","id":"23308d18-32ba-5461-b5aa-74f746e5049f","slug":"/blog/2017/01/17/scm-api-2.0-release/","strippedHtml":"The regressions\ndiscovered after release have now been resolved and this post has been updated with the correct plugin version numbers.\n\nSee this post for more details.\n\nWe are announcing the\nSCM API\n2.0.x and\nBranch API\n2.0.x release lines.\n\nDownstream of this there are also some great improvements to a number of popular plugins including:\n\nGitHub Branch Source\n\nBitBucket branch source\n\nGit\n\nMercurial\n\nPipeline Multibranch\n\nGitHub Organization Folders\n\nThere are some gotcha’s that Jenkins administrators will need to be aware of.\n\nAlways take a backup of your JENKINS_HOME before upgrading any plugins.\n\nWe want to give you the whole story, but the take home message is this:\n\nWhen updating the\nSCM API\nand/or\nBranch API\nplugins to the 2.0.x release lines, if you have any of the\nGitHub Organization Folders,\nGitHub Branch Source\nand/or\nBitBucket branch source\nplugins installed then you must upgrade them all to 2.0.x at the same time or Bad Things™ will happen.\n\n— A Jenkins Administrator\n\nDo NOT upgrade some of these plugins but not others!\nDoing so may cause your jobs to fail to load.\n\nIf you don’t care about the hows and whys, you can just skip down to this section but if you are curious…​ here we go!\n\nThe back-story\n\nWay back in September 2013 we announced the\nLiterate plugin,\nas an experimental new way of modeling branch development in Jenkins.\n\nWhen you are performing an experiment, the recommendation is to do just enough work to let you perform the test.\nHowever, the culture in Jenkins is to always try and produce reusable components that others can use in ways you have not anticipated.\n\nSo when releasing the initial version of the\nLiterate plugin\nwe also separated the Literate specific bits from the SCM specific concepts and multi-branch concepts.\nThese were lower level concepts were organized into the following plugins:\n\nSCM API -\nwhich was intended to be a plugin to hold a next generation API for interacting with source control systems.\n\nBranch API -\nwhich was intended to be a plugin to hold the multi-branch functionality that was abstracted from the usage by the Literate plugin.\n\nIn addition, we released updates to three of the more common SCM plugins which included implementations of the SCM API:\n\nGit plugin\n\nSubversion plugin\n\nMercurial plugin\n\nWhile there was some interest in the Literate plugin, it did not gain much traction - there are only 39 Jenkins instances running the Literate plugin as of December 2016.\n\nIn terms of the reusable components, we had only made a minimal implementation with some limitations:\n\nVery basic event support - events can only trigger a re-scan of the entire repository.\nThis was acceptable at the time because the only three implementations use a local cache of the remote state so re-scanning is quick.\n\nNo implementation of the SCMFileSystem API.\nAs a result it is not possible for plugins like\nPipeline Multibranch\nto get the Jenkinsfile from the remote repository without needing to checkout the repository into a workspace.\n\nNo documentation on how plugin developers are supposed to implement the SCM API\n\nNo documentation on how plugin developers are supposed to consume the SCM API (if they wanted to do something like Branch API but not the same way as Branch API)\n\nNo documentation on how plugin developers are supposed to implement the Branch API to create their own multi-branch project types\n\nNo documentation on for users on how the Branch API based project types are expected to work.\n\nRoll forward to November 2015 and Jenkins Pipeline got a release of the\nPipeline Multibranch.\nIt seems that pairing Pipeline with Branch API style multi-branch is much more successful than Literate - there are close to 60,000 instances running the pipeline multi-branch plugin as of December 2016.\n\nThere also were two new SCM plugins implementing the SCM API:\n\nGitHub Branch Source Plugin\n\nBitBucket Branch Source Plugin\n\nUnlike the previous implementations of the SCM API, however, these plugins do not maintain a local cache of the repository state.\nRather they make queries via the GitHub / BitBucket REST APIs on demand.\n\nThe above design decision exposed one of the initial MVP compromises of the SCM API plugin: very basic event support.\nUnder the SCM API 1.x model, the only event that an SCMSource can signal is something changed, go look at everything again.\nWhen you are accessing an API that only allows 5,000 API calls per hour, performing a full scan of the entire repository just to pick up a change in one branch does not make optimum usage of that 5,000 calls/hour rate limit.\n\nSo we decided that perhaps the SCM API and Branch API plugins have left their Minimum Viability Experiment state and the corresponding limitations should be addressed.\n\nEnter SCM API 2.0.x and Branch API 2.0.x\n\nSo what has changed in the\nSCM API\n2.0.x and\nBranch API\n2.0.x release lines?\nThese plugin releases include:\n\ndocumentation on how plugin developers are supposed to\nimplement the SCM API\n\ndocumentation on how plugin developers are supposed to\nconsume the SCM API\n(if they wanted to do something like Branch API but not the same way as Branch API)\n\ndocumentation on how plugin developers are supposed to\nimplement the Branch API\nto create their own multi-branch project types\n\ngeneric documentation for users on\nhow Branch API based project types are intended to work\n\na full featured\nevent system\nthat allows implementers to provide fine grained notifications to consumers\n\nlots\nand\nlots\nof new automated tests\n\na mock implementation\nof the SCM API to help consumers of the SCM API test their usage.\n\nIn addition, we have upgraded the following plugins to include the new fine-grained event support:\n\nGit Plugin\n\nMercurial Plugin\n\nOk, that was the good news.\nHere is the bad news.\n\nWe found out that the GitHub Branch Source and BitBucket Branch Source plugins had made invalid assumptions about how to implement the SCM API.\nTo be clear, this was not the plugin developers fault: at the time there was no documentation on how to implement the SCM API.\n\nBut fixing the issues that we found means that you have to be careful about which specific combinations of plugin versions you have installed.\n\nSCM API Plugin\n\nTechnically, the 2.0.x line of this plugin is both API and on-disk compatible with plugins compiled against older version lines.\n\nHowever, the 1.x lines of both the GitHub Branch Source and BitBucket Branch Source plugins have hard-coded assumptions about internal implementation of the SCM API that are no longer valid in the 2.0.x line.\n\nIf you upgrade to SCM API 2.0.x and you have either the GitHub Branch Source or the BitBucket Branch Source plugins and you do not upgrade those instances to the 2.0.x line then your Jenkins instance will fail to start-up correctly.\n\nThe solution is just to upgrade the GitHub Branch Source or the BitBucket Branch Source plugin (as appropriate) to the 2.0.x line.\n\nIf you upgrade the SCM API plugin to the 2.0.x line and do not upgrade the Branch API plugin to the 2.0.x line then you will not get any of the benefits of the new version of the SCM API plugin.\n\nBranch API Plugin\n\nThe 2.0.x line of this plugin makes on-disk file format changes that mean you will be unable to roll back to the 1.x line after an upgrade without restoring the old data files from a back-up.\nTechnically, the API is compatible with plugins compiled against older version lines.\n\nThe 1.x lines of both the GitHub Branch Source and BitBucket Branch Source plugins have implemented hacks that make assumptions about internal implementation of the Branch API that are no longer valid in the 2.0.x line.\n\nThe Pipeline Multibranch plugin made a few minor invalid assumptions about how to implement a Multibranch project type.\nFor example, if you do not upgrade the Pipeline Multibranch plugin in tandem then you will be unable to manually delete an orphaned item before the orphaned item retention strategy runs, which should be significantly less frequently with the new event support.\n\nIf you upgrade to Branch API 2.0.x and you have either the GitHub Branch Source or the BitBucket Branch Source plugins and you do not upgrade those instances to the 2.0.x line then your Jenkins instance will fail to start-up correctly.\n\nThe solution is just to upgrade the GitHub Branch Source or the BitBucket Branch Source plugin (as appropriate) to the 2.0.x line.\n\nGit Plugin\n\nThe new releases of this plugin are both API and on-disk compatible with plugins compiled against the previous releases.\n\nThe 2.0.x lines of both the GitHub Branch Source and BitBucket Branch Source plugins require that you upgrade your Git Plugin to one of the versions that supports SCM API 2.0.x.\nIn general, the required upgrade will be performed automatically when you upgrade your GitHub Branch Source and BitBucket Branch Source plugins.\n\nMercurial Plugin\n\nThe new release of this plugin is both API and on-disk compatible with plugins compiled against the previous releases.\n\nThe 2.0.x line of the BitBucket Branch Source plugins require that you upgrade your Mercurial Plugin to the 2.0.x line.\nIn general, the required upgrade will be performed automatically when you upgrade your  BitBucket Branch Source plugins.\n\nBitBucket Branch Source Plugin\n\nThe 2.0.x line of this plugin makes on-disk file format changes that mean you will be unable to roll back to the 1.x line after an upgrade without restoring the old data files from a back-up.\n\nGitHub Branch Source Plugin\n\nThe 2.0.x line of this plugin makes on-disk file format changes that mean you will be unable to roll back to the 1.x line after an upgrade without restoring the old data files from a back-up.\n\nIf you upgrade to GitHub Branch Source 2.0.x and you have the GitHub Organization Folders plugin installed, you must upgrade that plugin to the tombstone release.\n\nGitHub Organization Folders Plugin\n\nThe functionality of this plugin has been migrated to the GitHub Branch Source plugin.\nYou will need to upgrade to the tombstone release in order to ensure all the data has been migrated to the classes in the GitHub Branch Source plugin.\n\nOnce you have upgraded to the tombstone version and all GitHub Organization Folders have had a full scan completed successfully, you can disable and uninstall the GitHub Organization Folders plugin.\nThere will be no more releases of this plugin after the tombstone.\nThe tombstone is only required for data migration.\n\nSummary for busy Jenkins Administrators\n\nUpgrading should make multi-branch projects much better.\nWhen you are ready to upgrade you must ensure that you upgrade all the required plugins.\nIf you miss some, just upgrade them and restart to fix the issue.\n\nFolders Plugin\n\n5.16 5.17 or newer\n\nSCM API Plugin\n\n2.0.1 2.0.2 or newer\n\nBranch API Plugin\n\n2.0.0 2.0.2 or newer\n\nGit Plugin\n\nEither 2.6.2 2.6.4 or newer in the 2.6.x line or 3.0.2 3.0.4 or newer\n\nMercurial Plugin\n\n2.0.0 or newer\n\nGitHub Branch Source Plugin\n\n2.0.0 2.0.1 or newer\n\nBitBucket Branch Source Plugin\n\n2.0.0 2.0.2 or newer\n\nGitHub Organization Folders Plugin\n\n1.6\n\nPipeline Multibranch Plugin\n\n2.10 2.12 or newer\n\nIf you are using the Blue Ocean plugin\n\nBlue Ocean Plugin\n\n1.0.0-b22 or newer\n\nOther plugins that may require updating:\n\nGitHub API Plugin\n\n1.84 or newer\n\nGitHub Plugin\n\n1.25.0 or newer\n\nAfter an upgrade you will see the data migration warning (see the screenshot in JENKINS-41608 for an example) this is normal and expected.\nThe unreadable data will be removed by the next scan / index or can be removed manually using the Discard Unreadable Data button.\nThe warning will disappear on the next restart after the unreadable data has been removed.\n\nSummary for busy Jenkins users\n\nSCM API 2.0.x adds fine-grained event support.\nThis should significantly improve the responsiveness of multi-branch projects.\nThis should significantly reduce your GitHub API rate limit usage.\n\nIf you are using the\nGitHub Branch Source\nor\nGitHub Organization Folders\nplugins then upgrading will significantly reduce the API calls made by Jenkins to GitHub.\n\nIf you are using any of the upgraded SCM plugins (e.g. Git, Mercurial, GitHub Branch Source, BitBucket Branch Source) then upgrading will significantly improve the responsiveness to push event notifications.\n\nSummary for busy SCM plugin developers\n\nYou should read the new\ndocumentation\non how plugin developers are supposed to implement the SCM API\n\nWhere to now dear Literate Plugin\n\nThe persistent reader may be wondering what happens now to the Literate plugin.\n\nFor me, the logical heir of the Literate Plugin is the\nPipeline Model Definition plugin.\nThis new plugin has the advantage of an easy to read pipeline syntax with the extra functionality that I suspect was preventing people from adopting Literate.\n\nThe good news is that the Pipeline Model Definition already has 5000 installations as of December 2016 and I expect up-take to keep on growing.","title":"SCM API turns 2.0 and what that means for you","tags":["development","plugins"],"authors":[{"avatar":null,"blog":null,"github":"stephenc","html":"","id":"stephenc","irc":null,"linkedin":null,"name":"Stephen Connolly","slug":"/blog/authors/stephenc","twitter":"connolly_s"}]}},{"node":{"date":"2017-01-12T00:00:00.000Z","id":"2ff4e12b-c7b0-5650-8366-ce14906b5f15","slug":"/blog/2017/01/12/declarative-pipeline-beta-2/","strippedHtml":"This week, we released the second beta of the new\nDeclarative Pipeline syntax,\navailable in the Update Center now as version 0.8.1 of Pipeline: Model Definition.\nYou can read more about Declarative Pipeline\nin the blog post introducing the first beta\nfrom December, but we wanted to update you all on the syntax changes in the\nsecond beta. These syntax changes are the last compatibility-breaking changes to\nthe syntax before the 1.0 release planned for February, so you can safely start\nusing the 0.8.1 syntax now without needing to change it when 1.0 is released.\n\nA full syntax reference is available on the wiki as well.\n\nSyntax Changes\n\nChanged \"agent\" configuration to block structure\n\nIn order to support more detailed and clear configuration of agents, as well as\nmaking agent syntax more consistent with the rest of the Declarative Pipeline\nsyntax, we’ve moved the agent configuration into blocks. The agent any and\nagent none configurations work the same as previously, but label, docker\nand dockerfile now look like the following:\n\nJust specifying a label is simple.\n\n// Declarative //\nagent {\n    label \"some-label\"\n}\n// Script //\n\nIf you’re just specifying a Docker image, you can use this simple syntax.\n\n// Declarative //\nagent {\n    docker \"ubuntu:16.04\"\n}\n// Script //\n\nWhen you are specifying a label or other arguments, docker looks like this:\n\n// Declarative //\nagent {\n    docker {\n        image \"ubuntu:16.04\"\n        label \"docker-label\"\n        args \"-v /tmp:/tmp -p 8000:8000\"\n    }\n}\n// Script //\n\nWhen you’re building an image from \"Dockerfile\" in your repository and\ndon’t care what node is used or have additional arguments, you can again\nuse a simple syntax.\n\n// Declarative //\nagent {\n    dockerfile true\n}\n// Script //\n\nWhen you’re building an image from a different file, or have a label or other\narguments, use the following syntax:\n\n// Declarative //\nagent {\n    dockerfile {\n        filename \"OtherDockerfile\"\n        label \"docker-label\"\n        args \"-v /tmp:/tmp -p 8000:8000\"\n    }\n}\n// Script //\n\nImproved \"when\" conditions\n\nWe introduced the when section a couple releases ago, but have made some\nchanges to its syntax here in 0.8.1. We wanted to add some simpler ways to\nspecify common conditions, and that required we re-work the syntax accordingly.\n\nBranch\n\nOne of the most common conditions is running a stage only if you’re on a\nspecific branch. You can also use wildcards like \"*/master\".\n\n// Declarative //\nwhen {\n    branch \"master\"\n}\n// Script //\n\nEnvironment\n\nAnother built-in condition is the environment condition, which checks to see\nif a given environment variable is set to a given value.\n\n// Declarative //\nwhen {\n    environment name: \"SOME_ENV_VAR\", value: \"SOME_VALUE\"\n}\n// Script //\n\nExpression\n\nLastly, there’s the expression condition, which resolves an arbitrary\nPipeline expression. If the return value of that expression isn’t false or\nnull, the stage will execute.\n\n// Declarative //\nwhen {\n    expression {\n        echo \"Should I run?\"\n        return \"foo\" == \"bar\"\n    }\n}\n// Script //\n\n\"options\" replaces \"properties\" and \"wrappers\"\n\nWe’ve renamed the properties section to options, due to needing to add new\nDeclarative-specific options and to cut down on confusion. The options section\nis now where you’ll put general Pipeline options like buildDiscarder,\nDeclarative-specific options like skipDefaultCheckout, and block-scoped steps\nthat should wrap the execution of the entire build, like timeout or\ntimestamps.\n\n// Declarative //\n\noptions {\n    buildDiscarder(logRotator(numToKeepStr:'1'))\n    skipDefaultCheckout()\n    timeout(time: 5, unit: 'MINUTES')\n}\n// Script //\n\nHeading towards 1.0!\n\nWhile we may still add more functionality to the Declarative Pipeline syntax,\nwe won’t be making any changes to existing syntax for the 1.0 release. This\nmeans that any pipelines you write against the 0.8.1 syntax will keep working\nfor the foreseeable future without any changes. So if you’re already using\nDeclarative Pipelines, make sure to update your `Jenkinsfile`s after upgrading\nto 0.8.1, and if you haven’t been using Declarative Pipelines yet, install the\nPipeline: Model Definition plugin and\ngive them a try!","title":"Declarative Pipeline Syntax Beta 2 release","tags":["plugins","pipeline"],"authors":[{"avatar":null,"blog":null,"github":"abayer","html":"<div class=\"paragraph\">\n<p>Andrew was a core committer to Hudson and the author of numerous plugins.</p>\n</div>","id":"abayer","irc":null,"linkedin":null,"name":"Andrew Bayer","slug":"/blog/authors/abayer","twitter":"abayer"}]}},{"node":{"date":"2017-01-10T00:00:00.000Z","id":"ce9069d6-e9c4-5b3e-b147-2b3d3ac2f09d","slug":"/blog/2017/01/10/jenkins-lifx-notifier-plugin/","strippedHtml":"This is a\ncross\npost by Veaceslav Gaidarji, open source\ndeveloper and contributor to the Jenkins and Bitrise projects.\n\nSome time ago I encountered a LIFX smart bulbs.\nThese are the bulbs with a chip inside - 50% bulb, 50% chip. There are mobile\napplications for easy configuration and remote control of the bulb. Nothing\nspecial here, it simply works and is very convenient to have such bulbs in\ndormitory.\n\nBrilliant idea time\n\n99% of ideas which come to our minds either were already implemented by someone\nelse or they are useless.\n\n— Veaceslav Gaidarji\n\nAnd as it always happens, the developer inside me generated an idea which, as\nit always happens, was implemented by someone else already.\n\nThe idea was: to connect a LIFX bulb to Jenkins server and update the color\naccording to a job’s state.\n\nBefore starting to work on such Jenkins plugin, I searched for similar projects\non Google and the first links pointed me to existing\nLIFX notifier plugin\nand a\nblog post\nfrom\nMichael Neale\nwho created the plugin. Michael’s post describes exactly what I had in mind.\n\nAt this point I had 2 options:\n\nforget about building something new and just use the plugin\n\nimprove existing plugin\n\nFirst option is always easy and effortless, but second one is more challenging.\n\nImproving an existing plugin\n\nThe existing LIFX notifier plugin\ndid its job really well and I was able to connect my bulb to Jenkins and test\nit. But it wasn’t complete and had no configurable at all, therefore no\npossibility to change the colors.\n\nFirst, I read Jenkins contribution guidelines, which\nencourage\ndevelopers to improve existing plugins (if any) and not create other versions\nof plugins with similar functionality. Then I contacted the plugin author, Michael Neale,\nvia email and kindly asked for the contributor access in GitHub\nfor the existing plugin version. After a short discussion about my plans on this\nplugin, Michael added me as a contributor to GitHub\nrepo and wished me\ngood luck. Thanks Michael!\n\nI wanted to improve the LIFX notifier plugin to add the ability\ncustomize the colors ( in progress, build success and build failure). This\nis not a hard task actually.\nA 1000+ plugins were\ndeveloped for Jenkins by the hackers like me, which means that I should have no\nproblem to do it as well.\nFortunately for me, I have used some plugins already which had a UI similar to\nthat I had planned to add to the LIFX notifier, such as:\n\nHockeyApp plugin\n\nFabric Beta publisher plugin\n\nDifferent Build notifiers plugins\n\nReviewing the code for these plugins, plus Jenkins\nplugin\ndevelopment documentation, and of course looking over\nJelly components helped\nme to:\n\nBetter understand the Jenkins architecture.\n\nLearn how Jenkins plugins work in general.\n\nLearn how to create the UI components for a plugin.\n\nLearn how to subscribe to Jenkins job state changes using appropriate\nextension points.\n\nIn a few weeks I’ve finished my plugin modifications and added unit tests for\nits major parts.  As a result, the plugin now has a UI configuration section in\nPost-build Actions which is self descriptive:\n\nThe last step was to prepare new plugin version and publish it to the world!\nThe Jenkins\"Hosting\nplugins\" document describes step by step process of how to publish a plugin.\n\nThis includes many steps which should be respected very carefully.\n\nDemo\n\nWhat I’ve learned\n\nIt was my first experience in Jenkins plugins development. I should say that\nsteep learning curve is high enough, and sometimes is really hard to find\nanswers on appearing questions. But in general it’s all about Java, XML,\nMaven and it’s a lot of fun developing Jenkins plugins.\n\nCheck out the LIFX notifier page\nfor more information about the latest releases!\n\nBonus : bitrise.io users, I’ve developed step LIFX notifier for bitrise as well.","title":"Learning plugin development by improving the LIFX notifier plugin","tags":["plugins","lifx"],"authors":[{"avatar":null,"blog":"http://vgaidarji.me","github":"vgaidarji","html":"<div class=\"paragraph\">\n<p>Veaceslav is a software developer with the main focus on Android platform.\nIn his free time, he enjoys working on different open-source projects.</p>\n</div>","id":"vgaidarji","irc":null,"linkedin":null,"name":"Veaceslav Gaidarji","slug":"/blog/authors/vgaidarji","twitter":"v_gaidarji"}]}},{"node":{"date":"2016-10-31T00:00:00.000Z","id":"dcf580a2-8de4-526e-8237-399ee53b3b39","slug":"/blog/2016/10/31/xunit-reporting/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nThe\nJUnit plugin\nis the go-to test result reporter for many Jenkins projects,\nbut the it is not the only one available.  The\nxUnit plugin\nis a viable alternative that supports JUnit and many other test result file formats.\n\nIntroduction\n\nNo matter the project, you need to gather and report test results.\nJUnit is one of the most widely supported formats for recording test results.\nFor a scenarios where your tests are stable and your framework can produce JUnit output,\nthis makes the JUnit plugin ideal for reporting results in Jenkins.\nIt will consume results from a specified file or path, create a report,\nand if it finds test failures it will set the the job state to \"unstable\" or \"failed\".\n\nThere are also plenty of scenarios where the JUnit plugin is not enough.\nIf your project has some failing tests that will take some time to fix,\nor if there are some flaky tests,\nthe JUnit plugin’s simplistic view of test failures may be difficult to work with.\n\nNo problem, the Jenkins plugin model lets us replace the JUnit\nplugin functionality with similar\nfunctionality from another plugin and Jenkins Pipeline lets us do this in safe\nstepwise fashion where we can test and debug each of our changes.\n\nIn this article, I will show you how to replace the JUnit plugin with the\nxUnit plugin in Pipeline code to address a few common test reporting scenarios.\n\nInitial Setup\n\nI’m going to use the \"JS-Nightwatch.js\" sample project from my\nprevious post to demonstrate a couple\ncommon scenarios that the xUnit handles better.\nI already have the latest\nJUnit plugin\nand\nxUnit plugin\ninstalled on my Jenkins server.\n\nI’ll be keeping my changes in\nlink: my fork\nof the \"JS-Nightwatch.js\" sample project on GitHub, under the\n\" blog/xunit\" branch.\n\nHere’s what the Jenkinsfile looked like at the end of that previous post and what\nthe report page looks like after a few runs:\n\nJenkinsfile\n\nnode {\n    stage \"Build\"\n    checkout scm\n\n    // Install dependencies\n    sh 'npm install'\n\n    stage \"Test\"\n    // Add sauce credentials\n    sauce('f0a6b8ad-ce30-4cba-bf9a-95afbc470a8a') {\n        // Start sauce connect\n        sauceconnect(options: '', useGeneratedTunnelIdentifier: false, verboseLogging: false) {\n\n            // List of browser configs we'll be testing against.\n            def platform_configs = [\n                'chrome',\n                'firefox',\n                'ie',\n                'edge'\n            ].join(',')\n\n            // Nightwatch.js supports color ouput, so wrap this step for ansi color\n            wrap([$class: 'AnsiColorBuildWrapper', 'colorMapName': 'XTerm']) {\n                // Run selenium tests using Nightwatch.js\n                // Ignore error codes. The junit publisher will cover setting build status.\n                sh \"./node_modules/.bin/nightwatch -e ${platform_configs} || true\"\n            }\n\n            junit 'reports/**'\n\n            step([$class: 'SauceOnDemandTestPublisher'])\n        }\n    }\n}\n\nSwitching from JUnit to xUnit\n\nI’ll start by replacing JUnit with xUnit in my pipeline.\nI use the Snippet Generator to create the step with the right parameters.\nThe main downside of using the xUnit plugin is that while it is Pipeline compatible,\nit still uses the more verbose step() syntax and has some very rough edges around that, too.\nI’ve filed\nJENKINS-37611\nbut in the meanwhile, we’ll work with what we have.\n\n// Original JUnit step\njunit 'reports/**'\n\n// Equivalent xUnit step - generated (reformatted)\nstep([$class: 'XUnitBuilder', testTimeMargin: '3000', thresholdMode: 1,\n    thresholds: [\n        [$class: 'FailedThreshold', failureNewThreshold: '', failureThreshold: '', unstableNewThreshold: '', unstableThreshold: '1'],\n        [$class: 'SkippedThreshold', failureNewThreshold: '', failureThreshold: '', unstableNewThreshold: '', unstableThreshold: '']],\n    tools: [\n        [$class: 'JUnitType', deleteOutputFiles: false, failIfNotNew: false, pattern: 'reports/**', skipNoTestFiles: false, stopProcessingIfError: true]]\n    ])\n\n// Equivalent xUnit step - cleaned\nstep([$class: 'XUnitBuilder',\n    thresholds: [[$class: 'FailedThreshold', unstableThreshold: '1']],\n    tools: [[$class: 'JUnitType', pattern: 'reports/**']]])\n\nIf I replace the junit step in my Jenkinsfile with that last step above,\nit produces a report and job result identical to the JUnit plugin but using the xUnit plugin.  Easy!\n\nnode {\n    stage \"Build\"\n    // ... snip ...\n\n    stage \"Test\"\n    // Add sauce credentials\n    sauce('f0a6b8ad-ce30-4cba-bf9a-95afbc470a8a') {\n        // Start sauce connect\n        sauceconnect(options: '', useGeneratedTunnelIdentifier: false, verboseLogging: false) {\n\n            // ... snip ...\n\n            // junit 'reports/**'\n            step([$class: 'XUnitBuilder',\n                thresholds: [[$class: 'FailedThreshold', unstableThreshold: '1']],\n                tools: [[$class: 'JUnitType', pattern: 'reports/**']]])\n\n            // ... snip ...\n        }\n    }\n}\n\nAccept a Baseline\n\nMost projects don’t start off with automated tests passing or even running.\nThey start with a people hacking and prototyping, and eventually they start to write tests.\nAs new tests are written, having tests checked-in, running, and failing can be valuable information.\nWith the xUnit plugin we can accept a baseline of failed cases and drive that number down over time.\n\nI’ll start by changing the Jenkinsfile to fail jobs only if the number of failures is greater than an expected baseline,\nin this case four failures. When I run the job with this change, the reported numbers remain the same, but the job passes.\n\nJenkinsfile\n\n// The rest of the Jenkinsfile is unchanged.\n// Only the xUnit step() call is modified.\nstep([$class: 'XUnitBuilder',\n    thresholds: [[$class: 'FailedThreshold', failureThreshold: '4']],\n    tools: [[$class: 'JUnitType', pattern: 'reports/**']]])\n\nNext, I can also check that the plugin reports the job as failed if more failures occur.\nSince this is sample code, I’ll do this by adding another failing test and checking the job\nreports as failed.\n\ntests/guineaPig.js\n\n// ... snip ...\n\n    'Guinea Pig Assert Title 0 - D': function(client) { /* ... */ },\n\n    'Guinea Pig Assert Title 0 - E': function(client) {\n        client\n            .url('https://saucelabs.com/test/guinea-pig')\n            .waitForElementVisible('body', 1000)\n            //.assert.title('I am a page title - Sauce Labs');\n            .assert.title('I am a page title - Sauce Labs - Cause a Failure');\n    },\n\n    afterEach: function(client, done) { /* ... */ }\n\n// ... snip ...\n\nIn a real project, we’d make fixes over a number of commits bringing the number of failures down and adjusting our baseline.\nSince this is a sample, I’ll just make all tests pass and set the job failure threshold for failed and skipped cases to zero.\n\nJenkinsfile\n\n// The rest of the Jenkinsfile is unchanged.\n// Only the xUnit step() call is modified.\nstep([$class: 'XUnitBuilder',\n    thresholds: [\n        [$class: 'SkippedThreshold', failureThreshold: '0'],\n        [$class: 'FailedThreshold', failureThreshold: '0']],\n    tools: [[$class: 'JUnitType', pattern: 'reports/**']]])\n\ntests/guineaPig.js\n\n// ... snip ...\n\n    'Guinea Pig Assert Title 0 - D': function(client) { /* ... */ },\n\n    'Guinea Pig Assert Title 0 - E': function(client) {\n        client\n            .url('https://saucelabs.com/test/guinea-pig')\n            .waitForElementVisible('body', 1000)\n            .assert.title('I am a page title - Sauce Labs');\n    },\n\n    afterEach: function(client, done) { /* ... */ }\n\n// ... snip ...\n\ntests/guineaPig_1.js\n\n// ... snip ...\n\n    'Guinea Pig Assert Title 1 - A': function(client) {\n        client\n            .url('https://saucelabs.com/test/guinea-pig')\n            .waitForElementVisible('body', 1000)\n            .assert.title('I am a page title - Sauce Labs');\n    },\n\n// ... snip ...\n\nAllow for Flakiness\n\nWe’ve all known the frustration of having one flaky test that fails once every ten jobs.\nYou want to keep it active so you can working isolating the source of the problem,\nbut you also don’t want to destablize your CI pipeline or reject commits that are actually okay.\nYou could move the test to a separate job that runs the \"flaky\" tests,\nbut in my experience that just leads to a job that is always in a failed state\nand a pile of flaky tests no one looks at.\n\nWith the xUnit plugin, we can keep the this flaky test in main test suite but allow\nthe our job to still pass.\n\nI’ll start by adding a sample flaky test.  After a few runs, we can see the test\nfails intermittently and causes the job to fail too.\n\ntests/guineaPigFlaky.js\n\n// New test file: tests/guineaPigFlaky.js\nvar https = require('https');\nvar SauceLabs = require(\"saucelabs\");\n\nmodule.exports = {\n\n    '@tags': ['guineaPig'],\n\n    'Guinea Pig Flaky Assert Title 0': function(client) {\n        var expectedTitle = 'I am a page title - Sauce Labs';\n        // Fail every fifth minute\n        if (Math.floor(Date.now() / (1000 * 60)) % 5 === 0) {\n            expectedTitle += \" - Cause failure\";\n        }\n\n        client\n            .url('https://saucelabs.com/test/guinea-pig')\n            .waitForElementVisible('body', 1000)\n            .assert.title(expectedTitle);\n    }\n\n    afterEach: function(client, done) {\n        client.customSauceEnd();\n\n        setTimeout(function() {\n            done();\n        }, 1000);\n\n    }\n\n};\n\nI can almost hear my teammates screaming in frustration just looking at this report.\nTo allow specific tests to be unstable but not others,\nI’m going to add a guard \"suite completed\" test to the suites that should be stable,\nand keep flaky test on it’s own.\nThen I’ll tell xUnit to allow for a number of failed tests, but no skipped ones.\nIf any test fails other than the ones I allow to be flaky,\nit will also result in one or more skipped tests and will fail the build.\n\n// The rest of the Jenkinsfile is unchanged.\n// Only the xUnit step() call is modified.\nstep([$class: 'XUnitBuilder',\n    thresholds: [\n        [$class: 'SkippedThreshold', failureThreshold: '0'],\n        // Allow for a significant number of failures\n        // Keeping this threshold so that overwhelming failures are guaranteed\n        //     to still fail the build\n        [$class: 'FailedThreshold', failureThreshold: '10']],\n    tools: [[$class: 'JUnitType', pattern: 'reports/**']]])\n\ntests/guineaPig.js\n\n// ... snip ...\n\n    'Guinea Pig Assert Title 0 - E': function(client) { /* ... */ },\n\n    'Guinea Pig Assert Title 0 - Suite Completed': function(client) {\n      // No assertion needed\n    },\n\n    afterEach: function(client, done) { /* ... */ }\n\n// ... snip ...\n\ntests/guineaPig_1.js\n\n// ... snip ...\n\n    'Guinea Pig Assert Title 1 - E': function(client) { /* ... */ },\n\n    'Guinea Pig Assert Title 1 - Suite Completed': function(client) {\n      // No assertion needed\n    },\n\n    afterEach: function(client, done) { /* ... */ }\n\n// ... snip ...\n\nAfter a few more runs, you can see the flaky test is still being flaky,\nbut it is no longer failing the build.  Meanwhile, if another test fails,\nit will cause the \"suite completed\" test to be skipped, failing the job.\nIf this were a real project, the test owner could instrument and eventually fix\nthe test.  When they were confident they had stabilized the test the could add\na \"suite completed\" test after it to enforce it passing without changes to other\ntests or framework.\n\nConclusion\n\nThis post has shown how to migrate from the JUnit plugin to the\nxUnit plugin on an existing project in Jenkins pipeline.  It also covered how to\nuse the features of xUnit plugin to get more meaningful and effective Jenkins\nreporting behavior.\n\nWhat I didn’t show was how many other formats xUnit supports - from CCPUnit to MSTest.  You can\nalso write your own XSL for result formats not on the known/supported list.\n\nLinks\n\nxUnit plugin\n\nbitwiseman/JS-Nightwatch.js\n\nsaucelabs-sample-test-frameworks","title":"xUnit and Pipeline","tags":["pipeline","plugins","xunit","nightwatch"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2016-08-29T00:00:00.000Z","id":"48307a4d-711a-56d1-885f-e9d1945fa4d5","slug":"/blog/2016/08/29/sauce-pipeline/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nTesting web applications across multiple browsers on different platforms can be challenging even for smaller applications.\nWith Jenkins and the\nSauce OnDemand Plugin,\nyou can wrangle that complexity by defining your Pipeline as Code.\n\nPipeline ♥ UI Testing, Too\n\nI recently started looking for a way to do browser UI testing for an open-source JavaScript project to which I contribute.\nThe project is targeted primarily at\nNode.js\nbut we’re committed to maintaining browser-client compatibility as well.\nThat means we should run tests on a matrix of browsers.\nSauce Labs\nhas an \"open-sauce\" program that provides free test instances to open-source projects.\nI decided to try using the\nSauce OnDemand Plugin\nand\nNightwatch.js\nto run Selenium tests on a sample project first, before trying a full-blown suite of tests.\n\nStarting from Framework\n\nI started off by following Sauce Labs' instructions on\n\" Setting up Sauce Labs with Jenkins\"\nas far as I could.\nI installed the\nJUnit and\nSauce OnDemand\nplugins, created an account with Sauce Labs, and\nadded my Sauce Labs credentials to Jenkins.\nFrom there I started to get a little lost.\nI’m new to Selenium and I had trouble understanding how to translate the instructions to my situation.\nI needed a working example that I could play with.\n\nHappily, there’s a whole range of sample projects in\n\" saucelabs-sample-test-frameworks\"\non GitHub, which show how to integrate Sauce Labs with various test frameworks, including Nightwatch.js.\nI forked the Nightwatch.js sample to\nbitwiseman/JS-Nightwatch.js\nand set to writing my Jenkinsfile.\nBetween the sample and the Sauce Labs instructions,\nI was able to write a pipeline that ran five tests on one browser via\nSauce Connect :\n\nnode {\n    stage \"Build\"\n    checkout scm\n\n    sh 'npm install' (1)\n\nstage \"Test\"\n    sauce('f0a6b8ad-ce30-4cba-bf9a-95afbc470a8a') { (2)\nsauceconnect(options: '', useGeneratedTunnelIdentifier: false, verboseLogging: false) { (3)\nsh './node_modules/.bin/nightwatch -e chrome --test tests/guineaPig.js || true' (4)\njunit 'reports/**' (5)\nstep([$class: 'SauceOnDemandTestPublisher']) (6)\n}\n    }\n}\n\n1\nInstall dependencies\n\n2\nUse my\npreviously added sauce credentials\n\n3\nStart up the\nSauce Connect\ntunnel to Sauce Labs\n\n4\nRun Nightwatch.js\n\n5\nUse JUnit to track results and show a trend graph\n\n6\nLink result details from Sauce Labs\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit url:'https://github.com/bitwiseman/JS-Nightwatch.js', branch: 'sauce-pipeline'.\n\nI ran this job a few times to get the JUnit report to show a trend graph.\n\nThis sample app generates the SauceOnDemandSessionID for each test, enabling the Jenkins Sauce OnDemand Plugin’s result publisher to link results to details Sauce Labs captured during the run.\n\nAdding Platforms\n\nNext I wanted to add a few more platforms to my matrix.\nThis would require changing both the test framework configuration and the pipeline.\nI’d need to add new named combinations of platform, browser, and browser version (called \"environments\") to the Nightwatch.js configuration file,\nand modify the pipeline to run tests in those new environments.\n\nThis is a perfect example of the power of pipeline as code.\nIf I were working with a separately configured pipeline,\nI’d have to make the change to the test framework, then change the pipeline manually.\nWith my pipeline checked in as code,\nI could change both in one commit,\npreventing errors resulting from pipeline configurations going out of sync from the rest of the project.\n\nI added three new environments to nightwatch.json :\n\n\"test_settings\" : {\n  \"default\": { /*----8 <----*/ },\n  \"chrome\": { /*----8 <----*/ },\n\n  \"firefox\": {\n    \"desiredCapabilities\": {\n      \"platform\": \"linux\",\n      \"browserName\": \"firefox\",\n      \"version\": \"latest\"\n    }\n  },\n  \"ie\": {\n    \"desiredCapabilities\": {\n      \"platform\": \"Windows 10\",\n      \"browserName\": \"internet explorer\",\n      \"version\": \"latest\"\n    }\n  },\n  \"edge\": {\n    \"desiredCapabilities\": {\n      \"platform\": \"Windows 10\",\n      \"browserName\": \"MicrosoftEdge\",\n      \"version\": \"latest\"\n    }\n  }\n}\n\nAnd I modified my Jenkinsfile to call them:\n\n//----8 (1)\n'chrome',\n        'firefox',\n        'ie',\n        'edge'\n    ].join(',')\n    // Run selenium tests using Nightwatch.js\n    sh \"./node_modules/.bin/nightwatch -e ${configs} --test tests/guineaPig.js\" (2)\n} //----8\n\n1\nUsing an array to improve readability and make it easy to add more platforms later.\n\n2\nChanged from single-quoted string to double-quoted to support variable substitution.\n\nTest frameworks have bugs too. Nightwatch.js (v0.9.8) generates incomplete JUnit files,\nreporting results without enough information in them to distinguish between platforms.\nI implemented a fix for it and\nsubmitted a PR to Nightwatch.js.\nThis blog shows output with that fix applied locally.\n\nAs expected, Jenkins picked up the new pipeline and ran Nightwatch.js on four platforms.\nSauce Labs of course recorded the results and correctly linked them into this build.\nNightwatch.js was already configured to use multiple worker threads to run tests against those platforms in parallel, and\nmy Sauce Labs account supported running them all at the same time,\nletting me cover four configurations in less that twice the time,\nand that added time was most due to individual new environments taking longer to complete.\nWhen I move to the actual project, this will let me run broad acceptance passes quickly.\n\nConclusion: To Awesome and Beyond\n\nConsidering the complexity of the system, I was impressed with how easy it was to integrate Jenkins with Sauce OnDemand to start testing on multiple browsers.\nThe plugin worked flawlessly with Jenkins Pipeline.\nI went ahead and ran some additional tests to show that failure reporting also behaved as expected.\n\n//----8 (1)\n//----8\n\n1\nRemoved --test filter to run all tests\n\nEpilogue: Pipeline vs. Freestyle\n\nJust for comparison here’s the final state of this job in Freestyle UI versus fully-commented pipeline code:\n\nThis includes the\nAnsiColor Plugin\nto support Nightwatch.js' default ANSI color output.\n\nFreestyle\n\nPipeline\n\nnode {\n    stage \"Build\"\n    checkout scm\n\n    // Install dependencies\n    sh 'npm install'\n\n    stage \"Test\"\n\n    // Add sauce credentials\n    sauce('f0a6b8ad-ce30-4cba-bf9a-95afbc470a8a') {\n        // Start sauce connect\n        sauceconnect(options: '', useGeneratedTunnelIdentifier: false, verboseLogging: false) {\n\n            // List of browser configs we'll be testing against.\n            def platform_configs = [\n                'chrome',\n                'firefox',\n                'ie',\n                'edge'\n            ].join(',')\n\n            // Nightwatch.js supports color ouput, so wrap this step for ansi color\n            wrap([$class: 'AnsiColorBuildWrapper', 'colorMapName': 'XTerm']) {\n\n                // Run selenium tests using Nightwatch.js\n                // Ignore error codes. The junit publisher will cover setting build status.\n                sh \"./node_modules/.bin/nightwatch -e ${platform_configs} || true\"\n            }\n\n            junit 'reports/**'\n\n            step([$class: 'SauceOnDemandTestPublisher'])\n        }\n    }\n}\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit url:'https://github.com/bitwiseman/JS-Nightwatch.js', branch: 'sauce-pipeline'.\n\nNot only is the pipeline as code more compact,\nit also allows for comments to further clarify what is being done.\nAnd as I noted earlier,\nchanges to this pipeline code are committed the same as changes to the rest of the project,\nkeeping everything synchronized, reviewable, and testable at any commit.\nIn fact, you can view the full set of commits for this blog post in the\nblog/sauce-pipeline\nbranch of the\nbitwiseman/JS-Nightwatch.js\nrepository.\n\nLinks\n\nSauce OnDemand Plugin\n\nbitwiseman/JS-Nightwatch.js\n\nsaucelabs-sample-test-frameworks","title":"Browser-testing with Sauce OnDemand and Pipeline","tags":["tutorial","pipeline","plugins","saucelabs","selenium","nightwatch"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}}]}},"pageContext":{"tag":"plugins","limit":8,"skip":56,"numPages":14,"currentPage":8}},
    "staticQueryHashes": ["3649515864"]}