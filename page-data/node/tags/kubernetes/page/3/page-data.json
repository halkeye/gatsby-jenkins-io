{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/kubernetes/page/3",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2018-04-10T00:00:00.000Z","id":"de012828-154d-5d83-8b81-e1199eaa8685","slug":"/blog/2018/04/10/opinionated-cd-jenkins-x/","strippedHtml":"I\nrecently wrote\nabout how all the cloud platforms are all in Kubernetes and so are developers.\nIt is an exciting time, but the problem for many is that this is\na huge blank sheet of paper for how to build and deploy applications.\nA white space, a void, a limitless canvas of possibilities.\nInsert metaphors here.\n\nThe problem, as you may guess, is that few people really like or are able to start with a blank canvas.\nI know I prefer to start with something working and iterate towards a solution,\nor be given some rails to stay on (again with the metaphors).\n\nThat’s where the Jenkins X project comes in.\nJenkins X is a Kubernetes-native continuous integration and continuous delivery platform\nfor developing cloud native applications that was recently introduced as a\nJenkins Enhancement Proposal,\nsponsored by James Strachan.\n\nThere is a lot to take in but at it’s heart,\nthis is an open source opinionated way to do continuous delivery with Kubernetes,\nnatively, without necessarily having to learn all the things I talked about in my last blog post.\nI shall attempt to explain what this is all about and why it matters to developers.\nAs someone said on the jenkins-dev mailing list\n“We have the two glued together with baling wire and twine” -\nJenkins X aims to simplify how to work with continuous delivery and Kubernetes.\n\nFirst and most importantly, let’s see the logo:\n\nYou can see the nautical theme leaking through (and Kubernetes).\nWhilst it is called Jenkins X, it is about quite a lot more than Jenkins.\n\nJenkins X makes decisions for you\n\nJenkins X presents itself to you initially as a handy sleek command line\n(a native binary you can install called jx - the debate is on as to how pronounce it).\nLet’s take a tour (sail?):\n\njx import my-app\n\nIf you have an existing project, this will detect what type of project it is, build a pipeline for you (and a bunch of Kubernetes things, like Helm Charts), add it to your project and set it up in GitHub, WebHooks and all, build the project (run the pipeline) and deploy a version to a “staging” environment.\n\nIf it looks ok, you can promote it to production:\n\njx promote --env production --version 1.0.1 my-app\n\nIf something went wrong in production, you can roll back an app to any version (the version numbers are made for you):\n\njx promote --env production --version 1.0.0 my-app\n> jx get apps # list versions\n\nAn environment is a well-established concept for web developers using\ncontinuous delivery: out of the box Jenkins X makes three of them for you\n(dev, staging and production), but you can make as many as you like.\nEnvironments have rules around how things are promoted into them\n(and they also have their own extensible pipelines,\nbut you can just use them as-is to start).\n\nYou can also create a Spring Boot microservice app:\n\njx create spring\n\nAnswer a few questions and it will set everything up for you.\n\nAny changes you make to your app automatically are built,\nand if the build looks good, they go through to the staging environment.\nWebHooks are setup for you (if you are using GitHub) to smooth this over.\n\nFor those looking at starting from pre-made apps, there are \"quickstarts\":\n\njx create quickstart\n\nThey are based on a growing set of starter apps, in a variety of languages and tech stacks.\n\nReview apps for your changes: Each pull request is built/tested,\nand a “review app” is made available in a temporary environment.\nThat means each proposed change, before it goes to the default branch (master),\nhas an environment made (temporary) that it can be tried out in.\nIn GitHub, this shows up as a comment in the pull request:\n\nProject type detection\n\nAs you can see, so far there is no editing or manual creation of pipelines,\nor scripting or setup, just importing or creating your app and go.\nThis is powered by\nDraft “packs”\n(a handy project that came out of Azure).\n What you end up with is a Jenkinsfile in your project repository.\n You may want to edit it one day, or you may be happy with it as is!\n Jenkins is famous for being un-opinionated in what you do,\n but Jenkins X has strong opinions (but you can extend and customise).\n\nimage::/images/jenkins-x/draft-logo.png[Draft Logo, width=300]\n\nDeploying or promoting to environments\n\nDeploying happens via pipelines behind the scenes -\nwhen a change is pushed, or a version promoted.\nYou don’t need to directly interact with Kubernetes if you don’t need to.\nA tool called Helm does the heavy lifting:\nHelm is used to package and perform installations and upgrade for your apps.\n\nThere is a little more magic going on here with environments, which you don’t see at first.\nEach environment, for a team, is represented by a Git repository behind the scenes.\nConfiguration as code is a well-established best practice these days,\nso why not use it to track deployments and initiate deployments.\nI also mentioned in my previous post how declarative Kubernetes is:\nit is perfect for keeping all config in a repository, of the desired system state.\n\nEach promotion is actually a pull request to a per-environment repository.\nThis repository is made and managed for you (and kept outside of the\nmain application code repository), you don’t have to look at it,\nbut you can extend things there should you need to.\nSpecific environment repositories may have different access rules,\nor be controlled by a different team (perhaps even deploy to a different cluster).\nSome have coined the term for this as “GitOps.”\nI first came across this concept on a\nWeaveWorks blog.\n\nI’ll try and explain this one with a diagram:\n\nThe pipeline is actually split in the middle.\nOn the left is the more familiar continuous integration pipeline.\nThis works on pull requests, pre-release version of things\nand is all about testing(automated and manual review).\nThe source of truth for this is the configuration in the\napplications repository: branches, pull requests and so on.\n\nThe right-hand side is the continuous delivery pipeline.\nThis kicks in when the application is ready to be updated with a new release.\nThis is the “GitOps” repo behind the scenes that controls the state of things in Kubernetes.\nA promotion on this side is a pull request, and then a merge,\nfrom the staging repository to the production repository.\n\nInstalling Jenkins X\n\nThe jx command line has a jx install command that installs it into a Kubernetes cluster.\n\nThe best experience initially is using Google’s excellent GKE service:\n\njx create cluster gke\n\nThis will ask a few questions, and go and set it all up for you in a\ncluster set aside for Jenkins X (recommended).\nJenkins X runs entirely as services on top of a Kubernetes cluster.\n\njx install\n\nIs designed to work with a Kubernetes cluster (if it already exists,\nrecommendation is to have a cluster put aside for Jenkins X if possible).\nAmazon EKS support is coming (mostly it is around testing),\nthat service is in beta/early access so it is still a work in progress,\nas is Microsoft Azures excellent AKS service.\n\nSo where is Jenkins?\n\nGood question, thanks for asking. Well, it is behind the scenes.\nAs you have seen, there was no direct interaction with Jenkins,\nbut it is there, running the pipelines for continuous integration and\ncontinuous delivery of the respective repositories, and orchestrating things with Kubernetes.\n\nIf you run jx get pipelines you can see URLs to the various pipelines\nthat have been setup for you are part of interacting with Jenkins X.\n\nBy the way,\nJames Strachan has written an extensive blog on jenkins.io\nthat really explores the Jenkins X project in-depth.\nOnce you finish reading this blog, take a stroll on over there and read James'.\nHe also provides several ways you can get involved in the project.\n\nWhat else can I do with the command line?\n\nLots, the jx command line has built in help:\n\njx open\n\nopen apps, services or pipelines in your browser\n\njx activity\n\nexplains how things got to where they are, a history\n\njx get environments\n\nlist environments\n\njx get apps\n\nshow the state of applications, what versions are in what environments.\n\nWhat’s next\n\nThere is a whole lot more to this, and lots more moving parts and services\nthat are set up for you that are very useful, but it is best to head over\nto jenkins-x.io and have a look.\n\nThis project is obviously in early stages (it is stll a Draft JEP after all) and there is lots happening.\nCheck out the Jenkins X community\nif you want to chat on slack, IRC, issues or email.\nAlso, read the\nJenkins Enhancement Proposal doc.","title":"Opinionated Kubernetes and Jenkins X","tags":["jenkins-x","kubernetes","pipeline"],"authors":[]}},{"node":{"date":"2018-03-19T00:00:00.000Z","id":"13be0610-c1c9-59de-ab77-d3fdaf0d1add","slug":"/blog/2018/03/19/introducing-jenkins-x/","strippedHtml":"We are excited to share and invite the community to join us on a project we’ve been thinking about over the last few months called Jenkins X which extends the Jenkins ecosystem to solve the problem of automating CI/CD in the cloud.\n\nBackground\n\nThe last few years have seen massive changes in the software industry:\n\nuse of immutable container images for distributing software which are smaller, easier to work with and lead to cheaper infrastructure costs than VMs alone (approx 20% less on average)\n\nKubernetes has become the defacto way of installing, upgrading, operating and managing containers at scale on any public or hybrid cloud\n\n2018 is the year all the major public clouds, operating system vendors and PaaS offerings support Kubernetes natively\n\nwe now have an open source industry standard for distributing, installing and managing applications on any cloud!\n\nincreased adoption of microservices and cloud native applications leading to massive increase in the number of components which require CI/CD along with increased release frequency\n\nimprovements in DevOps practices coming from the community such as the State of DevOps Report which show the approach of high performing teams\n\nincreasingly many businesses now realise that to compete you have to deliver value quickly via software\n\nteams need to become high performing if the business is to succeed\n\nAll of this adds up to an increased demand for teams to have a solution for cloud native CI/CD with lots of automation!\n\nIntroducing Jenkins X\n\nJenkins X is a project which rethinks how developers should interact with CI/CD in the cloud with a focus on making development teams productive through automation, tooling and DevOps best practices.\n\nJenkins X is open source and we invite you to give us feedback and to contribute to the project.\n\nWhats the big deal?\n\nFor many years Jenkins has been capable of doing pretty much anything in the CI/CD space; the challenge has always been figuring out how to get the right plugins, configuration and code to work together in your Jenkinsfile.\n\nFor me the big deal about Jenkins X is as a developer you can type one command jx create or jx import and get your source code, git repository and application created, automatically built and deployed to Kubernetes on each Pull Request or git push with full CI/CD complete with Environments and Promotion via GitOps!\n\nDevelopers and teams don’t have to spend time figuring out how to package software as docker images, create the Kubernetes YAML to run their application on kubernetes, create Preview environments or even learn how to implement CI/CD pipelines with declarative pipeline-as-code Jenkinsfiles. It’s all automated for you out of the box! So you can focus instead on delivering value!\n\nAt the same time, Jenkins X doesn’t hide anything. If you do want to hack the Dockerfile, Jenkinsfile or Helm charts for your apps or their environments then go right ahead - those are all available versioned in git with the rest of your source code with full CI/CD on it all. GitOps FTW!\n\nJenkins X automates CI/CD and DevOps best practices for you - so you can become a faster performing team! Let your butler do more work for you!\n\nDemo\n\nHere’s a demonstration of Jenkins X running on GKE:\n\nYou can check out more demos here.\n\nJenkins X Features\n\nNow lets walk through the features of Jenkins X that we showed in the demo:\n\nAutomated CI/CD Pipelines\n\nCreate new Spring Boot projects, new quickstarts or import existing source code quickly into Jenkins X via the jx command line tool and:\n\nget a Pipeline automatically setup for you that implements best practice CI/CD features:\n\ncreates a Jenkinsfile for defining the CI/CD pipelines through declarative pipeline-as-code\n\ncreates a Dockerfile for packaging the application up as an immutable container image (for applications which generate images)\n\ncreates a Helm chart for deploying and running your application on Kubernetes\n\nensures your code is in a git repository (e.g. GitHub) with the necessary webhooks to trigger the Jenkins CI/CD pipelines on push events\n\ntriggers the first release pipeline to promote your application to your teams Staging Environment\n\nThen on each Pull Request:\n\na CI pipeline is triggered to build your application and run all the tests ensuring you keep the master branch in a ready to release state\n\nyour Pull Request is deployed to a Preview Environment (more on this later)\n\nWhen a Pull Request is merged to the master branch the Release pipeline is triggered to create a new release:\n\na new semantic version number is generated\n\nthe source code is modified for the new version (e.g. pom.xml files get their elements modified) and then tagged in git\n\nnew versioned artifacts are published including:\n\ndocker image, helm chart and any language specific artifacts (e.g. pom.xml and jar files for Java, npm packages for node or binaries for go etc)\n\nthe new version is promoted to Environments (more on this later)\n\nEnvironment Promotion via GitOps\n\nIn Jenkins X each team gets their own environments. The default environments are Staging and Production but teams can create as many environments as they wish and call them whatever they prefer.\n\nAn Environment is a place to deploy code and each Environment maps to a separate namespace in Kubernetes so they are isolated from each other and can be managed independently.\n\nWe use something called GitOps to manage environments and perform promotion. This means that:\n\nEach environment gets its own git repository to store all the environment specific configuration together with a list of all the applications and their version and configuration.\n\nPromotion of new versions of applications to an environment results in:\n\na Pull Request is created for the configuration change that triggers the CI pipeline tests on the Environment along with code review and approval\n\nonce the Pull Request is merged the release pipeline for the environment which updates the applications running in that environment by applying the helm chart metadata from the git repository.\n\nEnvironments can be configured to either promote automatically as part of a release pipeline or they can use manual promotion.\n\nThe defaults today are for the Staging environment to use automatic promotion; so all merges to master are automatically promoted to Staging. Then the Production environment is configured to use manual promotion; so you choose when do promote.\n\nHowever it is easy to change the  configuration of how many environments you need and how they are configured via the jx create environment and jx edit environment commands\n\nPreview Environments\n\nJenkins X lets you create Preview Environments for Pull Requests. Typically this happens automatically in the Pull Request Pipelines when a Pull Request is submitted but you can also perform this manually yourself via the jx preview command.\n\nThe following happens when a Preview Environment is created:\n\na new Environment of kind Preview is created along with a kubernetes namespace which show up the jx get environments command along with the jx environment and jx namespace commands so you can see which preview environments are active and switch into them to look around\n\nthe Pull Request is built as a preview docker image and chart and deployed into the preview environment\n\na comment is added to the Pull Request to let your team know the preview application is ready for testing with a link to open the application. So in one click your team members can try out the preview!\n\nThis is particularly useful if you are working on a web application or REST endpoint; it lets your team interact with the running Pull Request to help folks approve changes.\n\nFeedback\n\nIf the commit comments reference issues (e.g. via the text fixes #123) then Jenkins X pipelines will generate release notes like those of the jx releases.\n\nAlso, as the version associated with those new commits is promoted to Staging or Production, you will get automated comments on each fixed issue that the issue is now available for review in the corresponding environment along with a link to the release notes and a link to the app running in that environment. e.g.\n\nGetting started\n\nHopefully you now want to give Jenkins X a try. One of the great features of Jenkins is that it’s super easy to get started: install Java, download a war and run via java -jar jenkins.war.\n\nWith Jenkins X we’ve tried to follow a similarly simple experience. One complication is that Jenkins X has more moving pieces than a single JVM; it also needs a Kubernetes cluster :)\n\nFirst you need to download and install the jx command line tool so its on your PATH.\n\nThen you need to run a single command to create a new Kubernetes cluster and install Jenkins X (in this example, on GKE).\n\njx create cluster gke\n\nToday we support creating Kubernetes clusters and installing Jenkins X on Amazon (AWS), Google (GKE), Microsoft Azure, and even locally using minikube.\nWe plan to support AWS EKS soon.\n\nAt the time of this writing the easiest cloud to get started with is Google’s GKE so we recommend you start there unless you already use AWS or Azure. Amazon and Microsoft are working hard to make Kubernetes clusters as easy to create and manage as they are on GKE.\n\nAll the public clouds have a free tier so you should be able to spin up a Kubernetes cluster and install Jenkins X for a few hours then tear it down and it should be cheaper than a cup of coffee (probably free!). Just remember to tear down the cluster when you are done!\n\nHere’s a demo of creating a kuberentes cluster and installing Jenkins X :\n\nIf you really don’t want to use the public cloud, you can install Jenkins X on an existing kubernetes cluster (if it has RBAC enabled!). Or, if you can install and run minikube, then you should be able to install Jenkins X on it as well.\n\nRelationship between Jenkins and Jenkins X\n\nJenkins is the core CI/CD engine within Jenkins X. So Jenkins X is built on the massive shoulders of Jenkins and its awesome community.\n\nWe are proposing Jenkins X as a sub project within the Jenkins foundation as Jenkins X has a different focus: automating CI/CD for the cloud using Jenkins plus other open source tools like Kubernetes, Helm, Git, Nexus/Artifactory etc.\n\nOver time we are hoping Jenkins X can help drive some changes in Jenkins itself to become more cloud native, which will benefit the wider Jenkins community in addition to Jenkins X.\n\nPlease join us!\n\nSo I hope the above has given you a feel for the vision of where we are heading with Jenkins X and to show where we are today. The project is still very young, we have lots to do and we are looking for more input on where to go next and what to focus on. We’re also working on high level roadmap.\n\nTo make Jenkins X a success we’d love you to get involved, try it out and give us feedback in the community! We love contributions whether its email, chat, issues or even better Pull Requests ;).\n\nIf you’re thinking of contributing here’s some ideas:\n\nGive us feedback. What could we improve? Anything you don’t like or you think is missing?\n\nHelp improve the documentation so its more clear how to get started and use Jenkins X\n\nAdd your own quickstarts so the Jenkins X community can easily bootstrap new projects using your quickstart. If you work on an open source project is there a good quickstart we could add to Jenkins X?\n\nIf you’d like to contribute to the code then try browse the current issues.\n\nwe have marked issues help wanted or good first issue to save you hunting around too much\n\nin particular we would love help on getting Jenkins X working well on windows or the integrations with cloud services, git providers and issues trackers\n\nfor more long term goals we’ve the roadmap\n\nwe could always use more test cases and to improve test coverage!\n\nTo help get faster feedback we are using Jenkins X as the CI/CD platform to develop Jenkins X itself. For example Jenkins X creates all the releases and release notes. We’ll talk more about UpdateBot in a future blog post but you can see all the automated pull requests generated in the various Jenkins X pipelines via UpdateBot pushing version changes from upstream dependencies into downstream repositories.\n\nNote that the Jenkins community tends to use IRC for chat and the Kubernetes community uses Slack, so Jenkins X has rooms for both IRC and slack depending on which chat technology you prefer - as the Jenkins X community will be working closely with both the Jenkins community and the various Kubernetes communities (Kubernetes, Helm, Skaffold, Istio et al).\n\nOne of the most rewarding things about open source is being able to learn from others in the community. So I’m hoping that even if you are not yet ready to use Kubernetes in your day job or are not yet interested in automating your Continuous Delivery - that you’ll at least consider taking a look at Jenkins X, if for no other reason than to help you learn more about all these new ideas, technologies and approaches!\n\nThanks for listening and I’m looking forward to seeing you in the community.\n\nLinks\n\nJenkins X JEP proposal\n\nJenkins X website\n\nGetting Started Guide\n\nDemos","title":"Introducing Jenkins X: a CI/CD solution for modern cloud applications on Kubernetes","tags":["jenkins-x","kubernetes","pipeline"],"authors":[]}},{"node":{"date":"2017-08-10T00:00:00.000Z","id":"7a11360a-42a4-5d64-8401-89a9e4f35904","slug":"/blog/2017/08/10/kubernetes-with-pipeline-acs/","strippedHtml":"This is a guest post by Pui Chee Chen,\nProduct Manager at Microsoft working on\nAzure\nDevOps open source integrations.\n\nRecently, we improved the Azure Credential plugin by\nadding a custom binding for Azure Credentials which allows you to use an\nAzure\nservice principal (the analog to a service or system account) via  the\nCredentials Binding plugin. This means it’s now trivial to run Azure CLI\ncommands from a Jenkins Pipeline. We also recently published the first version\nof the Azure App Service plugin which makes it very\neasy to deploy\nAzure Web\nApps directly from Jenkins Pipeline. While we’ll have\nmuch more to discuss in our Jenkins World presentation on\nAzure\nDevOps open source integrations, in this blog post I wanted to share some good\nsnippets of what is possible today with Jenkins Pipeline and Azure.\n\nFirst, a simple example using the Azure CLI to list resources in the\nsubscription:\n\n// Scripted //\nnode {\n    /* .. snip .. */\n    stage('Deploy') {\n        withCredentials([azureServicePrincipal('principal-credentials-id')]) {\n            sh 'az login --service-principal -u $AZURE_CLIENT_ID -p $AZURE_CLIENT_SECRET -t $AZURE_TENANT_ID'\n            sh 'az account set -s $AZURE_SUBSCRIPTION_ID'\n            sh 'az resource list'\n        }\n    }\n}\n// Declarative //\n\nazureServicePrincipal() cannot be used in Declarative Pipeline until\nJENKINS-46103 is\nresolved.\n\nOnce a Pipeline can interact with Azure, there are countless ways one could\nimplement continuous delivery with Jenkins and Azure. From a deploying a simple\nwebapp with the\nAzure\nApp Service plugin and the azureWebAppPublish step, or a more advanced\ncontainer-based delivery pipeline to deliver new containers to\nKubernetes\nvia Azure Container Service.\n\nWith the Docker Pipeline plugin and a little bit of\nextra scripting, a Jenkins Pipeline can also build and publish a Docker\ncontainer to an\nAzure\nContainer Registry :\n\n// Scripted //\nimport groovy.json.JsonSlurper\n\nnode {\n    def container\n    def acrSettings\n\n    withCredentials([azureServicePrincipal('principal-credentials-id')]) {\n        stage('Prepare Environment') {\n            sh 'az login --service-principal -u $AZURE_CLIENT_ID -p $AZURE_CLIENT_SECRET -t $AZURE_TENANT_ID'\n            sh 'az account set -s $AZURE_SUBSCRIPTION_ID'\n            acrSettings = new JsonSlurper().parseText(\n                                            sh(script: \"az acs show -o json -n my-acr\", returnStdout: true))\n        }\n\n        stage('Build') {\n            container = docker.build(\"${acrSettings.loginServer}/my-app:${env.BUILD_ID}\")\n        }\n\n        stage('Publish') {\n            /* https://issues.jenkins.io/browse/JENKINS-46108 */\n            sh \"docker login -u ${AZURE_CLIENT_ID} -p ${AZURE_CLIENT_SECRET} ${acrSettings.loginServer}\"\n            container.push()\n        }\n\n        stage('Deploy') {\n            echo 'Orchestrating a new deployment with kubectl is a simple exercise left to the reader ;)'\n        }\n    }\n}\n// Declarative //\n\nIf you have been following our\nAzure Blog, you may\nhave noticed we have shipped a lot of updates to provide better support for\nAzure on Jenkins, and vice versa, such as:\n\nHosted Jenkins. New\nSolution\nTemplate in Azure Marketplace lets you spin up a\nJenkins Controller on Azure in minutes. Not only is it easy and fast, the solution\ntemplate gives you option to scale up by selecting the VM disk type and size.\nAnd guess what? You can even select the Jenkins release type you want to use -\nLTS, weekly build or Azure verified - all under your control.\n\nContinuous integration experience. In the latest version of our\nAzure VM Agents plugin, we improved the user\nexperience and added the option to let you to select Managed Disk for disk\ntype (which is currently used extensively on\nci.jenknis.io. You no longer need to worry about\nexceeding the number of VMs on your subscription.\n\nContinuous deployment experience. Now, if\nAzure CLI is not your cup of tea, we released our first plugin to provide\ncontinuous deployment support to Azure App Service. The plugin supports all\nlanguages Azure App Service supports. We even have a walkthrough\nhere in the\nbrand new Jenkins Hub where you can find all Jenkins on Azure resources.\n\nPipeline readiness. Also, all Azure plugins are and will be pipeline ready.\nHave you been leveraging our\nAzure Storage plugin in your Pipeline?\n\nSo, what’s next? We have a big surprise in store at Jenkins World! :)\n\nWe are serious about supporting open source and the open source community.\nBe sure to catch our talk on\nAzure\nDevOps open source integrations.\nSee you at\nJenkins World 2017!\n\nJoin the Azure DevOps team at\nJenkins World in August,\nregister with the code JWFOSS for a 30% discount off your pass.","title":"CI/CD with Jenkins Pipeline and Azure","tags":["plugins","kubernetes","pipeline"],"authors":[]}},{"node":{"date":"2017-07-21T00:00:00.000Z","id":"215b578b-be74-5ce1-b9f3-38b9bfc35f5c","slug":"/blog/2017/07/21/scaling-jenkins-with-kubernetes-on-google-container-engine/","strippedHtml":"This is a guest post by Guillaume Laforge,\nDeveloper Advocate for Google Cloud\n\nLast week, I had the pleasure to speak at the\nJenkins Community Day conference, in Paris,\norganized by my friends from JFrog,\nprovider of awesome tools for software management and distribution.\nI covered how to scale Jenkins with Kubernetes on\nGoogle Container Engine.\n\nFor the impatient, here are the slides of the presentation I’ve given:\n\nBut let’s step back a little. In this article, I’d like to share with you why you would want to run Jenkins in the cloud,\nas well as give you some pointers to interesting resources on the topic.\n\nWhy running Jenkins in the cloud?\n\nSo why running Jenkins in the cloud? First of all, imagine your small team, working on a single project.\nYou have your own little server, running under a desk somewhere, happily building your application on each commit,\na few times a day. So far so good, your build machine running Jenkins isn’t too busy, and stays idle most of the day.\n\nLet’s do some bottom of the napkin calculations. Let’s say you have a team of 3 developers,\ncommitting roughly 4 times a day, on one single project, and the build takes roughly 10 minutes to go.\n\n3 developers * 4 commits / day / developer * 10 minutes build time * 1 project = 1 hour 20 minutes\n\nSo far so good, your server indeed stays idle most of the day. Usually, at most,\nyour developers will wait just 10 minutes to see the result of their work.\n\nBut your team is growing to 10 persons, the team is still as productive, but the project becoming bigger,\nthe build time goes up to 15 minutes:\n\n10 developers * 4 commits / day / developer * 15 minutes build time * 1 project = 10 hours\n\nYou’re already at 10 hours build time, so your server is busy the whole day, and at times,\nyou might have several build going on at the same time, using several CPU cores in parallel.\nAnd instead of building in 15 minutes, sometimes, the build might take longer, or your build might be queued.\nSo in theory, it might be 15 minutes, but in practice, it could be half an hour because of the length of the queue\nor the longer time to build parallel projects.\n\nNow, the company is successful, and has two projects instead of one (think a backend and a mobile app).\nYour teams grow further up to 20 developers per project. The developers are a little less productive\nbecause of the size of the codebase and project, so they only commit 3 times a day.\nThe build takes more time too, at 20 minutes (in ideal time). Let’s do some math again:\n\n20 developers * 3 commits / day / developer * 20 minutes build time * 2 projects = 40 hours\n\nWoh, that’s already 40 hours of total build time, if all the builds are run serially.\nFortunately, our server is multi-core, but still, there are certainly already many builds that are enqueued,\nand many of them, perhaps up to 2-3 or perhaps even 4 could be run in parallel.\nBut as we said, the build queue increases further, the real effective time of build is certainly longer than 30 minutes.\nPerhaps at times, developers won’t see the result of their developments before at least an hour, if not more.\n\nOne last calculation? With team sizes of 30 developers, decreased productivity of 2 commits, 25 build time,\nand 3 projects? And you’ll get 75 hours total build time. You may start creating a little build farm,\nwith a controller and several build agents. But you also increase the burden of server management.\nAlso, if you move towards a full Continuous Delivery or Continuous Deployment approach,\nyou may further increase your build times to go up to deployment, make more but smaller commits, etc.\nYou could think of running builds less often, or even on a nightly basis, to cope with the demand, but then,\nyour company is less agile, and the time-to-market for fixes of new features might increase,\nand your developers may also become more frustrated because they are developing in the blind,\nnot knowing before the next day if their work was successful or not.\n\nWith my calculations, you might think that it makes more sense for big companies, with tons of projects and developers.\nThis is quite true, but when you’re a startup, you also want to avoid taking care of local server management,\nprovisioning, etc. You want to be agile, and use only compute resources you need for the time you need them.\nSo even if you’re a small startup, a small team, it might still make sense to take advantage of the cloud.\nYou pay only for the actual time taken by your builds as the build agent containers are automatically provisioned\nand decommissioned. The builds can scale up via Kubernetes, as you need more (or less) CPU time for building everything.\n\nAnd this is why I was happy to dive into scaling Jenkins in the cloud. For that purpose,\nI decided to go with building with containers, with Kubernetes, as my app was also containerized as well.\nGoogle Cloud offers Container Engine, which is basically just Kubernetes in the cloud.\n\nUseful pointers\n\nI based my presentation and demo on some great solutions that are published on the Google Cloud documentation portal.\nLet me give you some pointers.\n\nOverview of Jenkins on Container Engine\n\nSetting up Jenkins on Container Engine\n\nConfiguring Jenkins for Container Engine\n\nContinuous Deployment to Container Engine using Jenkins\n\nLab: Build a Continuous Deployment Pipeline with Jenkins and Kubernetes\n\nThe latter one is the tutorial I actually followed for the demo that I presented during the conference.\nIt’s a simple Go application, with a frontend and backend.\nIt’s continuously build, on each commit (well, every minute to check if there’s a new commit),\nand deployed automatically in different environments: dev, canary, production.\nThe sources of the project are stored in Cloud Source Repository (it can be mirrored from Github, for example).\nThe containers are stored in Cloud Container Registry.\nAnd both the Jenkins controller and agents, as well as the application are running inside Kubernetes clusters in Container Engine.\n\nSummary and perspective\n\nDon’t bother with managing servers! Quickly, you’ll run out of CPU cycles,\nand you’ll have happier developers with builds that are super snappy!\n\nAnd for the record, at Google, dev teams are also running Jenkins!\nThere was a presentation ( video and\nslides\navailable) given last year by David Hoover at Jenkins World\ntalking about how developers inside Google are running hundreds of build agents to build projects on various platforms.","title":"Scaling Jenkins with Kubernetes on Google Container Engine","tags":["jenkins","kubernetes","jenkins-community-day-paris"],"authors":[]}},{"node":{"date":"2017-05-15T00:00:00.000Z","id":"355e4b5a-0c27-5f01-94a5-7a981139c2fb","slug":"/blog/2017/05/15/kubernetes-journey-on-azure/","strippedHtml":"With the\nongoing migration to Azure,\nI would like to share my thoughts regarding one of the biggest challenges we\nhave faced thus far: orchestrating container infrastructure. Many of the\nJenkins project’s applications are run as Docker containers, making Kubernetes\na logical choice as far as running our containers, but it presents its own set\nof challenges. For example, what would the workflow from development to\nproduction look like?\n\nBefore going deeper into the challenges, let’s review the requirements we\nstarted with:\n\nGit\n\nWe found it mandatory to keep track of all the infrastructure changes in Git\nrepositories, including secrets, in order to facilitate reviewing,\nvalidation, rollback, etc of all infra changes.\n\nTests\n\nInfrastructure contributors are geographically distributed and in different\ntimezones.  Getting feedback can take time, so we heavily rely on a lot of\ntests before any changes can be merged.\n\nAutomation\n\nThe change submitter is not necessarily the person who will deploy it.\nRepetitive tasks are error prone and a waste of time.\nFor these reasons, all steps must be automated and stay as simple as possible.\n\nA high level overview of our \"infrastructure as code\" workflow would look like:\n\nInfrastructure as Code Workflow\n\n__________       _________       ______________\n  |         |      |        |      |             |\n  | Changes | ---->|  Test  |----->| Deployment  |\n  |_________|      |________|  ^   |_____________|\n                               |\n                        ______________\n                       |             |\n                       | Validation  |\n                       |_____________|\n\nWe identified two possible approaches for implementing our container\norchestration with Kubernetes:\n\nThe Jenkins Way: Jenkins is triggered by a Git commit, runs the tests, and\nafter validation, Jenkins deploys changes into production.\n\nThe Puppet Way: Jenkins is triggered by a Git commit, runs the tests, and\nafter validation, it triggers Puppet to deploy into production.\n\nLet’s discuss these two approaches in detail.\n\nThe Jenkins Way\n\nWorkflow\n\n_________________       ____________________       ______________\n  |                |      |                   |      |             |\n  |    Github:     |      |     Jenkins:      |      |   Jenkins:  |\n  | Commit trigger | ---->| Test & Validation | ---->|  Deployment |\n  |________________|      |___________________|      |_____________|\n\nIn this approach, Jenkins is used to test, validate, and deploy our Kubernetes\nconfiguration files. kubectl can be run on a directory and is idempotent.\nThis means that we can run it as often as we want: the result will not change.\nTheoretically, this is the simplest way. The only thing needed is to run\nkubectl command each time Jenkins detects changes.\n\nThe following Jenkinsfile gives an example of this workflow.\n\nJenkinsfile\n\npipeline {\n    agent any\n    stages {\n      stage('Init'){\n        steps {\n          sh 'curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl'\n        }\n      }\n      stage('Test'){\n        steps {\n          sh 'Run tests'\n        }\n      }\n      stage('Deploy'){\n        steps {\n          sh './kubectl apply -R true -f my_project'\n        }\n      }\n    }\n  }\n\nThe devil is in the details of course, and it was not as easy as it looked at\nfirst sight.\n\nOrder matters\n\nSome resources needed to be deployed before others. A workaround was to use\nnumbers as file names. But this added extra logic at file name level, for\nexample:\n\nproject/00-nginx-ingress\nproject/09-www.jenkins.io\n\nPortability\n\nThe deployment environments needed to be the same across development machines\nand the Jenkins host. Although this a well-known problem, it was not easy to\nsolve.  The more the project grew, the more our scripts needed additional tools\n( make, bats, jq gpg, etc).  The more tools we used, the more issues\nappeared because of the different versions used.\n\nAnother challenge that emerged when dealing with different environments was:\nhow should we manage environment-specific configurations (dev, prod, etc)?\nWould it be better to define different configuration files per environment?\nPerhaps, but this means code duplication, or using file templates which would require\nmore tools ( sed, jinja2, erb), and more work.\n\nThere wasn’t a golden rule we discovered, and the answer is probably somewhere in between.\n\nIn any case, the good news is that a Jenkinsfile provides an easy way to\nexecute tasks from a Docker image, and an image can contain all the necessary\ntools in our environment. We can even use different Docker images for each\nstage along the way.\n\nIn the following example, I use the my_env Docker image. It contains all the\ntools needed to test, validate, and deploy changes.\n\nJenkinsfile\n\npipeline{\n  agent {\n    docker{\n      image 'my_env:1.0'\n    }\n  }\n  options{\n    buildDiscarder(logRotator(numToKeepStr: '10'))\n    disableConcurrentBuilds()\n    timeout(time: 1, unit: 'HOURS')\n  }\n  triggers{\n    pollSCM('* * * * *')\n  }\n  stages{\n    stage('Init'){\n      steps{\n        // Init everything required to deploy our infra\n        sh 'make init'\n      }\n    }\n    stage('Test'){\n      steps{\n       // Run tests to validate changes\n       sh 'make test'\n      }\n    }\n    stage('Deploy'){\n      steps{\n       // Deploy changes in production\n       sh 'make deploy'\n      }\n    }\n  }\n  post{\n    always {\n      sh 'make notify'\n    }\n  }\n}\n\nSecret credentials\n\nManaging secrets is a big subject and brings with it many different\nrequirements which are very hard to fulfill.  For obvious reasons, we couldn’t\npublish the credentials used within the infra project.  On the other hand, we\nneeded to keep track and share them, particularly for the Jenkins node that\ndeploys our cluster.  This means that we needed a way to encrypt or decrypt\nthose credentials depending on permissions, environments, etc.  We analyzed two\ndifferent approaches to handle this:\n\nStoring secrets in a key management tool like Key Vault or Vault and use them like a Kubernetes \"secret\" type of resource.\n→ Unfortunately, these tools are not yet integrated in Kubernetes. But we may come back to this option later.\nKubernetes issue: 10439\n\nPublishing and encrypting using a public GPG key.\nThis means that everybody can encrypt credentials for the infrastructure project but only the owner of the private key can decrypt credentials.\nThis solution implies:\n\nScripting: as secrets need to be decrypted at deployment time.\n\nTemplates: as secret values will change depending on the environment.\n→ Each Jenkins node should only have the private key to decrypt secrets associated to its environment.\n\nScripting\n\nFinally, the system we had built was hard to work with.  Our initial\nJenkinsfile which only ran one kubectl command slowly become a bunch of\nscripts to accommodate for:\n\nResources needing to be updated only in some situations.\n\nSecrets needing to be encrypted/decrypted.\n\nTests needing to be run.\n\nIn the end, the amount of scripts required to deploy the Kubernetes resources\nstarted to become unwieldy and we began asking ourselves: \"aren’t we\nre-inventing the wheel?\"\n\nThe Puppet Way\n\nThe Jenkins project already uses Puppet, so we decided to look at using Puppet\nto orchestrate our container deployment with Kubernetes.\n\nWorkflow\n\n_________________       ____________________       _____________\n  |                |      |                   |      |            |\n  |    Github:     |      |     Jenkins:      |      | Puppet:    |\n  | Commit trigger | ---->| Test & Validation | ---->| Deployment |\n  |________________|      |___________________|      |____________|\n\nIn this workflow, Puppet is used to template and deploy all Kubernetes\nconfigurations files needed to orchestrate our cluster.\nPuppet is also used to automate basic kubectl operations such as 'apply' or\n'remove' for various resources based on file changes.\n\nPuppet workflow\n\n______________________\n|                     |\n|  Puppet Code:       |\n|    .                |\n|    ├── apply.pp     |\n|    ├── kubectl.pp   |\n|    ├── params.pp    |\n|    └── resources    |\n|        ├── lego.pp  |\n|        └── nginx.pp |\n|_____________________|\n          |                                        _________________________________\n          |                                       |                                |\n          |                                       |  Host: Prod orchestrator       |\n          |                                       |    /home/k8s/                  |\n          |                                       |    .                           |\n          |                                       |    └── resources               |\n          | Puppet generate workspace             |        ├── lego                |\n          └-------------------------------------->|        │   ├── configmap.yaml  |\n            Puppet apply workspaces' resources on |        │   ├── deployment.yaml |\n          ----------------------------------------|        │   └── namespace.yaml  |\n          |                                       |        └── nginx               |\n          v                                       |            ├── deployment.yaml |\n ______________                                   |            ├── namespace.yaml  |\n |     Azure:  |                                  |            └── service.yaml    |\n | K8s Cluster |                                  |________________________________|\n |_____________|\n\nThe main benefit of this approach is letting Puppet manage the environment and run\ncommon tasks. In the following example, we define a Puppet class for Datadog.\n\nPuppet class for resource Datadog\n\n# Deploy datadog resources on kubernetes cluster\n#   Class: profile::kubernetes::resources::datadog\n#\n#   This class deploy a datadog agent on each kubernetes node\n#\n#   Parameters:\n#     $apiKey:\n#       Contain datadog api key.\n#       Used in secret template\nclass profile::kubernetes::resources::datadog (\n    $apiKey = base64('encode', $::datadog_agent::api_key, 'strict')\n  ){\n  include ::stdlib\n  include profile::kubernetes::params\n  require profile::kubernetes::kubectl\n\n  file { \"${profile::kubernetes::params::resources}/datadog\":\n    ensure => 'directory',\n    owner  => $profile::kubernetes::params::user,\n  }\n\n  profile::kubernetes::apply { 'datadog/secret.yaml':\n    parameters => {\n        'apiKey' => $apiKey\n    },\n  }\n  profile::kubernetes::apply { 'datadog/daemonset.yaml':}\n  profile::kubernetes::apply { 'datadog/deployment.yaml':}\n\n  # As secrets change do not trigger pods update,\n  # we must reload pods 'manually' in order to use updated secrets.\n  # If we delete a pod defined by a daemonset,\n  # this daemonset will recreate pods automatically.\n  exec { 'Reload datadog pods':\n    path        => [\"${profile::kubernetes::params::bin}/\"],\n    command     => 'kubectl delete pods -l app=datadog',\n    refreshonly => true,\n    environment => [\"KUBECONFIG=${profile::kubernetes::params::home}/.kube/config\"] ,\n    logoutput   => true,\n    subscribe   => [\n      Exec['apply datadog/secret.yaml'],\n      Exec['apply datadog/daemonset.yaml'],\n    ],\n  }\n}\n\n→\nMore \"resources\" examples\n\nLet’s compare the Puppet way with the challenges discovered with the Jenkins\nway.\n\nOrder Matters\n\nWith Puppet, it becomes easier to define priorities as\nPuppet provides relationship meta parameters and the function 'require' (see\nalso:\nPuppet\nrelationships).\n\nIn our Datadog example, we can be sure that deployment will respect the following order:\n\ndatadog/secret.yaml -> datadog/daemonset.yaml -> datadog/deployment.yaml\n\nCurrently, our Puppet code only applies configuration when it detects file\nchanges.  It would be better to compare local files with the cluster\nconfiguration in order to trigger the required updates, but we haven’t found a\ngood way to implement this yet.\n\nPortability\n\nAs Puppet is used to configure working environments, it becomes easier to be\nsure that all tools are present and correctly configured.  It’s also easier to\nreplicate environments and run tests on them with tools like\nRSpec-puppet, Serverspec or\nVagrant.\n\nIn our Datadog example, we can also easily change the Datadog API key depending\non the environment with Hiera.\n\nSecret credentials\n\nAs we were already using Hiera GPG\nwith Puppet, we decided to continue to use it, making managing secrets for\ncontainers very simple.\n\nScripting\n\nOf course the Puppet DSL is used, and even if it seems harder at the beginning,\nPuppet simplifies a lot the management of Kubernetes configuration files.\n\nConclusion\n\nIt was much easier to bootstrap the project with a full CI workflow within\nJenkins as long as the Kubernetes project itself stays basic. But as soon as\nthe project grew, and we started deploying different applications with\ndifferent configurations per environment, it became easier to delegate\nKubernetes management to Puppet.\n\nIf you have any comments feel free to send a message to\nJenkins Infra mailing list.\n\nThanks\n\nThanks to Lindsay Vanheyste, Jean Marc Meessen, and Damien Duportal for their feedback.","title":"A journey to Kubernetes on Azure","tags":["puppet","kubernetes","docker","azure"],"authors":[]}}]}},"pageContext":{"tag":"kubernetes","limit":8,"skip":16,"numPages":3,"currentPage":3}},
    "staticQueryHashes": ["3649515864"]}