{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/azure",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2020-03-30T00:00:00.000Z","id":"479ec095-b427-5e47-95a5-1bca6fbc97c5","slug":"/blog/2020/03/30/azure-key-vault-cred-provider/","strippedHtml":"Azure Key Vault is a product for securely managing keys, secrets and certificates.\n\nI’m happy to announce two new features in the Azure Key Vault plugin:\n\na credential provider to tightly link Jenkins and Azure Key Vault.\n\nhuge thanks to Jie Shen for contributing this\n\nintegration with the configuration-as-code plugin.\n\nThese changes were released in v1.8 but make sure to run the latest version of the plugin, there has been some fixes since then.\n\nSome advantages of using the credential provider rather than your own scripts:\n\nyour Jenkins jobs consume the credentials with no knowledge of Azure Key Vault, so they stay vendor-independent.\n\nthe provider integrates with the ecosystem of existing Jenkins credential consumers, such as the Slack Notifications plugin.\n\ncredential usage is recorded in the central Jenkins credentials tracking log.\n\nJenkins can use multiple credentials providers concurrently, so you can incrementally migrate credentials to Azure Key Vault while consuming other credentials from your existing providers.\n\nNote: Currently only secret text credentials are supported via the credential provider, you can use the configuration-as-code integration to load the secret from Azure Key Vault into the System Credential Provider to work around this limitation.\n\nGetting started\n\nInstall the Azure Key Vault plugin\n\nThen you will need to configure the plugin.\n\nAzure authentication\n\nThere’s two types of authentication you can use 'Microsoft Azure Service Principal' or 'Managed Identities for Azure Resources'\n\nThe easiest one to set this up quickly with is the 'Microsoft Azure Service Principal',\n\n$ az ad sp create-for-rbac --name http://service-principal-name\nCreating a role assignment under the scope of \"/subscriptions/ff251390-d7c3-4d2f-8352-f9c6f0cc8f3b\"\n  Retrying role assignment creation: 1/36\n  Retrying role assignment creation: 2/36\n{\n  \"appId\": \"021b5050-9177-4268-a300-7880f2beede3\",\n  \"displayName\": \"service-principal-name\",\n  \"name\": \"http://service-principal-name\",\n  \"password\": \"d9d0d1ba-d16f-4e85-9b48-81ea45a46448\",\n  \"tenant\": \"7e593e3e-9a1e-4c3d-a26a-b5f71de28463\"\n}\n\nIf this doesn’t work then take a look at the Microsoft documentation for creating a service principal.\n\nNote: for production 'Managed Identities for Azure Resources' is more secure as there’s no password involved and you don’t need to worry about the service principal’s password or certificate expiring.\n\nVault setup\n\nYou need to create a vault and give your service principal access to it:\n\nRESOURCE_GROUP_NAME=my-resource-group\naz group create --location uksouth --name $RESOURCE_GROUP_NAME\n\nVAULT=my-vault # you will need a unique name for the vault\naz keyvault create --resource-group $RESOURCE_GROUP_NAME --name $VAULT\naz keyvault set-policy --resource-group $RESOURCE_GROUP_NAME --name $VAULT \\\n  --secret-permissions get list --spn http://service-principal-name\n\nJenkins credential\n\nThe next step is to configure the credential in Jenkins:\n\nclick 'Credentials'\n\nclick 'System' (it’ll appear below the Credentials link in the side bar)\n\nclick 'Global credentials (unrestricted)'\n\nclick 'Add Credentials'\n\nselect 'Microsoft Azure Service Principal'\n\nfill out the form from the credential created above, appId is 'Client ID', password is 'Client Secret'\n\nclick 'Verify Service Principal', you should see 'Successfully verified the Microsoft Azure Service Principal'.\n\nclick 'Save'\n\nJenkins Azure Key Vault plugin configuration\n\nYou now have a credential you can use to interact with Azure resources from Jenkins, now you need to configure the plugin:\n\ngo back to the Jenkins home page\n\nclick 'Manage Jenkins'\n\nclick 'Configure System'\n\nsearch for 'Azure Key Vault Plugin'\n\nenter your vault url and select your credential\n\nclick 'Save'\n\nStore a secret in Azure Key Vault\n\nFor the step after this you will need a secret, so let’s create one now:\n\n$ az keyvault secret set --vault-name $YOUR_VAULT --name secret-key --value my-super-secret\n\nCreate a pipeline\n\nInstall the Pipeline plugin if you don’t already have it.\n\nFrom the Jenkins home page, click 'New item', and then:\n\nenter a name, i.e. 'key-vault-test'\n\nclick on 'Pipeline'\n\nadd the following to the pipeline definition:\n\n// Declarative //\npipeline {\n  agent any\n  environment {\n    SECRET_KEY = credentials('secret-key')\n  }\n  stages {\n    stage('Foo') {\n      steps {\n        echo SECRET_KEY\n        echo SECRET_KEY.substring(0, SECRET_KEY.size() - 1) // shows the right secret was loaded, don't do this for real secrets unless you're debugging\n      }\n    }\n  }\n}\n\n// Scripted //\nwithCredentials([string(credentialsId: 'secret-key', variable: 'SECRET_KEY')]) {\n    echo SECRET_KEY\n    echo SECRET_KEY.substring(0, SECRET_KEY.size() - 1) // shows the right secret was loaded, don't do this for real secrets unless you're debugging\n}\n\nYou have now successfully retrieved a credential from Azure Key Vault using native Jenkins credentials integration.\n\nconfiguration-as-code integration\n\nThe Configuration as Code plugin has been designed as an opinionated way to configure Jenkins based on human-readable declarative configuration files. Writing such a file should be easy without being a Jenkins expert.\n\nFor many secrets the credential provider is enough,\nbut when integrating with other plugins you will likely need more than string credentials.\n\nYou can use the configuration-as-code plugin (aka JCasC) to allow integrating with other credential types.\n\nconfigure authentication\n\nAs the JCasC plugin runs during initial startup the Azure Key Vault credential provider needs to be configured before JCasC runs during startup.\n\nThe easiest way to do that is via environment variables set before Jenkins starts up:\n\nexport AZURE_KEYVAULT_URL=https://my.vault.azure.net\nexport AZURE_KEYVAULT_SP_CLIENT_ID=...\nexport AZURE_KEYVAULT_SP_CLIENT_SECRET=...\nexport AZURE_KEYVAULT_SP_SUBSCRIPTION_ID=...\nexport AZURE_KEYVAULT_SP_SUBSCRIPTION_ID=...\n\nSee the azure-keyvault documentation for other authentication options.\n\nYou will now be able to refer to Azure Key Vault secret IDs in your jenkins.yaml file:\n\ncredentials:\n  system:\n    domainCredentials:\n      - credentials:\n        - usernamePassword:\n            description: \"GitHub\"\n            id: \"jenkins-github\"\n            password: \"${jenkins-github-apikey}\"\n            scope: GLOBAL\n            username: \"jenkinsadmin\"\n\nThanks for reading, send feedback on twitter using the tweet button in the top right, any issues or feature requests use GitHub issues.","title":"Introducing the Azure Key Vault Credentials Provider for Jenkins","tags":["jenkins","pipeline","security","azure","credentials","credential-provider","configuration-as-code"],"authors":[{"avatar":null,"blog":null,"github":"timja","html":"<div class=\"paragraph\">\n<p>Jenkins core maintainer, along with slack, azure-keyvault and configuration-as-code plugins.\nTim started using Jenkins in 2013 and became an active contributor in 2018.\nTim enjoys working on open source software in his “free” time.</p>\n</div>","id":"timja","irc":null,"linkedin":"tim-jacomb-98043174","name":"Tim Jacomb","slug":"/blog/authors/timja","twitter":"Tjaynz"}]}},{"node":{"date":"2019-07-25T00:00:00.000Z","id":"c8dcfc2d-fe9b-51ed-b1d8-8da5973e144c","slug":"/blog/2019/07/25/azure-artifact-manager/","strippedHtml":"Jenkins stores all generated artifacts on the controller server filesystem. This presents a couple of challenges especially when you try to run Jenkins in the cloud:\n\nAs the number of artifacts grow, your Jenkins controller will run out of disk space. Eventually, performance can be impacted.\n\nFrequent transfer of files between agents and controller may cause load, CPU or network issues which are always hard to diagnose.\n\nSeveral existing plugins allow you to manage your artifacts externally. To use these plugins, you need to know how they work and perform specific steps in your job’s configuration. And if you are new to Jenkins, you may find it hard to follow existing samples in Jenkins tutorial like Recording tests and artifacts.\n\nSo, if you are running Jenkins in Azure, you can consider automatically managing new artifacts on Azure Storage. The new Azure Artifact Management plugin allows you to store artifacts in Azure blob storage and simplify your existing Jenkins jobs that contain Jenkins general artifacts management steps. This approach will give you all the advantages of a cloud storage, with less effort on your part to maintain your Jenkins instance.\n\nConfiguration\n\nAzure storage account\n\nFirst, you need to have an Azure Storage account. You can skip this section if you already have one. Otherwise, create an Azure storage account for storing your artifacts. Follow this tutorial to quickly create one. Then navigate to Access keys in the Settings section to get the storage account name and one of its keys.\n\nExisting Jenkins instance\n\nFor existing Jenkins instance, make sure you install the Azure Artifact Manager plugin. Then you can go to your Jenkins System Configuration page and locate the Artifact Management for Builds section. Select the Add button to configure an Azure Artifact Storage. Fill in the following parameters:\n\nStorage Type: Azure storage supports several storage types like blob, file, queue etc. This plugin currently supports blob storage only.\n\nStorage Credentials: Credentials used to authenticate with Azure storage. If you do not have an existing Azure storage credential in you Jenkins credential store, click the Add button and choose Microsoft Azure Storage kind to create one.\n\nAzure Container Name: The container under which to keep your artifacts. If the container name does not exist in the blob, this plugin automatically creates one for you when artifacts are uploaded to the blob.\n\nBase Prefix: Prefix added to your artifact paths stored in your container, a forward slash will be parsed as a folder. In the following screenshot, all your artifacts will be stored in the “staging” folder in the container “Jenkins”.\n\nNew Jenkins instance\n\nIf you need to create a new Jenkins controller, follow this tutorial to quickly create an Jenkins instance on Azure. In the Integration Settings section, you can now set up Azure Artifact Manager directly. Note that you can change any of the configuration after your Jenkins instance is created. Azure storage account and credential, in this case, are still prerequisites.\n\nUsage\n\nJenkins Pipeline\n\nHere are a few commonly used artifact related steps in pipeline jobs; all are supported to push artifacts to the Azure Storage blob specified.\n\nYou can use archiveArtifacts step to archive target artifacts into Azure storage. For more details about archiveArtifacts step, see the Jenkins archiveArtifacts setp documentation.\n\nnode {\n  //...\n  stage('Archive') {\n    archiveArtifacts \"pattern\"\n  }\n}\n\nYou can use the unarchive step to retrieve the artifacts from Azure storage. For more details about unarchive step, please see unarchive step documentation.\n\nnode {\n  //...\n  stage('Unarchive') {\n    unarchive mapping: [\"pattern\": '.']\n  }\n}\n\nTo save a set of files so that you can use them later in the same build (generally on another node or workspace), you can use stash step to store files into Azure storage for later use. Stash step documentation can be found here.\n\nnode {\n  //...\n  stash name: 'name', includes: '*'\n}\n\nYou can use unstash step to retrieve the files saved with stash step from Azure storage to the local workspace. Unstash documentation can be found here.\n\nnode {\n  //...\n  unstash 'name'\n}\n\nFreeStyle Job\n\nFor a FreeStyle Jenkins job, you can use Archive the artifacts step in Post-build Actions to upload the target artifacts into Azure storage.\n\nThis Azure Artifact Manager plugin is also compatible with some other popular management plugins, such as the Copy Artifact plugin. You can still use these plugins without changing anything.\n\nTroubleshooting\n\nIf you have any problems or suggestions when using Azure Artifact Manager plugin, you can file a ticket on Jenkins JIRA for the azure-artifact-manager-plugin component.\n\nConclusion\n\nThe Azure Artifact Manager enables a more cloud-native Jenkins. This is the first step in the Cloud Native project. We have a long way to go to get Jenkins to run on cloud environments as a true “Cloud Native” application. We need help and welcome your participation and contributions to make Jenkins better. Please start contributing and/or give us feedback!","title":"Managing Jenkins Artifacts with the Azure Artifact Manager Plugin","tags":["general","azure","plugin"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/5f4c376818aa25b85cee206264e2692f/19e71/jshen.jpg","srcSet":"/gatsby-jenkins-io/static/5f4c376818aa25b85cee206264e2692f/77b35/jshen.jpg 32w,\n/gatsby-jenkins-io/static/5f4c376818aa25b85cee206264e2692f/d4a57/jshen.jpg 64w,\n/gatsby-jenkins-io/static/5f4c376818aa25b85cee206264e2692f/19e71/jshen.jpg 128w,\n/gatsby-jenkins-io/static/5f4c376818aa25b85cee206264e2692f/68974/jshen.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/5f4c376818aa25b85cee206264e2692f/ef6ff/jshen.webp 32w,\n/gatsby-jenkins-io/static/5f4c376818aa25b85cee206264e2692f/8257c/jshen.webp 64w,\n/gatsby-jenkins-io/static/5f4c376818aa25b85cee206264e2692f/6766a/jshen.webp 128w,\n/gatsby-jenkins-io/static/5f4c376818aa25b85cee206264e2692f/22bfc/jshen.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"gavinfish","html":"<div class=\"paragraph\">\n<p>Software engineer at Microsoft. Focusing on DevOps and cloud native.</p>\n</div>","id":"jshen","irc":null,"linkedin":null,"name":"Jie Shen","slug":"/blog/authors/jshen","twitter":null}]}},{"node":{"date":"2018-09-14T00:00:00.000Z","id":"9d454b50-9959-5557-8c15-e1041ef6bff4","slug":"/blog/2018/09/14/speaker-blog-jenkins-builds-jenkins/","strippedHtml":"Next week Olivier Vernin from CloudBees and Brian Benz from Microsoft will be presenting a session at DevOps World | Jenkins World about how Microsoft has been working with Jenkins to build Jenkins plugins and produce Jenkins on Microsoft Azure.\nThese plugins run Jenkins on Azure Linux and Windows VMs, Kubernetes, azure App service, as well as deploy artifacts to those Azure platforms and more.\nAll are open source and available on GitHub.\n\nHere’s our session, where we’ll be sharing successes and challenges of getting the infrastructure up and running:\n\nTuesday, September 18\n\nSession: Developing and Delivering Jenkins in the cloud\n11:15am - 12:00pm Brian Benz with Olivier Vernin, CloudBees\n\nIn this session, we’ll discuss the real-life implementation of Jenkins' development and delivery infrastructure in the cloud as it has evolved from a mix of platforms to Microsoft Azure.\nExpect a frank discussion of how issues that were encountered along the way were overcome, how the architecture has evolved, and what’s on the roadmap.\nWe’ll share important tips and tricks for implementing your own Jenkins infrastructure on any cloud, based on Jenkins' own experience with their implementation.\n\nSee you in San Francisco!\n\nCome meet us at\nDevOps World | Jenkins World 2018 on September 16-19th in San Francisco.\nWe will be hanging out around the OSS space, eager to answer more questions.\n\nRegister with the code JWFOSS for a 30% discount off your pass.","title":"Want to know how Jenkins builds Jenkins? Catch this session at DevOps World | Jenkins World next week in San Francisco!","tags":["jenkinsworld","jenkinsworld2018","azure","infrastructure"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/bf8e1/olblak.png","srcSet":"/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/914ee/olblak.png 32w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/1c9ce/olblak.png 64w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/bf8e1/olblak.png 128w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/acb7c/olblak.png 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/ef6ff/olblak.webp 32w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/8257c/olblak.webp 64w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/6766a/olblak.webp 128w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/22bfc/olblak.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"olblak","html":"<div class=\"paragraph\">\n<p>Olivier is the Jenkins infrastructure officer and Senior Operations Engineer at CloudBees.\nAs a regular contributor to the Jenkins infrastructure projects, he works on a wide range of tasks from services reliability to applications maintenance.</p>\n</div>","id":"olblak","irc":"olblak","linkedin":null,"name":"Olivier Vernin","slug":"/blog/authors/olblak","twitter":"0lblak"}]}},{"node":{"date":"2018-07-23T00:00:00.000Z","id":"bc08dba7-169b-5284-bbaa-6f3d64408472","slug":"/blog/2018/07/23/javadoc-service-improvements/","strippedHtml":"Jenkins infrastructure is continuously improving.\nThe latest service to get some attention and major improvement is the Jenkins javadoc.\n\nThere were a number of issues affecting that service:\n\nIrregular updates -\nDevelopers wouldn’t find the latest java documentation because of inadequate update frequence.\n\nBroken HTTPS support -\nwhen users would go to the Javadoc site they would get an unsafe site warning and then an incorrect redirection.\n\nObsolete content - Javadoc was not cleaned up correctly and plenty of obsolete pages remained which confused end users.\n\nAs Jenkins services\nmigrate to Azure infrastructure,\nsomething that needed to be done was to move the javadoc service there as a standalone service.\nI took the same approach as jenkins.io, putting data on an azure file storage, using a nginx proxy in front of it and running on kubernetes.\nThis approach brings multiple benefits:\n\nWe store static files on an azure file storage which brings data reliability, redundancy, etc.\n\nWe use Kubernetes Ingress to configure HTTP/HTTPS endpoint\n\nWe use Kubernetes Service to provide load balancing\n\nWe use Kubernetes deployment to deploy default nginx containers with azure file storage volume.\n\nHTTP/HTTPS workflow\n\n+----------------------+     goes on     +------------------------------+\n  |  Jenkins Developer   |---------------->+  https://javadoc.jenkins.io  |\n  +----------------------+                 +------------------------------+\n                                                                      |\n  +-------------------------------------------------------------------|---------+\n  | Kubernetes Cluster:                                               |         |\n  |                                                                   |         |\n  | +---------------------+     +-------------------+     +-----------v------+  |\n  | | Deployment: Javadoc |     | Service: javadoc  <-----| Ingress: javadoc |  |\n  + +---------------------+     +-------------------+     +------------------+  |\n  |                                           |                                 |\n  |                          -----------------+                                 |\n  |                          |                |                                 |\n  |                          |                |                                 |\n  | +------------------------v--+    +--------v------------------+              |\n  | | Pod: javadoc              |    | Pod: javadoc              |              |\n  | | container: \"nginx:alpine\" |    | container: \"nginx:alpine\" |              |\n  | | +-----------------------+ |    | +-----------------------+ |              |\n  | | | Volume:               | |    | | Volume:               | |              |\n  | | | /usr/share/nginx/html | |    | | /usr/share/nginx/html | |              |\n  | | +-------------------+---+ |    | +----+------------------+ |              |\n  | +---------------------|-----+    +------|--------------------+              |\n  |                       |                 |                                   |\n  +-----------------------|-----------------|-----------------------------------+\n                          |                 |\n                          |                 |\n                       +--+-----------------+-----------+\n                       |   Azure File Storage: javadoc  |\n                       +--------------------------------+\n\nThe javadoc static files are now generated by a Jenkins\njob regularly and then published from a trusted jenkins instance.\nWe only update what has changed and remove obsolete documents.\nMore information can be find\nhere\n\nThe next thing in continuously improving is also to look at the user experience of the javadoc to make it easier to discover javadoc for other components or versions.\n( Help Needed)\n\nThese changes all go towards improving the developer experience for those using javadocs and making life easier for core and plugin developers.\nSee the new and improved javadoc service here\nJenkins Javadoc.","title":"Jenkins Javadoc: Service Improvements","tags":["javadoc","azure","infrastructure","kubernetes"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/bf8e1/olblak.png","srcSet":"/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/914ee/olblak.png 32w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/1c9ce/olblak.png 64w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/bf8e1/olblak.png 128w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/acb7c/olblak.png 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/ef6ff/olblak.webp 32w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/8257c/olblak.webp 64w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/6766a/olblak.webp 128w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/22bfc/olblak.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"olblak","html":"<div class=\"paragraph\">\n<p>Olivier is the Jenkins infrastructure officer and Senior Operations Engineer at CloudBees.\nAs a regular contributor to the Jenkins infrastructure projects, he works on a wide range of tasks from services reliability to applications maintenance.</p>\n</div>","id":"olblak","irc":"olblak","linkedin":null,"name":"Olivier Vernin","slug":"/blog/authors/olblak","twitter":"0lblak"}]}},{"node":{"date":"2017-05-15T00:00:00.000Z","id":"355e4b5a-0c27-5f01-94a5-7a981139c2fb","slug":"/blog/2017/05/15/kubernetes-journey-on-azure/","strippedHtml":"With the\nongoing migration to Azure,\nI would like to share my thoughts regarding one of the biggest challenges we\nhave faced thus far: orchestrating container infrastructure. Many of the\nJenkins project’s applications are run as Docker containers, making Kubernetes\na logical choice as far as running our containers, but it presents its own set\nof challenges. For example, what would the workflow from development to\nproduction look like?\n\nBefore going deeper into the challenges, let’s review the requirements we\nstarted with:\n\nGit\n\nWe found it mandatory to keep track of all the infrastructure changes in Git\nrepositories, including secrets, in order to facilitate reviewing,\nvalidation, rollback, etc of all infra changes.\n\nTests\n\nInfrastructure contributors are geographically distributed and in different\ntimezones.  Getting feedback can take time, so we heavily rely on a lot of\ntests before any changes can be merged.\n\nAutomation\n\nThe change submitter is not necessarily the person who will deploy it.\nRepetitive tasks are error prone and a waste of time.\nFor these reasons, all steps must be automated and stay as simple as possible.\n\nA high level overview of our \"infrastructure as code\" workflow would look like:\n\nInfrastructure as Code Workflow\n\n__________       _________       ______________\n  |         |      |        |      |             |\n  | Changes | ---->|  Test  |----->| Deployment  |\n  |_________|      |________|  ^   |_____________|\n                               |\n                        ______________\n                       |             |\n                       | Validation  |\n                       |_____________|\n\nWe identified two possible approaches for implementing our container\norchestration with Kubernetes:\n\nThe Jenkins Way: Jenkins is triggered by a Git commit, runs the tests, and\nafter validation, Jenkins deploys changes into production.\n\nThe Puppet Way: Jenkins is triggered by a Git commit, runs the tests, and\nafter validation, it triggers Puppet to deploy into production.\n\nLet’s discuss these two approaches in detail.\n\nThe Jenkins Way\n\nWorkflow\n\n_________________       ____________________       ______________\n  |                |      |                   |      |             |\n  |    Github:     |      |     Jenkins:      |      |   Jenkins:  |\n  | Commit trigger | ---->| Test & Validation | ---->|  Deployment |\n  |________________|      |___________________|      |_____________|\n\nIn this approach, Jenkins is used to test, validate, and deploy our Kubernetes\nconfiguration files. kubectl can be run on a directory and is idempotent.\nThis means that we can run it as often as we want: the result will not change.\nTheoretically, this is the simplest way. The only thing needed is to run\nkubectl command each time Jenkins detects changes.\n\nThe following Jenkinsfile gives an example of this workflow.\n\nJenkinsfile\n\npipeline {\n    agent any\n    stages {\n      stage('Init'){\n        steps {\n          sh 'curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl'\n        }\n      }\n      stage('Test'){\n        steps {\n          sh 'Run tests'\n        }\n      }\n      stage('Deploy'){\n        steps {\n          sh './kubectl apply -R true -f my_project'\n        }\n      }\n    }\n  }\n\nThe devil is in the details of course, and it was not as easy as it looked at\nfirst sight.\n\nOrder matters\n\nSome resources needed to be deployed before others. A workaround was to use\nnumbers as file names. But this added extra logic at file name level, for\nexample:\n\nproject/00-nginx-ingress\nproject/09-www.jenkins.io\n\nPortability\n\nThe deployment environments needed to be the same across development machines\nand the Jenkins host. Although this a well-known problem, it was not easy to\nsolve.  The more the project grew, the more our scripts needed additional tools\n( make, bats, jq gpg, etc).  The more tools we used, the more issues\nappeared because of the different versions used.\n\nAnother challenge that emerged when dealing with different environments was:\nhow should we manage environment-specific configurations (dev, prod, etc)?\nWould it be better to define different configuration files per environment?\nPerhaps, but this means code duplication, or using file templates which would require\nmore tools ( sed, jinja2, erb), and more work.\n\nThere wasn’t a golden rule we discovered, and the answer is probably somewhere in between.\n\nIn any case, the good news is that a Jenkinsfile provides an easy way to\nexecute tasks from a Docker image, and an image can contain all the necessary\ntools in our environment. We can even use different Docker images for each\nstage along the way.\n\nIn the following example, I use the my_env Docker image. It contains all the\ntools needed to test, validate, and deploy changes.\n\nJenkinsfile\n\npipeline{\n  agent {\n    docker{\n      image 'my_env:1.0'\n    }\n  }\n  options{\n    buildDiscarder(logRotator(numToKeepStr: '10'))\n    disableConcurrentBuilds()\n    timeout(time: 1, unit: 'HOURS')\n  }\n  triggers{\n    pollSCM('* * * * *')\n  }\n  stages{\n    stage('Init'){\n      steps{\n        // Init everything required to deploy our infra\n        sh 'make init'\n      }\n    }\n    stage('Test'){\n      steps{\n       // Run tests to validate changes\n       sh 'make test'\n      }\n    }\n    stage('Deploy'){\n      steps{\n       // Deploy changes in production\n       sh 'make deploy'\n      }\n    }\n  }\n  post{\n    always {\n      sh 'make notify'\n    }\n  }\n}\n\nSecret credentials\n\nManaging secrets is a big subject and brings with it many different\nrequirements which are very hard to fulfill.  For obvious reasons, we couldn’t\npublish the credentials used within the infra project.  On the other hand, we\nneeded to keep track and share them, particularly for the Jenkins node that\ndeploys our cluster.  This means that we needed a way to encrypt or decrypt\nthose credentials depending on permissions, environments, etc.  We analyzed two\ndifferent approaches to handle this:\n\nStoring secrets in a key management tool like Key Vault or Vault and use them like a Kubernetes \"secret\" type of resource.\n→ Unfortunately, these tools are not yet integrated in Kubernetes. But we may come back to this option later.\nKubernetes issue: 10439\n\nPublishing and encrypting using a public GPG key.\nThis means that everybody can encrypt credentials for the infrastructure project but only the owner of the private key can decrypt credentials.\nThis solution implies:\n\nScripting: as secrets need to be decrypted at deployment time.\n\nTemplates: as secret values will change depending on the environment.\n→ Each Jenkins node should only have the private key to decrypt secrets associated to its environment.\n\nScripting\n\nFinally, the system we had built was hard to work with.  Our initial\nJenkinsfile which only ran one kubectl command slowly become a bunch of\nscripts to accommodate for:\n\nResources needing to be updated only in some situations.\n\nSecrets needing to be encrypted/decrypted.\n\nTests needing to be run.\n\nIn the end, the amount of scripts required to deploy the Kubernetes resources\nstarted to become unwieldy and we began asking ourselves: \"aren’t we\nre-inventing the wheel?\"\n\nThe Puppet Way\n\nThe Jenkins project already uses Puppet, so we decided to look at using Puppet\nto orchestrate our container deployment with Kubernetes.\n\nWorkflow\n\n_________________       ____________________       _____________\n  |                |      |                   |      |            |\n  |    Github:     |      |     Jenkins:      |      | Puppet:    |\n  | Commit trigger | ---->| Test & Validation | ---->| Deployment |\n  |________________|      |___________________|      |____________|\n\nIn this workflow, Puppet is used to template and deploy all Kubernetes\nconfigurations files needed to orchestrate our cluster.\nPuppet is also used to automate basic kubectl operations such as 'apply' or\n'remove' for various resources based on file changes.\n\nPuppet workflow\n\n______________________\n|                     |\n|  Puppet Code:       |\n|    .                |\n|    ├── apply.pp     |\n|    ├── kubectl.pp   |\n|    ├── params.pp    |\n|    └── resources    |\n|        ├── lego.pp  |\n|        └── nginx.pp |\n|_____________________|\n          |                                        _________________________________\n          |                                       |                                |\n          |                                       |  Host: Prod orchestrator       |\n          |                                       |    /home/k8s/                  |\n          |                                       |    .                           |\n          |                                       |    └── resources               |\n          | Puppet generate workspace             |        ├── lego                |\n          └-------------------------------------->|        │   ├── configmap.yaml  |\n            Puppet apply workspaces' resources on |        │   ├── deployment.yaml |\n          ----------------------------------------|        │   └── namespace.yaml  |\n          |                                       |        └── nginx               |\n          v                                       |            ├── deployment.yaml |\n ______________                                   |            ├── namespace.yaml  |\n |     Azure:  |                                  |            └── service.yaml    |\n | K8s Cluster |                                  |________________________________|\n |_____________|\n\nThe main benefit of this approach is letting Puppet manage the environment and run\ncommon tasks. In the following example, we define a Puppet class for Datadog.\n\nPuppet class for resource Datadog\n\n# Deploy datadog resources on kubernetes cluster\n#   Class: profile::kubernetes::resources::datadog\n#\n#   This class deploy a datadog agent on each kubernetes node\n#\n#   Parameters:\n#     $apiKey:\n#       Contain datadog api key.\n#       Used in secret template\nclass profile::kubernetes::resources::datadog (\n    $apiKey = base64('encode', $::datadog_agent::api_key, 'strict')\n  ){\n  include ::stdlib\n  include profile::kubernetes::params\n  require profile::kubernetes::kubectl\n\n  file { \"${profile::kubernetes::params::resources}/datadog\":\n    ensure => 'directory',\n    owner  => $profile::kubernetes::params::user,\n  }\n\n  profile::kubernetes::apply { 'datadog/secret.yaml':\n    parameters => {\n        'apiKey' => $apiKey\n    },\n  }\n  profile::kubernetes::apply { 'datadog/daemonset.yaml':}\n  profile::kubernetes::apply { 'datadog/deployment.yaml':}\n\n  # As secrets change do not trigger pods update,\n  # we must reload pods 'manually' in order to use updated secrets.\n  # If we delete a pod defined by a daemonset,\n  # this daemonset will recreate pods automatically.\n  exec { 'Reload datadog pods':\n    path        => [\"${profile::kubernetes::params::bin}/\"],\n    command     => 'kubectl delete pods -l app=datadog',\n    refreshonly => true,\n    environment => [\"KUBECONFIG=${profile::kubernetes::params::home}/.kube/config\"] ,\n    logoutput   => true,\n    subscribe   => [\n      Exec['apply datadog/secret.yaml'],\n      Exec['apply datadog/daemonset.yaml'],\n    ],\n  }\n}\n\n→\nMore \"resources\" examples\n\nLet’s compare the Puppet way with the challenges discovered with the Jenkins\nway.\n\nOrder Matters\n\nWith Puppet, it becomes easier to define priorities as\nPuppet provides relationship meta parameters and the function 'require' (see\nalso:\nPuppet\nrelationships).\n\nIn our Datadog example, we can be sure that deployment will respect the following order:\n\ndatadog/secret.yaml -> datadog/daemonset.yaml -> datadog/deployment.yaml\n\nCurrently, our Puppet code only applies configuration when it detects file\nchanges.  It would be better to compare local files with the cluster\nconfiguration in order to trigger the required updates, but we haven’t found a\ngood way to implement this yet.\n\nPortability\n\nAs Puppet is used to configure working environments, it becomes easier to be\nsure that all tools are present and correctly configured.  It’s also easier to\nreplicate environments and run tests on them with tools like\nRSpec-puppet, Serverspec or\nVagrant.\n\nIn our Datadog example, we can also easily change the Datadog API key depending\non the environment with Hiera.\n\nSecret credentials\n\nAs we were already using Hiera GPG\nwith Puppet, we decided to continue to use it, making managing secrets for\ncontainers very simple.\n\nScripting\n\nOf course the Puppet DSL is used, and even if it seems harder at the beginning,\nPuppet simplifies a lot the management of Kubernetes configuration files.\n\nConclusion\n\nIt was much easier to bootstrap the project with a full CI workflow within\nJenkins as long as the Kubernetes project itself stays basic. But as soon as\nthe project grew, and we started deploying different applications with\ndifferent configurations per environment, it became easier to delegate\nKubernetes management to Puppet.\n\nIf you have any comments feel free to send a message to\nJenkins Infra mailing list.\n\nThanks\n\nThanks to Lindsay Vanheyste, Jean Marc Meessen, and Damien Duportal for their feedback.","title":"A journey to Kubernetes on Azure","tags":["puppet","kubernetes","docker","azure"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/bf8e1/olblak.png","srcSet":"/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/914ee/olblak.png 32w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/1c9ce/olblak.png 64w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/bf8e1/olblak.png 128w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/acb7c/olblak.png 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/ef6ff/olblak.webp 32w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/8257c/olblak.webp 64w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/6766a/olblak.webp 128w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/22bfc/olblak.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"olblak","html":"<div class=\"paragraph\">\n<p>Olivier is the Jenkins infrastructure officer and Senior Operations Engineer at CloudBees.\nAs a regular contributor to the Jenkins infrastructure projects, he works on a wide range of tasks from services reliability to applications maintenance.</p>\n</div>","id":"olblak","irc":"olblak","linkedin":null,"name":"Olivier Vernin","slug":"/blog/authors/olblak","twitter":"0lblak"}]}},{"node":{"date":"2017-04-20T00:00:00.000Z","id":"1824bec7-be57-5f54-a27b-537ad8ff5a34","slug":"/blog/2017/04/20/secure-jenkins-on-azure/","strippedHtml":"This is a guest post by Claudiu Guiman and Eric Jizba,\nSoftware Engineers in the Azure DevOps team at Microsoft. If you have any questions, please email us at azdevopspub@microsoft.com.\n\nOne of the most frequently asked questions for managing a Jenkins instance is\n\"How do I make it secure?\" Like any other web application, these issues must be\nsolved:\n\nHow do I securely pass secrets between the browser and the server?\n\nHow do I hide certain parts from unauthorized users and show other parts to anonymous users?\n\nThis blog post details how to securely connect to a Jenkins instance and how to\nsetup a read-only public dashboard.  We’ll cover topics like: setting up a\nreverse proxy, blocking inbound requests to certain URLs and ports, enabling\nproject-based authorization, and making the Jenkins agents accessible through\nthe JNLP protocol.\n\nDeploy Jenkins\n\nThe simplest way to deploy a secure Jenkins instance is by using the Azure Marketplace offer. If you have an existing Jenkins instance or want to setup your instance manually, follow the steps below.\n\nSecurely log in to Jenkins\n\nAfter you’ve deployed your new virtual machine with a hosted Jenkins instance, you will notice that by default the instance listens on port 8080 using 'HTTP'. If you want to set up 'HTTPS' communication, you will need to provide an SSL certificate. Unfortunately, most certificate authorities are not cheap and other free services like Let’s Encrypt have a very small quota (about 20 certificates per week for the entire 'azure.com' subdomain). The only other option is to use a self-signed certificate, but then users must explicitly verify and mark your certificate as trusted, which is not recommended.\n\nIf you do not setup 'HTTPS' communication, the best way to make sure the sign-in credentials are not leaked due to a Man-in-the-middle attack is to only log in using SSH tunneling.\nAn SSH tunnel is an encrypted tunnel created through an SSH protocol connection, which can be used to transfer unencrypted traffic over an unsecured network. Simply run this command:\n\nLinux or Mac\n\nssh -L 8080:localhost:8080 @\n\nWindows ( using PuTTY)\n\nputty.exe -ssh -L 8080:localhost:8080 @\n\nThis command will open an SSH connection to your remote host and bind remote port 8080 to listen to requests coming from your local machine. Navigate to http://localhost:8080 on your local machine to view your Jenkins dashboard and you’ll be able to log in securely.\n\nSetup a reverse proxy\n\nNow that you can securely log in to your Jenkins instance, you should prevent people from accidentally authenticating through the public (unsecured) interface. To achieve this, you can setup a reverse proxy on the Jenkins hosting machine that will listen on a different port (80 is the best candidate) and redirect only certain requests to port 8080.\n\nSpecifically, it is recommended to block the login and the CLI requests. Some CLI versions fall back to unsecure HTTP connections if they have problems establishing the secured connection. In most cases, users don’t need the CLI and it should be enabled on an as-needed basis.\n\nInstall Nginx:\n\nsudo apt-get update\nsudo apt-get install nginx\n\nOpen the Nginx config file:\n\nsudo nano /etc/nginx/sites-enabled/default\n\nModify the file to configure Nginx to work as a reverse proxy (you’ll need to update):\n\nserver {\n    listen 80;\n    server_name;\n    # Uncomment the line bellow to change the default 403 error page\n    # error_page 403 /secure-jenkins;\n    location / {\n        proxy_set_header        Host \\$host:\\$server_port;\n        proxy_set_header        X-Real-IP \\$remote_addr;\n        proxy_set_header        X-Forwarded-For \\$proxy_add_x_forwarded_for;\n        proxy_set_header        X-Forwarded-Proto \\$scheme;\n        proxy_pass              http://localhost:8080;\n        proxy_redirect          http://localhost:8080 http://;\n        proxy_read_timeout      90;\n    }\n    #block requests to /cli\n    location /cli {\n        deny all;\n    }\n    #block requests to /login\n    location ~ /login* {\n        deny all;\n    }\n    # Uncomment the lines bellow to redirect /secure-jenkins\n    #location /secure-jenkins {\n    #  alias /usr/share/nginx/secure-jenkins;\n    #}\n}\n\nThe first section tells the Nginx server to listen to any requests that come from port 80. It also contains a commented redirect of the 403 error to a custom location (we’ll get back to this later).\n\nlisten 80;\n    server_name;\n    # error_page 403 /secure-jenkins;\n\nThe next section describes the reverse proxy configuration. This tells the Nginx server to take all incoming requests and proxy them to the Jenkins instance that is listening to port 8080 on the local network interface.\n\nlocation / {\n        proxy_set_header        Host \\$host:\\$server_port;\n        proxy_set_header        X-Real-IP \\$remote_addr;\n        proxy_set_header        X-Forwarded-For \\$proxy_add_x_forwarded_for;\n        proxy_set_header        X-Forwarded-Proto \\$scheme;\n        proxy_pass              http://localhost:8080;\n        proxy_redirect          http://localhost:8080 http://;\n        proxy_read_timeout      90;\n    }\n\nThe last section filters out specific URLs (login, cli) and denies access to them.\n\nlocation /cli {\n        deny all;\n    }\n    location ~ /login* {\n        deny all;\n    }\n\nRestart Nginx:\n\nsudo service nginx restart\n\nGo to http:// and verify you can access your Jenkins instance.\n\nVerify clicking 'login' returns a '403 Forbidden' page. If you want to customize that page, update the Nginx configuration and remove the comments around /secure-jenkins. This will redirect all 403 errors to the file /usr/share/nginx/secure-jenkins. You can add any content to that file, for example:\n\nsudo mkdir /usr/share/nginx/secure-jenkins\necho \"Access denied! Use SSH tunneling to log in to your Jenkins instance!\" | sudo tee /usr/share/nginx/secure-jenkins/index.html\n\nIf restart fails or you cannot access your instance, check the error log: cat /var/log/nginx/error.log\n\nSecure your Jenkins dashboard\n\nIf you go to http:// :8080 you’ll notice you can still\nbypass the reverse proxy and access the Jenkins instance directly through an\nunsecure channel. You can easily block all inbound requests on port 8080 on\nAzure with a\nNetwork\nSecurity Group (NSG).\n\nCreate the NSG and add it to your existing network interface or to the subnet your Azure Virtual Machine is bound to.\n\nAdd 2 inbound security rules:\n\nAllow requests to port 22 so you can SSH into the machine.\n\nAllow requests to port 80 so the reverse proxy can be reached\n\nBy default, all other external traffic will be blocked\n\nNavigate to http:// :8080 and verify you cannot connect.\n\nIf you don’t want to deploy an Azure Network Security Group, you can block port 8080 using the Uncomplicated Firewall (ufw)\n\nConfigure read-only access to your dashboard\n\nAfter installing Jenkins, the default security strategy is 'Logged-in users can do anything'. If you want to allow read-only access to anonymous users, you need to set up Matrix-based security. In this example, we’ll set up a project-based authorization matrix, so that you can make certain projects private and others public.\n\nInstall the Matrix Authorization Strategy Plugin and restart Jenkins.\n\nGo to http://localhost:8080/configureSecurity/ ('Configure Global Security' page under 'Manage Jenkins') and select 'Project-base Matrix Authorization Strategy' from the 'Authorization' options.\n\nAs an example, you can grant read-only access to anonymous users (Overall/Read, Job/Discover and Job/Read should be enough) and grant all logged in users full access in a  group called 'authenticated':\n\nConnect JNLP-based agents\n\nSince your Jenkins instance is only accessible through the reverse proxy on port 80, any Jenkins agents that use the JNLP protocol will not be able to register to the controller anymore. To overcome this problem, all agents must be in the same virtual network as the Jenkins controller and must connect using their private IP (by default, the NSG allows all internal traffic).\n\nMake sure that the Jenkins virtual machine will always be assigned the same private IP by going to the Azure Portal, opening the Network Interface of your virtual machine, opening 'IP configuration', and clicking on the configuration.\n\nMake sure the Private IP has a static assignment and restart the virtual machine if necessary.\n\nCopy the static IP Address and go to http://localhost:8080/configure ('Configure System' page under 'Manage Jenkins') and update the 'Jenkins URL' to point to that private IP ( https://10.0.0.5:8080/ in this example)\n\nNow agents can communicate through JNLP. If you want to streamline the process,\nyou can use the\nAzure VM Agents plugin,\nwhich automatically deploys agents in the same virtual network\nand connects them to the controller.","title":"Securing a Jenkins instance on Azure","tags":["azure","cloud"],"authors":[{"avatar":null,"blog":null,"github":"clguimanMSFT","html":"","id":"clguiman","irc":null,"linkedin":null,"name":"Claudiu Guiman","slug":"/blog/authors/clguiman","twitter":null}]}},{"node":{"date":"2016-12-31T00:00:00.000Z","id":"3267c8fe-f10e-530a-9caa-bae9eeb34fb0","slug":"/blog/2016/12/31/what-a-year/","strippedHtml":"I do not think it is an exaggeration to say: 2016 was the best year yet for the\nJenkins project. Since the first commit in 2006, the project has reached a\nnumber of significant milestones in its ten years but we have never experienced\nthe breadth of major milestones in such a short amount of time. From\nJenkins 2\nand\nBlue Ocean\nto the\nGoogle Summer of Code\nand\nJenkins World,\n\nI wanted to take a moment and celebrate the myriad of accomplishments which\ncouldn’t have happened without the help from everybody who participates in the\nJenkins project. The 1,300+ contributors to the\njenkinsci GitHub organization,\nthe 4,000+ members of the\ndevelopers mailing list,\nthe 8,000+ members of the\nusers mailing list,\nand countless others who have reported issues, submitted pull requests, and\npresented at meetups and conferences.\n\nJenkins 2\n\nThrough the course of 2016, the Jenkins project published 16\nLTS releases\nand 54\nWeekly releases.\nOf those 70 releases, the most notable may have been the\nJenkins 2.0 release\nwhich was published in April.\n\nJenkins 2 made Pipeline as Code front-and-center in the user experience,\nintroduced a new \"Getting Started\" experience, and included a number of other\nsmall UI improvements, all while maintaining backwards compatibility with\nexisting Jenkins environments.\n\nSince April, we have released a number of LTS\nreleases using Jenkins 2 as a baseline, meaning the Jenkins project no longer\nmaintains any 1.x release lines.\n\nThe\nPipeline\nefforts have continuted to gain steam since April, covered on this blog with a\nnumber of\nposts tagged \"pipeline\". Closing out 2016 with the\nannouncement of the beta for\nDeclarative Pipeline syntax\nwhich is expected in early 2017.\n\nBlue Ocean\n\nHot on the heels of Jenkins 2 announcement\"Blue Ocean, a new user experience for Jenkins\",\nwas\nopen sourced in May.\nBlue Ocean is a new project that rethinks the user experience of Jenkins.\nDesigned from the ground up for Jenkins Pipeline and compatible with Freestyle\njobs. The goal for the project is to reduce clutter and increase clarity for\nevery member of a team using Jenkins.\n\nThe Blue Ocean beta can be installed from the Update Center and can be run in\nproduction Jenkins environments alongside the existing UI. It adds the new user experience under\n/blue in the environment but does not disturb the existing UI.\n\nBlue Ocean is expected to reach \"1.0\" in the first half of 2017.\n\nAzure\n\nAlso in May of 2016, the Jenkins project announced an exciting\nPartnership with Microsoft\nto run our project infrastructure on\nAzure. While the migration of Jenkins project\ninfrastructure into Azure is still on-going, there have been some notable\nmilestones reached already:\n\nEnd-to-end TLS encrypted delivery for Debian/openSUSE/Red Hat repositories which are\nconfigured to use https://pkg.jenkins.io by the end-user.\n\nMajor capacity improvements to\nci.jenkins.io\nproviding on-demand Ubuntu and Windows build/test infrastructure.\n\nA full continuous delivery Pipeline for all Azure-based infrastructure using\nTerraform from Jenkins.\n\nThe migration to Azure is expected to complete in 2017.\n\nGoogle Summer of Code\n\nFor the first time in the history of the project, Jenkins was accepted into\nGoogle Summer of Code\n2016. Google Summer of Code (GSoC) is an annual, international, program\nwhich encourages college-aged students to participate with open source projects\nduring the summer break between classes. Students accepted into the program\nreceive a stipend, paid by Google, to work well-defined projects to improve or\nenhance the Jenkins project.\n\nIn exchange, numerous Jenkins community members volunteered as \"mentors\" for\nstudents to help integrate them into the open source community and succeed in\ncompleting their summer projects.\n\nA lot was learned during the summer which we look forward to applying to Google\nSummer of Code 2017\n\nJenkins World\n\nIn September, over one thousand people attended\nJenkins World,\nin Santa Clara, California.\n\nFollowing the event,\nLiam\nposted a series of blog posts which highlight some of the fantastic content\nshared by Jenkins users and contributors from around the world, such as:\n\nThe demos from the \"Experts\"\n\nSessions on Scaling Jenkins\n\nUsing Jenkins Pipeline\n\nThe Contributor Summit\n\nJenkins World was the first global event of its kind for Jenkins, it brought users\nand contributors together to exchange ideas on the current state of the\nproject, celebrate accomplishments of the past year, and look ahead at all the\nexiting enhancements coming down the pipe(line).\n\nIt was such a smashing success that\nJenkins World 2017\nis already scheduled for August 30-31st in San Francisco, California.\n\nJAM\n\nFinally, 2016 saw tremendous growth in the number of\nJenkins Area Meetups\n(JAMs) hosted around the world. JAMs are local meetups intended to bring\nJenkins users and contributors together for socializing and learning. JAMs are\norganized by local Jenkins community members who have a passion for sharing new\nJenkins concepts, patterns and tools.\n\nDriven by current Jenkins Events Officer,\nAlyssa Tong,\nand the dozens of passionate organizers, JAMs have become a great way to meet\nother Jenkins users near you.\n\nWhile we don’t yet have JAMs on each of the seven continents, you can always join the\nJenkins Online Meetup.\nThough we’re hoping more groups will be founded near you in 2017!\n\nI am personally grateful for the variety and volume of contributions made by\nthousands of people to the Jenkins project this year. I believe I can speak for\nproject founder,\nKohsuke Kawaguchi,\nin stating that the Jenkins community has grown beyond our anything we could\nhave imagined five years ago, let alone ten!\n\nThere are number of ways to\nparticipate\nin the Jenkins project, so if you didn’t have an opportunity to join in during\n2016, we hope to see you next year!","title":"Thank you for an amazing 2016","tags":["jam","jenkins2","pipeline","blueocean","azure","gsoc","new-year-blogpost"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-05-18T00:00:00.000Z","id":"3098cab4-cd65-52ec-a4a9-2b36e2ac20d7","slug":"/blog/2016/05/18/announcing-azure-partnership/","strippedHtml":"I am pleased to announce that we have partnered with Microsoft to migrate and\npower the Jenkins project’s infrastructure with\nMicrosoft Azure. The partnership comes\nat an important time, after the recent launch of Jenkins 2.0,\nJenkins users are more readily adopting Pipeline as\nCode and many other plugins at an increasing rate, elevating the importance of\nJenkins infrastructure to the overall success of the project. That strong and\ncontinued growth has brought new demands to our infrastructure’s design and\nimplementation, requiring the next step in its evolution. This partnership helps\nus grow with the rest of the project by unifying our existing infrastructure\nunder one comprehensive, modern and scalable platform.\n\nIn March we\ndiscussed\nthe potential partnership in our regularly scheduled\nproject\nmeeting,\nhighlighting some of the infrastructure challenges that we face:\n\nCurrently we have infrastructure in four different locations, with four\ndifferent infrastructure providers, each with their own APIs and tools for\nmanaging resources, each with varying capabilities and capacities.\n\nProject infrastructure is managed by a team of volunteers, operating\nmore than 15 different services and managing a number of additional external\nservices.\n\nOur current download/mirror network, while geographically distributed, is\nrelatively primitive and its implementation prevents us from using more modern\ndistribution best practices.\n\nIn essence, five years of tremendous growth for Jenkins has outpaced our\norganically grown, unnecessarily complex, project infrastructure. Migrating to\nAzure simplifies and improves our infrastructure in a dramatic way that would\nnot be possible without a comprehensive platform consisting of: compute, CDN,\nstorage and data-store services. Our partnership covers, at minimum, the next\nthree years of the project’s infrastructure needs, giving us a great home for\nthe future.\n\nAzure also enables a couple of projects that I\nhave long been dreaming of providing to Jenkins users and contributors:\n\nEnd-to-end TLS encrypted distribution of Jenkins packages, plugins and\nmetadata via the Azure CDN.\n\nMore complete build/test/release support and capacity on\nci.jenkins.io for plugin developers using\nAzure\nContainer Service and generic VMs.\n\nThe Jenkins infrastructure is all open\nsource which means  all of our Docker containers, Puppet code and many of our\ntools are all available on GitHub. Not\nonly can you watch the migration process to Azure as it happens, but I also\ninvite you to participate in making our project’s infrastructure better (join\nus in the #jenkins-infra channel on Freenode or our\nmailing list).\n\nSuffice it to say, I’m very excited about the bright [blue] future for the\nJenkins project and the infrastructure that powers it!","title":"Partnering with Microsoft to run Jenkins infrastructure on Azure","tags":["azure","infra","infrastructure"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}}]}},"pageContext":{"tag":"azure","limit":8,"skip":0,"numPages":2,"currentPage":1}},
    "staticQueryHashes": ["3649515864"]}