{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/tutorial/page/2",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2017-02-10T00:00:00.000Z","id":"06a04f0b-7823-5a11-8c3a-d385a336b68c","slug":"/blog/2017/02/10/declarative-html-publisher/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nDeclare Your Pipelines!\nDeclarative Pipeline 1.0 is here!\nThis is the second post in a series showing some of the cool features of\nDeclarative Pipeline.\n\nIn the\nprevious blog post,\nwe created a simple Declarative Pipeline.\nIn this blog post, we’ll go back and look at the Scripted Pipeline for the\nPublishing HTML Reports in Pipeline blog post.\nWe’ll convert that Pipeline to Declarative syntax (including properties), go\ninto more detail on the post section, and then we’ll use the agent\ndirective to switch our Pipeline to run in Docker.\n\nSetup\n\nFor this post, I’m going to use the\nblog/add-declarative/html\nbranch of\nmy fork of the\nhermann project.\nI’ve set up a Multibranch Pipeline and pointed it at my repository\nthe same as did it previous post.\nAlso the same as before, I’ve set this Pipeline’s Git configuration to\nautomatically \"Clean after checkout\".\n\nThis time we already have a Pipeline checked in.\nI’ll run it a few times to get a baseline.\n\nConverting to Declarative\n\nLet’s start by converting the Scripted Pipeline straight to Declarative.\n\n// Declarative //\npipeline {\n  agent any // <1> (2)\noptions {\n    // Keep the 10 most recent builds\n    buildDiscarder(logRotator(numToKeepStr:'10')) (3)\n}\n  stages {\n    stage ('Build') { (4)\nsteps {\n        // install required gems\n        sh 'bundle install'\n\n        // build and run tests with coverage\n        sh 'bundle exec rake build spec'\n\n        // Archive the built artifacts\n        archive includes: 'pkg/*.gem'\n\n        // publish html\n        publishHTML target: [\n            allowMissing: false,\n            alwaysLinkToLastBuild: false,\n            keepAll: true,\n            reportDir: 'coverage',\n            reportFiles: 'index.html',\n            reportName: 'RCov Report'\n          ]\n      }\n    }\n  }\n}\n// Scripted //\nproperties([[$class: 'BuildDiscarderProperty',\n                strategy: [$class: 'LogRotator', numToKeepStr: '10']]]) (3)\n\nnode { (1)\nstage ('Build') { (4)\n\n// Checkout\n    checkout scm (2)\n\n// install required gems\n    sh 'bundle install'\n\n    // build and run tests with coverage\n    sh 'bundle exec rake build spec'\n\n    // Archive the built artifacts\n    archive includes: 'pkg/*.gem'\n\n    // publish html\n    publishHTML [\n        allowMissing: false,\n        alwaysLinkToLastBuild: false,\n        keepAll: true,\n        reportDir: 'coverage',\n        reportFiles: 'index.html',\n        reportName: 'RCov Report'\n      ]\n\n  }\n}\n\n1\nSelect where to run this Pipeline, in this case \"any\" agent, regardless of label.\n\n2\nDeclarative automatically performs a checkout of source code on the agent,\nwhereas Scripted Pipeline users must explicitly call checkout scm.\n\n3\nSet the Pipeline option to preserve the ten most recent runs.\nThis overrides the default behavior from the Multibranch parent of this Pipeline.\n\n4\nRun the \"Build\" stage.\n\nNow that we have this Pipeline in Declarative form, let’s take a minute to do a\nlittle clean up.  We’ll split out the bundle actions a little more and move\nsteps into logically grouped stages.  Rather than having one monolithic \"Build\"\nstage, we’ll have details for each stage.  As long as we’re prettying things\nup, let’s switch to using Blue Ocean to view our\nbuilds, as well.\n\n// Declarative //\npipeline {\n  agent any\n  options {\n    // Keep the 10 most recent builds\n    buildDiscarder(logRotator(numToKeepStr:'10'))\n  }\n  stages {\n    stage ('Install') {\n      steps {\n        // install required gems\n        sh 'bundle install'\n      }\n    }\n    stage ('Build') {\n      steps {\n        // build\n        sh 'bundle exec rake build'\n\n        // Archive the built artifacts\n        archive includes: 'pkg/*.gem'\n      }\n    }\n    stage ('Test') {\n      steps {\n        // run tests with coverage\n        sh 'bundle exec rake spec'\n\n        // publish html\n        publishHTML target: [\n            allowMissing: false,\n            alwaysLinkToLastBuild: false,\n            keepAll: true,\n            reportDir: 'coverage',\n            reportFiles: 'index.html',\n            reportName: 'RCov Report'\n          ]\n      }\n    }\n  }\n}\n// Scripted //\n\nUsing post sections\n\nThis looks pretty good, but if we think about it\nthe archive and publishHTML steps are really post-stage actions.\nThey should only occur when the rest of their stage succeeds.\nAs our Pipeline gets more complex we might need to add actions that always happen\neven if a stage or the Pipeline as a whole fail.\n\nIn Scripted Pipeline, we would use try-catch-finally,\nbut we cannot do that in Declarative.\nOne of the defining features of the Declarative Pipeline\nis that it does not allow script-based control structures\nsuch as for loops, if-then-else blocks, or try-catch-finally blocks.\nOf course, internally Step implementations can still contain whatever conditional logic they want,\nbut the Declarative Pipeline cannot.\n\nInstead of free-form conditional logic,\nDeclarative Pipeline provides a set of Pipeline-specific controls:\nwhen directives, which we’ll look at in\na later blog post in this series, control whether to execute the steps in a stage,\nand\npost sections\ncontrol which actions to take based on result of a single stage\nor a whole Pipeline. post supports a number of\nrun conditions,\nincluding always (execute no matter what) and changed\n(execute when the result differs from previous run).\nWe’ll use success to run archive and publishHTML when their respective stages complete.\nWe’ll also use an always block with a placeholder for sending notifications,\nwhich I’ll implement in the next blog post.\n\n// Declarative //\npipeline {\n  agent any\n  options {\n    // Only keep the 10 most recent builds\n    buildDiscarder(logRotator(numToKeepStr:'10'))\n  }\n  stages {\n    stage ('Install') {\n      steps {\n        // install required gems\n        sh 'bundle install'\n      }\n    }\n    stage ('Build') {\n      steps {\n        // build\n        sh 'bundle exec rake build'\n      }\n\n      post {\n        success {\n          // Archive the built artifacts\n          archive includes: 'pkg/*.gem'\n        }\n      }\n    }\n    stage ('Test') {\n      steps {\n        // run tests with coverage\n        sh 'bundle exec rake spec'\n      }\n\n      post {\n        success {\n          // publish html\n          publishHTML target: [\n              allowMissing: false,\n              alwaysLinkToLastBuild: false,\n              keepAll: true,\n              reportDir: 'coverage',\n              reportFiles: 'index.html',\n              reportName: 'RCov Report'\n            ]\n        }\n      }\n    }\n  }\n  post {\n    always {\n      echo \"Send notifications for result: ${currentBuild.result}\"\n    }\n  }\n}\n// Scripted //\n\nSwitching agent to run in Docker\n\nagent can actually accept\nseveral other parameters instead of any.\nWe could filter on label \"some-label\", for example,\nwhich would be the equivalent of node ('some-label') in Scripted Pipeline.\nHowever, agent also lets us just as easily switch to using a Docker container,\nwhich replaces a more complicated set of changes in Scripted Pipeline:\n\npipeline {\n  agent {\n    // Use docker container\n    docker {\n      image 'ruby:2.3'\n    }\n  }\n  /* ... unchanged ... */\n}\n\nIf I needed to, I could add a label filter under docker\nto select a node to host the Docker container.\nI already have Docker available on all my agents, so I don’t need label -\nthis works as is.\nAs you can see below, the Docker container spins up at the start of the run\nand the pipeline runs inside it.  Simple!\n\nConclusion\n\nAt first glance, the Declarative Pipeline’s removal of control structures seems\nlike it would be too constrictive.  However, it replaces those structures with\nfacilities like the post section, that give us reasonable control over the\nflow our our Pipeline while still improving readability and maintainability.\nIn the next blog post, we’ll add notifications to this pipeline\nand look at how to use Shared Libraries with Declarative\nPipeline to share code and keep Pipelines easy to understand.\n\nLinks\n\nDeclarative Pipeline plugin\n\nDeclarative Pipeline Syntax Reference\n\nPipeline source for this post","title":"Declarative Pipeline: Publishing HTML Reports","tags":["tutorial","pipeline","declarative","plugins","ruby"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}},"publicURL":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/lnewman.jpeg"},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2017-02-07T00:00:00.000Z","id":"df12ba62-13a1-5a1b-88f3-afc58f167e79","slug":"/blog/2017/02/07/declarative-maven-project/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nDeclare Your Pipelines!\nDeclarative Pipeline 1.0 is here!\nThis is first in a series of blog posts that will show some of the cool features of\nDeclarative Pipeline.\nFor several of these posts, I’ll be revisiting some of my\nprevious posts\non using various plugins with (Scripted) Pipeline,\nand seeing how those are implemented in Declarative Pipeline.\n\nTo start though, let’s get familiar with the basic structure of a Declarative Pipeline\nby creating a simple Pipeline for a Maven-based Java project - the\nJenkins JUnit plugin.\nWe’ll create a minimal Declarative Pipeline,\nadd the settings needed to install Maven and the JDK,\nand finally we’ll actually run Maven to build the plugin.\n\nSet up\n\nWith Declarative, it is still possible to run Pipelines edited directly in the\nJenkins web UI, but one of the key features of \"Pipeline as Code\" is\nchecking-in and being able to track changes.  For this post, I’m going to use\nthe\nblog/add-declarative-pipeline\nbranch of\nmy fork of the JUnit plugin.\nI’m going to set up a Multi-branch Pipeline and point it at my repository.\n\nI’ve also set this Pipeline’s Git configuration to automatically \"clean after\ncheckout\" and to only keep the ten most recent runs.\n\nWriting a Minimal Pipeline\n\nAs has been said before, Declarative Pipeline provides a more structured,\n\"opinionated\" way to create Pipelines. I’m going to start by creating a minimal\nDeclarative Pipeline and adding it to my branch.  Below is a minimal Pipeline\n(with annotations) that just prints a message:\n\n// Declarative //\npipeline { (1)\nagent any // <2> (3)\nstages { (4)\nstage('Build') { (5)\nsteps { (6)\necho 'This is a minimal pipeline.' (7)\n}\n        }\n    }\n}\n// Scripted //\nnode { (2)\ncheckout scm (3)\nstage ('Build') { (5)\necho 'This is a minimal pipeline.' (6)\n}\n}\n\n1\nAll Declarative Pipelines start with a pipeline section.\n\n2\nSelect where to run this Pipeline, in this case \"any\" agent, regardless of label.\n\n3\nDeclarative automatically performs a checkout of source code on the agent,\nwhereas Scripted Pipeline users must explicitly call checkout scm,\n\n4\nA Declarative Pipeline is defined as a series of stages.\n\n5\nRun the \"Build\" stage.\n\n6\nEach stage in a Declarative Pipeline runs a series of steps.\n\n7\nRun the echo step to print a message in the Console Output.\n\nIf you are familiar with Scripted Pipeline, you can toggle the above\nDeclarative code sample to show the Scripted equivalent.\n\nOnce I add the Pipeline above to my Jenkinsfile and run \"Branch Indexing\", my\nJenkins will pick it up and run run it.  We see that the Declarative Pipeline\nhas added stage called \"Declarative: Checkout SCM\":\n\nThis a \"dynamic stage\", one of several the kinds that Declarative Pipeline adds\nas needed for clearer reporting.  In this case, it is a stage in which the\nDeclarative Pipeline automatically checkouts out source code on the agent.\n\nAs you can see above, we didn’t have to tell it do any of this,\n\nConsole Output\n\n[Pipeline] node\nRunning on osx_mbp in /Users/bitwiseman/jenkins/agents/osx_mbp/workspace/blog_add-declarative-pipeline\n[Pipeline] {\n[Pipeline] stage\n[Pipeline] { (Declarative: Checkout SCM)\n[Pipeline] checkout\nCloning the remote Git repository\n{ ... truncated 20 lines ... }\n[Pipeline] }\n[Pipeline] // stage\n[Pipeline] stage\n[Pipeline] { (Build)\n[Pipeline] echo\nThis is a minimal pipeline\n[Pipeline] }\n[Pipeline] // stage\n[Pipeline] }\n[Pipeline] // node\n[Pipeline] End of Pipeline\nFinished: SUCCESS\n\nDeclarative Pipeline syntax is a little more verbose than the equivalent Scripted Pipeline,\nbut the added detail gives a clearer, more consistent view of what the Pipeline is supposed to do.\nIt also gives us a structure into which we can add more configuration details about this Pipeline.\n\nAdding Tools to Pipeline\n\nThe next thing we’ll add in this Pipeline is a tools section to let us use\nMaven.  The tools section is one of several sections we can add under\npipeline, which affect the configuration of the rest of the Pipeline.  (We’ll\nlook at the others, including agent, in later posts.) Each tool entry will\nmake whatever settings changes, such as updating PATH or other environment\nvariables, to make the named tool available in the current pipeline.  It will\nalso automatically install the named tool if that tool is configured to do so\nunder \"Managing Jenkins\" → \"Global Tool Configuration\".\n\n// Declarative //\npipeline {\n    agent any\n    tools { (1)\nmaven 'Maven 3.3.9' (2)\njdk 'jdk8' (3)\n}\n    stages {\n        stage ('Initialize') {\n            steps {\n                sh '''\n                    echo \"PATH = ${PATH}\"\n                    echo \"M2_HOME = ${M2_HOME}\"\n                ''' (4)\n}\n        }\n\n        stage ('Build') {\n            steps {\n                echo 'This is a minimal pipeline.'\n            }\n        }\n    }\n}\n// Scripted Not Defined //\n\n1\ntools section for adding tool settings.\n\n2\nConfigure this Pipeline to use the Maven version matching \"Maven 3.3.9\"\n(configured in \"Managing Jenkins\" → \"Global Tool Configuration\").\n\n3\nConfigure this Pipeline to use the Maven version matching \"jdk8\"\n(configured in \"Managing Jenkins\" → \"Global Tool Configuration\").\n\n4\nThese will show the values of PATH and M2_HOME environment variables.\n\nWhen we run this updated Pipeline the same way we ran the first, we see that\nthe Declarative Pipeline has added another stage called \"Declarative: Tool\nInstall\": In the console output, we see that during this particular stage \"Maven 3.3.9\" gets installed,\nand the PATH and M2_HOME environment variables are set:\n\nConsole Output\n\n{ ... truncated lines ... }\n[Pipeline] // stage\n[Pipeline] stage\n[Pipeline] { (Declarative: Tool Install)\n[Pipeline] tool\nUnpacking https://repo.maven.apache.org/maven2/org/apache/maven/apache-maven/3.3.9/apache-maven-3.3.9-bin.zip\nto /Users/bitwiseman/jenkins/agents/osx_mbp/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.3.9\non osx_mbp\n[Pipeline] envVarsForTool\n[Pipeline] tool\n[Pipeline] envVarsForTool\n[Pipeline] }\n[Pipeline] // stage\n{ ... }\nPATH = /Library/Java/JavaVirtualMachines/jdk1.8.0_92.jdk/Contents/Home/bin:/Users/bitwiseman/jenkins/agents/osx_mbp/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.3.9/bin:...\nM2_HOME = /Users/bitwiseman/jenkins/agents/osx_mbp/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.3.9\n{ ... }\n\nRunning a Maven Build\n\nFinally, running a Maven build is trivial.  The tools section already added\nMaven and JDK8 to the PATH, all we need to do is call mvn install.  It\nwould be nice if I could split the build and the tests into separate stages,\nbut Maven is famous for not liking when people do that, so I’ll leave it alone\nfor now.\n\nInstead, let’s load up the results of the build using the JUnit plugin,\nhowever the version that was just built, sorry.\n\n// Declarative //\npipeline {\n    agent any\n    tools {\n        maven 'Maven 3.3.9'\n        jdk 'jdk8'\n    }\n    stages {\n        stage ('Initialize') {\n            steps {\n                sh '''\n                    echo \"PATH = ${PATH}\"\n                    echo \"M2_HOME = ${M2_HOME}\"\n                '''\n            }\n        }\n\n        stage ('Build') {\n            steps {\n                sh 'mvn -Dmaven.test.failure.ignore=true install' (1)\n}\n            post {\n                success {\n                    junit 'target/surefire-reports/**/*.xml' (2)\n}\n            }\n        }\n    }\n}\n// Scripted //\nnode {\n    checkout scm\n\n    String jdktool = tool name: \"jdk8\", type: 'hudson.model.JDK'\n    def mvnHome = tool name: 'mvn'\n\n    /* Set JAVA_HOME, and special PATH variables. */\n    List javaEnv = [\n        \"PATH+MVN=${jdktool}/bin:${mvnHome}/bin\",\n        \"M2_HOME=${mvnHome}\",\n        \"JAVA_HOME=${jdktool}\"\n    ]\n\n    withEnv(javaEnv) {\n    stage ('Initialize') {\n        sh '''\n            echo \"PATH = ${PATH}\"\n            echo \"M2_HOME = ${M2_HOME}\"\n        '''\n    }\n    stage ('Build') {\n        try {\n            sh 'mvn -Dmaven.test.failure.ignore=true install'\n        } catch (e) {\n            currentBuild.result = 'FAILURE'\n        }\n    }\n    stage ('Post') {\n        if (currentBuild.result == null || currentBuild.result == 'SUCCESS') {\n            junit 'target/surefire-reports/**/*.xml' (2)\n}\n    }\n}\n\n1\nCall mvn, the version configured by the tools section will be first on the path.\n\n2\nIf the maven build succeeded, archive the JUnit test reports for display in the Jenkins web UI.\nWe’ll discuss the\npost section in detail in the next blog post.\n\nIf you are familiar with Scripted Pipeline, you can toggle the above\nDeclarative code sample to show the Scripted equivalent.\n\nBelow is the console output for this last revision:\n\nConsole Output\n\n{ ... truncated lines ... }\n+ mvn install\n[INFO] Scanning for projects...\n[WARNING] The POM for org.jenkins-ci.tools:maven-hpi-plugin:jar:1.119 is missing, no dependency information available\n[WARNING] Failed to build parent project for org.jenkins-ci.plugins:junit:hpi:1.20-SNAPSHOT\n[INFO]\n[INFO] ------------------------------------------------------------------------\n[INFO] Building JUnit Plugin 1.20-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO]\n[INFO] --- maven-hpi-plugin:1.119:validate (default-validate) @ junit ---\n[INFO]\n[INFO] --- maven-enforcer-plugin:1.3.1:display-info (display-info) @ junit ---\n[INFO] Maven Version: 3.3.9\n[INFO] JDK Version: 1.8.0_92 normalized as: 1.8.0-92\n[INFO] OS Info: Arch: x86_64 Family: mac Name: mac os x Version: 10.12.3\n[INFO]\n{ ... }\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 03:25 min\n[INFO] Finished at: 2017-02-06T22:43:41-08:00\n[INFO] Final Memory: 84M/1265M\n[INFO] ------------------------------------------------------------------------\n\nConclusion\n\nThe new Declarative syntax is a significant step forward for Jenkins Pipeline.\nIt trades some verbosity and constraints for much greater clarity and\nmaintainability.  In the coming weeks, I’ll be adding new blog posts\ndemonstrating various features of the Declarative syntax along with some recent\nJenkins Pipeline improvements.\n\nLinks\n\nDeclarative Pipeline\n\nDeclarative Pipeline Syntax Reference\n\nJenkins JUnit plugin","title":"Declarative Pipeline for Maven Projects","tags":["tutorial","pipeline","declarative","maven","java"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}},"publicURL":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/lnewman.jpeg"},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2017-01-19T00:00:00.000Z","id":"437a3a39-d6ca-5875-b27d-0189cefc4150","slug":"/blog/2017/01/19/converting-conditional-to-pipeline/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nIntroduction\n\nWith all the new developments in\nJenkins Pipeline (and\nDeclarative Pipeline on the horizon),\nit’s easy to forget what we did to create \"pipelines\" before\nPipeline.\nThere are number of plugins, some that have been around since the very beginning,\nthat enable users to create \"pipelines\" in Jenkins.\nFor example, basic job chaining worked well in many cases, and the\nParameterized Trigger plugin\nmade chaining more flexible.\nHowever, creating chained jobs with conditional behavior was\nstill one of the harder things to do in Jenkins.\n\nThe\nConditional BuildStep plugin\nis a powerful tool that has allowed Jenkins users to write Jenkins jobs with complex conditional logic.\nIn this post, we’ll take a look at how we might converting Freestyle jobs that\ninclude conditional build steps to Jenkins Pipeline.\nUnlike Freestyle jobs, implementing conditional operations in Jenkins Pipeline is trivial,\nbut matching the behavior of complex conditional build steps will require a bit more care.\n\nGraphical Programming\n\nThe Conditional BuildStep plugin lets users add conditional logic to Freestyle\njobs from within the Jenkins web UI.  It does this by:\n\nAdding two types of Conditional BuildStep (\"Single\" and \"Multiple\") -\nthese build steps contain one or more other build steps to be run when the configured\ncondition is met\n\nAdding a set of Condition operations -\nthese control whether the Conditional BuildStep execute the contained step(s)\n\nLeveraging the Token Macro facility -\nthese provide values to the Conditions for evaluation\n\nIn the example below, this project will run the shell script step when the value of the\nREQUESTED_ACTION token equals \"greeting\".\n\nHere’s the output when I run this project with REQUESTED_ACTION set to \"greeting\":\n\nRun condition [Strings match] enabling prebuild for step [Execute shell]\nStrings match run condition: string 1=[greeting], string 2=[greeting]\nRun condition [Strings match] enabling perform for step [Execute shell]\n[freestyle-conditional] $ /bin/sh -xe /var/folders/hp/f7yc_mwj2tq1hmbv_5n10v2c0000gn/T/hudson5963233933358491209.sh\n+ echo 'Hello, bitwiseman!'\nHello, bitwiseman!\nFinished: SUCCESS\n\nAnd when I pass the value \"silence\":\n\nRun condition [Strings match] enabling prebuild for step [Execute shell]\nStrings match run condition: string 1=[silence], string 2=[greeting]\nRun condition [Strings match] preventing perform for step [Execute shell]\nFinished: SUCCESS\n\nThis is a simple example but the conditional step can contain any regular build step.\nWhen combined with other plugins, it can control whether to send notifications,\ngather data from other sources, wait for user feedback, or call other projects.\n\nThe Conditional BuildStep plugin does a great job of leveraging strengths of\nthe Jenkins web UI, Freestyle jobs, and UI-based programming,\nbut it is also hampered by their limitations.\nThe Jenkins web UI can be clunky and confusing at times.\nLike the steps in any Freestyle job, these conditional steps are only\nstored and viewable in Jenkins.\nThey are not versioned with other product or build code and can’t be code reviewed.\nLike any number of UI-based programming tools, it has to make trade-offs between clarity\nand flexibility: more options or clearer presentation.\nThere’s only so much space on the screen.\n\nConverting to Pipeline\n\nJenkins Pipeline, on the other hand, enables users to implement their pipeline as code.\nPipeline code can be written directly in the Jenkins Web UI or in any text editor.\nIt is a full-featured programming language,\nwhich gives users access to much broader set of conditional statements\nwithout the restrictions of UI-based programming.\n\nSo, taking the example above, the Pipeline equivalent is:\n\n// Declarative //\npipeline {\n    agent any\n    parameters {\n        choice(\n            choices: ['greeting' , 'silence'],\n            description: '',\n            name: 'REQUESTED_ACTION')\n    }\n\n    stages {\n        stage ('Speak') {\n            when {\n                // Only say hello if a \"greeting\" is requested\n                expression { params.REQUESTED_ACTION == 'greeting' }\n            }\n            steps {\n                echo \"Hello, bitwiseman!\"\n            }\n        }\n    }\n}\n// Script //\nproperties ([\n    parameters ([\n        choice (\n            choices: ['greeting', 'silence'],\n            description: '',\n            name : 'REQUESTED_ACTION')\n    ])\n])\n\nnode {\n    stage ('Speak') {\n        // Only say hello if a \"greeting\" is requested\n        if (params.REQUESTED_ACTION == 'greeting') {\n            echo \"Hello, bitwiseman!\"\n        }\n    }\n}\n\nWhen I run this project with REQUESTED_ACTION set to \"greeting\", here’s the output:\n\n[Pipeline] node\nRunning on osx_mbp in /Users/bitwiseman/jenkins/agents/osx_mbp/workspace/pipeline-conditional\n[Pipeline] {\n[Pipeline] stage\n[Pipeline] { (Speak)\n[Pipeline] echo\nHello, bitwiseman!\n[Pipeline] }\n[Pipeline] // stage\n[Pipeline] }\n[Pipeline] // node\n[Pipeline] End of Pipeline\nFinished: SUCCESS\n\nWhen I pass the value \"silence\", the only change is \"Hello, bitwiseman!\" is not printed.\n\nSome might argue that the Pipeline code is a bit harder to understand on first reading.\nOthers would say the UI is just as confusing if not more so.\nEither way, the Pipeline representation is considerably more compact than the Jenkins UI presentation.\nPipeline also lets us add helpful comments, which we can’t do in the Freestyle UI.\nAnd we can easily put this Pipeline in a Jenkinsfile to be code-reviewed, checked-in, and versioned\nalong with the rest of our code.\n\nConditions\n\nThe previous example showed the \"Strings match\" condition and its Pipeline equivalent.\nLet’s look at couple more interesting conditions and their Jenkins Pipeline equivalents.\n\nBoolean condition\n\nYou might think that a boolean condition would be the simplest condition, but it isn’t.\nSince it works with string values from tokens, the Conditional BuildStep plugin offers\na number of ways to indicate true or false.\nTruth is a case insensitive match of one of the following:\n1 (the number one), Y, YES, T, TRUE, ON or RUN.\n\nPipeline can duplicate these, but depending on the scenario we might consider\nwhether a simpler expression would suffice.\n\nPipeline\n\n// Declarative //\nwhen {\n    // case insensitive regular expression for truthy values\n    expression { return token ==~ /(?i)(Y|YES|T|TRUE|ON|RUN)/ }\n}\nsteps {\n    /* step */\n}\n\n// Script //\n// case insensitive regular expression for truthy values\nif (token ==~ /(?i)(Y|YES|T|TRUE|ON|RUN)/) {\n    /* step */\n}\n\nLogical \"OR\" of conditions\n\nThis condition wraps other conditions.\nIt takes their results as inputs and performs a logical \"or\" of the results.\nThe AND and NOT conditions do the same, performing their respective operations.\n\nPipeline\n\n// Declarative //\nwhen {\n    // A or B\n    expression { return A || B }\n}\nsteps {\n    /* step */\n}\n\n// Script //\n// A or B\nif (A || B) {\n    /* step */\n}\n\nTokens\n\nTokens can be considerably more work than conditions.\nThere are more of them and they cover a much broader range of behaviors.\nThe previous example showed one of the simpler cases, accessing a build parameter,\nwhere the token has a direct equivalent in Pipeline.\nHowever, many tokens don’t have direct equivalents,\nsome take a parameters (adding to their complexity),\nand some provide information that is simply not exposed in Pipeline yet.\nSo, determining how to migrate tokens needs to be done on case-by-case basis.\n\nLet’s look at a few examples.\n\n\"FILE\" token\n\nExpands to the contents of a file. The file path is relative to the build workspace root.\n\n${FILE,path=\"PATH\"}\n\nThis token maps directly to the readFile step.\nThe only difference is the file path for readFile is relative to the\ncurrent working directory on the agent, but that is the workspace root by default.\nNo problem.\n\nPipeline\n\n// Declarative //\nwhen {\n    expression { return readFile('pom.xml').contains('mycomponent') }\n}\nsteps {\n    /* step */\n}\n\n// Script //\nif (readFile('pom.xml').contains('mycomponent')) {\n    /* step */\n}\n\nGIT_BRANCH\n\nExpands to the name of the branch that was built.\n\nParameters (descriptions omitted): all, fullName.\n\nThis information may or may not be exposed in Pipeline.  If you’re using the\nPipeline Multibranch plugin\nenv.BRANCH_NAME will give similar basic information, but doesn’t offer the parameters.\nThere are also\nseveral\nissues\nfiled around GIT_* tokens in Pipeline.\nUntil they are addressed fully, we can follow the pattern shown in\npipeline-examples,\nexecuting a shell to get the information we need.\n\nPipeline\n\nGIT_BRANCH = sh(returnStdout: true, script: 'git rev-parse --abbrev-ref HEAD').trim()\n\nCHANGES_SINCE_LAST_SUCCESS\n\nDisplays the changes since the last successful build.\n\nParameters (descriptions omitted):\nreverse, format, changesFormat, showPaths, pathFormat,\nshowDependencies, dateFormat, regex, replace, default.\n\nNot only is the information provided by this token not exposed in Pipeline,\nthe token has ten optional parameters, including format strings and regular expression\nsearches. There are a number of ways we might get similar information in Pipeline.\nEach have their own particular limitations and ways they differ from the token output.\nThen we’ll need to consider how each of the parameters changes the output.\nIf nothing else, translating this token is clearly beyond the scope of this post.\n\nSlightly More Complex Example\n\nLet’s do one more example that shows some of these conditions and tokens.\nThis time we’ll perform different build steps depending on what branch we’re building.\nWe’ll take two build parameters: BRANCH_PATTERN and FORCE_FULL_BUILD.\nBased on BRANCH_PATTERN, we’ll checkout a repository.\nIf we’re building on the master branch or the user checked FORCE_FULL_BUILD,\nwe’ll call three other builds in parallel\n( full-build-linux, full-build-mac, and full-build-windows),\nwait for them to finish, and report the result.\nIf we’re not building on the master branch and the user did not check FORCE_FULL_BUILD,\nwe’ll print a message saying we skipped the full builds.\n\nFreestyle\n\nHere’s the configuration for Freestyle version.\n(It’s pretty long.  Feel free to skip down to the Pipeline version):\n\nThe Pipeline version of this job determines the GIT_BRANCH branch by\nrunning a shell script that returns the current local branch name.\nThis means that the Pipeline version must checkout to a local branch (not a detached head).\n\nFreestyle version of this job does not require a local branch, GIT_BRANCH is set automatically.\nHowever, to maintain functional parity, the Freestyle version of this job includes\n\"Checkout to Specific Local Branch\" as well.\n\nPipeline\n\nHere’s the equivalent Pipeline:\n\nFreestyle version of this job is not stored in source control.\n\nIn general, the Pipeline version of this job would be stored in source control,\nwould checkout scm, and would run that same repository.\nHowever, to maintain functional parity, the Pipeline version shown does a checkout\nfrom source control but is not stored in that repository.\n\nPipeline\n\n// Script //\nproperties ([\n    parameters ([\n        string (\n            defaultValue: '*',\n            description: '',\n            name : 'BRANCH_PATTERN'),\n        booleanParam (\n            defaultValue: false,\n            description: '',\n            name : 'FORCE_FULL_BUILD')\n    ])\n])\n\nnode {\n    stage ('Prepare') {\n        checkout([$class: 'GitSCM',\n            branches: [[name: \"origin/${BRANCH_PATTERN}\"]],\n            doGenerateSubmoduleConfigurations: false,\n            extensions: [[$class: 'LocalBranch']],\n            submoduleCfg: [],\n            userRemoteConfigs: [[\n                credentialsId: 'bitwiseman_github',\n                url: 'https://github.com/bitwiseman/hermann']]])\n    }\n\n    stage ('Build') {\n        GIT_BRANCH = 'origin/' + sh(returnStdout: true, script: 'git rev-parse --abbrev-ref HEAD').trim()\n        if (GIT_BRANCH == 'origin/master' || params.FORCE_FULL_BUILD) {\n\n            // Freestyle build trigger calls a list of jobs\n            // Pipeline build() step only calls one job\n            // To run all three jobs in parallel, we use \"parallel\" step\n            // https://jenkins.io/doc/pipeline/examples/#jobs-in-parallel\n            parallel (\n                linux: {\n                    build job: 'full-build-linux', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                },\n                mac: {\n                    build job: 'full-build-mac', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                },\n                windows: {\n                    build job: 'full-build-windows', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                },\n                failFast: false)\n\n        } else {\n            echo 'Skipped full build.'\n        }\n    }\n}\n// Declarative //\npipeline {\n    agent any\n    parameters {\n        string (\n            defaultValue: '*',\n            description: '',\n            name : 'BRANCH_PATTERN')\n        booleanParam (\n            defaultValue: false,\n            description: '',\n            name : 'FORCE_FULL_BUILD')\n    }\n\n    stages {\n        stage ('Prepare') {\n            steps {\n                checkout([$class: 'GitSCM',\n                    branches: [[name: \"origin/${BRANCH_PATTERN}\"]],\n                    doGenerateSubmoduleConfigurations: false,\n                    extensions: [[$class: 'LocalBranch']],\n                    submoduleCfg: [],\n                    userRemoteConfigs: [[\n                        credentialsId: 'bitwiseman_github',\n                        url: 'https://github.com/bitwiseman/hermann']]])\n            }\n        }\n\n        stage ('Build') {\n            when {\n                expression {\n                    GIT_BRANCH = 'origin/' + sh(returnStdout: true, script: 'git rev-parse --abbrev-ref HEAD').trim()\n                    return GIT_BRANCH == 'origin/master' || params.FORCE_FULL_BUILD\n                }\n            }\n            steps {\n                // Freestyle build trigger calls a list of jobs\n                // Pipeline build() step only calls one job\n                // To run all three jobs in parallel, we use \"parallel\" step\n                // https://jenkins.io/doc/pipeline/examples/#jobs-in-parallel\n                parallel (\n                    linux: {\n                        build job: 'full-build-linux', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                    },\n                    mac: {\n                        build job: 'full-build-mac', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                    },\n                    windows: {\n                        build job: 'full-build-windows', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                    },\n                    failFast: false)\n            }\n        }\n        stage ('Build Skipped') {\n            when {\n                expression {\n                    GIT_BRANCH = 'origin/' + sh(returnStdout: true, script: 'git rev-parse --abbrev-ref HEAD').trim()\n                    return !(GIT_BRANCH == 'origin/master' || params.FORCE_FULL_BUILD)\n                }\n            }\n            steps {\n                echo 'Skipped full build.'\n            }\n        }\n    }\n}\n\nConclusion\n\nAs I said before, the Conditional BuildStep plugin is great.\nIt provides a clear, easy to understand way to add conditional logic to any Freestyle job.\nBefore Pipeline, it was one of the few plugins to do this and it remains one of the most popular plugins.\nNow that we have Pipeline, we can implement conditional logic directly in code.\n\nThis is blog post discussed how to approach converting conditional build steps to Pipeline\nand showed a couple concrete examples.  Overall, I’m pleased with the results so far.\nI found scenarios which could not easily be migrated to Pipeline, but even those\nare only more difficult, rather than impossible.\n\nThe next thing to do is add a section to the\nJenkins Handbook documenting the Pipeline\nequivalent of all of the Conditions and the most commonly used Tokens.\nLook for it soon!\n\nLinks\n\nConditional BuildStep plugin","title":"Converting Conditional Build Steps to Jenkins Pipeline","tags":["pipeline","freestyle","plugins","conditional-build-step","tutorial"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}},"publicURL":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/lnewman.jpeg"},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2016-08-29T00:00:00.000Z","id":"48307a4d-711a-56d1-885f-e9d1945fa4d5","slug":"/blog/2016/08/29/sauce-pipeline/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nTesting web applications across multiple browsers on different platforms can be challenging even for smaller applications.\nWith Jenkins and the\nSauce OnDemand Plugin,\nyou can wrangle that complexity by defining your Pipeline as Code.\n\nPipeline ♥ UI Testing, Too\n\nI recently started looking for a way to do browser UI testing for an open-source JavaScript project to which I contribute.\nThe project is targeted primarily at\nNode.js\nbut we’re committed to maintaining browser-client compatibility as well.\nThat means we should run tests on a matrix of browsers.\nSauce Labs\nhas an \"open-sauce\" program that provides free test instances to open-source projects.\nI decided to try using the\nSauce OnDemand Plugin\nand\nNightwatch.js\nto run Selenium tests on a sample project first, before trying a full-blown suite of tests.\n\nStarting from Framework\n\nI started off by following Sauce Labs' instructions on\n\" Setting up Sauce Labs with Jenkins\"\nas far as I could.\nI installed the\nJUnit and\nSauce OnDemand\nplugins, created an account with Sauce Labs, and\nadded my Sauce Labs credentials to Jenkins.\nFrom there I started to get a little lost.\nI’m new to Selenium and I had trouble understanding how to translate the instructions to my situation.\nI needed a working example that I could play with.\n\nHappily, there’s a whole range of sample projects in\n\" saucelabs-sample-test-frameworks\"\non GitHub, which show how to integrate Sauce Labs with various test frameworks, including Nightwatch.js.\nI forked the Nightwatch.js sample to\nbitwiseman/JS-Nightwatch.js\nand set to writing my Jenkinsfile.\nBetween the sample and the Sauce Labs instructions,\nI was able to write a pipeline that ran five tests on one browser via\nSauce Connect :\n\nnode {\n    stage \"Build\"\n    checkout scm\n\n    sh 'npm install' (1)\n\nstage \"Test\"\n    sauce('f0a6b8ad-ce30-4cba-bf9a-95afbc470a8a') { (2)\nsauceconnect(options: '', useGeneratedTunnelIdentifier: false, verboseLogging: false) { (3)\nsh './node_modules/.bin/nightwatch -e chrome --test tests/guineaPig.js || true' (4)\njunit 'reports/**' (5)\nstep([$class: 'SauceOnDemandTestPublisher']) (6)\n}\n    }\n}\n\n1\nInstall dependencies\n\n2\nUse my\npreviously added sauce credentials\n\n3\nStart up the\nSauce Connect\ntunnel to Sauce Labs\n\n4\nRun Nightwatch.js\n\n5\nUse JUnit to track results and show a trend graph\n\n6\nLink result details from Sauce Labs\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit url:'https://github.com/bitwiseman/JS-Nightwatch.js', branch: 'sauce-pipeline'.\n\nI ran this job a few times to get the JUnit report to show a trend graph.\n\nThis sample app generates the SauceOnDemandSessionID for each test, enabling the Jenkins Sauce OnDemand Plugin’s result publisher to link results to details Sauce Labs captured during the run.\n\nAdding Platforms\n\nNext I wanted to add a few more platforms to my matrix.\nThis would require changing both the test framework configuration and the pipeline.\nI’d need to add new named combinations of platform, browser, and browser version (called \"environments\") to the Nightwatch.js configuration file,\nand modify the pipeline to run tests in those new environments.\n\nThis is a perfect example of the power of pipeline as code.\nIf I were working with a separately configured pipeline,\nI’d have to make the change to the test framework, then change the pipeline manually.\nWith my pipeline checked in as code,\nI could change both in one commit,\npreventing errors resulting from pipeline configurations going out of sync from the rest of the project.\n\nI added three new environments to nightwatch.json :\n\n\"test_settings\" : {\n  \"default\": { /*----8 <----*/ },\n  \"chrome\": { /*----8 <----*/ },\n\n  \"firefox\": {\n    \"desiredCapabilities\": {\n      \"platform\": \"linux\",\n      \"browserName\": \"firefox\",\n      \"version\": \"latest\"\n    }\n  },\n  \"ie\": {\n    \"desiredCapabilities\": {\n      \"platform\": \"Windows 10\",\n      \"browserName\": \"internet explorer\",\n      \"version\": \"latest\"\n    }\n  },\n  \"edge\": {\n    \"desiredCapabilities\": {\n      \"platform\": \"Windows 10\",\n      \"browserName\": \"MicrosoftEdge\",\n      \"version\": \"latest\"\n    }\n  }\n}\n\nAnd I modified my Jenkinsfile to call them:\n\n//----8 (1)\n'chrome',\n        'firefox',\n        'ie',\n        'edge'\n    ].join(',')\n    // Run selenium tests using Nightwatch.js\n    sh \"./node_modules/.bin/nightwatch -e ${configs} --test tests/guineaPig.js\" (2)\n} //----8\n\n1\nUsing an array to improve readability and make it easy to add more platforms later.\n\n2\nChanged from single-quoted string to double-quoted to support variable substitution.\n\nTest frameworks have bugs too. Nightwatch.js (v0.9.8) generates incomplete JUnit files,\nreporting results without enough information in them to distinguish between platforms.\nI implemented a fix for it and\nsubmitted a PR to Nightwatch.js.\nThis blog shows output with that fix applied locally.\n\nAs expected, Jenkins picked up the new pipeline and ran Nightwatch.js on four platforms.\nSauce Labs of course recorded the results and correctly linked them into this build.\nNightwatch.js was already configured to use multiple worker threads to run tests against those platforms in parallel, and\nmy Sauce Labs account supported running them all at the same time,\nletting me cover four configurations in less that twice the time,\nand that added time was most due to individual new environments taking longer to complete.\nWhen I move to the actual project, this will let me run broad acceptance passes quickly.\n\nConclusion: To Awesome and Beyond\n\nConsidering the complexity of the system, I was impressed with how easy it was to integrate Jenkins with Sauce OnDemand to start testing on multiple browsers.\nThe plugin worked flawlessly with Jenkins Pipeline.\nI went ahead and ran some additional tests to show that failure reporting also behaved as expected.\n\n//----8 (1)\n//----8\n\n1\nRemoved --test filter to run all tests\n\nEpilogue: Pipeline vs. Freestyle\n\nJust for comparison here’s the final state of this job in Freestyle UI versus fully-commented pipeline code:\n\nThis includes the\nAnsiColor Plugin\nto support Nightwatch.js' default ANSI color output.\n\nFreestyle\n\nPipeline\n\nnode {\n    stage \"Build\"\n    checkout scm\n\n    // Install dependencies\n    sh 'npm install'\n\n    stage \"Test\"\n\n    // Add sauce credentials\n    sauce('f0a6b8ad-ce30-4cba-bf9a-95afbc470a8a') {\n        // Start sauce connect\n        sauceconnect(options: '', useGeneratedTunnelIdentifier: false, verboseLogging: false) {\n\n            // List of browser configs we'll be testing against.\n            def platform_configs = [\n                'chrome',\n                'firefox',\n                'ie',\n                'edge'\n            ].join(',')\n\n            // Nightwatch.js supports color ouput, so wrap this step for ansi color\n            wrap([$class: 'AnsiColorBuildWrapper', 'colorMapName': 'XTerm']) {\n\n                // Run selenium tests using Nightwatch.js\n                // Ignore error codes. The junit publisher will cover setting build status.\n                sh \"./node_modules/.bin/nightwatch -e ${platform_configs} || true\"\n            }\n\n            junit 'reports/**'\n\n            step([$class: 'SauceOnDemandTestPublisher'])\n        }\n    }\n}\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit url:'https://github.com/bitwiseman/JS-Nightwatch.js', branch: 'sauce-pipeline'.\n\nNot only is the pipeline as code more compact,\nit also allows for comments to further clarify what is being done.\nAnd as I noted earlier,\nchanges to this pipeline code are committed the same as changes to the rest of the project,\nkeeping everything synchronized, reviewable, and testable at any commit.\nIn fact, you can view the full set of commits for this blog post in the\nblog/sauce-pipeline\nbranch of the\nbitwiseman/JS-Nightwatch.js\nrepository.\n\nLinks\n\nSauce OnDemand Plugin\n\nbitwiseman/JS-Nightwatch.js\n\nsaucelabs-sample-test-frameworks","title":"Browser-testing with Sauce OnDemand and Pipeline","tags":["tutorial","pipeline","plugins","saucelabs","selenium","nightwatch"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}},"publicURL":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/lnewman.jpeg"},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2016-08-10T00:00:00.000Z","id":"4f2eb2b7-1bb1-501e-8b2c-1b8c4ac3e6e3","slug":"/blog/2016/08/10/rails-cd-with-pipeline/","strippedHtml":"This is a guest post by R. Tyler Croy, who is a\nlong-time contributor to Jenkins and the primary contact for Jenkins project\ninfrastructure. He is also a Jenkins Evangelist at\nCloudBees, Inc.\n\nWhen the Ruby on Rails framework debuted it\nchanged the industry in two noteworthy ways: it created a trend of opinionated web\napplication frameworks ( Django,\nPlay, Grails) and it\nalso strongly encouraged thousands of developers to embrace test-driven\ndevelopment along with many other modern best practices (source control, dependency\nmanagement, etc). Because Ruby, the language underneath Rails, is interpreted\ninstead of compiled there isn’t a \"build\" per se but rather tens, if not\nhundreds, of tests, linters and scans which are run to ensure the application’s\nquality. With the rise in popularity of Rails, the popularity of application\nhosting services with easy-to-use deployment tools like Heroku or\nEngine Yard rose too.\n\nThis combination of good test coverage and easily automated deployments\nmakes Rails easy to continuously deliver with Jenkins. In this post we’ll cover\ntesting non-trivial Rails applications with Jenkins\nPipeline and, as an added bonus, we will add security scanning via\nBrakeman and the\nBrakeman\nplugin.\n\nTopics\n\nPreparing the app\n\nPreparing Jenkins\n\nWriting the Pipeline\n\nSecurity scanning\n\nDeploying the good stuff\n\nWrap up\n\nFor this demonstration, I used Ruby Central 's\ncfp-app :\n\nA Ruby on Rails application that lets you manage your conference’s call for\nproposal (CFP), program and schedule. It was written by Ruby Central to run the\nCFPs for RailsConf and RubyConf.\n\nI chose this Rails app, not only because it’s a sizable application with lots\nof tests, but it’s actually the application we used to collect talk proposals\nfor the \" Community Tracks\" at this\nyear’s Jenkins World. For the most part,\ncfp-app is a standard Rails application. It uses\nPostgreSQL for its database,\nRSpec for its tests and\nRuby 2.3.x as its runtime.\n\nIf you prefer to just to look at the code, skip straight to the\nJenkinsfile.\n\nPreparing the app\n\nFor most Rails applications there are few, if any, changes needed to enable\ncontinuous delivery with Jenkins. In the case of\ncfp-app, I added two gems to get\nthe most optimal integration into Jenkins:\n\nci_reporter, for test report\nintegration\n\nbrakeman, for security scanning.\n\nAdding these was simple, I just needed to update the Gemfile and the\nRakefile in the root of the repository to contain:\n\nGemfile\n\n# .. snip ..\ngroup :test do\n  # RSpec, etc\n  gem 'ci_reporter'\n  gem 'ci_reporter_rspec'\n  gem \"brakeman\", :require => false\nend\n\nRakefile\n\n# .. snip ..\nrequire 'ci/reporter/rake/rspec'\n# Make sure we setup ci_reporter before executing our RSpec examples\ntask :spec => 'ci:setup:rspec'\n\nPreparing Jenkins\n\nWith the cfp-app project set up, next on the list is to ensure that Jenkins itself\nis ready. Generally I suggest running the latest LTS of\nJenkins; for this demonstration I used Jenkins 2.7.1 with the following\nplugins:\n\nPipeline plugin\n\nBrakeman plugin\n\nCloudBees\nDocker Pipeline plugin\n\nI also used the\nGitHub\nOrganization Folder plugin to automatically create pipeline items in my\nJenkins instance; that isn’t required for the demo, but it’s pretty cool to see\nrepositories and branches with a Jenkinsfile automatically show up in\nJenkins, so I recommend it!\n\nIn addition to the plugins listed above, I also needed at least one\nJenkins agent with the Docker daemon installed and\nrunning on it. I label these agents with \"docker\" to make it easier to assign\nDocker-based workloads to them in the future.\n\nAny Linux-based machine with Docker installed will work, in my case I was\nprovisioning on-demand agents with the\nAzure\nplugin which, like the\nEC2 plugin,\nhelps keep my test costs down.\n\nIf you’re using Amazon Web Services, you might also be interested in\nthis blog post from\nearlier this year unveiling the\nEC2\nFleet plugin for working with EC2 Spot Fleets.\n\nWriting the Pipeline\n\nTo make sense of the various things that the Jenkinsfile needs to do, I find\nit easier to start by simply defining the stages of my pipeline. This helps me\nthink of, in broad terms, what order of operations my pipeline should have.\nFor example:\n\n/* Assign our work to an agent labelled 'docker' */\nnode('docker') {\n    stage 'Prepare Container'\n    stage 'Install Gems'\n    stage 'Prepare Database'\n    stage 'Invoke Rake'\n    stage 'Security scan'\n    stage 'Deploy'\n}\n\nAs mentioned previously, this Jenkinsfile is going to rely heavily on the\nCloudBees\nDocker Pipeline plugin. The plugin provides two very important features:\n\nAbility to execute steps inside of a running Docker container\n\nAbility to run a container in the \"background.\"\n\nLike most Rails applications, one can effectively test the application with two\ncommands: bundle install followed by bundle exec rake. I already had some\nDocker images prepared with RVM and Ruby 2.3.0 installed,\nwhich ensures a common and consistent starting point:\n\nnode('docker') {\n    // .. 'stage' steps removed\n    docker.image('rtyler/rvm:2.3.0').inside { (1)\nrvm 'bundle install' (2)\nrvm 'bundle exec rake'\n    } (3)\n}\n\n1\nRun the named container. The inside method can take optional additional flags for the docker run command.\n\n2\nExecute our shell commands using our tiny sh step wrapper\nrvm . This ensures that the shell code is executed in the correct RVM environment.\n\n3\nWhen the closure completes, the container will be destroyed.\n\nUnfortunately, with this application, the bundle exec rake command will fail\nif PostgreSQL isn’t available when the process starts. This is where the\nsecond important feature of the CloudBees Docker Pipeline plugin comes\ninto effect: the ability to run a container in the \"background.\"\n\nnode('docker') {\n    // .. 'stage' steps removed\n    /* Pull the latest `postgres` container and run it in the background */\n    docker.image('postgres').withRun { container -> (1)\necho \"PostgreSQL running in container ${container.id}\" (2)\n} (3)\n}\n\n1\nRun the container, effectively docker run postgres\n\n2\nAny number of steps can go inside the closure\n\n3\nWhen the closure completes, the container will be destroyed.\n\nRunning the tests\n\nCombining these two snippets of Jenkins Pipeline is, in my opinion, where the\npower of the DSL\nshines:\n\nnode('docker') {\n    docker.image('postgres').withRun { container ->\n        docker.image('rtyler/rvm:2.3.0').inside(\"--link=${container.id}:postgres\") { (1)\nstage 'Install Gems'\n            rvm \"bundle install\"\n\n            stage 'Invoke Rake'\n            withEnv(['DATABASE_URL=postgres://postgres@postgres:5432/']) { (2)\nrvm \"bundle exec rake\"\n            }\n            junit 'spec/reports/*.xml' (3)\n}\n    }\n}\n\n1\nBy passing the --link argument, the Docker daemon will allow the RVM container to talk to the PostgreSQL container under the host name 'postgres'.\n\n2\nUse the withEnv step to set environment variables for everything that is in the closure. In this case, the cfp-app DB scaffolding will look for the DATABASE_URL variable to override the DB host/user/dbname defaults.\n\n3\nArchive the test reports generated by ci_reporter so that Jenkins can display test reports and trend analysis.\n\nWith this done, the basics are in place to consistently run the tests for\ncfp-app in fresh Docker containers for each execution of the pipeline.\n\nSecurity scanning\n\nUsing Brakeman, the security scanner for Ruby\non Rails, is almost trivially easy inside of Jenkins Pipeline, thanks to the\nBrakeman\nplugin which implements the publishBrakeman step.\n\nBuilding off our example above, we can implement the \"Security scan\" stage:\n\nnode('docker') {\n    /* --8 (1)\npublishBrakeman 'brakeman-output.tabs' (2)\n/* --8\n\n1\nRun the Brakeman security scanner for Rails and store the output for later in brakeman-output.tabs\n\n2\nArchive the reports generated by Brakeman so that Jenkins can display detailed reports with trend analysis.\n\nAs of this writing, there is work in progress\n( JENKINS-31202) to\nrender trend graphs from plugins like Brakeman on a pipeline project’s main\npage.\n\nDeploying the good stuff\n\nOnce the tests and security scanning are all working properly, we can start to\nset up the deployment stage. Jenkins Pipeline provides the variable\ncurrentBuild which we can use to determine whether our pipeline has been\nsuccessful thus far or not. This allows us to add the logic to only deploy when\neverything is passing, as we would expect:\n\nnode('docker') {\n    /* --8 (1)\nsh './deploy.sh' (2)\n}\n    else {\n        mail subject: \"Something is wrong with ${env.JOB_NAME} ${env.BUILD_ID}\",\n                  to: 'nobody@example.com',\n                body: 'You should fix it'\n    }\n    /* --8\n\n1\ncurrentBuild has the result property which would be 'SUCCESS', 'FAILED', 'UNSTABLE', 'ABORTED'\n\n2\nOnly if currentBuild.result is successful should we bother invoking our deployment script (e.g. git push heroku master)\n\nWrap up\n\nI have gratuitously commented the full\nJenkinsfile\nwhich I hope is a useful summation of the work outlined above. Having worked\non a number of Rails applications in the past, the consistency provided by\nDocker and Jenkins Pipeline above would have definitely improved those\nprojects' delivery times. There is still room for improvement however, which\nis left as an exercise for the reader. Such as: preparing new containers with\nall their\ndependencies\nbuilt-in instead of installing them at run-time. Or utilizing the parallel\nstep for executing RSpec across multiple Jenkins agents simultaneously.\n\nThe beautiful thing about defining your continuous delivery, and continuous\nsecurity, pipeline in code is that you can continue to iterate on it!","title":"Continuous Security for Rails apps with Pipeline and Brakeman","tags":["tutorial","ruby","pipeline","rails","brakeman","continuousdelivery"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}},"publicURL":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/rtyler.jpeg"},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-07-18T00:00:00.000Z","id":"523da562-6777-53bd-978f-45ab4cb9092c","slug":"/blog/2016/07/18/pipeline-notifications/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nRather than sitting and watching Jenkins for job status, I want Jenkins to send\nnotifications when events occur.  There are Jenkins plugins for\nSlack,\nHipChat,\nor even email\namong others.\n\nNote: Something is happening!\n\nI think we can all agree getting notified when events occur is preferable to\nhaving to constantly monitor them just in case.  I’m going to continue from\nwhere I left off in my\nprevious post with the\nhermann project.  I added a Jenkins\nPipeline with an HTML publisher for code coverage. This week, I’d like to make\nJenkins to notify me when builds start and when they succeed or fail.\n\nSetup and Configuration\n\nFirst, I select targets for my notifications. For this blog post, I’ll use sample\ntargets that I control.  I’ve created Slack and HipChat organizations called\n\"bitwiseman\", each with one member - me.  And for email I’m running a Ruby SMTP server called\nmailcatcher, that is perfect for local testing\nsuch as this.  Aside for these concessions, configuration would be much the\nsame in a non-demo situation.\n\nNext, I install and add server-wide configuration for the\nSlack,\nHipChat,\nand Email-ext\nplugins.  Slack and HipChat use API tokens - both products have integration\npoints on their side that generate tokens which I copy into my Jenkins\nconfiguration. Mailcatcher SMTP runs locally. I just point Jenkins\nat it.\n\nHere’s what the Jenkins configuration section for each of these looks like:\n\nOriginal Pipeline\n\nNow I can start adding notification steps. The same as\nlast week, I’ll use the\nJenkins Pipeline Snippet Generator\nto explore the step syntax for the notification plugins.\n\nHere’s the base pipeline before I start making changes:\n\nstage 'Build'\n\nnode {\n  // Checkout\n  checkout scm\n\n  // install required bundles\n  sh 'bundle install'\n\n  // build and run tests with coverage\n  sh 'bundle exec rake build spec'\n\n  // Archive the built artifacts\n  archive (includes: 'pkg/*.gem')\n\n  // publish html\n  // snippet generator doesn't include \"target:\"\n  // https://issues.jenkins.io/browse/JENKINS-29711.\n  publishHTML (target: [\n      allowMissing: false,\n      alwaysLinkToLastBuild: false,\n      keepAll: true,\n      reportDir: 'coverage',\n      reportFiles: 'index.html',\n      reportName: \"RCov Report\"\n    ])\n}\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit 'https://github.com/reiseburo/hermann.git'.\n\nJob Started Notification\n\nFor the first change, I decide to add a \"Job Started\" notification.  The\nsnippet generator and then reformatting makes this straightforward:\n\nnode {\n\n  notifyStarted()\n\n  /* ... existing build steps ... */\n}\n\ndef notifyStarted() {\n  // send to Slack\n  slackSend (color: '#FFFF00', message: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\")\n\n  // send to HipChat\n  hipchatSend (color: 'YELLOW', notify: true,\n      message: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\"\n    )\n\n  // send to email\n  emailext (\n      subject: \"STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\",\n      body: \"\"\" STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':\nCheck console output at \" ${env.JOB_NAME} [${env.BUILD_NUMBER}]\"\"\"\",\n      recipientProviders: [[$class: 'DevelopersRecipientProvider']]\n    )\n}\n\nSince Pipeline is a Groovy-based DSL, I can use\nstring interpolation\nand variables to add exactly the details I want in my notification messages. When\nI run this I get the following notifications:\n\nJob Successful Notification\n\nThe next logical choice is to get notifications when a job succeeds.  I’ll\ncopy and paste based on the notifyStarted method for now and do some refactoring\nlater.\n\nnode {\n\n  notifyStarted()\n\n  /* ... existing build steps ... */\n\n  notifySuccessful()\n}\n\ndef notifyStarted() { /* .. */ }\n\ndef notifySuccessful() {\n  slackSend (color: '#00FF00', message: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\")\n\n  hipchatSend (color: 'GREEN', notify: true,\n      message: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\"\n    )\n\n  emailext (\n      subject: \"SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\",\n      body: \"\"\" SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':\nCheck console output at \" ${env.JOB_NAME} [${env.BUILD_NUMBER}]\"\"\"\",\n      recipientProviders: [[$class: 'DevelopersRecipientProvider']]\n    )\n}\n\nAgain, I get notifications, as expected.  This build is fast enough,\nsome of them are even on the screen at the same time:\n\nJob Failed Notification\n\nNext I want to add failure notification.  Here’s where we really start to see the power\nand expressiveness of Jenkins pipeline.  A Pipeline is a Groovy script, so as we’d\nexpect in any Groovy script, we can handle errors using try-catch blocks.\n\nnode {\n  try {\n    notifyStarted()\n\n    /* ... existing build steps ... */\n\n    notifySuccessful()\n  } catch (e) {\n    currentBuild.result = \"FAILED\"\n    notifyFailed()\n    throw e\n  }\n}\n\ndef notifyStarted() { /* .. */ }\n\ndef notifySuccessful() { /* .. */ }\n\ndef notifyFailed() {\n  slackSend (color: '#FF0000', message: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\")\n\n  hipchatSend (color: 'RED', notify: true,\n      message: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})\"\n    )\n\n  emailext (\n      subject: \"FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\",\n      body: \"\"\" FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':\nCheck console output at \" ${env.JOB_NAME} [${env.BUILD_NUMBER}]\"\"\"\",\n      recipientProviders: [[$class: 'DevelopersRecipientProvider']]\n    )\n}\n\nCode Cleanup\n\nLastly, now that I have it all working, I’ll do some refactoring. I’ll unify\nall the notifications in one method and move the final success/failure notification\ninto a finally block.\n\nstage 'Build'\n\nnode {\n  try {\n    notifyBuild('STARTED')\n\n    /* ... existing build steps ... */\n\n  } catch (e) {\n    // If there was an exception thrown, the build failed\n    currentBuild.result = \"FAILED\"\n    throw e\n  } finally {\n    // Success or failure, always send notifications\n    notifyBuild(currentBuild.result)\n  }\n}\n\ndef notifyBuild(String buildStatus = 'STARTED') {\n  // build status of null means successful\n  buildStatus = buildStatus ?: 'SUCCESS'\n\n  // Default values\n  def colorName = 'RED'\n  def colorCode = '#FF0000'\n  def subject = \"${buildStatus}: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]'\"\n  def summary = \"${subject} (${env.BUILD_URL})\"\n  def details = \"\"\" STARTED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]':\nCheck console output at \" ${env.JOB_NAME} [${env.BUILD_NUMBER}]\"\"\"\"\n\n  // Override default values based on build status\n  if (buildStatus == 'STARTED') {\n    color = 'YELLOW'\n    colorCode = '#FFFF00'\n  } else if (buildStatus == 'SUCCESS') {\n    color = 'GREEN'\n    colorCode = '#00FF00'\n  } else {\n    color = 'RED'\n    colorCode = '#FF0000'\n  }\n\n  // Send notifications\n  slackSend (color: colorCode, message: summary)\n\n  hipchatSend (color: color, notify: true, message: summary)\n\n  emailext (\n      subject: subject,\n      body: details,\n      recipientProviders: [[$class: 'DevelopersRecipientProvider']]\n    )\n}\n\nYou have been notified!\n\nI now get notified twice per build on three different channels.  I’m not sure I\nneed to get notified this much for such a short build.  However, for a longer\nor complex CD pipeline, I might want exactly that.  If needed, I could even\nimprove this to handle other status strings and call it as needed throughout\nmy pipeline.\n\nLinks\n\nSlack Plugin\n\nHipChat Plugin\n\nEmail-ext Plugin\n\nJenkins Pipeline Snippet Generator","title":"Sending Notifications in Pipeline","tags":["tutorial","pipeline","plugins","notifications","slack","hipchat","emailext"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}},"publicURL":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/lnewman.jpeg"},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2016-07-01T00:00:00.000Z","id":"4848db1b-feac-54a0-8b3e-2a0f5e3fbfc6","slug":"/blog/2016/07/01/html-publisher-plugin/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nMost projects need more that just JUnit result reporting.  Rather than writing a\ncustom plugin for each type of report, we can use the\nHTML Publisher Plugin.\n\nLet’s Make This Quick\n\nI’ve found a Ruby project,\nhermann, I’d like to build using Jenkins Pipeline. I’d\nalso like to have the code coverage results published with each build job.  I could\nwrite a plugin to publish this data, but I’m in a bit of hurry and\nthe build already creates an HTML report file using SimpleCov\nwhen the unit tests run.\n\nSimple Build\n\nI’m going to use the\nHTML Publisher Plugin\nto add the HTML-formatted code coverage report to my builds.  Here’s a simple\npipeline for building the hermann\nproject.\n\nstage 'Build'\n\nnode {\n  // Checkout\n  checkout scm\n\n  // install required bundles\n  sh 'bundle install'\n\n  // build and run tests with coverage\n  sh 'bundle exec rake build spec'\n\n  // Archive the built artifacts\n  archive (includes: 'pkg/*.gem')\n}\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit 'https://github.com/reiseburo/hermann.git'.\n\nSimple enough, it builds, runs tests, and archives the package.\n\nNow I just need to add the step to publish the code coverage report.\nI know that rake spec creates an index.html file in the coverage directory.\nI’ve already installed the\nHTML Publisher Plugin.\nHow do I add the HTML publishing step to the pipeline?  The plugin page doesn’t\nsay anything about it.\n\nSnippet Generator to the Rescue\n\nDocumentation is hard to maintain and easy to miss, even more so in a system\nlike Jenkins with hundreds of plugins the each potential have one or more\ngroovy fixtures to add to the Pipeline.  The Pipeline Syntax\"Snippet Generator\" helps users\nnavigate this jungle by providing a way to generate a code snippet for any step using\nprovided inputs.\n\nIt offers a dynamically generated list of steps, based on the installed plugins.\nFrom that list I select the publishHTML step:\n\nThen it shows me a UI similar to the one used in job configuration.  I fill in\nthe fields, click \"generate\", and it shows me snippet of groovy generated from\nthat input.\n\nHTML Published\n\nI can use that snippet directly or as a template for further customization.\nIn this case, I’ll just reformat and copy it in at the end of my\npipeline.  (I ran into a minor bug\nin the snippet generated for this plugin step. Typing\nerror string in my search bar immediately found the bug and a workaround.)\n\n/* ...unchanged... */\n\n  // Archive the built artifacts\n  archive (includes: 'pkg/*.gem')\n\n  // publish html\n  // snippet generator doesn't include \"target:\"\n  // https://issues.jenkins.io/browse/JENKINS-29711.\n  publishHTML (target: [\n      allowMissing: false,\n      alwaysLinkToLastBuild: false,\n      keepAll: true,\n      reportDir: 'coverage',\n      reportFiles: 'index.html',\n      reportName: \"RCov Report\"\n    ])\n\n}\n\nWhen I run this new pipeline I am rewarded with an RCov Report link on left side,\nwhich I can follow to show the HTML report.\n\nI even added the keepAll setting to let I can also go back an look at reports on old jobs as\nmore come in.  As I said to to begin with, this is not as slick as what I\ncould do with a custom plugin, but it is much easier and works with any static\nHTML.\n\nLinks\n\nHTML Publisher Plugin\n\nJenkins Pipeline Snippet Generator","title":"Publishing HTML Reports in Pipeline","tags":["tutorial","pipeline","plugins","ruby"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}},"publicURL":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/lnewman.jpeg"},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2016-06-16T00:00:00.000Z","id":"9d5b4fb7-1151-5470-985c-045b0dd79455","slug":"/blog/2016/06/16/parallel-test-executor-plugin/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nIn this blog post, I’ll show you how to speed up your pipeline by using the\nParallel Test Executor Plugin.\n\nSo much to do, so little time…​\n\nIn my career, I’ve helped many teams move to continuous integration and delivery. One problem\nwe always encounter is how to run all the tests needed to ensure high-quality\nchanges while still keeping pipeline times reasonable and changes flowing\nsmoothly. More tests mean greater confidence, but also longer wait times.\nBuild systems may or may not support running tests in parallel, but they still only use one\nmachine even while other lab machines sit idle. In these cases, parallelizing\ntest execution across multiple machines is a great way to speed up pipelines.\nThe Parallel Test Executor plugin lets us leverage Jenkins do just that with no\ndisruption to the rest of the build system.\n\nSerial Test Execution\n\nFor this post, I’ll be running a pipeline based on the\nJenkins Git Plugin. I’ve modified\nthe Jenkinsfile from that project to allow us to compare execution times to our\nlater changes, and I’ve truncated the \"mvn\" utility method since it remains\nunchanged.  You can find the original file\nhere.\n\nnode {\n  stage 'Checkout'\n  checkout scm\n\n  stage 'Build'\n\n  /* Call the Maven build without tests. */\n  mvn \"clean install -DskipTests\"\n\n  stage 'Test'\n  runTests()\n\n  /* Save Results. */\n  stage 'Results'\n\n  /* Archive the build artifacts */\n  archive includes: 'target/*.hpi,target/*.jpi'\n}\n\nvoid runTests(def args) {\n  /* Call the Maven build with tests. */\n  mvn \"install -Dmaven.test.failure.ignore=true\"\n\n  /* Archive the test results */\n  junit '**/target/surefire-reports/TEST-*.xml'\n}\n\n/* Run Maven */\nvoid mvn(def args) { /* ... */ }\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit 'https://github.com/jenkinsci/git-plugin.git'.\n\nThis is a Maven project, so the Jenkinsfile is pretty simple.\nI’ve split the Maven build into separate “Build” and “Test”\nstages. Maven doesn’t support this split very well, it wants to run all\nthe steps of the lifecycle in order every time. So, I have to call Maven twice:\nfirst using the “skipTests” property to do only build steps in the first call,\nand then a second time with out that property to run tests.\n\nOn my quad-core machine, executing this pipeline takes about 13 minutes and 30\nseconds.  Of that time, it takes 13 minutes to run about 2.7 thousand tests in\nserial.\n\nParallel Test Execution\n\nThis looks like an ideal project for parallel test execution: a short build\nfollowed by a large number of serially executed tests that consume the most of\nthe pipeline time. There are a number of things I could try to speed this up.\nFor example, I could modify test harness to look for ways to parallelize\nthe test execution on this single machine. Or I could try speed up the tests\nthemselves. Both of those can be time-consuming and both risk destabilizing the\ntests. I’d need to know more about the project to do it well.\n\nI’ll avoid that risk by using Jenkins and the\nParallel Test Executor Plugin to\nparallelize the tests across multiple nodes instead. This will isolate the tests\nfrom each other, while still giving us speed gains from parallel execution.\n\nThe plugin reads the list of tests from the results archived in the previous execution of this\njob and splits that list into a specified number of sublists. I can then use\nthose sublists to execute the tests in parallel, passing a different sublist to\neach node.\n\nLet’s look at how this changes the pipeline:\n\nnode { /* ...unchanged... */ }\n\nvoid runTests(def args) {\n  /* Request the test groupings.  Based on previous test results. */\n  /* see https://wiki.jenkins.io/display/JENKINS/Parallel+Test+Executor+Plugin and demo on github\n  /* Using arbitrary parallelism of 4 and \"generateInclusions\" feature added in v1.8. */\n  def splits = splitTests parallelism: [$class: 'CountDrivenParallelism', size: 4], generateInclusions: true\n\n  /* Create dictionary to hold set of parallel test executions. */\n  def testGroups = [:]\n\n  for (int i = 0; i }. */\n    /*     includes = whether list specifies tests to include (true) or tests to exclude (false). */\n    /*     list = list of tests for inclusion or exclusion. */\n    /* The list of inclusions is constructed based on results gathered from */\n    /* the previous successfully completed job. One additional record will exclude */\n    /* all known tests to run any tests not seen during the previous run.  */\n    testGroups[\"split-${i}\"] = {  // example, \"split3\"\n      node {\n        checkout scm\n\n        /* Clean each test node to start. */\n        mvn 'clean'\n\n        def mavenInstall = 'install -DMaven.test.failure.ignore=true'\n\n        /* Write includesFile or excludesFile for tests.  Split record provided by splitTests. */\n        /* Tell Maven to read the appropriate file. */\n        if (split.includes) {\n          writeFile file: \"target/parallel-test-includes-${i}.txt\", text: split.list.join(\"\\n\")\n          mavenInstall += \" -Dsurefire.includesFile=target/parallel-test-includes-${i}.txt\"\n        } else {\n          writeFile file: \"target/parallel-test-excludes-${i}.txt\", text: split.list.join(\"\\n\")\n          mavenInstall += \" -Dsurefire.excludesFile=target/parallel-test-excludes-${i}.txt\"\n        }\n\n        /* Call the Maven build with tests. */\n        mvn mavenInstall\n\n        /* Archive the test results */\n        junit '**/target/surefire-reports/TEST-*.xml'\n      }\n    }\n  }\n  parallel testGroups\n}\n\n/* Run Maven */\nvoid mvn(def args) { /* ... */ }\n\nThat’s it!  The change is significant but it is all encapsulated in this one\nmethod in the Jenkinsfile.\n\nGreat (ish) Success!\n\nHere’s the results for the new pipeline with parallel test execution:\n\nThe tests ran almost twice as fast, without changes outside pipeline.  Great!\n\nHowever, I used 4 test executors, so why am I not seeing a 4x? improvement.\nA quick review of the logs shows the problem: A small number of tests are taking up\nto 5 minutes each to complete! This is actually good news. It means that I\nshould be able to see further improvement in pipeline throughput just by refactoring\nthose few long running tests into smaller parts.\n\nConclusion\n\nWhile I would like to have seen closer to a 4x improvement to match to number\nof executors, 2x is still perfectly respectable. If I were working on a group of projects\nwith similar pipelines, I’d be completely comfortable reusing these same changes\non my other project and I’d expect to similar improvement without any disruption to\nother tools or processes.\n\nLinks\n\nhttps://wiki.jenkins.io/display/JENKINS/Parallel+Test+Executor+Plugin","title":"Faster Pipelines with the Parallel Test Executor Plugin","tags":["tutorial","pipeline","plugins"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}},"publicURL":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/lnewman.jpeg"},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}}]}},"pageContext":{"tag":"tutorial","limit":8,"skip":8,"numPages":4,"currentPage":2}},
    "staticQueryHashes": ["3649515864"]}