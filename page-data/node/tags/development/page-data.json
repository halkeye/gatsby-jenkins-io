{
    "componentChunkName": "component---src-templates-tag-blog-list-template-js",
    "path": "/node/tags/development",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2018-11-07T00:00:00.000Z","id":"f78fd338-8583-5f00-82a2-c7210a261909","slug":"/blog/2018/11/07/Validate-Jenkinsfile/","strippedHtml":"In my daily work I often have to create or modify Jenkinsfiles and more often than I would like, I make mistakes. It is a very tedious workflow when you make a change to your Jenkinsfile, create a commit, push the commit and wait for your Jenkins Server to tell you, that you have missed a bracket.\n\nThe Command-line Pipeline Linter ( https://jenkins.io/doc/book/pipeline/development/) does a great job of reducing the turnaround times when writing a Jenkinsfile, but its usage has its own inconveniences. You need tools like curl or ssh to make a connection to your Jenkins Server and you need to remember the correct command to validate your Jenkinsfile. I still did not like the solution.\n\nAs VS Code is my daily driver, I started to look at writing extensions for it and out of it came a little extension which makes validating Jenkinsfiles just a little bit more comfortable.\n\nWhat the 'Jenkins Pipeline Linter Connector' does is, that it takes the file that you have currently opened, pushes it to your Jenkins Server and displays the validation result in VS Code.\n\n​You can find the extension from within the VS Code extension browser or at the following url: https://marketplace.visualstudio.com/items?itemName=janjoerke.jenkins-pipeline-linter-connector\n\nThe extension adds four settings entries to VS Code which you have to use to configure the Jenkins Server you want to use for validation.\n\njenkins.pipeline.linter.connector.url is the endpoint at which your Jenkins Server expects the POST request, containing your Jenkinsfile which you want to validate. Typically this points to /pipeline-model-converter/validate\" class=\"bare\">http:// /pipeline-model-converter/validate .\n\njenkins.pipeline.linter.connector.user allows you to specify your Jenkins username.\n\njenkins.pipeline.linter.connector.pass allows you to specify your Jenkins password.\n\njenkins.pipeline.linter.connector.crumbUrl has to be specified if your Jenkins Server has CRSF protection enabled. Typically this points to /crumbIssuer/api/xml?xpath=concat(//crumbRequestField,%22:%22,//crumb\" class=\"bare\">http:// /crumbIssuer/api/xml?xpath=concat(//crumbRequestField,%22:%22,//crumb).\n​","title":"Validate your Jenkinsfile from within VS Code","tags":["jenkinsfile","validation","vscode","pipeline","pipeline authoring","development"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#887878","images":{"fallback":{"src":"/gatsby-jenkins-io/static/a8787b55a793681f4a2418b0594f7055/5236e/janjoerke.jpg","srcSet":"/gatsby-jenkins-io/static/a8787b55a793681f4a2418b0594f7055/534e5/janjoerke.jpg 32w,\n/gatsby-jenkins-io/static/a8787b55a793681f4a2418b0594f7055/99887/janjoerke.jpg 64w,\n/gatsby-jenkins-io/static/a8787b55a793681f4a2418b0594f7055/5236e/janjoerke.jpg 128w,\n/gatsby-jenkins-io/static/a8787b55a793681f4a2418b0594f7055/76fd4/janjoerke.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/a8787b55a793681f4a2418b0594f7055/59a6b/janjoerke.webp 32w,\n/gatsby-jenkins-io/static/a8787b55a793681f4a2418b0594f7055/cbb78/janjoerke.webp 64w,\n/gatsby-jenkins-io/static/a8787b55a793681f4a2418b0594f7055/1a93d/janjoerke.webp 128w,\n/gatsby-jenkins-io/static/a8787b55a793681f4a2418b0594f7055/50511/janjoerke.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":170}}},"blog":null,"github":"janjoerke","html":"<div class=\"paragraph\">\n<p>Software Engineer from northern Germany.</p>\n</div>","id":"janjoerke","irc":null,"linkedin":null,"name":"Jan Jörke","slug":"/blog/authors/janjoerke","twitter":"janjoerke"}]}},{"node":{"date":"2018-08-31T00:00:00.000Z","id":"0d561c02-de0c-5799-a5a4-74b3cc361577","slug":"/blog/2018/08/31/shifting-gears/","strippedHtml":"Kohsuke here. This is a message for my fellow Jenkins developers.\n\nJenkins has been on an amazing run, but I believe we are trapped in a local optimum, and losing appeal to people who fall outside of our traditional sweet spot.\nWe need to take on new efforts to solve this. One is “cloud native Jenkins” that creates a flavor of Jenkins that runs well on Kubernetes.\nThe other is “gear shift”, where we take an evolutionary line from the current Jenkins 2, but with breaking changes in order to gain higher development speed.\n\nI say it’s time we tackle these problems head on. I’ve been talking to various folks, and I think we need to take on two initiatives.\nOne is what I call \"Cloud Native Jenkins,\" and the other is to insert a jolt in Jenkins.\n\nSome of you have already seen the presentation I posted on the Jenkins YouTube channel.  In this post, I’ll expand on that with some additional details.\n\nJenkins: Shifting Gears Presentation ( Slides)\n\nCome hear more in Kohsuke’s keynote at\nJenkins World on September 16-19th,\nregister with the code JWFOSS for a 30% discount off your pass.\n\nOur Amazing Success\n\nOur project has been an amazing success over the past 10+ years, thanks to you all. What started as my hobby project became a huge community that boasts thousands of contributors and millions of users.\nWhen I think about what enabled this amazing journey, I can think of several magic sauces:\n\nExtensible:\nthe ability to take the system, or a portion of the system, then build on top of it to achieve what you need, without anyone else’s permission.\nHere, I’m not talking about the specific technical mechanism of Guice, extension point, etc, but rather I’m talking more broadly about the governance, culture, distribution mechanism, and so on.\n\nGeneral purpose:\nAt the base level, Jenkins can be used for any kind of automation around the area of software development.\nThis matched the reality of the software engineering world well.\nCombined with extensibility, this general purpose system that is Jenkins can specialize into any domain, much like Linux and JetBrains IDEs.\n\nCommunity:\nTogether we created a community where different people push envelopes in different directions and share the fruits with others.\nThis meant everyone can benefit from somebody else’s work, and great ideas and best practices spread more quickly.\n\nOur Challenges\n\nThe way we set up our community meant that collectively we were able to work toward solving certain kinds of problems locally and organically, such as Android application development, new UX, more expressive pipeline description language, …​\n\nBut at the same time, the incremental, autonomous nature of our community made us demonstrably unable to solve certain kinds of problems.\nAnd after 10+ years, these unsolved problems are getting more pronounced, and they are taking a toll — segments of users correctly feel that the community doesn’t get them, because we have shown an inability to address some of their greatest difficulties in using Jenkins.\nAnd I know some of those problems, such as service instability, matter to all of us.\n\nIn a way, we are stuck in a local optimum, and that is a dangerous place to be when there is growing competition from all sides.\nSo we must solve these problems to ensure our continued relevance and popularity in the space.\n\nSolving those problems starts with correctly understanding them, so let’s look at those.\n\nService Instability\n\nCI/CD service was once a novelty and a nice-to-have.\nToday, it is very much a mission critical service, in no small part because of us!\nIncreasingly, people are running bigger and bigger workloads, loading up more and more plugins, and expect higher and higher availability.\n\nAdmins today are unable to meet that heightened expectation using Jenkins easily enough.\nA Jenkins instance, especially a large one, requires too much overhead just to keep it running.\nIt’s not unheard of that somebody restarts Jenkins every day.\n\nAdmins expect errors to be contained and not impact the entire service.\nThey expect Jenkins to defend itself better from issues such as pipeline execution problems, run-away processes, over resource consumption so that they don’t have to constantly babysit the service.\n\nEvery restart implies degraded service for the software delivery teams where they have to wait longer for their builds to start or complete.\n\nBrittle Configuration\n\nEvery Jenkins admin must have been burnt at least once in the past by making changes that have caused unintended side effects.\nBy “changes,” I’m talking about installing/upgrading plugins, tweaking job settings, etc.\n\nAs a result, too many admins today aren’t confident that they can make changes safely.\nThey fear that their changes might cause issues for their software delivery teams, that those teams will notice regressions before they do, and that they may not be able to back out somes changes easily.\nIt feels like touching a Jenga tower for them, even when a change is small.\n\nUpgrading Jenkins and plugins is an important sub case of this, where admins often do not have understanding of the impact.\nThis decreases the willingness to upgrade, which in turn makes it difficult for the project to move forward more rapidly, and instead we get trapped with the long tail of compatibility burden.\n\nAssembly Required\n\nI’ve often described Jenkins as a bucket full of LEGO blocks — you can build any car you want, but everyone first has to assemble their own car in order to drive one.\n\nAs CI/CD has gone mainstream, this is no longer OK.\nPeople want something that works out of the box, something that gets people to productivity within 5 clicks in 5 minutes.\nToo many choices are confusing users, and we are not helping them toward “the lit path.”\nEveryone feels uncertain if they are doing the right thing, contributors are spread thin, and the whole thing feels a bit like a Frankenstein.\n\nThis is yet another problem we can’t solve by “writing more plugins.”\n\nReduced Development Velocity\n\nThis one is a little different from others that our users face, but nonetheless a very important one, because it impacts our ability to expand and sustain the developer community, and influences how fast we can solve challenges that our users face.\n\nSome of these problems are not structural and rather just a matter of doing it (for example, Java 11 upgrade), but there are some problems here that are structural.\n\nI think the following ones are the key ones:\n\nAs a contributor, a change that spans across multiple plugins is difficult.\nTooling gets in the way, users might not always upgrade a group of changes together, reviewing changes is hard.\n\nAs a contributor, the tests that we have do not give me enough confidence to ship code.\nNot enough of them run automatically, coverage is shallow, and there just isn’t anything like production workload of real users/customers.\n\nThese core problems create other downstream problems, for example:\n\nAs a non-regular contributor, what I think of as a small and reasonable change takes forever and a 100 comments going back & forth to get in. I get discouraged from ever doing it again.\n\nAs a regular contributor, I feel people are throwing crap over the wall, and if they cause problems after a release, I’m on the hook to clean up that mess.\n\nAs a user, I get a half-baked change that wreaks havoc, which results in loss of their confidence to Jenkins, an even slower pace of change, etc. This is a vicious cycle as it makes us even more conservative, and slow down the development velocity.\n\nPath Forward\n\nIn the past, my frustration and regret is that we couldn’t take on an effort of this magnitude.\nBut that is NO MORE!\nAs CTO of CloudBees, I’m excited that these challenges are important enough for CloudBees now that we want to solve these efforts within the Jenkins project.\n\nI’ve been talking to many of you, and there are a number of existing efforts going on that touch this space already.\nFrom there, the vision emerged is that we organize around two key efforts:\n\nCloud Native Jenkins: a general purpose CI/CD engine that runs on Kubernetes, and embraces a fundamentally different architecture and extensibility mechanism.\n\nJolt in Jenkins: continue the incremental trajectory of Jenkins 2 today, but with renegotiated “contract” with users to gain what we really need, such as faster pace of development and better stability.\n\nCloud Native Jenkins\n\nIn order to solve these problems that we can’t solve incrementally,\nI’m proposing the “Cloud Native Jenkins” sub-project in the context of the\nCloud Native SIG\nwith Carlos, who is the leader of this SIG.\n\nWe don’t have all the answers, that’s something we’ll discuss and figure out collectively, but based on numerous conversations with various folks, I think there are many clear pieces of puzzles.\n\nKubernetes as the Runtime\n\nJust like Java was the winning server application platform in the early 2000s, today, Kubernetes is the dominant, winning platform.\nCloud Native Jenkins should embrace the paradigm this new platform encourages. For example,\n\nServerless / function-as-a-service build execution (ala\nJenkinsfile runner)\nthat are isolated.\n\nVarious pieces of functionalities deployed as separate microservices.\n\nServices interacting through\nKubernetes CRDs\nin order to promote better reuse and composability.\n\nThese are the design principles that enable highly desirable properties like infinite scalability, pay-as-you-go cost model, immutability, zero down time operability, etc.\n\nNew Extensibility Mechanism\n\nWe need to introduce a new mechanism of extensibility in order to retain the magic sauces, and continue our incredible ecosystem.\n\nFor example, microservice or container-based extensibility avoids the service instability problem (ala\nKnative builder\nand the\nuserspace-scm work.)\nPipeline shared libraries is another example that concretely shows how extensibility mechanism can go beyond plugin, though it hasn’t fully flourished as one just yet.\n\nData on Cloud Managed Data Services\n\nThe long-term data storage must be moved from the file system to data services backed by cloud managed services, in order to achieve high availability and horizontal scalability, without burdening admins with additional operational responsibilities.\n\nConfiguration as Code\n\nJenkins Configuration as Code\nhas been incredibly well received, in part because it helps to solve some of the brittle configuration problems.\nIn Cloud Native Jenkins, JCasC must play a more central role, which in turn also helps us reduce the surface area for Blue Ocean to cover by eliminating many configuration screens.\n\nEvergreen\n\nJenkins Evergreen\nis another well received effort that’s already underway, which aims to solve the brittleness problem and developer velocity problem. This is a key piece of the puzzle that allows us to move faster without throwing users under the bus.\n\nSecure by Default Design\n\nOver the past years, we’ve learned that several different areas of Jenkins codebase, such as Remoting, are inherently prone to security vulnerabilities because of their design. Cloud Native Jenkins must address those problems by flipping those to “secure by design.”\n\nFollowing Footsteps of Jenkins X\n\nJenkins X\nhas been pioneering the use of Jenkins on Kubernetes for a while now, and it has been very well received, too.\nSo naturally, part of the aim of Cloud Native Jenkins is to grow and morph Jenkins into a shape that really works well for Jenkins X.\nCloud Native Jenkins will be the general purpose CI/CD engine that runs on Kubernetes, which Jenkins X uses to create an opinionated CD experience for developing cloud native apps.\n\nAll The Same Good Things, with New Foundation\n\nAnd then on top of these foundations, we need to rebuild or transplant all the good things that people love about Jenkins today, and all the good things people expect, such as:\n\nGreat “batteries included” onboarding experience for new users, where we are present in all the marketplaces, 5 clicks to get going and easy integration with key services.\n\nModern lovable UX in the direction of front-end web apps that Blue Ocean pioneered.\n\nGeneral purpose software that is useful for all sorts of software development.\n\nCloud Native Jenkins MVP\n\nAs I wrote, a number of good efforts are already ongoing today. Thus in order to get this effort off the ground, I believe the first MVP that we aim toward is pretty clear, which is to build a function-as-a-service style Jenkins build engine  that can be used underneath Jenkins X.\n\nCloud Native Jenkins MVP combines the spirits of Jenkins Pipeline, Jenkins Evergreen, Jenkinsfile Runner, and Jenkins Configuration as Code.\nIt consists of:\n\nWebhook receiver:\na service that receives webhooks from GitHub and triggers a build engine.\n\nBuild Engine:\ntake Jenkinsfile Runner and evolve it so that it can run as a “function” that carries out a pipeline execution, with some CasC sprinkled together in order to control Jenkins configuration and plugins  used.\nThis way, Jenkinsfile works as-is for the most part.\n\nContinuously delivered through Evergreen:\nIt allows us to solve the combinatorial version explosion problem, allow us to develop changes that span multiple plugins faster, and develop changes more confidently.\nOf all the projects out there, ours should be the community that believes in the value of Continuous Delivery and Evergreen is how we bring continuous delivery to the development of Cloud Native Jenkins itself.\n\nThis solves some of the key challenges listed above that are really hard to achieve today, so it’s already incredibly useful.\n\nThe catch is that this MVP has no GUI. There’s no Blue Ocean UI to look at. No parsing of test reports, no build history. It uses no persistent volumes, it keeps no record of builds. The only thing permanent at the end of a build is whatever data is pushed out from Jenkins Pipeline, such as images pushed to a Docker registry, email notifications, and GitHub commit status updates.  Load of other features in Jenkins will not be available here.\n\nThis is not that far from how some sophisticated users are deploying Jenkins today. All in all, I think this is the right trade off for the first MVP. As you can see, we have most of the pieces already.\n\nFrom here, the build engine will get continuously more polished and more cloud native, other services will get added to regain features that were  lost, new extensibility will get introduced to reduce the role of current in-VM plugins, and so on.\n\nJolt in Jenkins\n\nCloud Native Jenkins is a major effort and in particular initially it’s not usable for everyone; it only targets a subset of Jenkins functionalities, and it requires a platform whose adoption is still limited today.\nSo in parallel, we need to continue the incremental evolution of Jenkins 2, but in an accelerated speed. Said differently, we need to continue to serve the majority of production workload on Jenkins 2 today, but we are willing to break some stuff to gain what we really need, such as faster pace of development and better stability, in ways that were previously not possible. This requires us injecting a jolt in Jenkins.\n\nRelease Model Change\n\nThe kind of jolts that we need will almost certainly means we need to renegotiate the expectation around new releases with our users.\nMy inspiration source is what happened to the development of Java SE. It changed the release model and started moving faster, by shedding off more pieces faster, in ways that they haven’t done before.\nAgain, Jenkins Evergreen is the key piece that achieves this without throwing users under a bus, for the reasons I described in the Cloud Native MVP above.\n\nCompatibility\n\nThis jolt is aimed to put us on a different footing, one where our current “forever compatibility” expectation does not hold. If that requires us to use a new major version number, such as Jenkins 3, or new major version number every N months, I’m open to that.\n\nOf course, whatever move we do has to make sense to users. The accelerated pace of value delivery needs to justify any inconvenience we put on users, such as migration, breaking changes, and so on.\n\nIn practice, what that means is that we need to be largely compatible. We have to protect users’ investment into their existing job definitions as much as possible. We continue to run freestyle jobs, etc…​\n\nIngredients\n\nOther proposals CloudBees is putting forward with the intent to staff the effort are:\n\nConfiguration as Code: accelerate that and make it a more central  part of Jenkins.\n\nDeveloper experience improvements through buildpack style auto-detection of project types.\n\nContinued evolution of Jenkins Pipeline\n\nThere’s an effort going on to remove CPS execution of Pipeline and isolate any failures during pipeline execution.\n\nContinue to evolve Jenkins Pipeline toward the sweet spot that works well with the Cloud Native Jenkins effort.\n\nContinued tactical bug-by-bug improvements of Pipeline.\n\nEvergreen: I already talked about this above.\n\nPlugin spring cleaning: let’s actively guide users more toward the sweet spot of Jenkins and reduce our feature surface area, so that we can focus our contributors’ effort to important parts of Jenkins. I expect this to be a combination of governance and technical efforts.\n\nTable stakes service integration: let’s look at what kind of tablestake tool/service integrations today’s user need, and\nsee if we are meeting/exceeding the competition.\nWhere we fall short, let’s add/reimplement what are needed.\n\nUI Effort\n\nThe Web UI will be likely done differently in Cloud Native Jenkins, as its own app and not a plugin in Jenkins. JCasC will also play a bigger role in Cloud Native Jenkins, reducing UI surface area from Jenkins.\n\nGiven that, CloudBees will reconsider where to spend its effort in Blue Ocean. The current work where parts of Blue Ocean are made reusable as NPM modules is one example that aligns well with this new vision.\n\nConclusion\n\nThis document lays out the key directions and approaches in a broad stroke, which I discussed with a number of you in the past. Hopefully, this gives you the big picture of how I envision where to move Jenkins forward, not just as the creator of Jenkins but as the CTO of CloudBees, who employs a number of key contributors to the Jenkins project.\n\nCome meet Kohsuke and chat with him about the direction of Jenkins at\nJenkins World on September 16-19th,\nregister with the code JWFOSS for a 30% discount off your pass.","title":"Jenkins: Shifting Gears","tags":["development","core"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/edb43/kohsuke.jpg","srcSet":"/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/f81fe/kohsuke.jpg 32w,\n/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/01b1b/kohsuke.jpg 64w,\n/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/edb43/kohsuke.jpg 128w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/035c3/kohsuke.webp 32w,\n/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/273f8/kohsuke.webp 64w,\n/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/e3840/kohsuke.webp 128w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":148}}},"blog":null,"github":"kohsuke","html":"<div class=\"paragraph\">\n<p>Kohsuke is the creator of Jenkins.</p>\n</div>","id":"kohsuke","irc":null,"linkedin":null,"name":"Kohsuke Kawaguchi","slug":"/blog/authors/kohsuke","twitter":"kohsukekawa"}]}},{"node":{"date":"2017-02-06T00:00:00.000Z","id":"46885786-902f-55cb-b263-cae8df8f0611","slug":"/blog/2017/02/06/scm-api-2-take2/","strippedHtml":"In January we\nannounced the release of SCM API 2.0.\nAfter the original release was published we identified four new high-impact\nissues.  We decided to remove the new versions of the plugins from the update\ncenter until those issues could be resolved. The issues have now been resolved\nand the plugins are now available from the update center.\n\nSummary for busy Jenkins Administrators\n\nUpgrading should make multi-branch projects much better.  When you are ready to\nupgrade you must ensure that you upgrade all the required plugins.  If you miss\nsome, just upgrade them and restart to fix the issue. And of course, it’s\nalways a good idea to take a backup of your JENKINS_HOME before upgrading any\nplugins.\n\nIn the list below, version numbers in bold indicate a change from the\noriginal version in the\noriginal announcement\n\nFolders Plugin\n\n5.17 or newer\n\nSCM API Plugin\n\n2.0.2 or newer\n\nBranch API Plugin\n\n2.0.2 or newer\n\nGit Plugin\n\nThis depends on the exact release line of the Git plugin that you are using.\n\nFollowing the 2.6.x release line: 2.6.4 or newer\n\nFollowing the 3.0.x release line ( recommended): 3.0.4 or newer\n\nMercurial Plugin\n\n1.58 or newer\n\nGitHub Branch Source Plugin\n\n2.0.1 or newer\n\nBitBucket Branch Source Plugin\n\n2.0.2 or newer\n\nGitHub Organization Folders Plugin\n\n1.6\n\nPipeline Multibranch Plugin\n\n2.12 or newer\n\nIf you are using the Blue Ocean plugin\n\nBlue Ocean Plugin\n\n1.0.0-b22 or newer\n\nOther plugins that may require updating:\n\nGitHub API Plugin\n\n1.84 or newer\n\nGitHub Plugin\n\n1.25.0 or newer\n\nIf you upgrade to Branch API 2.0.x and you have either the GitHub Branch Source or the BitBucket Branch Source plugins and you do not upgrade those instances to the 2.0.x line then your Jenkins instance will fail to start-up correctly.\n\nThe solution is just to upgrade the GitHub Branch Source or the BitBucket Branch Source plugin (as appropriate) to the 2.0.x line.\n\nAfter an upgrade you will see the data migration warning (see the screenshot in\nJENKINS-41608 for an\nexample) this is normal and expected.  The unreadable data will be removed by\nthe next scan / index or can be removed manually using the Discard Unreadable\nData button.  The warning will disappear on the next restart after the\nunreadable data has been removed.\n\nPlease update to the versions listed above. If you want to know more about the\nissues and how they were resolved, see the next section.\n\nAnalysis of the issues\n\nThe issues described below are resolved with these plugin releases:\n\nFolders Plugin: 5.17\n\nSCM API Plugin: 2.0.2\n\nBranch API Plugin: 2.0.2\n\nGit Plugin: Either 2.6.4 or 3.0.4\n\nGitHub Branch Source Plugin: 2.0.1\n\nBitBucket Branch Source Plugin: 2.0.2\n\nPipeline Multibranch Plugin: 2.12\n\nJENKINS-41121: GitHub Branch Source upgrade can cause a lot of rebuilds :\n\nMigration of GitHub branches from 1.x to 2.x resulted in a change of the\nimplementation class used to identify branches.  Some other other bugs in\nBranch API had been fixed and the combined effect resulted in a rebuild of all\nGitHub Branches (not PRs) after an upgrade to GitHub Branch Source Plugin\n2.0.0.  This rebuild was referred to as a \"build storm\".\n\nResolution:\n\nThe SCM API plugin was enhanced to add an extension point that allows for a second round of data migration when upgrading.\n\nThe second round of data migration allows plugins implementing the SCM API contract to fix implementation class issues in context.\n\nThe Branch API plugin was enhanced to use this new extension point.\n\nThe GitHub Branch Source plugin was enhanced to provide an implementation of this extension point.\n\nJENKINS-41255: Upgrading from a navigator that did not assign consistent source ids to a version that does assign consistent source ids causes a build storm on first scan :\n\nThe GitHub Branch Source and BitBucket Branch Source plugins in 1.x were not\nassigning consistent IDs to multi-branch projects discovered in an Organization\nFolder.  Both plugins were fixed in 2.0.0 to assign consistent IDs as a change\nof ID would result in a rebuild of all projects.  What was missed is that the\nvery first scan of an Organization Folder after an upgrade will change the\nrandomly assigned ID assigned by the 1.x plugins into the consistent ID\nassigned by the 2.0.0 plugins and consequently trigger a rebuild of all\nbranches. This rebuild was referred to as a \"build storm\".\n\nResolution:\n\nThe Branch API plugin was enhanced to detect the case where a branch source has\nbeen changed but the change is only changing the ID.  When such changes are\nidentified, the downstream references of the ID are all updated which will\nprevent a build storm.\n\nJENKINS-41313: On first index after upgrade to 2.0.0 all open PRs are rebuilt :\n\nThe BitBucket Branch Source 1.x did not store all the information about PRs\nthat is required by the SCM API 2.0.x model.  This could well have resulted in\nsubtle effects when manually triggering a rebuild of a merge PR if the PR’s\ntarget branch has been modified after the PR branch was first detected by\nJenkins. Consequently, as the information is required, BitBucket Branch Source\nplugin 2.0.0 populated the information with dummy values which would force the\ncorrect information to be retrieved.  The side-effect is that all PR branches\nwould be rebuilt.\n\nResolution:\n\nThe changes in SCM API 2.0.2 introduced to resolve JENKINS-41121 provided a path to resolve this issue without causing a rebuild of all PR branches.\n\nThe BitBucket Branch Source plugin was enhanced to provide an implementation of the new SCM API extension point that connects to BitBucket and retrieves the missing information.\n\nJENKINS-41124: Can’t get a human readable job name anymore :\n\nDuring initial testing of the Branch API 2.0.0 release an issue was identified\nwith how Organization Folders handled unusual names.  None of the existing\nimplementations of the SCMNavigator API could generate such unusual names due\nto form validation on GitHub / BitBucket replacing unusual characters with -\nwhen creating a repository.\n\nIt would be irresponsible to rely on external services sanitizing their input\ndata for the correct operation of Organization Folders.  Consequently, in\nBranch API 2.0.0 the names were all transformed into URL safe names, with the\noriginal URLs still resolving to the original projects so that any existing\nsaved links would remain functional.\n\nQuite a number of people objected to this change of URL scheme.\n\nResolution:\n\nThere has been a convention in Jenkins that the on-disk storage structure for\njobs mirrors the URL structure. This is only a convention and there is nothing specific in the code that\nmandates following the convention.\n\nThe Folders Plugin was enhanced to allow for computed folders (where the item\nnames are provided by an external source) to provide a strategy to use when\ngenerating the on-disk storage names as well as the URL component names for\nthe folder’s child items.\n\nThe Branch API plugin was enhanced to use this new strategy for name transformation.\n\nThe net effect of this change is that the URLs remain the same as for 1.x but\nthe on-disk storage uses transformed names that are future proofed against\nany new SCMNavigator implementations where the backing service allows names\nthat are problematic to use as filesystem directory names.\n\nSide-effect:\n\nThe Branch API 2.0.0 approach handled the transformation of names by renaming the items using the Jenkins Item rename API.\n\nThe Branch API 2.0.2 approach does not rename the child items as it is only the on-disk storage location that is moved.\n\nThis means that the Jenkins Item rename API cannot be used.\n\nAt this time, the only known side-effect is in the Job Configuration History plugin.\nThe configuration history of each child item will still be tracked going\nforward after the upgrade.  The pre-upgrade configuration history is also\nretained.  Because the Jenkins Item rename API cannot be used to flag the\nconfiguration file location change, there is no association between the\npre-upgrade history chain and the post-upgrade history chain.","title":"SCM API 2.0 Release Take 2","tags":["development","plugins"],"authors":[{"avatar":null,"blog":null,"github":"stephenc","html":"","id":"stephenc","irc":null,"linkedin":null,"name":"Stephen Connolly","slug":"/blog/authors/stephenc","twitter":"connolly_s"}]}},{"node":{"date":"2017-01-17T00:00:00.000Z","id":"23308d18-32ba-5461-b5aa-74f746e5049f","slug":"/blog/2017/01/17/scm-api-2.0-release/","strippedHtml":"The regressions\ndiscovered after release have now been resolved and this post has been updated with the correct plugin version numbers.\n\nSee this post for more details.\n\nWe are announcing the\nSCM API\n2.0.x and\nBranch API\n2.0.x release lines.\n\nDownstream of this there are also some great improvements to a number of popular plugins including:\n\nGitHub Branch Source\n\nBitBucket branch source\n\nGit\n\nMercurial\n\nPipeline Multibranch\n\nGitHub Organization Folders\n\nThere are some gotcha’s that Jenkins administrators will need to be aware of.\n\nAlways take a backup of your JENKINS_HOME before upgrading any plugins.\n\nWe want to give you the whole story, but the take home message is this:\n\nWhen updating the\nSCM API\nand/or\nBranch API\nplugins to the 2.0.x release lines, if you have any of the\nGitHub Organization Folders,\nGitHub Branch Source\nand/or\nBitBucket branch source\nplugins installed then you must upgrade them all to 2.0.x at the same time or Bad Things™ will happen.\n\n— A Jenkins Administrator\n\nDo NOT upgrade some of these plugins but not others!\nDoing so may cause your jobs to fail to load.\n\nIf you don’t care about the hows and whys, you can just skip down to this section but if you are curious…​ here we go!\n\nThe back-story\n\nWay back in September 2013 we announced the\nLiterate plugin,\nas an experimental new way of modeling branch development in Jenkins.\n\nWhen you are performing an experiment, the recommendation is to do just enough work to let you perform the test.\nHowever, the culture in Jenkins is to always try and produce reusable components that others can use in ways you have not anticipated.\n\nSo when releasing the initial version of the\nLiterate plugin\nwe also separated the Literate specific bits from the SCM specific concepts and multi-branch concepts.\nThese were lower level concepts were organized into the following plugins:\n\nSCM API -\nwhich was intended to be a plugin to hold a next generation API for interacting with source control systems.\n\nBranch API -\nwhich was intended to be a plugin to hold the multi-branch functionality that was abstracted from the usage by the Literate plugin.\n\nIn addition, we released updates to three of the more common SCM plugins which included implementations of the SCM API:\n\nGit plugin\n\nSubversion plugin\n\nMercurial plugin\n\nWhile there was some interest in the Literate plugin, it did not gain much traction - there are only 39 Jenkins instances running the Literate plugin as of December 2016.\n\nIn terms of the reusable components, we had only made a minimal implementation with some limitations:\n\nVery basic event support - events can only trigger a re-scan of the entire repository.\nThis was acceptable at the time because the only three implementations use a local cache of the remote state so re-scanning is quick.\n\nNo implementation of the SCMFileSystem API.\nAs a result it is not possible for plugins like\nPipeline Multibranch\nto get the Jenkinsfile from the remote repository without needing to checkout the repository into a workspace.\n\nNo documentation on how plugin developers are supposed to implement the SCM API\n\nNo documentation on how plugin developers are supposed to consume the SCM API (if they wanted to do something like Branch API but not the same way as Branch API)\n\nNo documentation on how plugin developers are supposed to implement the Branch API to create their own multi-branch project types\n\nNo documentation on for users on how the Branch API based project types are expected to work.\n\nRoll forward to November 2015 and Jenkins Pipeline got a release of the\nPipeline Multibranch.\nIt seems that pairing Pipeline with Branch API style multi-branch is much more successful than Literate - there are close to 60,000 instances running the pipeline multi-branch plugin as of December 2016.\n\nThere also were two new SCM plugins implementing the SCM API:\n\nGitHub Branch Source Plugin\n\nBitBucket Branch Source Plugin\n\nUnlike the previous implementations of the SCM API, however, these plugins do not maintain a local cache of the repository state.\nRather they make queries via the GitHub / BitBucket REST APIs on demand.\n\nThe above design decision exposed one of the initial MVP compromises of the SCM API plugin: very basic event support.\nUnder the SCM API 1.x model, the only event that an SCMSource can signal is something changed, go look at everything again.\nWhen you are accessing an API that only allows 5,000 API calls per hour, performing a full scan of the entire repository just to pick up a change in one branch does not make optimum usage of that 5,000 calls/hour rate limit.\n\nSo we decided that perhaps the SCM API and Branch API plugins have left their Minimum Viability Experiment state and the corresponding limitations should be addressed.\n\nEnter SCM API 2.0.x and Branch API 2.0.x\n\nSo what has changed in the\nSCM API\n2.0.x and\nBranch API\n2.0.x release lines?\nThese plugin releases include:\n\ndocumentation on how plugin developers are supposed to\nimplement the SCM API\n\ndocumentation on how plugin developers are supposed to\nconsume the SCM API\n(if they wanted to do something like Branch API but not the same way as Branch API)\n\ndocumentation on how plugin developers are supposed to\nimplement the Branch API\nto create their own multi-branch project types\n\ngeneric documentation for users on\nhow Branch API based project types are intended to work\n\na full featured\nevent system\nthat allows implementers to provide fine grained notifications to consumers\n\nlots\nand\nlots\nof new automated tests\n\na mock implementation\nof the SCM API to help consumers of the SCM API test their usage.\n\nIn addition, we have upgraded the following plugins to include the new fine-grained event support:\n\nGit Plugin\n\nMercurial Plugin\n\nOk, that was the good news.\nHere is the bad news.\n\nWe found out that the GitHub Branch Source and BitBucket Branch Source plugins had made invalid assumptions about how to implement the SCM API.\nTo be clear, this was not the plugin developers fault: at the time there was no documentation on how to implement the SCM API.\n\nBut fixing the issues that we found means that you have to be careful about which specific combinations of plugin versions you have installed.\n\nSCM API Plugin\n\nTechnically, the 2.0.x line of this plugin is both API and on-disk compatible with plugins compiled against older version lines.\n\nHowever, the 1.x lines of both the GitHub Branch Source and BitBucket Branch Source plugins have hard-coded assumptions about internal implementation of the SCM API that are no longer valid in the 2.0.x line.\n\nIf you upgrade to SCM API 2.0.x and you have either the GitHub Branch Source or the BitBucket Branch Source plugins and you do not upgrade those instances to the 2.0.x line then your Jenkins instance will fail to start-up correctly.\n\nThe solution is just to upgrade the GitHub Branch Source or the BitBucket Branch Source plugin (as appropriate) to the 2.0.x line.\n\nIf you upgrade the SCM API plugin to the 2.0.x line and do not upgrade the Branch API plugin to the 2.0.x line then you will not get any of the benefits of the new version of the SCM API plugin.\n\nBranch API Plugin\n\nThe 2.0.x line of this plugin makes on-disk file format changes that mean you will be unable to roll back to the 1.x line after an upgrade without restoring the old data files from a back-up.\nTechnically, the API is compatible with plugins compiled against older version lines.\n\nThe 1.x lines of both the GitHub Branch Source and BitBucket Branch Source plugins have implemented hacks that make assumptions about internal implementation of the Branch API that are no longer valid in the 2.0.x line.\n\nThe Pipeline Multibranch plugin made a few minor invalid assumptions about how to implement a Multibranch project type.\nFor example, if you do not upgrade the Pipeline Multibranch plugin in tandem then you will be unable to manually delete an orphaned item before the orphaned item retention strategy runs, which should be significantly less frequently with the new event support.\n\nIf you upgrade to Branch API 2.0.x and you have either the GitHub Branch Source or the BitBucket Branch Source plugins and you do not upgrade those instances to the 2.0.x line then your Jenkins instance will fail to start-up correctly.\n\nThe solution is just to upgrade the GitHub Branch Source or the BitBucket Branch Source plugin (as appropriate) to the 2.0.x line.\n\nGit Plugin\n\nThe new releases of this plugin are both API and on-disk compatible with plugins compiled against the previous releases.\n\nThe 2.0.x lines of both the GitHub Branch Source and BitBucket Branch Source plugins require that you upgrade your Git Plugin to one of the versions that supports SCM API 2.0.x.\nIn general, the required upgrade will be performed automatically when you upgrade your GitHub Branch Source and BitBucket Branch Source plugins.\n\nMercurial Plugin\n\nThe new release of this plugin is both API and on-disk compatible with plugins compiled against the previous releases.\n\nThe 2.0.x line of the BitBucket Branch Source plugins require that you upgrade your Mercurial Plugin to the 2.0.x line.\nIn general, the required upgrade will be performed automatically when you upgrade your  BitBucket Branch Source plugins.\n\nBitBucket Branch Source Plugin\n\nThe 2.0.x line of this plugin makes on-disk file format changes that mean you will be unable to roll back to the 1.x line after an upgrade without restoring the old data files from a back-up.\n\nGitHub Branch Source Plugin\n\nThe 2.0.x line of this plugin makes on-disk file format changes that mean you will be unable to roll back to the 1.x line after an upgrade without restoring the old data files from a back-up.\n\nIf you upgrade to GitHub Branch Source 2.0.x and you have the GitHub Organization Folders plugin installed, you must upgrade that plugin to the tombstone release.\n\nGitHub Organization Folders Plugin\n\nThe functionality of this plugin has been migrated to the GitHub Branch Source plugin.\nYou will need to upgrade to the tombstone release in order to ensure all the data has been migrated to the classes in the GitHub Branch Source plugin.\n\nOnce you have upgraded to the tombstone version and all GitHub Organization Folders have had a full scan completed successfully, you can disable and uninstall the GitHub Organization Folders plugin.\nThere will be no more releases of this plugin after the tombstone.\nThe tombstone is only required for data migration.\n\nSummary for busy Jenkins Administrators\n\nUpgrading should make multi-branch projects much better.\nWhen you are ready to upgrade you must ensure that you upgrade all the required plugins.\nIf you miss some, just upgrade them and restart to fix the issue.\n\nFolders Plugin\n\n5.16 5.17 or newer\n\nSCM API Plugin\n\n2.0.1 2.0.2 or newer\n\nBranch API Plugin\n\n2.0.0 2.0.2 or newer\n\nGit Plugin\n\nEither 2.6.2 2.6.4 or newer in the 2.6.x line or 3.0.2 3.0.4 or newer\n\nMercurial Plugin\n\n2.0.0 or newer\n\nGitHub Branch Source Plugin\n\n2.0.0 2.0.1 or newer\n\nBitBucket Branch Source Plugin\n\n2.0.0 2.0.2 or newer\n\nGitHub Organization Folders Plugin\n\n1.6\n\nPipeline Multibranch Plugin\n\n2.10 2.12 or newer\n\nIf you are using the Blue Ocean plugin\n\nBlue Ocean Plugin\n\n1.0.0-b22 or newer\n\nOther plugins that may require updating:\n\nGitHub API Plugin\n\n1.84 or newer\n\nGitHub Plugin\n\n1.25.0 or newer\n\nAfter an upgrade you will see the data migration warning (see the screenshot in JENKINS-41608 for an example) this is normal and expected.\nThe unreadable data will be removed by the next scan / index or can be removed manually using the Discard Unreadable Data button.\nThe warning will disappear on the next restart after the unreadable data has been removed.\n\nSummary for busy Jenkins users\n\nSCM API 2.0.x adds fine-grained event support.\nThis should significantly improve the responsiveness of multi-branch projects.\nThis should significantly reduce your GitHub API rate limit usage.\n\nIf you are using the\nGitHub Branch Source\nor\nGitHub Organization Folders\nplugins then upgrading will significantly reduce the API calls made by Jenkins to GitHub.\n\nIf you are using any of the upgraded SCM plugins (e.g. Git, Mercurial, GitHub Branch Source, BitBucket Branch Source) then upgrading will significantly improve the responsiveness to push event notifications.\n\nSummary for busy SCM plugin developers\n\nYou should read the new\ndocumentation\non how plugin developers are supposed to implement the SCM API\n\nWhere to now dear Literate Plugin\n\nThe persistent reader may be wondering what happens now to the Literate plugin.\n\nFor me, the logical heir of the Literate Plugin is the\nPipeline Model Definition plugin.\nThis new plugin has the advantage of an easy to read pipeline syntax with the extra functionality that I suspect was preventing people from adopting Literate.\n\nThe good news is that the Pipeline Model Definition already has 5000 installations as of December 2016 and I expect up-take to keep on growing.","title":"SCM API turns 2.0 and what that means for you","tags":["development","plugins"],"authors":[{"avatar":null,"blog":null,"github":"stephenc","html":"","id":"stephenc","irc":null,"linkedin":null,"name":"Stephen Connolly","slug":"/blog/authors/stephenc","twitter":"connolly_s"}]}},{"node":{"date":"2016-04-11T00:00:00.000Z","id":"71b50dae-de34-5505-957d-a9c0f8d10044","slug":"/blog/2016/04/11/run-your-api-tests-continuously-with-jenkins-and-dhc/","strippedHtml":"This is a guest post by Guillaume Laforge.\nWell known for his contribution to the Apache Groovy project,\nGuillaume is also the \"Product Ninja and Advocate\" of Restlet,\na company focusing on Web APIs:\nwith DHC (an API testing client),\nRestlet Studio (an API designer),\nAPISpark (an API platform in the cloud),\nand the Restlet Framework\nopen source project for developing APIs.\n\nModern mobile apps, single-page web sites and applications, are more and more relying on Web APIs,\nas the nexus of the interaction between the frontend and the backend services.\nWeb APIs are also central to third-party integration, when you want to share your services with others,\nor when you need to consume existing APIs to build your own solution on top of their shoulders.\n\nWith APIs being a key element of your architecture and big picture,\nit’s obviously important to assess that this API is functioning the way it should, thanks to proper testing.\nYour framework of choice, regardless of the technology stack or programming language used,\nwill hopefully offer some facilities for testing your code,\nwhether in the form of unit tests, or ideally with integration tests.\n\nCoding Web API tests\n\nFrom a code perspective, as I said, most languages and frameworks provide approaches to testing APIs built with them.\nThere’s one I wanted to highlight in particular, which is one developed with a DSL approach (Domain-Specific Language),\nusing the Apache Groovy programming language, it’s\nAccuREST.\n\nTo get started, you can have a look at the introduction,\nand the usage guide.\nIf you use the contract DSL,\nyou’ll be able to write highly readable examples of requests you want to issue against your API,\nand the assertions that you expect to be true when getting the response from that call.\nHere’s a concrete example from the documentation:\n\nGroovyDsl.make {\n    request {\n        method 'POST'\n        urlPath('/users') {\n            queryParameters {\n                parameter 'limit': 100\n                parameter 'offset': containing(\"1\")\n                parameter 'filter': \"email\"\n            }\n        }\n        headers {\n            header 'Content-Type': 'application/json'\n        }\n        body '''{ \"login\" : \"john\", \"name\": \"John The Contract\" }'''\n    }\n    response {\n        status 200\n        headers {\n            header 'Location': '/users/john'\n        }\n    }\n}\n\nNotice that the response is expected to return a status code 200 OK, and a Location header pointing at /users/john.\nIndeed, a very readable way to express the requests and responses!\n\nTooling to test your APIs\n\nFrom a tooling perspective, there are some interesting tools that can be used to test Web APIs,\nlike Paw (on Macs),\nAdvanced REST client,\nPostman or\nInsomnia.\n\nBut in this article, I’ll offer a quick look at DHC,\na handy visual tool, that you can use both manually to craft your tests and assertions,\nand whose test scenarios you can export and integrate in your build and continuous integration pipeline,\nthanks to Maven and Jenkins.\n\nAt the end of this post, you should be able to see the following reporting in your Jenkins dashboard,\nwhen visualising the resulting API test execution:\n\nIntroducing DHC\n\nDHC is a Chrome extension, that you can\ninstall from the Chrome Web Store,\nin your Chrome browser. There’s also an online service available, with some limitations.\nFor the purpose of this article, we’ll use the Chrome extension.\n\nIn the main area, you can create your request, define the URL to call, specify the various request headers or params,\nchose the method you want to use, and then, you can click the send button to issue the request.\n\nIn the left pane, that’s where you’ll be able to see your request history, create and save your project in the cloud,\nor also set context variables.\n\nThe latter is important when testing your Web API, as you’ll be able to insert variables like for example\n{localhost} for testing locally on your machine or {staging} and {prod} to run your tests in different environments.\n\nIn the bottom pane, you have access to actual raw HTTP exchange, as well as the assertions pane.\n\nAgain, a very important pane to look at! With assertions, you’ll be able to ensure that your Web API works as expected.\nFor instance, you can check the status code of the call, check the payload contains a certain element,\nby using JSON Path or XPath to go through the JSON or XML payload respectively.\n\nBeyond assertions, what’s also interesting is that you can chain requests together.\nA call request can depend on the outcome of a previous request!\nFor example, in a new request, you could pass a query parameter whose value would be the value of some element\nof the JSON payload of a previously executed request.\nAnd by combining assertions, linked requests and context variables together, you can create full-blown test scenarios,\nthat you can then save in the cloud, but also export as a JSON file.\n\nTo export that test scenario, you can click on the little export icon in the bottom left hand corner,\nand you’ll be able to select exactly what you want to export:\n\nRunning your Web API tests with Maven\n\nNow things become even more interesting, as we’ll proceed to using Maven and Jenkins!\nAs the saying goes, there’s a Maven plugin for that! For running those Web API tests in your build!\nEven if your Web API is developed in another technology than Java, you can still create a small Maven build\njust for your Web API tests.\nAnd the icing on the cake, when you configure Jenkins to run this build, as the plugin outputs JUnit-friendly test reports,\nyou’ll be able to see the details of your successful and failed tests, just like you would see JUnit’s!\n\nLet’s sketch your Maven POM:\n\n4.0.0\n\ncom.example\nmy-first-api-test\n1.2.3\n\ncom.restlet.dhc\ndhc-maven-plugin\n1.1\n\ntest\n\ntest\n\ncompanies-scenario.json\n\nrestlet-maven\nRestlet public Maven repository Release Repository\nhttps://maven.restlet.com\n\nVisualizing Web API test executions in Jenkins\n\nOnce you’ve configured your Jenkins server to launch the test goal of this Maven project,\nyou’ll be able to see nice test reports for your Web API scenarios, like in the screenshot in introduction of this article!\n\nNext, you can easily run your Web API tests when developers commit changes to the API,\nor schedule regular builds with Jenkins to monitor an online Web API.\n\nFor more information, be sure to read the tutorial on\ntesting Web APIs with DHC.\nThere are also some more resources like a\nscreencast,\nas well as the\nuser guide, if you want to learn more.\nAnd above all, happy testing!","title":"Run Your API Tests Continuously with Jenkins and DHC","tags":["development","webapis","testing"],"authors":[{"avatar":null,"blog":"https://glaforge.appspot.com/","github":"glaforge","html":"","id":"glaforge","irc":null,"linkedin":null,"name":"Guillaume Laforge","slug":"/blog/authors/glaforge","twitter":"glaforge"}]}},{"node":{"date":"2015-11-01T00:00:00.000Z","id":"5c931f6f-0e08-5295-b4bb-ee8fc4aaf3b6","slug":"/blog/2015/11/01/adopt-a-plugin/","strippedHtml":"With more than a thousand public plugins in the Jenkins community now, it should come as no surprise that some of them are no longer actively maintained. Plugin authors move on when they change jobs, or lose interest in the plugin, and that’s fine. Plugins are hosted on the Jenkins project infrastructure after all, and when a maintainer moves on, others can continue their work.\n\nThe major problem of course is that it’s often difficult to tell whether a plugin is still maintained (and there’s just not a lot going on), or whether its maintainer has lost interest. Most plugins don’t get released every few weeks, or even every few months, and still do their job just fine.\n\nTo connect plugins that aren’t actively maintained with potential maintainers, we recently implemented the \"Adopt-a-plugin\" initiative: We built a list of plugins that are up for \"adoption\", and display a prominent message on the plugins' wiki pages. Anyone interested in taking over as a plugin maintainer can then contact us, and we’ll set you up.\n\nAre you interested in becoming a plugin maintainer? Maybe one of your favorite plugins isn’t actively maintained right now. Check out the Adopt a Plugin page for more details on this program, and a list of plugins that would benefit from your help.","title":"Adopt a plugin!","tags":["development","meta","plugins"],"authors":[{"avatar":null,"blog":null,"github":"daniel-beck","html":"<div class=\"paragraph\">\n<p>Daniel is a Jenkins core maintainer and, as security officer, leads the <a href=\"/security/#team\">Jenkins security team</a>.\nHe sometimes contributes to developer documentation and project infrastructure.</p>\n</div>","id":"daniel-beck","irc":null,"linkedin":null,"name":"Daniel Beck","slug":"/blog/authors/daniel-beck","twitter":null}]}},{"node":{"date":"2015-09-07T00:00:00.000Z","id":"2520f21a-bf41-5f6e-8001-bea5cf3ea0d8","slug":"/blog/2015/09/07/office-hour-on-proposed-ui-ux-changes/","strippedHtml":"Gus Reiber will host this week’s office hour on Wednesday, 11 AM PDT. He’ll talk about some of the UI/UX improvements in Jenkins that he’s working on, and will answer your questions about it.\n\nHe’s already given several talks about this, so you can check these out to learn more before the office hour:\n\nJUC US East\n\nJUC Europe\n\nJUC US West\n\nThere are also some mailing list threads where he’s discussing his designs with the community:\n\nFebruary to April discussion\n\nJuly discussion\n\nThe links to the Google Hangout (participate) and Youtube (watch live) will be posted to the wiki before the event. If you don’t get into the Hangout (limited number of participants), don’t worry: You’ll be able to send questions and suggestions to his Twitter account @gusreiber.","title":"Office hour on proposed UI/UX changes","tags":["development","screencast"],"authors":[{"avatar":null,"blog":null,"github":"daniel-beck","html":"<div class=\"paragraph\">\n<p>Daniel is a Jenkins core maintainer and, as security officer, leads the <a href=\"/security/#team\">Jenkins security team</a>.\nHe sometimes contributes to developer documentation and project infrastructure.</p>\n</div>","id":"daniel-beck","irc":null,"linkedin":null,"name":"Daniel Beck","slug":"/blog/authors/daniel-beck","twitter":null}]}},{"node":{"date":"2015-08-10T00:00:00.000Z","id":"69de0fa0-b2c1-5475-b88a-d08748f38442","slug":"/blog/2015/08/10/upcoming-office-hour-on-workflow/","strippedHtml":"Jesse Glick will host the next office hour this Wednesday, 11 AM PDT on Workflow.\n\nWorkflow has been Jesse’s project for the last year or so. If you don’t know what Workflow is, check out these talks about it from past JUCes:\n\nJune 2014\n\nOctober 2014\n\nJune 2015\n\nThis will be a developer-focused session on integrating with Workflow. He’ll discuss things like how to make sure your plugin can be used as part of workflows, and best practices for extending the workflow DSL. There’s already been a session on Workflow in January, but Jesse hasn’t been idle, and there’s new stuff to share.\n\nParticipate in the Hangout on Air or watch live on YouTube.","title":"Upcoming office hour on Workflow","tags":["development","tutorial","pipeline","workflow"],"authors":[{"avatar":null,"blog":null,"github":"daniel-beck","html":"<div class=\"paragraph\">\n<p>Daniel is a Jenkins core maintainer and, as security officer, leads the <a href=\"/security/#team\">Jenkins security team</a>.\nHe sometimes contributes to developer documentation and project infrastructure.</p>\n</div>","id":"daniel-beck","irc":null,"linkedin":null,"name":"Daniel Beck","slug":"/blog/authors/daniel-beck","twitter":null}]}}]}},"pageContext":{"tag":"development","limit":8,"skip":0,"numPages":9,"currentPage":1}},
    "staticQueryHashes": ["3649515864"]}