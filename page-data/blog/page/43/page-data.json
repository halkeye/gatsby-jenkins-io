{
    "componentChunkName": "component---src-templates-blog-list-template-js",
    "path": "/blog/page/43",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2017-02-03T00:00:00.000Z","id":"eba78b86-bbf2-5022-a3c9-27efc09a20e1","slug":"/blog/2017/02/03/declarative-pipeline-ga/","strippedHtml":"This is a guest post by\nPatrick Wolf,\nDirector of Product Management at\nCloudBees\nand contributor to\nthe Jenkins project.\n\nI am very excited to announce the addition of\nDeclarative Pipeline syntax\n1.0 to\nJenkins Pipeline.\nWe think this new syntax will enable everyone involved in DevOps, regardless of expertise,\nto participate in the continuous delivery process. Whether creating, editing or reviewing\na pipeline, having a straightforward structure helps to understand and predict the\nflow of the pipeline and provides a common foundation across all pipelines.\n\nPipeline as Code\n\nPipeline as Code was one of the pillars of the Jenkins 2.0 release and an\nessential part of implementing continuous delivery (CD). Defining all of the\nstages of an application’s CD pipeline within a Jenkinsfile and checking it\ninto the repository with the application code provides all of the benefits\ninherent in source control management (SCM):\n\nRetain history of all changes to Pipeline\n\nRollback to a previous Pipeline version\n\nView diffs and merge changes to the Pipeline\n\nTest new Pipeline steps in branches\n\nRun the same Pipeline on a different Jenkins server\n\nGetting Started with Declarative Pipeline\n\nWe recommend people begin using it for all their Pipeline definitions in Jenkins.\nThe plugin has been available for use and testing starting with the 0.1 release that was debuted at\nJenkins World\nin September. Since then, it has already been installed in over 5,000 Jenkins\nenvironments.\n\nIf you haven’t tried Pipeline or have considered Pipeline in the past, I\nbelieve this new syntax is much more approachable with an easier adoption curve\nto quickly realize all of the benefits of Pipeline as Code. In addition, the\npre-defined structure of Declarative makes it possible to create and edit\nPipelines with a graphical user interface (GUI). The Blue Ocean team is\nactively working on a\nVisual Pipeline Editor\nwhich will be included in an upcoming release.\n\nIf you have already begun using Pipelines in Jenkins, I believe that this new\nalternative syntax can help expand that usage.\n\nThe original syntax for defining Pipelines in Jenkins is a Groovy DSL that\nallows most of the features of full\nimperative programming.\n\nThis syntax is still fully supported and is now\nreferred to as \"Scripted Pipeline Syntax\" to distinguish it from \"Declarative\nPipeline Syntax.\" Both use the same underlying execution engine in Jenkins and\nboth will generate the same results in\nPipeline Stage View\nor Blue Ocean visualizations. All existing\nPipeline steps,\nGlobal Variables, and\nShared Libraries\ncan be used in either. You can now create more cookie-cutter Pipelines and\nextend the power of Pipeline to all users regardless of Groovy expertise.\n\nDeclarative Pipeline Features\n\nSyntax Checking\n\nImmediate runtime syntax checking with explicit error messages.\n\nAPI endpoint for linting a Jenkinsfile.\n\nCLI command to lint a Jenkinsfile.\n\nDocker Pipeline integration\n\nRun all stages in a single container.\n\nRun each stage in a different container.\n\nEasy configuration\n\nQuickly define parameters for your Pipeline.\n\nQuickly define environment variables and credentials for your Pipeline.\n\nQuickly define options (such as timeout, retry, build discarding) for your Pipeline.\n\nRound trip editing with the Visual Pipeline Editor (watch for preview release shortly).\n\nConditional actions\n\nSend notifications or take actions depending upon success or failure.\n\nSkip stages based on branches, environment, or other Boolean expression.\nrelease shortly)\n\nWhere Can I Learn More?\n\nBe on the lookout for future blog posts detailing specific examples of\nscenarios or features in Declarative Pipeline. Andrew Bayer, one of the primary\ndevelopers behind Declarative Pipeline, will be presenting at\nFOSDEM\nin Brussels, Belgium this weekend. We have also scheduled an online\nJenkins Meetup (JAM)\nlater this month to demo the features of Declarative Pipeline and give a sneak\npeek at the upcoming Blue Ocean Pipeline Editor.\n\nIn the meantime, all the\nPipeline documentation\nhas been updated to incorporate a\nGuided Tour,\nand a\nSyntax Reference\nwith numerous examples to help you get on your way to using Pipeline.  Simply\nupgrade to the latest version, 2.5 or later of the Pipeline in Jenkins to\nenable all of these great features.","title":"Declarative Pipeline Syntax 1.0 is now available","tags":["pipeline","blueocean"],"authors":[{"avatar":null,"blog":null,"github":"HRMPW","html":"","id":"hrmpw","irc":null,"linkedin":null,"name":"Patrick Wolf","slug":"/blog/authors/hrmpw","twitter":"hrmpw"}]}},{"node":{"date":"2017-02-01T00:00:00.000Z","id":"aa509c7b-3f24-5cce-8519-dda84cd1233e","slug":"/blog/2017/02/01/pipeline-scalability-best-practice/","strippedHtml":"This is a guest post by Sam Van Oort,\nSoftware Engineer at CloudBees and contributor to\nthe Jenkins project.\n\nToday I’m going to show you best practices to write scalable and robust Jenkins Pipelines. This is drawn from a\ncombination of work with the internals of Pipeline and observations with large-scale users.\n\nPipeline code works beautifully for its intended role of automating\nbuild/test/deploy/administer tasks.  As it is pressed into more complex roles\nand unanticipated uses, some users hit issues.  In these cases, applying the\nbest practices can make the difference between:\n\nA single controller running\nhundreds\nof concurrent builds on low end hardware (4 CPU cores and 4 GB of\nheap)\n\nRunning a couple dozen builds and bringing a controller to its knees or\ncrashing it…​even with 16+ CPU cores and 20+ GB of heap!\n\nThis has been seen in the wild.\n\nFundamentals\n\nTo understand Pipeline behavior you must understand a few points about\nhow it executes.\n\nExcept for the steps themselves, all of the Pipeline logic, the Groovy conditionals, loops, etc execute on the controller. Whether simple or complex! Even inside a node block!\n\nSteps may use executors to do work where appropriate, but each\nstep has a small on-controller overhead too.\n\nPipeline code is written as Groovy but the execution model is\nradically transformed at compile-time to Continuation Passing Style\n(CPS).\n\nThis transformation provides valuable safety and durability\nguarantees for Pipelines, but it comes with trade-offs:\n\nSteps can invoke Java and execute fast and efficiently, but Groovy\nis much slower to run than normal.\n\nGroovy logic requires far more memory, because an object-based\nsyntax/block tree is kept in memory.\n\nPipelines persist the program and its state frequently to be able to\nsurvive failure of the controller.\n\nFrom these we arrive at a set of best practices to make pipelines more\neffective.\n\nBest Practices For Pipeline Code\n\nThink of Pipeline code as glue: just enough Groovy code to connect\ntogether the Pipeline steps and integrate tools, and no more.\n\nThis makes code easier to maintain, more robust against bugs, and\nreduces load on controllers.\n\nKeep it simple: limit the amount of complex logic embedded in the\nPipeline itself (similarly to a shell script) and avoid treating it as a\ngeneral-purpose programming language.\n\nPipeline restricts all variables to Serializable types, so keeping\nPipeline logic simple helps avoid a NotSerializableException - see\nappendix at the bottom.\n\nUse @NonCPS -annotated functions for slightly more complex work.\nThis means more involved processing, logic, and transformations. This\nlets you leverage additional Groovy & functional features for more\npowerful, concise, and performant code.\n\nThis still runs on controllers so be mindful of complexity, but is much\nfaster than native Pipeline code because it doesn’t provide durability\nand uses a faster execution model. Still, be mindful of the CPU cost and\noffload to executors for complex work (see below).\n\n@NonCPS functions can use a much broader subset of the Groovy\nlanguage, such as iterators and functional features, which makes them\nmore terse and fast to write.\n\n@NonCPS functions should not use Pipeline steps internally, however\nyou can store the result of a Pipeline step to a variable and use it\nthat as the input to a @NonCPS function.\n\nGotcha: It’s not guaranteed that use of a step will generate an\nerror (there is an open RFE to implement that), but you should not rely\non that behavior. You may see improper handling of exceptions, in\nparticular.\n\nWhile normal Pipeline is restricted to serializable local variables\n(see appendix at bottom), @NonCPS functions can use more complex,\nnonserializable types internally (for example regex matchers, etc). Parameters\nand return types should still be Serializable, however.\n\nGotcha: improper usages are not guaranteed to raise an error with\nnormal Pipeline (optimizations may mask the issue), but it is unsafe to\nrely on this behavior.\n\nPrefer external scripts/tools for complex or CPU-expensive\nprocessing rather than Groovy language features. This offloads work\nfrom the controller to external executors, allowing for easy scale-out of\nhardware resources. It is also generally easier to test because these\ncomponents can be tested in isolation without the full on-controller\nexecution environment.\n\nMany software vendors will provide easy command-line clients for\ntheir tools in various programming languages. These are often robust,\nperformant, and easy to use. Plugins offer another option (see below).\n\nShell or batch steps are often the easiest way to integrate these\ntools, which can be written in any language. For example: sh “java -jar\nclient.jar $endPointUrl $inputData” for a Java client, or sh “python\njiraClient.py $issueId $someParam” for a Python client.\n\nGotcha: especially avoid Pipeline XML or JSON parsing using Groovy’s XmlSlurper and JsonSlurper!  Strongly prefer command-line tools or scripts.\n\nThe Groovy implementations are complex and as a result more brittle in Pipeline use.\n\nXmlSlurper and JsonSlurper can carry a high memory and CPU cost in pipelines\n\nxmllint and xmlstartlet are command-line tools offering XML extraction via xpath\n\njq offers the same functionality for JSON\n\nThese extraction tools may be coupled to curl or wget for fetching information from an HTTP API\n\nExamples of other places to use command-line tools:\n\nTemplating large files\n\nNontrivial integration with external APIs (for bigger vendors,\nconsider a Jenkins plugin if a quality offering exists)\n\nSimulations/complex calculations\n\nBusiness logic\n\nConsider existing plugins for external integrations. Jenkins has a\nwealth of plugins, especially for source control, artifact management,\ndeployment systems, and systems automation. These can greatly reduce the\namount of Pipeline code to maintain. Well-written plugins may be\nfaster and more robust than Pipeline equivalents.\n\nConsider both plugins and command-line clients (above) — one may be\neasier than the other.\n\nPlugins may be of widely varying quality. Look at the number of installations and how frequently and recently updates appear in the changelog. Poorly-maintained plugins\nwith limited installations may actually be worse than writing a little\ncustom Pipeline code.\n\nAs a last resort, if there is a good-quality plugin that is not\nPipeline-enabled, it is fairly easy to write a Pipeline wrapper to\nintegrate it or write a custom step that will invoke it.\n\nAssume things will go wrong: don’t rely on workspaces being clean\nof the remnants from previous executions, clean explicitly where needed.\nMake use of timeouts and retry steps (that’s what they’re there for).\n\nWithin a git repository, git clean -fdx is a good way to\naccomplish this and reduces the amount of SCM cloning\n\nDO use parameterized Pipelines and variables to make your Pipeline\nscripts more reusable. Passing in parameters is especially helpful for\nhandling different environments and should be preferred to applying\nconditional lookup logic; however, try to limit parameterized pipelines invoking each other.\n\nTry to limit business logic embedded in Pipelines. To some extent\nthis is inevitable, but try to focus on tasks to complete instead,\nbecause this yields more maintainable, reusable, and often more\nperformant Pipeline code.\n\nOne code smell that points to a problem is many hard-coded\nconstants. Consider taking advantage of the options above to refactor\ncode for better composability.\n\nFor complex cases, consider using Jenkins integration options\n(plugins, Jenkins API calls, invoking input steps externally) to offload\nimplementation of more complex business rules to an external system if\nthey fit more naturally there.\n\nPlease, think of these as guidelines, not strict rules – Jenkins\nPipeline provides a great deal of power and flexibility, and it’s there\nto be used.\n\nBreaking enough of these rules at scale can cause controllers to fail by\nplacing an unsustainable load on them.\n\nFor additional guidance, I also recommend\nthis\nJenkins World talk\non how to engineer Pipelines for speed and performance:\n\nAppendix: Serializable vs. Non-Serializable Types:\n\nTo assist with Pipeline development, here are common serializable and\nnon-serializable types, to assist with deciding if your logic can be CPS\nor should be in a @NonCPS function to avoid issues.\n\nCommon Serializable Types (safe everywhere):\n\nAll primitive types and their object wrappers: byte, boolean, int,\ndouble, short, char\n\nStrings\n\nenums\n\nArrays of serializable types\n\nArrayLists and normal Groovy Lists\n\nSets: HashSet\n\nMaps: normal Groovy Map, HashMap, TreeMap\n\nExceptions\n\nURLs\n\nDates\n\nRegex Patterns (compiled patterns)\n\nCommon non-Serializable Types (only safe in @NonCPS functions):\n\nIterators: this is a common problem. You need to use C-style loop, i.e.\nfor(int i=0; i\n\nRegex Matchers (you can use the\nbuilt-in functions in String, etc, just not the Matcher itself)\n\nImportant: JsonObject, JsonSlurper, etc in Groovy 2+ (used in some 2.x+\nversions of Jenkins).\n\nThis is due to an internal implementation change — earlier versions may serialize.","title":"Best Practices for Scalable Pipeline Code","tags":["pipeline","performance","scalability"],"authors":[{"avatar":null,"blog":null,"github":"svanoort","html":"","id":"svanoort","irc":null,"linkedin":null,"name":"Sam Van Oort","slug":"/blog/authors/svanoort","twitter":null}]}},{"node":{"date":"2017-02-01T00:00:00.000Z","id":"b68e4a15-e617-5570-bc2d-5ff59160e388","slug":"/blog/2017/02/01/security-updates/","strippedHtml":"We just released security updates to Jenkins, versions 2.44 and 2.32.2, that fix a high severity and several medium and low severity issues.\n\nFor an overview of what was fixed, see the security advisory.\nFor an overview on the possible impact of these changes on upgrading Jenkins LTS, see our LTS upgrade guide.\nI strongly recommend you read these documents, as there are a few possible side effects of these fixes.\n\nSubscribe to the jenkinsci-advisories mailing list to receive important notifications related to Jenkins security.","title":"Security updates for Jenkins core","tags":["core","security"],"authors":[{"avatar":null,"blog":null,"github":"daniel-beck","html":"<div class=\"paragraph\">\n<p>Daniel is a Jenkins core maintainer and, as security officer, leads the <a href=\"/security/#team\">Jenkins security team</a>.\nHe sometimes contributes to developer documentation and project infrastructure.</p>\n</div>","id":"daniel-beck","irc":null,"linkedin":null,"name":"Daniel Beck","slug":"/blog/authors/daniel-beck","twitter":null}]}},{"node":{"date":"2017-01-27T00:00:00.000Z","id":"47fad7c8-e5e2-5ee5-bca8-79356edc804b","slug":"/blog/2017/01/27/blueocean-dev-log-jan4/","strippedHtml":"As we get closer to\nBlue Ocean\n1.0, which is planned for the end of March, I have\nstarted\nhighlighting\nsome of the good stuff that has been going on. This week was 10 steps forward, and about 1.5 backwards…​\n\nThere were two releases this week, b19 and b20. Unfortunately, b20 had to\nbe released shortly after b19 hit the Update Center as an incompatible API\nchange in a 3rd party plugin was discovered.\n\nRegardless, the latest b20 has a lot of important improvements, and some\nvery nice new features.\n\nA first cut of the \"Create Pipeline\" UX, seen above, allowing you to create Git\nbased Multibranch Pipelines like you have never seen before.\n\nHandling network disconnections from the browser to server (eg server\nrestart, network etc) gracefully with a nice UI.\n\nMore precise time information for steps and running Pipelines.\n\nMore information when a Pipeline is blocked on infrastructure, such as when\nthe Pipeline is waiting for an agent to become available.\n\nFixed a really embarrassing typo (a prize if you spot it).\n\nTest reports now include stdout and stderr\n\nBetter support for parallel visualisation, such as when a parallel step exists outside of a stage.\n\nThe Visual Editor also had another release, with the \"sheets\" visual component\nand better validation.\n\nCreation\n\nCurrently this is hidden behind a\nfeature toggle,\nto access append?blueCreate to the URL in you browser, and then press the\n\"New Pipeline\" button. Currently it lets you quickly create a Pipeline from\nGit, add credentials, etc, in a very nice UX. More SCM types are being added to\nsupport this.\n\nReconnect/disconnect\n\nAs Blue Ocean is a very \"live\" style of UX, if your network becomes\nunavailable, or the server is restarted, it is good to know in case you\nwere staring at the screen waiting for something to happen (don’t you have\nanything better to do??). When this happens, now you get a polite message,\nand then when the connection is restored, even if you are waiting for a\nPipeline run to finish, it will then notice this, and refresh things for\nyou:\n\nNote the opacity changes to make it clear even if you don’t see the little\nmessage. Very nice addition for those of us who work on a train far to often.\n\nUp next\n\nWhat is up next:\n\nSCM Api changes should land, making things much better for users of\nGitHub, Bitbucket, and many more.\n\nCreating Pipelines from GitHub (including automatic discovery).\n\nLots of fixes and enhancements in the Pipeline from all over the place\n\nMore ATH [ 1 ] coverage against regressions\n\nMore Visual Editor releases as Declarative Pipeline reaches version 1.0\n\nImprovements to i18n\n\nThere was also a couple of \"alternative beta\" releases in the \"Experimental\nUpdate Center\" to help test the new SCM API improvements for better use of\nGitHub APIs (based on\nthis branch)\nI do not recommend trying this branch unless you know what you are doing,\nas this will migrate some data, but help testing it would be appreciated!\n\nEnjoy!\n\nIf you’re interested in helping to make Blue Ocean a great user experience for\nJenkins, please join the Blue Ocean development team on\nGitter!\n\n1. Acceptance Test Harness","title":"Blue Ocean Dev Log: January Week #4","tags":["blueocean"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/19e71/michaelneale.jpg","srcSet":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/77b35/michaelneale.jpg 32w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/d4a57/michaelneale.jpg 64w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/19e71/michaelneale.jpg 128w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/ef6ff/michaelneale.webp 32w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/8257c/michaelneale.webp 64w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/6766a/michaelneale.webp 128w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"michaelneale","html":"<div class=\"paragraph\">\n<p>Michael is a CD enthusiast with a interest in User Experience.\nHe is a co-founder of CloudBees and a long time OSS developer, and can often be found\nlurking around the jenkins-dev mailing list or #jenkins on irc (same nick as twitter name).\nBefore CloudBees he worked at Red Hat.</p>\n</div>","id":"michaelneale","irc":null,"linkedin":null,"name":"Michael Neale","slug":"/blog/authors/michaelneale","twitter":"michaelneale"}]}},{"node":{"date":"2017-01-20T00:00:00.000Z","id":"4d9875e3-c0ca-5b68-8c88-921f07767cc4","slug":"/blog/2017/01/20/blueocean-dev-log-jan2/","strippedHtml":"As we get closer to\nBlue Ocean\n1.0, which is planned for the end of March, I have started\nhighlighting\nsome of the good stuff that has been going on, and this week was a very busy week.\n\nA new Blue Ocean beta ( b18) was released with:\n\nParametrized pipelines are now supported!\n\ni18n improvements\n\nBetter support for matrix and the evil (yet somehow still used) Maven project type (don’t use it!)\n\nSSE fixes for IE and Edge browsers\n\nAn alpha release of the Visual Editor for Jenkinsfiles on top of\nDeclarative Pipeline\nhas snuck into the \"experimental\" update center. Andrew will be talking\nabout Declarative Pipelines at\nFOSDEM next week.\n\nParametrized Pipelines\n\nYou would know this if you followed\nThorsten’s twitter account.\n\nThat twitter account is mostly pics of Thorsten in running gear, but\noccasionally he announces new features as they land.\n\nWhen you run a pipeline that requires parameters, it will popup a dialog\nlike this no matter where you run it from. Most input types are supported\n(similar to input), with a planned extension point for custom input types.\n\nEditor\n\nA very-very early version of the\nBlue Ocean Pipeline Editor plugin\nthat will set your hair on fire of the editor is in the experimental update\ncenter.\n\nDeclarative pipelines are still not at version 1.0 status, but will be\nshortly. This editor allows you to roundtrip Jenkinsfiles written in this\nway, so they can be edited as text, or visually. The steps available are\ndiscovered form the installed plugins. One to watch.\n\nSo, what’s next?\n\nCreation of Git Pipelines, and likely GitHub too.\n\nShow parallel branches that aren’t in a stage visually\n\nShow stderr/out in test reports\n\nShow more information when Jenkins is \"busy\", such as when agents are coming online, in the Pipeline view\n\nEnjoy!\n\nIf you’re interested in helping to make Blue Ocean a great user experience for\nJenkins, please join the Blue Ocean development team on\nGitter!","title":"Blue Ocean Dev Log: January Week #3","tags":["blueocean"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/19e71/michaelneale.jpg","srcSet":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/77b35/michaelneale.jpg 32w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/d4a57/michaelneale.jpg 64w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/19e71/michaelneale.jpg 128w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/ef6ff/michaelneale.webp 32w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/8257c/michaelneale.webp 64w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/6766a/michaelneale.webp 128w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"michaelneale","html":"<div class=\"paragraph\">\n<p>Michael is a CD enthusiast with a interest in User Experience.\nHe is a co-founder of CloudBees and a long time OSS developer, and can often be found\nlurking around the jenkins-dev mailing list or #jenkins on irc (same nick as twitter name).\nBefore CloudBees he worked at Red Hat.</p>\n</div>","id":"michaelneale","irc":null,"linkedin":null,"name":"Michael Neale","slug":"/blog/authors/michaelneale","twitter":"michaelneale"}]}},{"node":{"date":"2017-01-19T00:00:00.000Z","id":"437a3a39-d6ca-5875-b27d-0189cefc4150","slug":"/blog/2017/01/19/converting-conditional-to-pipeline/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nIntroduction\n\nWith all the new developments in\nJenkins Pipeline (and\nDeclarative Pipeline on the horizon),\nit’s easy to forget what we did to create \"pipelines\" before\nPipeline.\nThere are number of plugins, some that have been around since the very beginning,\nthat enable users to create \"pipelines\" in Jenkins.\nFor example, basic job chaining worked well in many cases, and the\nParameterized Trigger plugin\nmade chaining more flexible.\nHowever, creating chained jobs with conditional behavior was\nstill one of the harder things to do in Jenkins.\n\nThe\nConditional BuildStep plugin\nis a powerful tool that has allowed Jenkins users to write Jenkins jobs with complex conditional logic.\nIn this post, we’ll take a look at how we might converting Freestyle jobs that\ninclude conditional build steps to Jenkins Pipeline.\nUnlike Freestyle jobs, implementing conditional operations in Jenkins Pipeline is trivial,\nbut matching the behavior of complex conditional build steps will require a bit more care.\n\nGraphical Programming\n\nThe Conditional BuildStep plugin lets users add conditional logic to Freestyle\njobs from within the Jenkins web UI.  It does this by:\n\nAdding two types of Conditional BuildStep (\"Single\" and \"Multiple\") -\nthese build steps contain one or more other build steps to be run when the configured\ncondition is met\n\nAdding a set of Condition operations -\nthese control whether the Conditional BuildStep execute the contained step(s)\n\nLeveraging the Token Macro facility -\nthese provide values to the Conditions for evaluation\n\nIn the example below, this project will run the shell script step when the value of the\nREQUESTED_ACTION token equals \"greeting\".\n\nHere’s the output when I run this project with REQUESTED_ACTION set to \"greeting\":\n\nRun condition [Strings match] enabling prebuild for step [Execute shell]\nStrings match run condition: string 1=[greeting], string 2=[greeting]\nRun condition [Strings match] enabling perform for step [Execute shell]\n[freestyle-conditional] $ /bin/sh -xe /var/folders/hp/f7yc_mwj2tq1hmbv_5n10v2c0000gn/T/hudson5963233933358491209.sh\n+ echo 'Hello, bitwiseman!'\nHello, bitwiseman!\nFinished: SUCCESS\n\nAnd when I pass the value \"silence\":\n\nRun condition [Strings match] enabling prebuild for step [Execute shell]\nStrings match run condition: string 1=[silence], string 2=[greeting]\nRun condition [Strings match] preventing perform for step [Execute shell]\nFinished: SUCCESS\n\nThis is a simple example but the conditional step can contain any regular build step.\nWhen combined with other plugins, it can control whether to send notifications,\ngather data from other sources, wait for user feedback, or call other projects.\n\nThe Conditional BuildStep plugin does a great job of leveraging strengths of\nthe Jenkins web UI, Freestyle jobs, and UI-based programming,\nbut it is also hampered by their limitations.\nThe Jenkins web UI can be clunky and confusing at times.\nLike the steps in any Freestyle job, these conditional steps are only\nstored and viewable in Jenkins.\nThey are not versioned with other product or build code and can’t be code reviewed.\nLike any number of UI-based programming tools, it has to make trade-offs between clarity\nand flexibility: more options or clearer presentation.\nThere’s only so much space on the screen.\n\nConverting to Pipeline\n\nJenkins Pipeline, on the other hand, enables users to implement their pipeline as code.\nPipeline code can be written directly in the Jenkins Web UI or in any text editor.\nIt is a full-featured programming language,\nwhich gives users access to much broader set of conditional statements\nwithout the restrictions of UI-based programming.\n\nSo, taking the example above, the Pipeline equivalent is:\n\n// Declarative //\npipeline {\n    agent any\n    parameters {\n        choice(\n            choices: ['greeting' , 'silence'],\n            description: '',\n            name: 'REQUESTED_ACTION')\n    }\n\n    stages {\n        stage ('Speak') {\n            when {\n                // Only say hello if a \"greeting\" is requested\n                expression { params.REQUESTED_ACTION == 'greeting' }\n            }\n            steps {\n                echo \"Hello, bitwiseman!\"\n            }\n        }\n    }\n}\n// Script //\nproperties ([\n    parameters ([\n        choice (\n            choices: ['greeting', 'silence'],\n            description: '',\n            name : 'REQUESTED_ACTION')\n    ])\n])\n\nnode {\n    stage ('Speak') {\n        // Only say hello if a \"greeting\" is requested\n        if (params.REQUESTED_ACTION == 'greeting') {\n            echo \"Hello, bitwiseman!\"\n        }\n    }\n}\n\nWhen I run this project with REQUESTED_ACTION set to \"greeting\", here’s the output:\n\n[Pipeline] node\nRunning on osx_mbp in /Users/bitwiseman/jenkins/agents/osx_mbp/workspace/pipeline-conditional\n[Pipeline] {\n[Pipeline] stage\n[Pipeline] { (Speak)\n[Pipeline] echo\nHello, bitwiseman!\n[Pipeline] }\n[Pipeline] // stage\n[Pipeline] }\n[Pipeline] // node\n[Pipeline] End of Pipeline\nFinished: SUCCESS\n\nWhen I pass the value \"silence\", the only change is \"Hello, bitwiseman!\" is not printed.\n\nSome might argue that the Pipeline code is a bit harder to understand on first reading.\nOthers would say the UI is just as confusing if not more so.\nEither way, the Pipeline representation is considerably more compact than the Jenkins UI presentation.\nPipeline also lets us add helpful comments, which we can’t do in the Freestyle UI.\nAnd we can easily put this Pipeline in a Jenkinsfile to be code-reviewed, checked-in, and versioned\nalong with the rest of our code.\n\nConditions\n\nThe previous example showed the \"Strings match\" condition and its Pipeline equivalent.\nLet’s look at couple more interesting conditions and their Jenkins Pipeline equivalents.\n\nBoolean condition\n\nYou might think that a boolean condition would be the simplest condition, but it isn’t.\nSince it works with string values from tokens, the Conditional BuildStep plugin offers\na number of ways to indicate true or false.\nTruth is a case insensitive match of one of the following:\n1 (the number one), Y, YES, T, TRUE, ON or RUN.\n\nPipeline can duplicate these, but depending on the scenario we might consider\nwhether a simpler expression would suffice.\n\nPipeline\n\n// Declarative //\nwhen {\n    // case insensitive regular expression for truthy values\n    expression { return token ==~ /(?i)(Y|YES|T|TRUE|ON|RUN)/ }\n}\nsteps {\n    /* step */\n}\n\n// Script //\n// case insensitive regular expression for truthy values\nif (token ==~ /(?i)(Y|YES|T|TRUE|ON|RUN)/) {\n    /* step */\n}\n\nLogical \"OR\" of conditions\n\nThis condition wraps other conditions.\nIt takes their results as inputs and performs a logical \"or\" of the results.\nThe AND and NOT conditions do the same, performing their respective operations.\n\nPipeline\n\n// Declarative //\nwhen {\n    // A or B\n    expression { return A || B }\n}\nsteps {\n    /* step */\n}\n\n// Script //\n// A or B\nif (A || B) {\n    /* step */\n}\n\nTokens\n\nTokens can be considerably more work than conditions.\nThere are more of them and they cover a much broader range of behaviors.\nThe previous example showed one of the simpler cases, accessing a build parameter,\nwhere the token has a direct equivalent in Pipeline.\nHowever, many tokens don’t have direct equivalents,\nsome take a parameters (adding to their complexity),\nand some provide information that is simply not exposed in Pipeline yet.\nSo, determining how to migrate tokens needs to be done on case-by-case basis.\n\nLet’s look at a few examples.\n\n\"FILE\" token\n\nExpands to the contents of a file. The file path is relative to the build workspace root.\n\n${FILE,path=\"PATH\"}\n\nThis token maps directly to the readFile step.\nThe only difference is the file path for readFile is relative to the\ncurrent working directory on the agent, but that is the workspace root by default.\nNo problem.\n\nPipeline\n\n// Declarative //\nwhen {\n    expression { return readFile('pom.xml').contains('mycomponent') }\n}\nsteps {\n    /* step */\n}\n\n// Script //\nif (readFile('pom.xml').contains('mycomponent')) {\n    /* step */\n}\n\nGIT_BRANCH\n\nExpands to the name of the branch that was built.\n\nParameters (descriptions omitted): all, fullName.\n\nThis information may or may not be exposed in Pipeline.  If you’re using the\nPipeline Multibranch plugin\nenv.BRANCH_NAME will give similar basic information, but doesn’t offer the parameters.\nThere are also\nseveral\nissues\nfiled around GIT_* tokens in Pipeline.\nUntil they are addressed fully, we can follow the pattern shown in\npipeline-examples,\nexecuting a shell to get the information we need.\n\nPipeline\n\nGIT_BRANCH = sh(returnStdout: true, script: 'git rev-parse --abbrev-ref HEAD').trim()\n\nCHANGES_SINCE_LAST_SUCCESS\n\nDisplays the changes since the last successful build.\n\nParameters (descriptions omitted):\nreverse, format, changesFormat, showPaths, pathFormat,\nshowDependencies, dateFormat, regex, replace, default.\n\nNot only is the information provided by this token not exposed in Pipeline,\nthe token has ten optional parameters, including format strings and regular expression\nsearches. There are a number of ways we might get similar information in Pipeline.\nEach have their own particular limitations and ways they differ from the token output.\nThen we’ll need to consider how each of the parameters changes the output.\nIf nothing else, translating this token is clearly beyond the scope of this post.\n\nSlightly More Complex Example\n\nLet’s do one more example that shows some of these conditions and tokens.\nThis time we’ll perform different build steps depending on what branch we’re building.\nWe’ll take two build parameters: BRANCH_PATTERN and FORCE_FULL_BUILD.\nBased on BRANCH_PATTERN, we’ll checkout a repository.\nIf we’re building on the master branch or the user checked FORCE_FULL_BUILD,\nwe’ll call three other builds in parallel\n( full-build-linux, full-build-mac, and full-build-windows),\nwait for them to finish, and report the result.\nIf we’re not building on the master branch and the user did not check FORCE_FULL_BUILD,\nwe’ll print a message saying we skipped the full builds.\n\nFreestyle\n\nHere’s the configuration for Freestyle version.\n(It’s pretty long.  Feel free to skip down to the Pipeline version):\n\nThe Pipeline version of this job determines the GIT_BRANCH branch by\nrunning a shell script that returns the current local branch name.\nThis means that the Pipeline version must checkout to a local branch (not a detached head).\n\nFreestyle version of this job does not require a local branch, GIT_BRANCH is set automatically.\nHowever, to maintain functional parity, the Freestyle version of this job includes\n\"Checkout to Specific Local Branch\" as well.\n\nPipeline\n\nHere’s the equivalent Pipeline:\n\nFreestyle version of this job is not stored in source control.\n\nIn general, the Pipeline version of this job would be stored in source control,\nwould checkout scm, and would run that same repository.\nHowever, to maintain functional parity, the Pipeline version shown does a checkout\nfrom source control but is not stored in that repository.\n\nPipeline\n\n// Script //\nproperties ([\n    parameters ([\n        string (\n            defaultValue: '*',\n            description: '',\n            name : 'BRANCH_PATTERN'),\n        booleanParam (\n            defaultValue: false,\n            description: '',\n            name : 'FORCE_FULL_BUILD')\n    ])\n])\n\nnode {\n    stage ('Prepare') {\n        checkout([$class: 'GitSCM',\n            branches: [[name: \"origin/${BRANCH_PATTERN}\"]],\n            doGenerateSubmoduleConfigurations: false,\n            extensions: [[$class: 'LocalBranch']],\n            submoduleCfg: [],\n            userRemoteConfigs: [[\n                credentialsId: 'bitwiseman_github',\n                url: 'https://github.com/bitwiseman/hermann']]])\n    }\n\n    stage ('Build') {\n        GIT_BRANCH = 'origin/' + sh(returnStdout: true, script: 'git rev-parse --abbrev-ref HEAD').trim()\n        if (GIT_BRANCH == 'origin/master' || params.FORCE_FULL_BUILD) {\n\n            // Freestyle build trigger calls a list of jobs\n            // Pipeline build() step only calls one job\n            // To run all three jobs in parallel, we use \"parallel\" step\n            // https://jenkins.io/doc/pipeline/examples/#jobs-in-parallel\n            parallel (\n                linux: {\n                    build job: 'full-build-linux', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                },\n                mac: {\n                    build job: 'full-build-mac', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                },\n                windows: {\n                    build job: 'full-build-windows', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                },\n                failFast: false)\n\n        } else {\n            echo 'Skipped full build.'\n        }\n    }\n}\n// Declarative //\npipeline {\n    agent any\n    parameters {\n        string (\n            defaultValue: '*',\n            description: '',\n            name : 'BRANCH_PATTERN')\n        booleanParam (\n            defaultValue: false,\n            description: '',\n            name : 'FORCE_FULL_BUILD')\n    }\n\n    stages {\n        stage ('Prepare') {\n            steps {\n                checkout([$class: 'GitSCM',\n                    branches: [[name: \"origin/${BRANCH_PATTERN}\"]],\n                    doGenerateSubmoduleConfigurations: false,\n                    extensions: [[$class: 'LocalBranch']],\n                    submoduleCfg: [],\n                    userRemoteConfigs: [[\n                        credentialsId: 'bitwiseman_github',\n                        url: 'https://github.com/bitwiseman/hermann']]])\n            }\n        }\n\n        stage ('Build') {\n            when {\n                expression {\n                    GIT_BRANCH = 'origin/' + sh(returnStdout: true, script: 'git rev-parse --abbrev-ref HEAD').trim()\n                    return GIT_BRANCH == 'origin/master' || params.FORCE_FULL_BUILD\n                }\n            }\n            steps {\n                // Freestyle build trigger calls a list of jobs\n                // Pipeline build() step only calls one job\n                // To run all three jobs in parallel, we use \"parallel\" step\n                // https://jenkins.io/doc/pipeline/examples/#jobs-in-parallel\n                parallel (\n                    linux: {\n                        build job: 'full-build-linux', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                    },\n                    mac: {\n                        build job: 'full-build-mac', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                    },\n                    windows: {\n                        build job: 'full-build-windows', parameters: [string(name: 'GIT_BRANCH_NAME', value: GIT_BRANCH)]\n                    },\n                    failFast: false)\n            }\n        }\n        stage ('Build Skipped') {\n            when {\n                expression {\n                    GIT_BRANCH = 'origin/' + sh(returnStdout: true, script: 'git rev-parse --abbrev-ref HEAD').trim()\n                    return !(GIT_BRANCH == 'origin/master' || params.FORCE_FULL_BUILD)\n                }\n            }\n            steps {\n                echo 'Skipped full build.'\n            }\n        }\n    }\n}\n\nConclusion\n\nAs I said before, the Conditional BuildStep plugin is great.\nIt provides a clear, easy to understand way to add conditional logic to any Freestyle job.\nBefore Pipeline, it was one of the few plugins to do this and it remains one of the most popular plugins.\nNow that we have Pipeline, we can implement conditional logic directly in code.\n\nThis is blog post discussed how to approach converting conditional build steps to Pipeline\nand showed a couple concrete examples.  Overall, I’m pleased with the results so far.\nI found scenarios which could not easily be migrated to Pipeline, but even those\nare only more difficult, rather than impossible.\n\nThe next thing to do is add a section to the\nJenkins Handbook documenting the Pipeline\nequivalent of all of the Conditions and the most commonly used Tokens.\nLook for it soon!\n\nLinks\n\nConditional BuildStep plugin","title":"Converting Conditional Build Steps to Jenkins Pipeline","tags":["pipeline","freestyle","plugins","conditional-build-step","tutorial"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2017-01-17T00:00:00.000Z","id":"51dc643a-7195-5ed6-b44d-6081914b07ff","slug":"/blog/2017/01/17/Jenkins-is-upgrading-to-Java-8/","strippedHtml":"In the next few months, Jenkins will require Java 8 as its runtime.\n\nBack in\nlast November,\nwe discussed interesting statistics showing that Jenkins was now running Java 8\non a majority of its running instances.\n\nTimeline\n\nHere is how we plan to roll that baseline upgrade in the next few months.\n\nNow: Announce the intention publicly.\n\nApril, 2017: Drop support for Java 7 in Jenkins weekly.\nWith the current rhythm, that means 2.52 will most likely be the first weekly to require Java 8.\n\nJune 2017: First LTS version requiring Java 8 is published.\nThis should be something around 2.60.1.\n\nIf you are still running Java 7, you will not be able to upgrade to the latest LTS version after some date probably around May 2017.\n\nWhy Upgrade to Java 8\n\nBalancing those numbers with many other criteria:\n\nJava 7 has been now end-of-lifed for 18+ months\n\nPeople are already moving away from Java 7, as show the numbers\n\n52.8% of instances were already running Java 8 back in last November, and now reaching 58% two months later.\n\nIf we only look at Jenkins 2.x, then we reach 72%.\n\nJava 8 runtime is known from the field to be more stable\n\nMany developers have been wanting to be allowed to leverage the improvements that Java 8 provides to the language and platform\n(lambdas, Date/Time API…​ just to name a few).\nBeing also a developer community, we want Jenkins to be appealing to contributors.\n\nIf you have questions or feedback about this announcement, please feel free to post it to the Jenkins developers mailing list.","title":"Jenkins Upgrades To Java 8","tags":["java8","upgrade"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8e8d8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/05e25bcf6699abfce74f0630971e7c78/f1e03/batmat.jpg","srcSet":"/gatsby-jenkins-io/static/05e25bcf6699abfce74f0630971e7c78/ede19/batmat.jpg 32w,\n/gatsby-jenkins-io/static/05e25bcf6699abfce74f0630971e7c78/bc20c/batmat.jpg 64w,\n/gatsby-jenkins-io/static/05e25bcf6699abfce74f0630971e7c78/f1e03/batmat.jpg 128w,\n/gatsby-jenkins-io/static/05e25bcf6699abfce74f0630971e7c78/b691b/batmat.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/05e25bcf6699abfce74f0630971e7c78/8ba60/batmat.webp 32w,\n/gatsby-jenkins-io/static/05e25bcf6699abfce74f0630971e7c78/a9ea7/batmat.webp 64w,\n/gatsby-jenkins-io/static/05e25bcf6699abfce74f0630971e7c78/51559/batmat.webp 128w,\n/gatsby-jenkins-io/static/05e25bcf6699abfce74f0630971e7c78/28f98/batmat.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":130}}},"blog":"http://batmat.net","github":"batmat","html":"<div class=\"paragraph\">\n<p>Baptiste has been using and contributing to Jenkins since it was called differently, and is a huge proponent of the Agile, Devops &amp; Continuous Delivery movements.\nHe loves to discuss not only the technical aspects, but also the even more essential cultural aspects of this all, working together to improve the value provided to customers in a great inclusive and blameless environment.</p>\n</div>","id":"batmat","irc":null,"linkedin":null,"name":"Baptiste Mathus","slug":"/blog/authors/batmat","twitter":"bmathus"}]}},{"node":{"date":"2017-01-17T00:00:00.000Z","id":"23308d18-32ba-5461-b5aa-74f746e5049f","slug":"/blog/2017/01/17/scm-api-2.0-release/","strippedHtml":"The regressions\ndiscovered after release have now been resolved and this post has been updated with the correct plugin version numbers.\n\nSee this post for more details.\n\nWe are announcing the\nSCM API\n2.0.x and\nBranch API\n2.0.x release lines.\n\nDownstream of this there are also some great improvements to a number of popular plugins including:\n\nGitHub Branch Source\n\nBitBucket branch source\n\nGit\n\nMercurial\n\nPipeline Multibranch\n\nGitHub Organization Folders\n\nThere are some gotcha’s that Jenkins administrators will need to be aware of.\n\nAlways take a backup of your JENKINS_HOME before upgrading any plugins.\n\nWe want to give you the whole story, but the take home message is this:\n\nWhen updating the\nSCM API\nand/or\nBranch API\nplugins to the 2.0.x release lines, if you have any of the\nGitHub Organization Folders,\nGitHub Branch Source\nand/or\nBitBucket branch source\nplugins installed then you must upgrade them all to 2.0.x at the same time or Bad Things™ will happen.\n\n— A Jenkins Administrator\n\nDo NOT upgrade some of these plugins but not others!\nDoing so may cause your jobs to fail to load.\n\nIf you don’t care about the hows and whys, you can just skip down to this section but if you are curious…​ here we go!\n\nThe back-story\n\nWay back in September 2013 we announced the\nLiterate plugin,\nas an experimental new way of modeling branch development in Jenkins.\n\nWhen you are performing an experiment, the recommendation is to do just enough work to let you perform the test.\nHowever, the culture in Jenkins is to always try and produce reusable components that others can use in ways you have not anticipated.\n\nSo when releasing the initial version of the\nLiterate plugin\nwe also separated the Literate specific bits from the SCM specific concepts and multi-branch concepts.\nThese were lower level concepts were organized into the following plugins:\n\nSCM API -\nwhich was intended to be a plugin to hold a next generation API for interacting with source control systems.\n\nBranch API -\nwhich was intended to be a plugin to hold the multi-branch functionality that was abstracted from the usage by the Literate plugin.\n\nIn addition, we released updates to three of the more common SCM plugins which included implementations of the SCM API:\n\nGit plugin\n\nSubversion plugin\n\nMercurial plugin\n\nWhile there was some interest in the Literate plugin, it did not gain much traction - there are only 39 Jenkins instances running the Literate plugin as of December 2016.\n\nIn terms of the reusable components, we had only made a minimal implementation with some limitations:\n\nVery basic event support - events can only trigger a re-scan of the entire repository.\nThis was acceptable at the time because the only three implementations use a local cache of the remote state so re-scanning is quick.\n\nNo implementation of the SCMFileSystem API.\nAs a result it is not possible for plugins like\nPipeline Multibranch\nto get the Jenkinsfile from the remote repository without needing to checkout the repository into a workspace.\n\nNo documentation on how plugin developers are supposed to implement the SCM API\n\nNo documentation on how plugin developers are supposed to consume the SCM API (if they wanted to do something like Branch API but not the same way as Branch API)\n\nNo documentation on how plugin developers are supposed to implement the Branch API to create their own multi-branch project types\n\nNo documentation on for users on how the Branch API based project types are expected to work.\n\nRoll forward to November 2015 and Jenkins Pipeline got a release of the\nPipeline Multibranch.\nIt seems that pairing Pipeline with Branch API style multi-branch is much more successful than Literate - there are close to 60,000 instances running the pipeline multi-branch plugin as of December 2016.\n\nThere also were two new SCM plugins implementing the SCM API:\n\nGitHub Branch Source Plugin\n\nBitBucket Branch Source Plugin\n\nUnlike the previous implementations of the SCM API, however, these plugins do not maintain a local cache of the repository state.\nRather they make queries via the GitHub / BitBucket REST APIs on demand.\n\nThe above design decision exposed one of the initial MVP compromises of the SCM API plugin: very basic event support.\nUnder the SCM API 1.x model, the only event that an SCMSource can signal is something changed, go look at everything again.\nWhen you are accessing an API that only allows 5,000 API calls per hour, performing a full scan of the entire repository just to pick up a change in one branch does not make optimum usage of that 5,000 calls/hour rate limit.\n\nSo we decided that perhaps the SCM API and Branch API plugins have left their Minimum Viability Experiment state and the corresponding limitations should be addressed.\n\nEnter SCM API 2.0.x and Branch API 2.0.x\n\nSo what has changed in the\nSCM API\n2.0.x and\nBranch API\n2.0.x release lines?\nThese plugin releases include:\n\ndocumentation on how plugin developers are supposed to\nimplement the SCM API\n\ndocumentation on how plugin developers are supposed to\nconsume the SCM API\n(if they wanted to do something like Branch API but not the same way as Branch API)\n\ndocumentation on how plugin developers are supposed to\nimplement the Branch API\nto create their own multi-branch project types\n\ngeneric documentation for users on\nhow Branch API based project types are intended to work\n\na full featured\nevent system\nthat allows implementers to provide fine grained notifications to consumers\n\nlots\nand\nlots\nof new automated tests\n\na mock implementation\nof the SCM API to help consumers of the SCM API test their usage.\n\nIn addition, we have upgraded the following plugins to include the new fine-grained event support:\n\nGit Plugin\n\nMercurial Plugin\n\nOk, that was the good news.\nHere is the bad news.\n\nWe found out that the GitHub Branch Source and BitBucket Branch Source plugins had made invalid assumptions about how to implement the SCM API.\nTo be clear, this was not the plugin developers fault: at the time there was no documentation on how to implement the SCM API.\n\nBut fixing the issues that we found means that you have to be careful about which specific combinations of plugin versions you have installed.\n\nSCM API Plugin\n\nTechnically, the 2.0.x line of this plugin is both API and on-disk compatible with plugins compiled against older version lines.\n\nHowever, the 1.x lines of both the GitHub Branch Source and BitBucket Branch Source plugins have hard-coded assumptions about internal implementation of the SCM API that are no longer valid in the 2.0.x line.\n\nIf you upgrade to SCM API 2.0.x and you have either the GitHub Branch Source or the BitBucket Branch Source plugins and you do not upgrade those instances to the 2.0.x line then your Jenkins instance will fail to start-up correctly.\n\nThe solution is just to upgrade the GitHub Branch Source or the BitBucket Branch Source plugin (as appropriate) to the 2.0.x line.\n\nIf you upgrade the SCM API plugin to the 2.0.x line and do not upgrade the Branch API plugin to the 2.0.x line then you will not get any of the benefits of the new version of the SCM API plugin.\n\nBranch API Plugin\n\nThe 2.0.x line of this plugin makes on-disk file format changes that mean you will be unable to roll back to the 1.x line after an upgrade without restoring the old data files from a back-up.\nTechnically, the API is compatible with plugins compiled against older version lines.\n\nThe 1.x lines of both the GitHub Branch Source and BitBucket Branch Source plugins have implemented hacks that make assumptions about internal implementation of the Branch API that are no longer valid in the 2.0.x line.\n\nThe Pipeline Multibranch plugin made a few minor invalid assumptions about how to implement a Multibranch project type.\nFor example, if you do not upgrade the Pipeline Multibranch plugin in tandem then you will be unable to manually delete an orphaned item before the orphaned item retention strategy runs, which should be significantly less frequently with the new event support.\n\nIf you upgrade to Branch API 2.0.x and you have either the GitHub Branch Source or the BitBucket Branch Source plugins and you do not upgrade those instances to the 2.0.x line then your Jenkins instance will fail to start-up correctly.\n\nThe solution is just to upgrade the GitHub Branch Source or the BitBucket Branch Source plugin (as appropriate) to the 2.0.x line.\n\nGit Plugin\n\nThe new releases of this plugin are both API and on-disk compatible with plugins compiled against the previous releases.\n\nThe 2.0.x lines of both the GitHub Branch Source and BitBucket Branch Source plugins require that you upgrade your Git Plugin to one of the versions that supports SCM API 2.0.x.\nIn general, the required upgrade will be performed automatically when you upgrade your GitHub Branch Source and BitBucket Branch Source plugins.\n\nMercurial Plugin\n\nThe new release of this plugin is both API and on-disk compatible with plugins compiled against the previous releases.\n\nThe 2.0.x line of the BitBucket Branch Source plugins require that you upgrade your Mercurial Plugin to the 2.0.x line.\nIn general, the required upgrade will be performed automatically when you upgrade your  BitBucket Branch Source plugins.\n\nBitBucket Branch Source Plugin\n\nThe 2.0.x line of this plugin makes on-disk file format changes that mean you will be unable to roll back to the 1.x line after an upgrade without restoring the old data files from a back-up.\n\nGitHub Branch Source Plugin\n\nThe 2.0.x line of this plugin makes on-disk file format changes that mean you will be unable to roll back to the 1.x line after an upgrade without restoring the old data files from a back-up.\n\nIf you upgrade to GitHub Branch Source 2.0.x and you have the GitHub Organization Folders plugin installed, you must upgrade that plugin to the tombstone release.\n\nGitHub Organization Folders Plugin\n\nThe functionality of this plugin has been migrated to the GitHub Branch Source plugin.\nYou will need to upgrade to the tombstone release in order to ensure all the data has been migrated to the classes in the GitHub Branch Source plugin.\n\nOnce you have upgraded to the tombstone version and all GitHub Organization Folders have had a full scan completed successfully, you can disable and uninstall the GitHub Organization Folders plugin.\nThere will be no more releases of this plugin after the tombstone.\nThe tombstone is only required for data migration.\n\nSummary for busy Jenkins Administrators\n\nUpgrading should make multi-branch projects much better.\nWhen you are ready to upgrade you must ensure that you upgrade all the required plugins.\nIf you miss some, just upgrade them and restart to fix the issue.\n\nFolders Plugin\n\n5.16 5.17 or newer\n\nSCM API Plugin\n\n2.0.1 2.0.2 or newer\n\nBranch API Plugin\n\n2.0.0 2.0.2 or newer\n\nGit Plugin\n\nEither 2.6.2 2.6.4 or newer in the 2.6.x line or 3.0.2 3.0.4 or newer\n\nMercurial Plugin\n\n2.0.0 or newer\n\nGitHub Branch Source Plugin\n\n2.0.0 2.0.1 or newer\n\nBitBucket Branch Source Plugin\n\n2.0.0 2.0.2 or newer\n\nGitHub Organization Folders Plugin\n\n1.6\n\nPipeline Multibranch Plugin\n\n2.10 2.12 or newer\n\nIf you are using the Blue Ocean plugin\n\nBlue Ocean Plugin\n\n1.0.0-b22 or newer\n\nOther plugins that may require updating:\n\nGitHub API Plugin\n\n1.84 or newer\n\nGitHub Plugin\n\n1.25.0 or newer\n\nAfter an upgrade you will see the data migration warning (see the screenshot in JENKINS-41608 for an example) this is normal and expected.\nThe unreadable data will be removed by the next scan / index or can be removed manually using the Discard Unreadable Data button.\nThe warning will disappear on the next restart after the unreadable data has been removed.\n\nSummary for busy Jenkins users\n\nSCM API 2.0.x adds fine-grained event support.\nThis should significantly improve the responsiveness of multi-branch projects.\nThis should significantly reduce your GitHub API rate limit usage.\n\nIf you are using the\nGitHub Branch Source\nor\nGitHub Organization Folders\nplugins then upgrading will significantly reduce the API calls made by Jenkins to GitHub.\n\nIf you are using any of the upgraded SCM plugins (e.g. Git, Mercurial, GitHub Branch Source, BitBucket Branch Source) then upgrading will significantly improve the responsiveness to push event notifications.\n\nSummary for busy SCM plugin developers\n\nYou should read the new\ndocumentation\non how plugin developers are supposed to implement the SCM API\n\nWhere to now dear Literate Plugin\n\nThe persistent reader may be wondering what happens now to the Literate plugin.\n\nFor me, the logical heir of the Literate Plugin is the\nPipeline Model Definition plugin.\nThis new plugin has the advantage of an easy to read pipeline syntax with the extra functionality that I suspect was preventing people from adopting Literate.\n\nThe good news is that the Pipeline Model Definition already has 5000 installations as of December 2016 and I expect up-take to keep on growing.","title":"SCM API turns 2.0 and what that means for you","tags":["development","plugins"],"authors":[{"avatar":null,"blog":null,"github":"stephenc","html":"","id":"stephenc","irc":null,"linkedin":null,"name":"Stephen Connolly","slug":"/blog/authors/stephenc","twitter":"connolly_s"}]}}]}},"pageContext":{"limit":8,"skip":336,"numPages":100,"currentPage":43}},
    "staticQueryHashes": ["3649515864"]}