{
    "componentChunkName": "component---src-templates-blog-list-template-js",
    "path": "/blog/page/37",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2017-07-13T00:00:00.000Z","id":"00ad337a-5c86-5032-be3e-f6f4934de80d","slug":"/blog/2017/07/13/speaker-blog-rosetta-stone/","strippedHtml":"This is a guest post by Kevin Burnett, DevOps Lead at\nRosetta Stone.\n\nHave you experienced that thing where you make a change in an app, and when you\ngo to check on the results of the build, you find an error that really doesn’t\nseem relevant to your change? And then you notice that your build is the first\nin over a year. And then you realize that you have accidentally become the\nsubject matter expert in this app.\n\nYou have no clue what change caused this failure or when that change occurred.\nDid one Jenkins agent become a\nsnowflake server,\naccruing cruft on the file system that is not cleaned up before each build?\nDid some unpinned external dependency upgrade in a backwards-incompatible fashion?\nDid the credentials the build plan was using to connect to source control get rotated?\nDid a dependent system go offline?\nOr - and I realize that this is unthinkable - did you legitimately break a test?\n\nNot only is this type of archaeological expedition often a bad time for the\nperson who happened to commit to this app (\"No good deed goes unpunished\"), but\nit’s also unnecessary. There’s a simple way to reduce the cognitive load it\ntakes to connect cause and effect: build more frequently.\n\nOne way we achieve this is by writing scripts to maintain our apps. When we\nbuild, the goal is that an equivalent artifact should be produced unless there\nwas a change to the app in source control. As such, we pin all of our\ndependencies to specific versions. But we also don’t want to languish on old\nversions of dependencies, whether internal or external. So we also have an\nauto-maintain script that bumps all of these versions and commits the result.\n\nI’ll give an example. We use docker to build and deploy our apps, and each app\ndepends on a base image that we host in a docker registry. So a Dockerfile in\none of our apps would have a line like this:\n\nFROM our.registry.example.com/rosettastone/sweet-repo:jenkins-awesome-project-sweet-repo-5\n\nWe build our base images in Jenkins and tag them with the Jenkins $BUILD_TAG,\nso this app is using build 5 of the rosettastone/sweet-repo base image.\nLet’s say we updated our sweet-repo base image to use ubuntu 16.04 instead of 14.04\nand this resulted in build 6 of the base image. Our auto-maintain script takes\ncare of upgrading an app that uses this base image to the most recent version.\nThe steps in the auto-maintain script look like this:\n\nFigure out what base image tag you’re using.\n\nFind the newest version of that base image tag by querying the docker registry.\n\nIf necessary, update the FROM line in the app’s Dockerfile to pull in the most recent version.\n\nWe do the same thing with library dependencies.\nIf our Gemfile.lock is referencing an old library, running auto-maintain will update things.\nThe same applies to the Jenkinsfile for each app. If we decide to implement a new policy where we\ndiscard old builds, we update auto-maintain so that it will bring each app into\ncompliance with the policy, by changing, for example, this Jenkinsfile :\n\nJenkinsfile (Before)\n\npipeline {\n  agent { label 'docker' }\n  stages {\n    stage('commit_stage') {\n      steps {\n        sh('./bin/ci')\n      }\n    }\n  }\n}\n\nto this:\n\nJenkinsfile (After)\n\npipeline {\n  agent { label 'docker' }\n  options {\n    buildDiscarder(logRotator(numToKeepStr: '100'))\n  }\n  stages {\n    stage('commit_stage') {\n      steps {\n        sh('./bin/ci')\n      }\n    }\n  }\n}\n\nWe try to account for these sorts of things (everything that we can) in our\nauto-maintain script rather than updating apps manually, since this reduces the\nfriction in keeping apps standardized.\n\nOnce you create an auto-maintain script (start small), you just have to run it.\nWe run ours based on both \"actions\" and \"non-actions.\" When an internal library\nchanges, we kick off app builds, so a library’s Jenkinsfile might look like\nthis:\n\nJenkinsfile\n\npipeline {\n  agent { label 'docker' }\n  stages {\n    stage('commit_stage') {\n      steps {\n        sh('./bin/ci')\n      }\n    }\n    stage('auto_maintain_things_that_might_be_using_me') {\n      steps {\n        build('hot-project/auto-maintain-all-apps/master')\n      }\n    }\n  }\n}\n\nWhen auto-maintain updates something in an app, we have it commit the change\nback to the app, which in turn triggers a build of that app, and—​if all is\nwell—​a production deployment.\n\nThe only missing link then for avoiding one-year build droughts is to get around\nthe problem where auto-maintain isn’t actually updating anything in a certain app.\nIf no dependencies are changing, or if the technology in question is not\nreceiving much attention, auto-maintain might not do anything for an\nextended period of time, even if the script is run on a schedule using\ncron . For those cases, putting\na cron trigger in the Pipeline for each app will ensure that builds still happen periodically:\n\nJenkinsfile\n\npipeline {\n  agent { label 'docker' }\n  triggers {\n    cron('@weekly')\n  }\n  stages {\n    stage('commit_stage') {\n      steps {\n        sh('./bin/ci')\n      }\n    }\n  }\n}\n\nIn most cases, these periodic builds won’t do anything different from the last\nbuild, but when something does break, this strategy will allow you to decide\nwhen you find out about it (by making your cron @weekly, @daily, etc)\ninstead of letting some poor developer find out about it when they do\nsomething silly like commit code to an infrequently-modified app.\n\nKevin will be\npresenting\nmore on this topic at\nJenkins World in August,\nregister with the code JWFOSS for a 30% discount off your pass.","title":"Automated Software Maintenance","tags":["event","jenkinsworld","jenkinsworld2017"],"authors":[]}},{"node":{"date":"2017-07-10T00:00:00.000Z","id":"8686ea9f-8c07-53fb-867e-e9ea74741ecf","slug":"/blog/2017/07/10/security-advisory/","strippedHtml":"Multiple Jenkins plugins received updates today that fix several security vulnerabilities, including high severity ones:\n\nDocker Commons Plugin\n\nGit Plugin\n\nGitHub Branch Source Plugin\n\nParameterized Trigger Plugin\n\nPeriodic Backup Plugin\n\nPipeline: Build Step Plugin\n\nPipeline: Groovy Plugin\n\nPoll SCM Plugin\n\nRole-based Authorization Strategy Plugin\n\nScript Security Plugin\n\nSidebar Link Plugin\n\nSubversion Plugin\n\nAdditionally, the SSH Plugin received a security update a few days ago.\n\nFor an overview of what was fixed, see the security advisory.\n\nSubscribe to the jenkinsci-advisories mailing list to receive important notifications related to Jenkins security.","title":"Security updates for multiple Jenkins plugins","tags":["plugins","security"],"authors":[]}},{"node":{"date":"2017-07-07T00:00:00.000Z","id":"7541af12-10e1-5d79-bcc7-41f0ae0d3010","slug":"/blog/2017/07/07/jenkins-conan/","strippedHtml":"This is a guest post by Luis Martínez de Bartolomé,\nConan Co-Founder\n\nC and C++ are present in very important industries today, including Operating Systems, embedded systems, finances, research, automotive, robotics, gaming, and many more. The main reason for this is performance, which is critical to many of these industries, and cannot be compared to any other technology.\nAs a counterpart, the C/C++ ecosystem has a few important challenges to face:\n\nHuge projects - With millions of lines of code, it’s very hard to manage your projects without using modern tools.\n\nApplication Binary Interface (ABI) incompatibility - To guarantee the compatibility of a library with other libraries and your application,  different configurations (such as the operating system, architecture, and compiler) need to be under control.\n\nSlow compilation times - Due to header inclusion and pre-processor bloat, together with the challenges mentioned above, it requires special attention to optimize the process and rebuild only the libraries that need to be rebuilt.\n\nCode linkage and inlining - A static C/C++ library can embed headers from a dependent library. Also, a shared library can embed a static library. In both cases, you need to manage the rebuild of your library when any of its dependencies change.\n\nVaried ecosystem - There are many different compilers and build systems, for different platforms, targets and purposes.\n\nThis post will show how to implement DevOps best practices for C/C++ development, using Jenkins CI, Conan C/C++ package manager, and JFrog Artifactory the universal artifact repository.\n\nConan, The C/C++ Package Manager\n\nConan was born to mitigate these pains.\n\nConan uses python recipes, describing how to build a library by explicitly calling any build system, and also describing the needed information for the consumers (include directories, library names etc.).\nTo manage the different configurations and the ABI compatibility, Conan uses \"settings\" (os, architecture, compiler…). When a setting is changed, Conan generates a different binary version for the same library:\n\nThe built binaries can be uploaded to JFrog Artifactory or Bintray, to be shared with your team or the whole community. The developers in your team won’t need to rebuild the libraries again, Conan will fetch only the needed Binary packages matching the user’s configuration from the configured remotes (distributed model).\nBut there are still some more challenges to solve:\n\nHow to manage the development and release process of your C/C++ projects?\n\nHow to distribute your C/C++ libraries?\n\nHow to test your C/C++ project?\n\nHow to generate multiple packages for different configurations?\n*How to manage the rebuild of the libraries when one of them changes?\n\nConan Ecosystem\n\nThe Conan ecosystem is growing fast, and DevOps with C/C++ is now a reality:\n\nJFrog Artifactory manages the full development and releasing cycles.\n\nJFrog Bintray is the universal distribution hub.\n\nJenkins automates the project testing, generates different binary configurations of your Conan packages, and automates the rebuilt libraries.\n\nJenkins Artifactory plugin\n\nProvides a Conan DSL, a very generic but powerful way to call Conan from a Jenkins Pipeline script.\n\nManages the remote configuration with your Artifactory instance, hiding the authentication details.\n\nCollects from any Conan operation (installing/uploading packages) all the involved artifacts to generate and publish the buildInfo to Artifactory. The buildInfo object is very useful, for example, to promote the created Conan packages to a different repository and to have full traceability of the Jenkins build:\n\nHere’s an example of the Conan DSL with the Artifactory plugin.  First we configure the Artifactory repository, then retrieve the dependencies and finally build it:\n\ndef artifactory_name = \"artifactory\"\ndef artifactory_repo = \"conan-local\"\ndef repo_url = 'https://github.com/memsharded/example-boost-poco.git'\ndef repo_branch = 'master'\n\nnode {\n   def server\n   def client\n   def serverName\n\nstage(\"Get project\"){\n    git branch: repo_branch, url: repo_url\n}\n\nstage(\"Configure Artifactory/Conan\"){\n    server = Artifactory.server artifactory_name\n    client = Artifactory.newConanClient()\n    serverName = client.remote.add server: server, repo: artifactory_repo\n}\n\nstage(\"Get dependencies and publish build info\"){\n    sh \"mkdir -p build\"\n    dir ('build') {\n      def b = client.run(command: \"install ..\")\n      server.publishBuildInfo b\n    }\n}\n\nstage(\"Build/Test project\"){\n        dir ('build') {\n          sh \"cmake ../ && cmake --build .\"\n        }\n    }\n}\n\nYou can see in the above example that the Conan DSL is very explicit. It helps a lot with common operations, but also allows powerful and custom integrations. This is very important for C/C++ projects, because every company has a very specific project structure, custom integrations etc.\n\nComplex Jenkins Pipeline operations: Managed and parallelized libraries building\n\nAs we saw at the beginning of this blog post, it’s crucial to save time when building a C/C++ project. Here are several ways to optimize the process:\n\nOnly re-build the libraries that need to be rebuilt. These are the libraries that  have been affected by a dependant library that has changed.\n\nBuild in parallel, if possible. When there is no relation between two or more libraries in the project graph, you can build them in parallel.\n\nBuild different configurations (os, compiler, etc) in parallel. Use different agents if needed.\n\nLet’s see an example using Jenkins Pipeline feature\n\nThe above graph represents our project P and its dependencies (A-G). We want to distribute the project for two different architectures, x86 and x86_64.\n\nWhat happens if we change library A?\n\nIf we bump the version to A(v1) there is no problem, we can update the B requirement and also bump its version to B(v1) and so on. The complete flow would be as follows:\n\nPush A(v1) version to Git, Jenkins will build the x86 and x86_64 binaries. Jenkins will upload all the packages to Artifactory.\n\nManually change B to v1, now depending on A1, push to Git, Jenkins will build the B(v1) for x86 and x86_64 using the retrieved new A1 from Artifactory.\n\nRepeat the same process for C, D, F, G and finally our project.\n\nBut if we are developing our libraries in a development repository, we probably depend on the latest A version or will override A (v0) packages on every git push, and we want to automatically rebuild the affected libraries in this case B, D, F, G and P.\n\nHow we can do this with Jenkins Pipelines?\n\nFirst we need to know which libraries need to be rebuilt. The \"conan info --build_order\" command identifies the libraries that were changed in our project, and also tells us which can be rebuilt in parallel.\n\nSo, we created two Jenkins pipelines tasks:\n\nThe\"SimpleBuild\" task which builds every single library. Similar to the first example using Conan DSL with the Jenkins Artifactory plugin. It’s a parameterized task that receives the libraries that need to built.\n\nThe\"MultiBuild\" task which coordinates and launches the \" SimpleBuild\" tasks, in parallel when possible.\n\nWe also have a repository with a configuration yml. The Jenkins tasks will use it to know where the recipe of each library is, and the different profiles to be used. In this case they are x86 and x86_64.\n\nleaves:\n  PROJECT:\n    profiles:\n       - ./profiles/osx_64\n       - ./profiles/osx_32\n\nartifactory:\n  name: artifactory\n  repo: conan-local\n\nrepos:\n LIB_A/1.0:\n   url: https://github.com/lasote/skynet_example.git\n   branch: master\n   dir: ./recipes/A\n\nLIB_B/1.0:\n url: https://github.com/lasote/skynet_example.git\n branch: master\n dir: ./recipes/b\n\n…\n\nPROJECT:\n url: https://github.com/lasote/skynet_example.git\n branch: master\n dir: ./recipes/PROJECT\n\nIf we change and push library A to the repository, the \" MultiBuild\" task will be triggered. It will start by checking which libraries need to be rebuilt, using the \"conan info\" command.\nConan will return something like this:\n[B, [D, F], G]\n\nThis means that we need to start building B, then we can build D and F in parallel, and finally build G. Note that library C does not need to be rebuilt, because it’s not affected by a change in library A.\n\nThe \" MultiBuild\" Jenkins pipeline script will create closures with the parallelized calls to the \" SimpleBuild\" task, and finally launch the groups in parallel.\n\n//for each group\n      tasks = [:]\n      // for each dep in group\n         tasks[label] = { -> build(job: \"SimpleBuild\",\n                            parameters: [\n                               string(name: \"build_label\", value: label),\n                               string(name: \"channel\", value: a_build[\"channel\"]),\n                               string(name: \"name_version\", value: a_build[\"name_version\"]),\n                               string(name: \"conf_repo_url\", value: conf_repo_url),\n                               string(name: \"conf_repo_branch\", value: conf_repo_branch),\n                               string(name: \"profile\", value: a_build[\"profile\"])\n                            ]\n                     )\n          }\n     parallel(tasks)\n\nEventually, this is what will happen:\n\nTwo SimpleBuild tasks will be  triggered, both for building library B, one for x86 and another for x86_64 architectures\n\nOnce \"A\" and \"B\" are built, \"F\" and \"D\" will be triggered, 4 workers will run the \"SimpleBuild\" task in parallel, (x86, x86_64)\n\nFinally \"G\" will be built. So 2 workers will run in parallel.\n\nThe Jenkins Stage View for the will looks similar to the figures below:\n\nMultiBuild\n\nSimpleBuild\n\nWe can configure the \" SimpleBuild\" task within different nodes (Windows, OSX, Linux…), and control the number of executors available in our Jenkins configuration.\n\nConclusions\n\nEmbracing DevOps for C/C++ is still marked as a to-do for many companies. It requires a big investment of time but can save huge amounts of time in the development and releasing life cycle for the long run. Moreover it increases the quality and the reliability of the C/C++ products. Very soon, adoption of DevOps for C/C++ companies will be a must!\n\nThe Jenkins example shown above that demonstrating how to control the library building in parallel is just Groovy code and a custom convenient yml file. The great thing about it is not the example or the code itself. The great thing is the possibility of defining your own pipeline scripts to adapt to your specific workflows, thanks to Jenkins Pipeline, Conan and JFrog Artifactory.\n\nMore on this topic will be presented at Jenkins Community Day Paris on\nJuly 11, and Jenkins User Conference Israel on July 13.","title":"Continuous Integration for C/C++ Projects with Jenkins and Conan","tags":["event","jenkins-user-conference","jenkins-community-day-paris"],"authors":[]}},{"node":{"date":"2017-07-05T00:00:00.000Z","id":"fd2d8b89-b05f-5caf-a6b1-923d33960065","slug":"/blog/2017/07/05/continuousdelivery-devops-artifactory/","strippedHtml":"This is a guest post by Michael Hüttermann. Michael is an expert\nin Continuous Delivery, DevOps and SCM/ALM. More information about him at huettermann.net, or\nfollow him on Twitter: @huettermann.\n\nIn a past blog post Delivery Pipelines,\nwith Jenkins 2, SonarQube, and Artifactory, we talked about pipelines which result in binaries for development versions. Now, in this blog post, I zoom in to different parts of the\nholistic pipeline and cover the handling of possible downstream steps once you have the binaries of development versions, in our example a Java EE WAR and a Docker image (which contains the WAR).\nWe discuss basic concept of staging software, including further information about quality gates, and show example toolchains. This contribution particularly examines the staging from binaries from\ndev versions to release candidate versions and from release candidate versions to final releases from the perspective of the automation server Jenkins, integrating with the binary\nrepository manager JFrog Artifactory and the distribution management platform JFrog Bintray, and ecosystem.\n\nStaging software\n\nStaging (also often called promoting) software is the process of completely and consistently transferring a release with all its configuration items\nfrom one environment to another. This is even more true with DevOps, where you want to accelerate the cycle time (see Michael Hüttermann, DevOps for Developers (Apress, 2012), 38ff).\nFor accelerating the cycle time, meaning to bring software to production, fast and in good quality, it is crucial to have fine processes and integrated tools to streamline the\ndelivery of software. The process of staging releases consists of deploying software to different staging levels, especially different test environments.\nStaging also involves configuring the software for various environments without needing to recompile or rebuild the software. Staging is necessary\nto transport the software to production systems in high quality. Many Agile projects make great experience with implementing a staging ladder in\norder to optimize the cycle time between development software and the point when the end user is able to use the software in production.\n\nCommonly, the staging ladder is illustrated on its side, with the higher rungs being the boxes further to the right. It’s good practice not to skip any rungs during staging.\nThe central development environment packages and integrates all respective configuration items and is the base for releasing. Software is staged over different environments by\nconfiguration, without rebuilding. All changes go through the entire staging process, although defined exception routines may be in place,\nfor details see Michael Hüttermann, Agile ALM (Manning, 2012).\n\nTo make concepts clearer, this blog post covers sample tools. Please note, that there are also alternative tools available. As one example: Sonatype Nexus is also able to host the covered binaries and also offers scripting functionality.\n\nWe nowadays often talk about delivery pipelines. A pipeline is just a set of stages and transition rules between those stages. From a DevOps perspective, a pipeline bridges multiple\nfunctions in organizations, above all development and operations. A pipeline is a staging ladder. A change enters the pipeline at the beginning and leaves it at the end. The processing\ncan be triggered automatically (typical for delivery pipelines) or by a human actor (typical for special steps at overall pipelines, e.g. pulling and thus cherry-picking specific\nversions to promote them to be release candidates are final releases).\n\nPipelines often look different, because they strongly depend on requirements and basic conditions, and can contain further sub pipelines. In our scenario, we have two sub pipelines to\nmanage the promotion of continuous dev versions to release candidates and the promotion of release candidates to final release. A change typically waits at a stage for further processing\naccording to the transition rules, aligned with defined requirements to meet, which are the Quality Gates, explored next.\n\nQuality Gates\n\nQuality gates allow the software to pass through stages only if it meets their defined requirements. The next illustration shows a staging ladder with quality gates injected. You and\nother engaged developers commit code to the version control system (please, use VCS as an abbreviation, not SCM, because the latter is much more) in order to update the central test\nenvironment only if the code satisfies the defined quality requirements; for instance, the local build may need to run successfully and have all tests pass locally. Build, test, and\nmetrics should pass out of the central development environment, and then automated and manual acceptance tests are needed to pass the system test. In our case, the last quality gate\nto pass is the one from the  production mirror to production. Here, for example, specific production tests are done or relevant documents must be filled in and signed.\n\nIt’s mandatory to define the quality requirements in advance and to resist customizing them after the fact, when the software has failed. Quality gates are different at lower and\nhigher stages; the latter normally consist of a more severe or broader set of quality requirements, and they often include the requirements of the lower gates. The binary repository\nmanager must underpin corresponding quality gates, while managing the binaries, what we cover next.\n\nThis blog post illustrates typical concepts and sample toolchains. For more information, please consult the respective documentation, good books or attend top notch conferences, e.g.\nJenkins World, powered by CloudBees.\n\nBinary repository manager\n\nA central backbone of the staging ladder is the binary repository manager, e.g. JFrog Artifactory. The binary repository manager manages all binaries including the self-produced\nones (producing view) and the 3rd party ones (consuming view), across all artifact types, in our case a Java EE WAR file and a Docker image. Basic idea here is that the repo manager serves\nas a proxy, thus all developers access the repo manager, and not remote binary pools directly, e.g. Maven Central. The binary repository manager offers cross-cutting services,\ne.g. role-based access control on specific logical repositories, which may correspond to specific stages of the staging ladder.\n\nLogical repositories can be generic ones (meaning they are agnostic regarding any tools and platforms, thus you can also just upload the menu of your local canteen) or repos\nspecific to tools and platforms. In our case, we need a repository for managing the Java EE WAR files and for the Docker images. This can be achieved by\n\na generic repository (preferred for higher stages) or a repo which is aligned with the layout of the Maven build tool, and\n\na repository for managing Docker images, which serves as a Docker registry.\n\nIn our scenario, preparing the staging of artifacts includes the following ramp-up activities\n\nCreating two sets of logical repositories, inside JFrog Artifactory, where each set has a repo for the WAR file and a repo for the Docker image, and one set is for managing dev\nversions and one set is for release candidate versions.\n\nDefining and implementing processes to promote the binaries from the one set of repositories (which is for dev versions) to the other set of repositories (which is for RC versions).\nPart of the process is defining roles, and JFrog Artifactory helps you to implement role-based access control.\n\nSetting up procedures or scripts to bring binaries from one set of repositories to the other set of repositories, reproducibly. Adding meta data to binaries is important if the degree of maturity\nof the binary cannot be easily derived from the context.\n\nThe following illustration shows a JFrog Artifactory instance with the involved logical repos in place. In our simplified example, the repo promotions are supposed to go from\ndocker-local to docker-prod-local, and from libs-release-local to libs-releases-staging-local. In our use case, we promote the software in version 1.0.0.\n\nAnother type of binary repository manager is JFrog Bintray, which serves as a universal distribution platform for many technologies. JFrog Bintray can be an interesting choice\nif you have strong requirements for scalability and worldwide coverage including IP restrictions and handy features around statistics. Most of the concepts and ramp up activities\n are similar compared to JFrog Artifactory, thus I do not want to repeat them here. Bintray is used by lot of projects e.g. by Groovy, to host their deliverables in the public.\n But keep in mind that you can of course also host your release binaries in JFrog Artifactory.\n In this blog post, I’d like to introduce different options, thus we promote our release candidates to JFrog Artifactory and our releases to JFrog Bintray.\n Bintray has the concept of products, packages and versions. A product can have multiple packages and has different versions. In our example, the product has two packages, namely the Java EE WAR and\n the Docker image, and the concrete version that will be processed is 1.0.0.\n\nSome tool features covered in this blog post are available as part of commercial offerings of tool vendors. Examples include the Docker support of JFrog Artifactory or the Firehose Event API of JFrog Bintray.\nPlease consult the respective documentation for more information.\n\nNow it is time to have a deeper look at the pipelines.\n\nImplementing Pipelines\n\nOur example pipelines are implemented with Jenkins, including its Blue Ocean and declarative pipelines facilities, JFrog Artifactory and JFrog Bintray. To derive your personal\npipelines, please check your individual requirements and basic conditions to come up with the best solution for your target architecture, and consult the respective documentation for\n more information, e.g. about scripting the tools.\n\nIn case your development versions are built with Maven, and have SNAPSHOT character, you need to either rebuild the software after setting the release version, as part of\nyour pipeline, or you solely use Maven releases from the very beginning. Many projects make great experience with morphing Maven snapshot versions into\nrelease versions, as part of the pipeline, by using a dedicated Maven plugin, and externalizing it into a Jenkins shared library. This can look like the following:\n\nsl.groovy (excerpt): A Jenkins shared library, to include in Jenkins pipelines.\n\n#!/usr/bin/groovy\n    def call(args) { (1)\necho \"Calling shared library, with ${args}.\"\n       sh \"mvn com.huettermann:versionfetcher:1.0.0:release versions:set -DgenerateBackupPoms=false -f ${args}\" (2)\n}\n\n1\nWe provide a global variable/function to include it in our pipelines.\n\n2\nThe library calls a Maven plugin, which dynamically morphs the snapshot version of a Maven project to a release version.\n\nAnd including it into the pipeline is then also very straight forward:\n\npipeline.groovy (excerpt): A stage calling a Jenkins shared library.\n\nstage('Produce RC') { (1)\nreleaseVersion 'all/pom.xml' (2)\n}\n\n1\nThis stage is part of a scripted pipeline and is dedicated to morphing a Maven snapshot version into a release version, dynamically.\n\n2\nWe call the Jenkins shared library, with a parameter pointing to the Maven POM file, which can be a parent POM.\n\nYou can find the code of the underlying Maven plugin here.\n\nLet’s now discuss how to proceed for the release candidates.\n\nRelease Candidate (RC)\n\nThe pipeline to promote a dev version to a RC version does contain a couple of different stages, including stages to certify the binaries (meaning labeling it or adding context information) and stages to process the concrete promotion.\nThe following illustration shows the successful run of the promotion, for software version 1.0.0.\n\nWe utilize Jenkins Blue Ocean that is a new user experience for Jenkins based on a personalizable, modern design that allows users to graphically create, visualize and diagnose\ndelivery pipelines. Besides the new approach in general, single Blue Ocean features help to boost productivity dramatically, e.g. to provide log information at your fingertips\nand the ability to search pipelines. The stages to perform the promote are as follows starting with the  Jenkins pipeline stage for promoting the WAR file. Keep in mind that all\nscripts are parameterized, including variables for versions and Artifactory domain names, which are either injected to the pipeline run by user input or set system wide in the Jenkins admin panel,\nand the underlying call is using the JFrog command line interface, CLI in short. JFrog Artifactory\nas well as JFrog Bintray can be used and managed by scripts, based on a REST API. The JFrog CLI\nis an abstraction on top of the JFrog REST API, and we show sample usages of both.\n\npipeline.groovy (excerpt): Staging WAR file to different logical repository\n\nstage('Promote WAR') { (1)\nsteps { (2)\nsh 'jfrog rt cp --url=https://$ARTI3 --apikey=$artifactory_key --flat=true libs-release-local/com/huettermann/web/$version/ ' + (3)\n'libs-releases-staging-local/com/huettermann/web/$version/'\n       }\n    }\n\n1\nThe dedicated stage for running the promotion of the WAR file.\n\n2\nHere we have the steps which make up the stage, based on Jenkins declarative pipeline syntax.\n\n3\nCopying the WAR file, with JFrog CLI, using variables, e.g. the domain name of the Artifactory installation. Many options available, check the docs.\n\nThe second stage to explore more is the promotion of the Docker image. Here, I want to show you a different way how to achieve the goal, thus in this use case we utilize the JFrog REST API.\n\npipeline.grovvy (excerpt): Promote Docker image\n\nstage('Promote Docker Image') {\n          sh '''curl -H \"X-JFrog-Art-Api:$artifactory_key\" -X POST https://$ARTI3/api/docker/docker-local/v2/promote ''' + (1)\n'''-H \"Content-Type:application/json\" ''' + (2)\n'''-d \\'{\"targetRepo\" : \"docker-prod-local\", \"dockerRepository\" : \"michaelhuettermann/tomcat7\", \"tag\": \"\\'$version\\'\", \"copy\": true }\\' (3)\n'''\n    }\n\n1\nThe shell script to perform the staging of Docker image is based on JFrog REST API.\n\n2\nPart of parameters are sent in JSON format.\n\n3\nThe payload tells the REST API endpoint what to to, i.e. gives information about target repo and tag.\n\nOnce the binaries are promoted (and hopefully deployed and tested on respective environments before), we can promote them to become final releases, which I like to call GA.\n\nGeneral Availability (GA)\n\nIn our scenario, JFrog Bintray serves as the distribution platform to manage and provide binaries for further usage. Bintray can also serve as a Docker registry, or can just\nprovide binaries for scripted or manual download. There are again different ways how to promote binaries, in this case from the RC repos inside JFrog Artifactory to the GA storage in JFrog Bintray, and I summarize one of those possible ways. First, let’s look at the Jenkins pipeline, showed in the next illustration. The processing is on its way, currently, and we again have a list of linked stages.\n\nZooming in now to the key stages, we see that promoting the WAR file is a set of steps that utilize JFrog REST API. We download the binary from JFrog Artifactory, parameterized,\nand upload it to JFrog Bintray.\n\npipeline.groovy (excerpt): Promote WAR to Bintray\n\nstage('Promote WAR to Bintray') {\n       steps {\n          sh '''\n             curl -u michaelhuettermann:${bintray_key} -X DELETE https://api.bintray.com/packages/huettermann/meow/cat/versions/$version (1)\ncurl -u michaelhuettermann:${bintray_key} -H \"Content-Type: application/json\" -X POST https://api.bintray.com/packages/huettermann/meow/cat/$version --data \"\"\"{ \"name\": \"$version\", \"desc\": \"desc\" }\"\"\" (2)\ncurl -T \"$WORKSPACE/all-$version-GA.war\" -u michaelhuettermann:${bintray_key} -H \"X-Bintray-Package:cat\" -H \"X-Bintray-Version:$version\" https://api.bintray.com/content/huettermann/meow/ (3)\ncurl -u michaelhuettermann:${bintray_key} -H \"Content-Type: application/json\" -X POST https://api.bintray.com/content/huettermann/meow/cat/$version/publish --data '{ \"discard\": \"false\" }' (4)\n'''\n       }\n    }\n\n1\nFor testing and demo purposes, we remove the existing release version.\n\n2\nNext we create the version in Bintray, in our case the created version is 1.0.0. The value was insert by user while triggering the pipeline.\n\n3\nThe upload of the WAR file.\n\n4\nBintray needs a dedicated publish step to make the binary publicly available.\n\nProcessing the Docker image is as easy as processing the WAR. In this case, we just push the Docker image to the Docker registry, which is served by JFrog Bintray.\n\npipeline.groovy (excerpt): Promote Docker image to Bintray\n\nstage('Promote Docker Image to Bintray') { (1)\nsteps {\n          sh 'docker push $BINTRAYREGISTRY/michaelhuettermann/tomcat7:$version' (2)\n}\n    }\n\n1\nThe stage for promoting the Docker image. Please note, depending on your setup, you may add further stages, e.g. to login to your Docker registry.\n\n2\nThe Docker push of the specific version. Note, that also here all variables are parameterized.\n\nWe now have promoted the binaries and uploaded them to JFrog Bintray. The overview page of our product lists two packages: the WAR file and the Docker image. Both can be downloaded\nnow and used, the Docker image can be pulled from the JFrog Bintray Docker registry with native Docker commands.\n\nAs part of its graphical visualization capabilitites, Bintray is able to show the single layers of the uploaded Docker images.\n\nBintray can also display usage statistics, e.g. download details. Now guess where I’m sitting right now while downloading the binary?\n\nBesides providing own statistics, Bintray provides the JFrog Firehose Event API. This API streams live usage data, which in turn can be integrated or aggregated with your ecosystem.\nIn our case, we visualize the data, particularly download, upload, and delete statistics, with the ELK stack, as part of a functional monitoring initiative.\n\nCrisp, isn’t it?\n\nSummary\n\nThis closes are quick ride through the world of staging binaries, based on Jenkins. We’ve discussed concepts and example DevOps enabler tools, which can help to implement\n the concepts. Along the way, we discussed some more options how to integrate with ecosystem, e.g. releasing Maven snapshots and functional monitoring with dedicated tools.\n After this appetizer you may want to now consider to double-check your staging processes and toolchains, and maybe you find some room for further adjustments.\n\nReferences\n\n'Agile ALM', Manning, 2011\n\nBinary Repository Manager Feature Matrix\n\n'DevOps for Developers', Apress, 2012\n\nDocker\n\nELK\n\nJFrog Artifactory\n\nJFrog Bintray\n\nJFrog CLI\n\nJFrog REST API\n\nSonatype Nexus","title":"Delivery pipelines, with Jenkins 2: how to promote Java EE and Docker binaries toward production.","tags":["devops","jenkins","artifactory","bintray"],"authors":[]}},{"node":{"date":"2017-07-03T00:00:00.000Z","id":"3eff5455-8949-570e-ae59-6f180d034c6f","slug":"/blog/2017/07/03/contributor-summit/","strippedHtml":"As in previous years, there’ll be a contributor summit at Jenkins World 2017 :\n\nLet’s talk about the future of Jenkins and how you can help shape it! The contributor summit is the place where the current and future contributors of the Jenkins project get together. This year, the theme is “working together”. Traditionally, most contributors are plugin maintainers focusing on their own plugins, but it’s important to look at Jenkins as a whole, because that’s how users experience it. There is more to the project beyond writing plugins, and even for plugin developers, there are increasing number of common libraries and modules that plugins should rely on. In short, we should work better together.\n\nA few contributors will prepare some presentations to help clarify what that means, and what all of us can do. And in the afternoon, there will be \"unconference\" sessions to brainstorm and discuss what we can do and how.\n\nWhether you are already a contributor or just thinking about becoming one, please join us for this full day free event.\n\nDetails about this year’s agenda are available on the event’s meetup page.\nAttending is free, and no Jenkins World ticket is needed, but RSVP if you’re going to attend to help us plan.\n\nSee you there!","title":"Jenkins World Contributor Summit","tags":["event","jenkinsworld","jenkinsworld2017"],"authors":[]}},{"node":{"date":"2017-06-27T00:00:00.000Z","id":"c68cdf2d-4588-5bfa-80b3-2a0c1581eedd","slug":"/blog/2017/06/27/speaker-blog-SAS-jenkins-world/","strippedHtml":"This is a guest post by Brent Laster, Senior Manager, Research and Development at\nSAS.\n\nJenkins Pipeline\nhas fundamentally changed how users can orchestrate their pipelines and workflows.\nEssentially, anything that you can do in a script or program can now be done in a Jenkinsfile or in a pipeline script created within the application.\nBut just because you can do nearly anything directly in those mechanisms doesn’t mean you necessarily should.\n\nIn some cases, it’s better to abstract the functionality out separately from your main Pipeline.\nPreviously, the main way to do this in Jenkins itself was through creating plugins.\nWith Jenkins 2 and the tight incorporation of Pipeline, we now have another approach – shared libraries.\n\nBrent will be\npresenting\nmore of this topic at Jenkins World in\nAugust, register with the code JWFOSS for a 30% discount off your pass.\n\nShared libraries\nprovide solutions for a number of situations that can be challenging or time-consuming to deal with in Pipeline.\nAmong them:\n\nProviding common routines that can be accessed across a number of pipelines or within a designated scope (more on scope later)\n\nAbstracting out complex or restricted code\n\nProviding a means to execute scripted code from calls in declarative pipelines (where scripted code is not normally allowed)\n\nSimplifying calls in a script to custom code that only differ by calling parameters\n\nTo understand how to use shared libraries in Pipeline, we first need to understand how they are constructed.\nA shared library for Jenkins consists of a source code repository with a structure like the one below:\n\nEach of the top-level directories has its own purpose.\n\nThe resources directory can have non-groovy resources that get loaded via the libraryResource step.\nThink of this as a place to store supporting data files such as json files.\n\nThe src directory uses a structure similar to the standard Java src layout.\nThis area is added to the classpath when a Pipeline that includes this shared library is executed.\n\nThe vars directory holds global variables that should be accessible from pipeline scripts.\nA corresponding.txt file can be included that defines documentation for objects here.\nIf found, this will be pulled in as part of the documentation in the Jenkins application.\n\nAlthough you might think that it would always be best to define library functions in the src structure, it actually works better in many cases to define them in the vars area.\nThe notion of a global variable may not correspond very well to a global function, but you can think of it as the function being a global value that can be pulled in and used in your pipeline.\nIn fact, to work in a declarative style pipeline, having your function in the vars area is the only option.\n\nLet’s look at a simple function that we can create for a shared library.\nIn this case, we’ll just wrap picking up the location of the Gradle installation from Jenkins and calling the corresponding executable with whatever tasks are passed in as arguments.\nThe code is below:\n\n/vars/gbuild.groovy\n\ndef call(args) {\n      sh \"${tool 'gradle3'}/bin/gradle ${args}\"\n}\n\nNotice that we are using a structured form here with the def call syntax.\nThis allows us to simply invoke the routine in our pipeline (assuming we have loaded the shared library) based on the name of the file in the vars area.\nFor example, since we named this file gbuild.groovy, then we can invoke it in our pipeline via a step like this:\n\ngbuild 'clean compileJava'\n\nSo, how do we get our shared library loaded to use in our pipeline?\nThe shared library itself is just code in the structure outlined above committed/pushed into a source code repository that Jenkins can access.\nIn our example, we’ll assume we’ve staged, committed, and pushed this code into a local Git repository on the system at /opt/git/shared-library.git.\n\nLike most other things in Jenkins, we need to first tell Jenkins where this shared library can be found and how to reference it \"globally\" so that pipelines can reference it specifically.\n\nFirst, though, we need to decide at what scope you want this shared library to be available.\nThe most common case is making it a \"global shared library\" so that all Pipelines can access it.\nHowever, we also have the option of only making shared libraries available for projects in a particular Jenkins Folder structure,\nor those in a Multibranch Pipeline, or those in a GitHub Organization pipeline project.\n\nTo keep it simple, we’ll just define ours to be globally available to all pipelines.\nDoing this is a two-step process.\nWe first tell Jenkins what we want to call the library and define some default behavior for Jenkins related to the library,\nsuch as whether we wanted it loaded implicitly for all pipelines.\nThis is done in the Global Pipeline Libraries section of the Configure System page.\n\nFor the second part, we need to tell Jenkins where the actual source repository for the shared library is located.\nSCM plugins that have been modified to understand how to work with shared libraries are called \" Modern SCM\".\nThe git plugin in one of these updated plugin, so we just supply the information in the same Configure System page.\n\nAfter configuring Jenkins so that it can find the shared library repository, we can load the shared library into our pipeline using the @Library(' ') annotation.\nSince Annotations\nare designed to annotate something that follows them,\nwe need to either include a specific import statement, or, if we want to include everything, we can use an underscore character as a placeholder.\nSo our basic step to load the library in a pipeline would be:\n\n@Library('Utilities2') _\n\nBased on this step, when Jenkins runs our Pipeline, it will first go out to the repository that holds the shared library and clone down a copy to use.\nThe log output during this part of the pipeline execution would look something like this:\n\nLoading library Utilities2@master\n > git rev-parse --is-inside-work-tree # timeout=10\nSetting origin to /opt/git/shared-libraries\n > git config remote.origin.url /opt/git/shared-libraries # timeout=10\nFetching origin...\nFetching upstream changes from origin\n > git --version # timeout=10\nusing GIT_SSH to set credentials Jenkins2 SSH\n > git fetch --tags --progress origin +refs/heads/*:refs/remotes/origin/*\n > git rev-parse master^{commit} # timeout=10\n > git rev-parse origin/master^{commit} # timeout=10\nCloning the remote Git repository\nCloning repository /opt/git/shared-libraries\n\nThen Pipeline can call our shared library gbuild function and translate it to the desired Gradle build commands.\n\nFirst time build.\nSkipping changelog.\n[Pipeline] }\n[Pipeline] // stage\n[Pipeline] stage\n[Pipeline] { (Compile)\n[Pipeline] tool\n[Pipeline] sh\n[gsummit17_lab2-4T357CUTJORMC2TIF7WW5LMRR37F7PM2QRUHXUNSRTWTTRHB3XGA]\nRunning shell script\n+ /usr/share/gradle/bin/gradle clean compileJava -x test\nStarting a Gradle Daemon (subsequent builds will be faster)\n\nThis is a very basic illustration of how using shared libraries work.\nThere is much more detail and functionality surrounding shared libraries, and extending your pipeline in general, than we can cover here.\n\nBe sure to catch my talk on\nExtending your Pipeline with Shared Libraries, Global Functions and External Code\nat Jenkins World 2017.\nAlso, watch for my new book on\nJenkins 2 Up and Running\nwhich will have a dedicated chapter on this – expected to be available later this year from O’Reilly.","title":"Extending your Pipeline with Shared Libraries, Global Functions and External Code","tags":["event","jenkinsworld","jenkinsworld2017"],"authors":[]}},{"node":{"date":"2017-06-26T00:00:00.000Z","id":"7b02e5d8-2c15-5100-9be1-7256902c46eb","slug":"/blog/2017/06/26/share-jenkins-world-keynote-stage/","strippedHtml":"Jenkins World is approaching fast,\nand the event staff are all busy preparing.\nI’ve decided to do something different this year as part of my keynote:\nI want to invite a few Jenkins users like you come up on stage with me.\n\nThere have been amazing developments in Jenkins over the past year.\nFor my keynote, I want highlight how the new Jenkins\n(Pipeline as code with the Jenkinsfile, no more creating jobs,\nBlue Ocean)\nis different and better than the old Jenkins (freestyle jobs, chaining jobs together, etc.).\nAll these developments have helped Jenkins users,\nand it would be more meaningful to have fellow users, like you, share their stories\nabout how recent Jenkins improvements like Pipeline and Blue Ocean have positively impacted them.\n\nIf you’re interested sharing your story, please complete\nthis form\nso that I can contact you.\nThis is a great opportunity to let\nthe rest of the world (and your boss!) hear about your accomplishments.\nYou’ll also get into Jenkins World for free and get to join me backstage.\nIf you concerns about traveling to Jenkins World,\nI’m happy to discuss helping with that as well.\n\nI look forward to hearing from you.","title":"Come Share the Jenkins World Keynote Stage with Me!","tags":["event","jenkinsworld","jenkinsworld2017"],"authors":[]}},{"node":{"date":"2017-06-14T00:00:00.000Z","id":"d17e39c6-5d60-518a-9ff6-8280ace32930","slug":"/blog/2017/06/14/jenkinsworld-awards-lastcall/","strippedHtml":"This is a guest post by Alyssa Tong, who runs\nthe Jenkins Area Meetup program and is also responsible for\nMarketing & Community Programs at CloudBees, Inc.\n\nWe have received a good number of nominations for the Jenkins World 2017 Community Awards. These nominations are indicative of the excellent work Jenkins members are doing for the betterment of Jenkins.\n\nThe deadline for nomination is this Friday, June 16.\n\nThis will be the first year we are commemorating community members who have\nshown excellence through commitment, creative thinking, and contributions to\ncontinue making Jenkins a great open source automation server. The award\ncategories includes:\n\nMost Valuable Contributor -\nThis award is presented to the Jenkins contributor who has helped move the Jenkins project forward the most through their invaluable feature contributions, bug fixes or plugin development efforts.\n\nJenkins Security MVP -\nThis award is presented to the individual most consistently providing excellent security reports or who helped secure Jenkins by fixing security issues.\n\nMost Valuable Advocate -\nThis award is presented to an individual who has helped advocate for Jenkins through organization of their local Jenkins Area Meetup.\n\nSubmit your story, or nominate someone today! Winners will be announced at Jenkins World 2017 in San Francisco on August 28-31.\n\nWe look forward to hearing about the great Jenkins work you are doing.","title":"Jenkins World 2017 Community Awards - Last Call for Nominations!","tags":["event","jenkinsworld","jenkinsworld2017"],"authors":[]}}]}},"pageContext":{"limit":8,"skip":288,"numPages":100,"currentPage":37}},
    "staticQueryHashes": ["3649515864"]}