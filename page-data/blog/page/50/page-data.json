{
    "componentChunkName": "component---src-templates-blog-list-template-js",
    "path": "/blog/page/50",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2016-07-01T00:00:00.000Z","id":"4848db1b-feac-54a0-8b3e-2a0f5e3fbfc6","slug":"/blog/2016/07/01/html-publisher-plugin/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nMost projects need more that just JUnit result reporting.  Rather than writing a\ncustom plugin for each type of report, we can use the\nHTML Publisher Plugin.\n\nLet’s Make This Quick\n\nI’ve found a Ruby project,\nhermann, I’d like to build using Jenkins Pipeline. I’d\nalso like to have the code coverage results published with each build job.  I could\nwrite a plugin to publish this data, but I’m in a bit of hurry and\nthe build already creates an HTML report file using SimpleCov\nwhen the unit tests run.\n\nSimple Build\n\nI’m going to use the\nHTML Publisher Plugin\nto add the HTML-formatted code coverage report to my builds.  Here’s a simple\npipeline for building the hermann\nproject.\n\nstage 'Build'\n\nnode {\n  // Checkout\n  checkout scm\n\n  // install required bundles\n  sh 'bundle install'\n\n  // build and run tests with coverage\n  sh 'bundle exec rake build spec'\n\n  // Archive the built artifacts\n  archive (includes: 'pkg/*.gem')\n}\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit 'https://github.com/reiseburo/hermann.git'.\n\nSimple enough, it builds, runs tests, and archives the package.\n\nNow I just need to add the step to publish the code coverage report.\nI know that rake spec creates an index.html file in the coverage directory.\nI’ve already installed the\nHTML Publisher Plugin.\nHow do I add the HTML publishing step to the pipeline?  The plugin page doesn’t\nsay anything about it.\n\nSnippet Generator to the Rescue\n\nDocumentation is hard to maintain and easy to miss, even more so in a system\nlike Jenkins with hundreds of plugins the each potential have one or more\ngroovy fixtures to add to the Pipeline.  The Pipeline Syntax\"Snippet Generator\" helps users\nnavigate this jungle by providing a way to generate a code snippet for any step using\nprovided inputs.\n\nIt offers a dynamically generated list of steps, based on the installed plugins.\nFrom that list I select the publishHTML step:\n\nThen it shows me a UI similar to the one used in job configuration.  I fill in\nthe fields, click \"generate\", and it shows me snippet of groovy generated from\nthat input.\n\nHTML Published\n\nI can use that snippet directly or as a template for further customization.\nIn this case, I’ll just reformat and copy it in at the end of my\npipeline.  (I ran into a minor bug\nin the snippet generated for this plugin step. Typing\nerror string in my search bar immediately found the bug and a workaround.)\n\n/* ...unchanged... */\n\n  // Archive the built artifacts\n  archive (includes: 'pkg/*.gem')\n\n  // publish html\n  // snippet generator doesn't include \"target:\"\n  // https://issues.jenkins.io/browse/JENKINS-29711.\n  publishHTML (target: [\n      allowMissing: false,\n      alwaysLinkToLastBuild: false,\n      keepAll: true,\n      reportDir: 'coverage',\n      reportFiles: 'index.html',\n      reportName: \"RCov Report\"\n    ])\n\n}\n\nWhen I run this new pipeline I am rewarded with an RCov Report link on left side,\nwhich I can follow to show the HTML report.\n\nI even added the keepAll setting to let I can also go back an look at reports on old jobs as\nmore come in.  As I said to to begin with, this is not as slick as what I\ncould do with a custom plugin, but it is much easier and works with any static\nHTML.\n\nLinks\n\nHTML Publisher Plugin\n\nJenkins Pipeline Snippet Generator","title":"Publishing HTML Reports in Pipeline","tags":["tutorial","pipeline","plugins","ruby"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2016-06-30T00:00:00.000Z","id":"6dc0bcbf-e180-50ae-86bf-0a881c810b38","slug":"/blog/2016/06/30/ewm-alpha-version/","strippedHtml":"Currently it’s quite difficult to share and reuse the same workspace between multiple jobs and across nodes.\nThere are some possible workarounds for achieving this, but each of them has its own drawback,\ne.g. stash/unstash pre-made artifacts, Copy Artifacts plugin or advanced job settings.\nA viable solution for this problem is the External Workspace Manager plugin, which facilitates workspace share and\nreuse across multiple Jenkins jobs and nodes.\nIt also eliminates the need to copy, archive or move files.\nYou can learn more about the design and goals of the External Workspace Manager project in\nthis introductory blog post.\n\nI’d like to announce that an alpha version of the External Manager Plugin has been released!\nIt’s now public available for testing.\nTo be able to install this plugin, you must follow the steps from the Experimental Plugins Update Center\nblog post.\n\nPlease be aware that it’s not recommended to use the Experimental Update Center in production installations of\nJenkins, since it may break it.\n\nThe plugin’s wiki page may be accessed\nhere.\nThe documentation that helps you get started with this plugin may be found on the\nREADME page.\nTo get an idea of what this plugin does, which are the features implemented so far and to see a working demo of it,\nyou can watch my mid-term presentation that is available here.\nThe slides for the presentation are shared on\nGoogle Slides.\n\nMy mentors, Martin and Oleg,\nand I have set up public meetings related to this plugin.\nYou are invited to join our discussions if you’d like to get more insight about the project.\nThe meetings are taking place twice a week on the Jenkins hangout,\nevery Monday at\n12 PM UTC\nand every Thursday at\n5 PM UTC.\n\nIf you have any issues in setting up or using the plugin, please feel free to ask me on the plugin’s Gitter\nchat.\nThe plugin is open-source, having the repository on\nGitHub, and you may contribute to it.\nAny feedback is welcome, and you may provide it either on the Gitter chat, or on\nJira by using the external-workspace-manager-plugin component.\n\nLinks\n\nProject repository\n\nPlugin wiki page\n\nMid-term presentation\n\nProject intro blog post\n\nGSoC page\n\nJenkins GSoC Page","title":"GSoC: External Workspace Manager Plugin alpha version","tags":["pipeline","plugins","external-workspace-manager","gsoc"],"authors":[{"avatar":null,"blog":null,"github":"alexsomai","html":"","id":"alexsomai","irc":null,"linkedin":null,"name":"Alexandru Somai","slug":"/blog/authors/alexsomai","twitter":"alex_somai"}]}},{"node":{"date":"2016-06-29T00:00:00.000Z","id":"72dc398a-0c3a-5b36-949e-7687e6ecbefc","slug":"/blog/2016/06/29/from-freestyle-to-pipeline/","strippedHtml":"This is a guest post by R. Tyler Croy, who is a\nlong-time contributor to Jenkins and the primary contact for Jenkins project\ninfrastructure. He is also a Jenkins Evangelist at\nCloudBees, Inc.\n\nFor ages I have used the \"Build After\" feature in Jenkins to cobble together\nwhat one might refer to as a \"pipeline\" of sorts. The Jenkins project itself, a\nmajor consumer of Jenkins, has used these daisy-chained Freestyle jobs to drive\na myriad of delivery pipelines in our infrastructure.\n\nOne such \"pipeline\" helped drive the complex process of generating the pretty\nblue charts on\nstats.jenkins.io.\nThis statistics generation process primarily performs two major tasks, on rather\nlarge sets of data:\n\nGenerate aggregate monthly \"census data.\"\n\nProcess the census data and create trend charts\n\nThe chained jobs allowed us to resume the independent stages of the pipeline,\nand allowed us to run different stages on different hardware (different\ncapabilities) as needed. Below is a diagram of what this looked like:\n\nThe infra_generate_monthly_json would run periodically creating the\naggregated census data, which would then be picked up by infra_census_push\nwhose sole responsibility was to take census data and publish it to the\nnecessary hosts inside the project’s infrastructure.\n\nThe second, semi-independent, \"pipeline\" would also run periodically. The\ninfra_statistics job’s responsibility was to use the census data, pushed\nearlier by infra_census_push, to generate the myriad of pretty blue charts\nbefore triggering the\ninfra_checkout_stats job which would make sure stats.jenkins.io was\nproperly updated.\n\nSuffice it to say, this \"pipeline\" had grown organically over a period time when\nmore advanced tools weren’t quite available.\n\nWhen we migrated to newer infrastructure for\nci.jenkins.io earlier this year I took the\nopportunity to do some cleaning up. Instead of migrating jobs verbatim, I pruned\nstale jobs and refactored a number of others into proper\nPipelines, statistics generation being an obvious\ntarget!\n\nOur requirements for statistics generation, in their most basic form, are:\n\nEnable a sequence of dependent tasks to be executed as a logical group (a\npipeline)\n\nEnable executing those dependent tasks on various pieces of infrastructure\nwhich support different requirements\n\nActually generate those pretty blue charts\n\nIf you wish to skip ahead, you can jump straight to the\nJenkinsfile\nwhich implements our new Pipeline.\n\nThe first iteration of the Jenkinsfile simply defined the conceptual stages we\nwould need:\n\nnode {\n    stage 'Sync raw data and census files'\n\n    stage 'Process raw logs'\n\n    stage 'Generate census data'\n\n    stage 'Generate stats'\n\n    stage 'Publish census'\n\n    stage 'Publish stats'\n}\n\nHow exciting! Although not terrifically useful. When I began actually\nimplementing the first couple stages, I noticed that the Pipeline might sync\ndozens of gigabytes of data every time it ran on a new agent in the cluster.\nWhile this problem will soon be solved by the\nExternal\nWorkspace Manager plugin, which is currently being developed. Until it’s ready,\nI chose to mitigate the issue by pinning the execution to a consistent agent.\n\n/* `census` is a node label for a single machine, ideally, which will be\n * consistently used for processing usage statistics and generating census data\n */\nnode('census && docker') {\n    /* .. */\n}\n\nRestricting a workload which previously used multiple agents to a single one\nintroduced the next challenge. As an infrastructure administrator, technically\nspeaking, I could just install all the system dependencies that I want on this\none special Jenkins agent. But what kind of example would that be setting!\n\nThe statistics generation process requires:\n\nJDK8\n\nGroovy\n\nA running MongoDB instance\n\nFortunately, with Pipeline we have a couple of useful features at our disposal:\ntool auto-installers and the\nCloudBees\nDocker Pipeline plugin.\n\nTool Auto-Installers\n\nTool Auto-Installers are exposed in Pipeline through the tool step and on\nci.jenkins.io we already had JDK8 and Groovy\navailable. This meant that the Jenkinsfile would invoke tool and Pipeline\nwould automatically install the desired tool on the agent executing the current\nPipeline steps.\n\nThe tool step does not modify the PATH environment variable, so it’s usually\nused in conjunction with the withEnv step, for example:\n\nnode('census && docker') {\n    /* .. */\n\n    def javaHome = tool(name: 'jdk8')\n    def groovyHome = tool(name: 'groovy')\n\n    /* Set up environment variables for re-using our auto-installed tools */\n    def customEnv = [\n        \"PATH+JDK=${javaHome}/bin\",\n        \"PATH+GROOVY=${groovyHome}/bin\",\n        \"JAVA_HOME=${javaHome}\",\n    ]\n\n    /* use our auto-installed tools */\n    withEnv(customEnv) {\n        sh 'java --version'\n    }\n\n    /* .. */\n}\n\nCloudBees Docker Pipeline plugin\n\nSatisfying the MongoDB dependency would still be tricky. If I caved in and installed\nMongoDB on a single unicorn agent in the cluster, what could I say the next time\nsomebody asked for a special, one-off, piece of software installed on our\nJenkins build agents?\n\nAfter doing my usual complaining and whining, I discovered that the CloudBees\nDocker Pipeline plugin provides the ability to run containers inside of a\nJenkinsfile. To make things even better, there are\nofficial MongoDB docker images readily\navailable on DockerHub!\n\nThis feature requires that the machine has a running Docker daemon which is\naccessible to the user running the Jenkins agent. After that, running a\ncontainer in the background is easy, for example:\n\nnode('census && docker') {\n    /* .. */\n\n    /* Run MongoDB in the background, mapping its port 27017 to our host's port\n     * 27017 so our script can talk to it, then execute our Groovy script with\n     * tools from our `customEnv`\n     */\n    docker.image('mongo:2').withRun('-p 27017:27017') { container ->\n        withEnv(customEnv) {\n            sh \"groovy parseUsage.groovy --logs ${usagestats_dir} --output ${census_dir} --incremental\"\n        }\n    }\n\n    /* .. */\n}\n\nThe beauty, to me, of this example is that you can pass a\nclosure to withRun which will\nexecute while the container is running. When the closure is finished executin,\njust the sh step in this case, the container is destroyed.\n\nWith that system requirement satisfied, the rest of the stages of the Pipeline\nfell into place. We now have a single source of truth, the\nJenkinsfile,\nfor the sequence of dependent tasks which need to be executed, accounting for\nvariations in systems requirements, and it actually generates\nthose pretty\nblue charts!\n\nOf course, a nice added bonus is the beautiful visualization of our\nnew Pipeline!\n\nLinks\n\nPipeline documentation\n\nCloudBees Docker Pipeline plugin documentation\n\nLive statistics Pipeline","title":"Migrating from chained Freestyle jobs to Pipelines","tags":["pipeline","infra"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-06-21T00:00:00.000Z","id":"719def0e-69a3-5e1c-b789-b2d19d5834dd","slug":"/blog/2016/06/21/gsoc-midterm-presentations-ann/","strippedHtml":"As you probably know, on this year Jenkins projects participates in\nGoogle Summer of Code 2016.\nYou can find more information about the accepted projects on the GSoC subproject page and in the\nJenkins Developer mailing list.\n\nOn this week GSoC students are going to present their projects as a part of mid-term evaluation,\nwhich covers one month of community bonding and one month of coding.\n\nWe would like to invite Jenkins developers to attend these meetings.\nThere are two additional months of coding ahead for successful students, so any feedback from Jenkins contributors and users will be appreciated.\n\nMeeting #1 - June 23, 7:00 PM UTC - 9:00 PM UTC\n\nSupport Core plugin improvements by Minudika Malshan\n\nIntro blogpost\n\nExternal Workspace Manager by Alex Somai\n\nIntro blogpost\n\nPlugin documentation publishing to jenkins.io by Cynthia Anyango\n\nIntro blogpost\n\nQ&A session\n\nMeeting link\n\nMeeting #2 - June 24, 8AM UTC - 9 AM UTC\n\nJenkins WebUI: Improving Job Creation/Configuration by Samat Davletshin\n\nIntro blogpost\n\nQ&A session\n\nMeeting link\n\nBoth meetings will be conducted and recorded via Hangouts on Air.\nThe recorded sessions will be made public after the meetup.\nThe agenda may change a bit.\n\nLinks\n\nMid-term presentations announcement on Jenkins Developer mailing list\n\nJenkins GSoC 2016 Wiki Page\n\nJenkins project page on the GSoC2016 website","title":"GSoC: Mid-term presentations by students on June 23 and 24","tags":["core","gsoc","plugin","general"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8c8e8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/bf8e1/oleg_nenashev.png","srcSet":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/914ee/oleg_nenashev.png 32w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/1c9ce/oleg_nenashev.png 64w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/bf8e1/oleg_nenashev.png 128w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/acb7c/oleg_nenashev.png 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/ef6ff/oleg_nenashev.webp 32w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/8257c/oleg_nenashev.webp 64w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/6766a/oleg_nenashev.webp 128w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/22bfc/oleg_nenashev.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"https://oleg-nenashev.github.io/","github":"oleg-nenashev","html":"<div class=\"paragraph\">\n<p>Jenkins core maintainer and board member.\nOleg started using Hudson for Hardware/Embedded projects in 2008 and became an active Jenkins contributor in 2012.\nNowadays he leads several Jenkins <a href=\"/sigs\">SIGs</a>, outreach programs (<a href=\"/projects/gsoc\">Google Summer of Code</a>, <a href=\"/events/hacktoberfest\">Hacktoberfest</a>) and <a href=\"/projects/jam/\">Jenkins meetups</a> in Switzerland and Russia.\nOleg works for <a href=\"https://www.cloudbees.com/\">CloudBees</a> and focuses on key projects in the community.</p>\n</div>","id":"oleg_nenashev","irc":"oleg_nenashev","linkedin":"onenashev","name":"Oleg Nenashev","slug":"/blog/authors/oleg_nenashev","twitter":"oleg_nenashev"}]}},{"node":{"date":"2016-06-16T00:00:00.000Z","id":"9d5b4fb7-1151-5470-985c-045b0dd79455","slug":"/blog/2016/06/16/parallel-test-executor-plugin/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nIn this blog post, I’ll show you how to speed up your pipeline by using the\nParallel Test Executor Plugin.\n\nSo much to do, so little time…​\n\nIn my career, I’ve helped many teams move to continuous integration and delivery. One problem\nwe always encounter is how to run all the tests needed to ensure high-quality\nchanges while still keeping pipeline times reasonable and changes flowing\nsmoothly. More tests mean greater confidence, but also longer wait times.\nBuild systems may or may not support running tests in parallel, but they still only use one\nmachine even while other lab machines sit idle. In these cases, parallelizing\ntest execution across multiple machines is a great way to speed up pipelines.\nThe Parallel Test Executor plugin lets us leverage Jenkins do just that with no\ndisruption to the rest of the build system.\n\nSerial Test Execution\n\nFor this post, I’ll be running a pipeline based on the\nJenkins Git Plugin. I’ve modified\nthe Jenkinsfile from that project to allow us to compare execution times to our\nlater changes, and I’ve truncated the \"mvn\" utility method since it remains\nunchanged.  You can find the original file\nhere.\n\nnode {\n  stage 'Checkout'\n  checkout scm\n\n  stage 'Build'\n\n  /* Call the Maven build without tests. */\n  mvn \"clean install -DskipTests\"\n\n  stage 'Test'\n  runTests()\n\n  /* Save Results. */\n  stage 'Results'\n\n  /* Archive the build artifacts */\n  archive includes: 'target/*.hpi,target/*.jpi'\n}\n\nvoid runTests(def args) {\n  /* Call the Maven build with tests. */\n  mvn \"install -Dmaven.test.failure.ignore=true\"\n\n  /* Archive the test results */\n  junit '**/target/surefire-reports/TEST-*.xml'\n}\n\n/* Run Maven */\nvoid mvn(def args) { /* ... */ }\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit 'https://github.com/jenkinsci/git-plugin.git'.\n\nThis is a Maven project, so the Jenkinsfile is pretty simple.\nI’ve split the Maven build into separate “Build” and “Test”\nstages. Maven doesn’t support this split very well, it wants to run all\nthe steps of the lifecycle in order every time. So, I have to call Maven twice:\nfirst using the “skipTests” property to do only build steps in the first call,\nand then a second time with out that property to run tests.\n\nOn my quad-core machine, executing this pipeline takes about 13 minutes and 30\nseconds.  Of that time, it takes 13 minutes to run about 2.7 thousand tests in\nserial.\n\nParallel Test Execution\n\nThis looks like an ideal project for parallel test execution: a short build\nfollowed by a large number of serially executed tests that consume the most of\nthe pipeline time. There are a number of things I could try to speed this up.\nFor example, I could modify test harness to look for ways to parallelize\nthe test execution on this single machine. Or I could try speed up the tests\nthemselves. Both of those can be time-consuming and both risk destabilizing the\ntests. I’d need to know more about the project to do it well.\n\nI’ll avoid that risk by using Jenkins and the\nParallel Test Executor Plugin to\nparallelize the tests across multiple nodes instead. This will isolate the tests\nfrom each other, while still giving us speed gains from parallel execution.\n\nThe plugin reads the list of tests from the results archived in the previous execution of this\njob and splits that list into a specified number of sublists. I can then use\nthose sublists to execute the tests in parallel, passing a different sublist to\neach node.\n\nLet’s look at how this changes the pipeline:\n\nnode { /* ...unchanged... */ }\n\nvoid runTests(def args) {\n  /* Request the test groupings.  Based on previous test results. */\n  /* see https://wiki.jenkins.io/display/JENKINS/Parallel+Test+Executor+Plugin and demo on github\n  /* Using arbitrary parallelism of 4 and \"generateInclusions\" feature added in v1.8. */\n  def splits = splitTests parallelism: [$class: 'CountDrivenParallelism', size: 4], generateInclusions: true\n\n  /* Create dictionary to hold set of parallel test executions. */\n  def testGroups = [:]\n\n  for (int i = 0; i }. */\n    /*     includes = whether list specifies tests to include (true) or tests to exclude (false). */\n    /*     list = list of tests for inclusion or exclusion. */\n    /* The list of inclusions is constructed based on results gathered from */\n    /* the previous successfully completed job. One additional record will exclude */\n    /* all known tests to run any tests not seen during the previous run.  */\n    testGroups[\"split-${i}\"] = {  // example, \"split3\"\n      node {\n        checkout scm\n\n        /* Clean each test node to start. */\n        mvn 'clean'\n\n        def mavenInstall = 'install -DMaven.test.failure.ignore=true'\n\n        /* Write includesFile or excludesFile for tests.  Split record provided by splitTests. */\n        /* Tell Maven to read the appropriate file. */\n        if (split.includes) {\n          writeFile file: \"target/parallel-test-includes-${i}.txt\", text: split.list.join(\"\\n\")\n          mavenInstall += \" -Dsurefire.includesFile=target/parallel-test-includes-${i}.txt\"\n        } else {\n          writeFile file: \"target/parallel-test-excludes-${i}.txt\", text: split.list.join(\"\\n\")\n          mavenInstall += \" -Dsurefire.excludesFile=target/parallel-test-excludes-${i}.txt\"\n        }\n\n        /* Call the Maven build with tests. */\n        mvn mavenInstall\n\n        /* Archive the test results */\n        junit '**/target/surefire-reports/TEST-*.xml'\n      }\n    }\n  }\n  parallel testGroups\n}\n\n/* Run Maven */\nvoid mvn(def args) { /* ... */ }\n\nThat’s it!  The change is significant but it is all encapsulated in this one\nmethod in the Jenkinsfile.\n\nGreat (ish) Success!\n\nHere’s the results for the new pipeline with parallel test execution:\n\nThe tests ran almost twice as fast, without changes outside pipeline.  Great!\n\nHowever, I used 4 test executors, so why am I not seeing a 4x? improvement.\nA quick review of the logs shows the problem: A small number of tests are taking up\nto 5 minutes each to complete! This is actually good news. It means that I\nshould be able to see further improvement in pipeline throughput just by refactoring\nthose few long running tests into smaller parts.\n\nConclusion\n\nWhile I would like to have seen closer to a 4x improvement to match to number\nof executors, 2x is still perfectly respectable. If I were working on a group of projects\nwith similar pipelines, I’d be completely comfortable reusing these same changes\non my other project and I’d expect to similar improvement without any disruption to\nother tools or processes.\n\nLinks\n\nhttps://wiki.jenkins.io/display/JENKINS/Parallel+Test+Executor+Plugin","title":"Faster Pipelines with the Parallel Test Executor Plugin","tags":["tutorial","pipeline","plugins"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2016-06-15T00:00:00.000Z","id":"92e6efae-588c-5a80-886a-8fe7822dcea3","slug":"/blog/2016/06/15/jenkins-pipeline-scalability/","strippedHtml":"This is a guest post by Damien\nCoraboeuf, Jenkins project contributor and Continuous Delivery consultant.\n\nImplementing a CI/CD solution based on Jenkins has become very easy. Dealing\nwith hundreds of jobs? Not so much. Having to scale to thousands of jobs?\nNow this is a real challenge.\n\nThis is the story of a journey to get out of the jungle of jobs…​\n\nStart of the journey\n\nAt the beginning of the journey there were several projects using roughly the same\ntechnologies. Those projects had several\nbranches, for maintenance of releases, for new features.\n\nIn turn, each of those branches had to be carefully built, deployed on different\nplatforms and versions, promoted so they could be tested for functionalities,\nperformances and security, and then promoted again for actual delivery.\n\nAdditionally, we had to offer the test teams the means to deploy any version of\ntheir choice on any supported platform in order to carry out some manual tests.\n\nThis represented, for each branch, around 20 jobs. Multiply this by the number of\nbranches and projects, and there you are: more than two years after the start\nof the story, we had more than 3500 jobs.\n\n3500 jobs. Half a dozen people to manage them all…​\n\nPreparing the journey\n\nHow did we deal with this load?\n\nWe were lucky enough to have several assets:\n\ntime - we had time to design a solution before the scaling went really out of\ncontrol\n\nforecast - we knew that the scaling would occur and we were not taken by\nsurprise\n\ntooling - the Jenkins Job DSL\nwas available, efficient and well documented\n\nWe also knew that, in order to scale, we’d have to provide a solution with the\nfollowing characteristics:\n\nself-service - we could not have a team of 6 people become a bottleneck for\nenabling CI/CD in projects\n\nsecurity - the solution had to be secure enough in order for it to be used by\nremote developers we never met and didn’t know\n\nsimplicity - enabling CI/CD had to be simple so that people having\nnever heard of it could still use it\n\nextensibility - no solution is a one-size-fits-all and must be flexible\nenough to allow for corner cases\n\nAll the mechanisms described in this article are available through the\nJenkins Seed plugin.\n\nCreating pipelines using the Job DSL and embedding the scripts in the code was\nsimple enough. But what about branching? We needed a mechanism to allow the\ncreation of pipelines per branch, by downloading the associated DSL and to\nrun it in a dedicated folder.\n\nBut then, all those projects, all those branches, they were mostly using the\nsame pipelines, give or take a few configurable items. Going this way would\nhave lead to a terrible duplication of code, transforming a job maintenance\nnightmare into a code maintenance nightmare.\n\nPipeline as configuration\n\nOur trick was to transform this vision of \"pipeline as code\" into a \"pipeline\nas configuration\":\n\nby maintaining well documented and tested \"pipeline libraries\"\n\nby asking projects to describe their pipeline not as code, but as property\nfiles which would:\n\ndefine the name and version of the DSL pipeline library to use\n\nuse the rest of the property file to configure the pipeline library, using\nas many sensible default values as possible\n\nPiloting the pipeline from the SCM\n\nOnce this was done, the only remaining trick was to automate the creation,\nupdate, start and deletion of the pipelines using SCM events. By enabling SCM\nhooks (in GitHub, BitBucket or even in Subversion), we could:\n\nautomatically create a pipeline for a new branch\n\nregenerate a pipeline when the branch’s pipeline description was modified\n\nstart the pipeline on any other commit on the branch\n\nremove the pipeline when the branch was deleted\n\nOnce a project wants to go in our ecosystem, the Jenkins team \"seeds\" the\nproject into Jenkins, by running a job and giving a few parameters.\n\nIt will create a folder for the project and grant proper authorisations, using\nActive Directory group names based on the project name.\n\nThe hook for the project must be registered into the SCM and you’re up and\nrunning.\n\nConfiguration and code\n\nMixing the use of strong pipeline libraries configured by properties and the\ndirect use of the Jenkins Job DSL is still possible. The Seed plugin\nsupports all kinds of combinations:\n\nuse of pipeline libraries only - this can even be enforced\n\nuse a DSL script which can in turn use some classes and methods defined in\na pipeline library\n\nuse of a Job DSL script only\n\nUsually, we tried to have a maximum reuse, through only pipeline libraries, for\nmost of our projects, but in other circumstances, we were less strict and\nallowed some teams to develop their own pipeline script.\n\nEnd of the journey\n\nIn the end, what did we achieve?\n\nSelf service ✔︎\n\nPipeline automation from SCM - no intervention from the Jenkins team but for\nthe initial bootstrapping\n\nGetting a project on board of this system can be done in a few minutes only\n\nSecurity ✔︎\n\nProject level authorisations\n\nNo code execution on the controller\n\nSimplicity ✔︎\n\nProperty files\n\nExtensibility ✔︎\n\nPipeline libraries\n\nDirect job DSL still possible\n\nSeed and Pipeline plugin\n\nNow, what about the Pipeline plugin? Both\nthis plugin and the Seed plugin have common functionalities:\n\nWhat we have found in our journey is that having a \"pipeline as configuration\"\nwas the easiest and most secure way to get a lot of projects on board, with\ndevelopers not knowing Jenkins and even less the DSL.\n\nThe outcome of the two plugins is different:\n\none pipeline job for the Pipeline plugin\n\na list of orchestrated jobs for the Seed plugin\n\nIf time allows, it would be probably a good idea to find a way to integrate the\nfunctionalities of the Seed plugin into the pipeline framework, and to keep\nwhat makes the strength of the Seed plugin:\n\npipeline as configuration\n\nreuseable pipeline libraries, versioned and tested\n\nLinks\n\nYou can find additional information about the Seed plugin and its usage at the\nfollowing links:\n\nthe Seed plugin itself\n\nJUC London, June 2015\n\nBruJUG Brussels, March 2016","title":"Jenkins Pipeline Scalability in the Enterprise","tags":["jenkins","scalability","dsl"],"authors":[{"avatar":null,"blog":null,"github":"dcoraboeuf","html":"<div class=\"paragraph\">\n<p>I&#8217;ve started many years ago in the Java development before switching\nprogressively toward continuous delivery aspects.  I&#8217;m now a consultant\nimplementing CD solutions based on Jenkins. Implementation of the Pipeline\nas Code principles have allowed one of my clients to be able to manage more\nthan 3000 jobs, using a self service approach based on the Seed plugin.</p>\n</div>\n<div class=\"paragraph\">\n<p>I&#8217;m also a contributor for some Jenkins plugins and the author of the\nOntrack application, which allows the monitoring of continuous delivery\npipelines.</p>\n</div>","id":"dcoraboeuf","irc":null,"linkedin":null,"name":"Damien Coraboeuf","slug":"/blog/authors/dcoraboeuf","twitter":"DamienCoraboeuf"}]}},{"node":{"date":"2016-06-14T00:00:00.000Z","id":"06cd0ff0-b842-5785-b6b9-ec7c24f8ea95","slug":"/blog/2016/06/14/jenkins-world-agenda/","strippedHtml":"Join us in Santa Clara, California on September 13-15, 2016!\n\nWe are excited to announce the Jenkins\nWorld agenda is now live. There will be 50+ sessions, keynotes, training,\ncertifications and workshops. Here are a few highlights of what you can expect:\n\nHigh level topics\n\nContinuous delivery\n\nDevOps\n\nMicroservices architectures\n\nTesting\n\nAutomation tools\n\nPlugin development\n\nPipeline\n\nBest practices\n\nAnd much more\n\nAdditionally, Jenkins World offers great opportunities for hands-on learning,\nexploring and networking:\n\nPlugin Development Workshop\n\nDue to its popularity in previous years, we are bringing back the plugin\ndevelopment workshop. This workshop will introduce developers to the Jenkins\nplugin ecosystem and terminology. The goal is to provide a cursory overview of\nthe resources available to Jenkins plugin developers. Armed with this\ninformation, Jenkins developers can learn how to navigate the project and\ncodebase to find answers to their questions.\n\nBirds of a Feather Sessions\n\nBoFs, as they are usually known, will be a new addition to Jenkins World this\nyear. Sessions will be curated on various technical topics from DevOps to how\nenterprises are integrating Jenkins in their environment. Discussions will be\nlead by the industry’s brightest minds who have an influence in shaping the\nfuture of Jenkins.\n\nAsk the Experts\n\nGot a Jenkins question that’s been keeping you up at night? Need to bounce ideas\noff somebody? Or you just need someone to fix your Jenkins issue? This is your chance\nto get connected with the Jenkins Experts. Experts will be on hand to help with\nall your Jenkins needs on Sept 14th & 15th.\n\nPrepare for Jenkins Certification\n\nThe objective of this session is to help you assess your level of readiness for\nthe certification exam - either the Certified Jenkins Engineer (CJE/open source)\ncertification or the Certified CloudBees Jenkins Platform Engineer\n(CCJPE/CloudBees-specific) certification. After an overview about the\ncertification program, a Jenkins expert from CloudBees will walk you through the\nvarious sections of the exam, highlighting the important things to controller ahead\nof time, not only from a pure knowledge perspective but also in terms of\npractical experience. This will be an interactive session.\n\nHope to see you at Jenkins World 2016!\n\nDon’t miss out on\nSuper\nEarly Bird Rate $399. Price goes up after July 1.\n\nLinks\n\nStart a JAM in your city if there isn’t one already.\n\nBecome a JAM member.\n\nBecome an online JAM member\n\nBe a JAM speaker or sponsor. Let us know jenkinsci-jam@googlegroups.com\n\nBecome a Jenkins project contributor","title":"Jenkins World Agenda is Live!","tags":["event"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#281818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg","srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/8d248/alyssat.jpg 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/c004c/alyssat.jpg 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/9e67b/alyssat.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/22924/alyssat.webp 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/89767/alyssat.webp 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/40d97/alyssat.webp 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/5028e/alyssat.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":166}}},"blog":null,"github":"alyssat","html":"<div class=\"paragraph\">\n<p>Member of the <a href=\"/sigs/advocacy-and-outreach/\">Jenkins Advocacy and Outreach SIG</a>.\nAlyssa drives and manages Jenkins participation in community events and conferences like <a href=\"https://fosdem.org/\">FOSDEM</a>, <a href=\"https://www.socallinuxexpo.org/\">SCaLE</a>, <a href=\"https://events.linuxfoundation.org/cdcon/\">cdCON</a>, and <a href=\"https://events19.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2019/\">KubeCon</a>.\nShe is also responsible for Marketing &amp; Community Programs at <a href=\"https://cloudbees.com\">CloudBees, Inc.</a></p>\n</div>","id":"alyssat","irc":null,"linkedin":null,"name":"Alyssa Tong","slug":"/blog/authors/alyssat","twitter":null}]}},{"node":{"date":"2016-06-14T00:00:00.000Z","id":"0e5d650b-c2ea-591f-9965-564c0e2df2c3","slug":"/blog/2016/06/14/gsoc-jenkins-support-core-plugin-improvements/","strippedHtml":"About me\n\nI am Minudika Malshan, an undergraduate student in Computer Science and Engineering from University of Moratuwa, Sri Lanka.\n\nAs a person who is passionate in open source software development and seeking for new knowledge and experience, I am willing to give my contribution for this project.\n\nLinkedIn | Twitter\n\nAbstract\n\nThe Support-Core Plugin provides the basic infrastructure for generating \"bundles\" of support information with Jenkins.\nThere are two kinds of bundles.\n\nAutomatic bundles: Bundles which are generated and get saved in $JENKINS_HOME/support once per hour starting 15 seconds after Jenkins starts the plugin.\nThe automatic bundles are retained using an exponential aging strategy. Therefore it’s possible to have a bunch of them over the entire lifetime after the plugin installing the plugin.\n\nOn demand bundles: These bundles are generated from the root \"Support\" action.\n\nHowever current support-core plugin is not much user friendly. The object of this project is to make it more user friendly by adding some features which make a sophisticated environment for the user who use support plugin.\n\nIn this project scope, there are three features and improvements we are going to consider.\n\nEase the bundles management by the administrator ( JENKINS-33090)\n\nAdding an option to anonymize customer labels (strings created by the user such as name of a job, folder, view, agent, and template etc). ( JENKINS-33091)\n\nAllowing user to create an issue and submit a bundle into the OSS tracker using the support-core plugin. ( JENKINS-21670)\n\nArnaud Héritier and Steven Christou are guiding me through the project as my mentors.\n\nTasks and Deliverables\n\nEase the bundles management by the administrator.\n\nUnder this task, the following functions are going to be implemented.\n\nListing bundles stored on the jenkins instance with their details.\n\nAllowing user to download each bundle.\n\nAllowing user to delete each bundle or all bundles.\n\nAllowing user to browse the content of each bundle.\n\nAutomatically purging old bundles.\n\nEnabling user to create an issue and submit a bundle into the OSS tracker\n\nWhen a Jenkins user sees an issue, he/she commonly contacts his support contacts (Jenkins instance admins) and then Jenkins admins troubleshoot the issue.\nThe objective of this task is to implement a feature which enables the user to report an issue to a admin through support core plugin.\n\nWhen creating bundles to attach with the ticket, it is important to protect the privacy of the user who creates the ticket. When considering doing that, anonymizing user created labels (texts) comes to the front.\n\nAdding  an option to anonymize customer labels\n\nThe following functions will be implemented under this taks.\n\nCreating randomized tokens for labels created by users.\n\nProducing a mapping for those labels.\n\nSubstituting encoded labels into all the files included in the support bundle.\n\nWhen creating randomized tokens, it would be much useful and effective if we can create those tokens in a way they make sense to humans. (i.e. readable to humans). For that, I am hoping to use a suitable java library to create human friendly random tokens. One of such libraries is wordnet-random-name.\n\nHowever in order to substitute randomized tokens, all files included in the bundle should be read. This can become inefficient when bundle consists of large number of files.  Therefore it’s important to follow an optimized method for this task.\n\nReferences\n\nInitial proposal of the project\n\nProject repository","title":"GSoC Project Intro: Support Core Plugin Improvements","tags":["core","gsoc","plugin","support-core"],"authors":[{"avatar":null,"blog":null,"github":"minudika","html":"","id":"minudika","irc":null,"linkedin":null,"name":"Minudika Malshan","slug":"/blog/authors/minudika","twitter":"minudika"}]}}]}},"pageContext":{"limit":8,"skip":392,"numPages":100,"currentPage":50}},
    "staticQueryHashes": ["3649515864"]}