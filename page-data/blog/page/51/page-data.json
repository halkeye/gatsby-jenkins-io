{
    "componentChunkName": "component---src-templates-blog-list-template-js",
    "path": "/blog/page/51",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2016-05-31T00:00:00.000Z","id":"ad068db4-bb27-52c3-9916-bb8b132c6e53","slug":"/blog/2016/05/31/pipeline-snippetizer/","strippedHtml":"Those of you updating the Pipeline Groovy plugin\nto 2.3 or later will notice a change to the appearance of the configuration form.\nThe Snippet Generator tool is no longer a checkbox enabled inside the configuration page.\nRather, there is a link Pipeline Syntax which opens a separate page with several options.\n(The link appears in the project’s sidebar; Jenkins 2 users will not see the sidebar from the configuration screen,\nso as of 2.4 there is also a link beneath the Pipeline definition.)\n\nSnippet Generator continues to be available for learning the available\nPipeline steps and creating sample calls given various configuration options.\nThe new page also offers clearer links to static reference documentation, online\nPipeline documentation resources, and an IntelliJ IDEA code completion file\n(Eclipse support is unfinished).\n\nOne motivation for this change\n( JENKINS-31831) was to\ngive these resources more visual space and more prominence.  But another\nconsideration was that people using multibranch projects or organization folders\nshould be able to use Snippet Generator when setting up the project, before\nany code is committed.\n\nThose using\nPipeline\nMultibranch plugin or organization folder plugins should upgrade to 2.4 or\nlater to see these improvements as well.","title":"New display of Pipeline’s \"snippet generator\"","tags":["pipeline"],"authors":[{"avatar":null,"blog":null,"github":"jglick","html":"<div class=\"paragraph\">\n<p>Jesse has been developing Jenkins core and plugins for years.\nHe is the coauthor with Kohsuke of the core infrastructure of the Pipeline system.</p>\n</div>","id":"jglick","irc":null,"linkedin":null,"name":"Jesse Glick","slug":"blog/author/jglick","twitter":"tyvole"}]}},{"node":{"date":"2016-05-26T00:00:00.000Z","id":"4a650522-ee39-5465-887f-963c348a9754","slug":"/blog/2016/05/26/gsoc-jenkins-web-ui-project/","strippedHtml":"About me\n\nMy name is Samat Davletshin and I am from HSE University from Moscow, Russia. I\ninterned at Intel and Yandex, and cofounded a startup\nproject where I personally developed front-end and back-end of the website.\n\nI am excited to participate in GSoC with Jenkins this summer as a chanсe to make\na positive change for thousands of users as well as to learn from great mentors.\n\nAbstract\n\nAlthough powerful, Jenkins new job creation and configuration process may be non\nobvious and time consuming. This can be improved by making UI more intuitive,\nconcise, and functional. I plan to achieve this by creating a simpler new job\ncreation, configuration process focused on essential elements, and embedding new\nfunctionality.\n\nMy mentors are Kirill Merkushev and\nMichael Neale\n\nDeliverables\n\nNew job creation\n\nNew job name validation\n\nInitially, job validation was unresponsive, job creation was still allowed with\nan invalid name, and some allowed characters even crashed Jenkins. Happily, two\nof this problems were fixed in recent improvements and I plan add only a real\ntime name check for invalid characters.\n\nPopup window\n\nJenkins has a lot of windows reloads that may time consuming. The creation of\nnew job is a simple process requiring only job name and job type. This way UI\nmay be improved by reducing page reloads and putting new job creation interface\nin a dialog window. Such popup would likely consist of three steps of\nimplementation: rendering a dialog window, receiving JSON with job types,\nsending a POST request to create the job.\n\nConfiguration page\n\nChanging help information\n\nAs reported by some users, it would be useful to have the functionality to\nchange help information. Installation administrators would be able to change the\nhelp info and choose editing rights for other users. That would likely require a\ncreation of extension points and a plugin using them. I also would like to\ninclude the ability to style the help information using markdown as shown above.\n\n[Optional] The functionality is extended to creation of crowd sourced \"wiki like\" documentation\n\nAs in\nlocalization\nplugin the changes are gathered and applied beyond installation of a particular\nuser.\n\nMore intuitive configuration page.\n\nPursuing to solve this  issue\n\nAlthough there are a lot improvements in new configuration page, there is always\na room for improvements. An advanced job still has a very complicated and hard\nto read configuration page. It is still open to discussion, but I may approach\nit by better division of configuration parts such as an accordion based\nnavigation.\n\nHome page\n\n[Optional] Removing \"My Views\" page\n\n\"My Views\" page may unnecessary complicate essential sidepanel navigation. Since\nit contains very small functionality, the functions may be moved to the home\npage and the whole page may be removed. That may be implemented by adding icons\nto \"My Views\" tabs. Additionally, the standard view creation page can create\neither of the types\n\n[Optional] Reducing number of UI elements\n\nThe home page may contain some UI elements that are not essential and rarely\nused. This way elements \"enable auto refresh\", “edit description”, “icon sizes”,\n”legend”, “RSS” may be removed from home page and placed under \"Manage Jenkins\"\nor an upper menu. It is also possible to create new extension points to support\nnew UI elements through plugins.\n\nCredentials store page\n\n[Optional] Grouping credentials and their domains\n\nCredentials page has too many reloads and requires many clicks to get to a\nrequired credentials page. That may be improved by removing the last page and\nshowing credentials under domains.\n\nCurrent progress\n\nBy May 25th I learned about the structure and tools of Jenkins and started\nworking on the first project:\n\nI started with New Job Name validation first. Luckily, in last updates the\nchanges of recena there\nwere implemented all of the changes I proposed except real time check on name\nvalidity. Here I proposed the change which fixes it by\nsending GET request on keyup event in addition to blur.\n\nI also made a New Job Popup with using existing interface.\n\nView the current\npop-up progress\n\nI used Remodal library for popup and put\nthere\nexisting\nNew Job container. Surprisingly, it was fully functional right away. On the GIF\nyou can see that popup receives all job types and then successfully submits the\npost form creating a new job. I think that could be a good first step. Further I\ncan start changing the window itself.\n\nLinks\n\nInitial proposal of the project\n\nThe project discussion on mailing list\n\nJenkins GSoC Page\n\nProject repository","title":"GSoC Project Intro: Improving Job Creation/Configuration","tags":["core","gsoc"],"authors":[{"avatar":null,"blog":null,"github":"samatdav","html":"","id":"samatdav","irc":null,"linkedin":null,"name":"Samat Davletshin","slug":"blog/author/samatdav","twitter":null}]}},{"node":{"date":"2016-05-26T00:00:00.000Z","id":"bbd7c8d9-ad20-56ae-b97b-834547d227f7","slug":"/blog/2016/05/26/introducing-blue-ocean/","strippedHtml":"In recent years developers have become rapidly attracted to tools that are not\nonly functional but are designed to fit into their workflow seamlessly and are\na joy to use. This shift represents a higher standard of design and user\nexperience that Jenkins needs to rise to meet.\n\nWe are excited to share and invite the community to join us on a project we’ve\nbeen thinking about over the last few months called Blue Ocean.\n\nBlue Ocean is a project that rethinks the user experience of Jenkins, modelling\nand presenting the process of software delivery by surfacing information that’s\nimportant to development teams with as few clicks as possible, while still\nstaying true to the extensibility that is core to Jenkins.\n\nWhile this project is in the alpha stage of development, the intent is that\nJenkins users can install Blue Ocean side-by-side with the Jenkins Classic UI\nvia a plugin.\n\nNot all the features listed on this blog are complete but we will be hard at\nwork over the next few months preparing Blue Ocean for general use. We intend\nto provide regular updates on this blog as progress is made.\n\nBlue Ocean is open source today\nand we invite you to give us feedback and to contribute to the project.\n\nBlue Ocean will provide development teams:\n\nNew modern user experience\n\nThe UI aims to improve clarity, reduce clutter and navigational depth to make\nthe user experience very concise. A modern visual design gives developers much\nneeded relief throughout their daily usage and screens respond instantly to\nchanges on the server making manual page refreshes a thing of the past.\n\nAdvanced Pipeline visualisations with built-in failure diagnosis\n\nPipelines are visualised on screen along with the\nsteps and logs to allow simplified comprehension of the continuous delivery\npipeline – from the simple to the most sophisticated scenarios.\n\nScrolling through 10,000 line log files is a thing of the past. Blue Ocean\nbreaks down your log per step and calls out where your build failed.\n\nBranch and Pull Request awareness\n\nModern pipelines make use of multiple Git branches, and Blue Ocean is designed\nwith this in mind. Drop a Jenkinsfile into your Git\nrepository that defines your pipeline and Jenkins will automatically discover\nand start automating any  Branches and validating Pull Requests.\n\nJenkins will report the status of your pipeline right inside Github or\nBitbucket on all your commits, branches or pull requests.\n\nPersonalised View\n\nFavourite any pipelines, branches or pull requests and see them appear on your\npersonalised dashboard. Intelligence is being built into the dashboard. Jobs\nthat need your attention, say a Pipeline waiting for approval or a failing job\nthat you have recently changed, appear on the top of the dashboard.\n\nYou can read more about Blue Ocean and its goals on the\nproject page and developers should watch the\nDevelopers list for more information.\n\nFor Jenkins developers and plugin authors:\n\nJenkins Design “Language”\n\nThe Jenkins Design Language (JDL) is a set of standardised React components and\na style guide that help developers create plugins that retain the look and feel\nof Blue Ocean in an effortless way. We will be publishing more on the JDL,\nincluding the style guide and developer documentation, over the next few weeks.\n\nModern JavaScript toolchain\n\nThe Jenkins plugin tool chain has been extended so that developers can use\nES6,\nReact, NPM\nin their plugins without endless yak-shaving. Jenkins\njs-modules are already in use in\nJenkins today, and this builds on this, using the same tooling.\n\nClient side Extension points\n\nClient Side plugins use Jenkins plugin infrastructure. The Blue Ocean libraries\nbuilt on ES6 and React.js provide an extensible client side component model\nthat looks familiar to developers who have built Jenkins plugins before. Client\nside extension points can help isolate failure, so one bad plugin doesn’t take\na whole page down.\n\nServer Sent Events\n\nServer Sent Events\n(SSE) allow plugin developers to tap into changes of state on the server and make\ntheir UI update in real time ( watch this for a\ndemo).\n\nTo make Blue Ocean a success, we’re asking for help and support from Jenkins\ndevelopers and plugin authors. Please join in our Blue Ocean discussions on the\nJenkins Developer\nmailing list and the #jenkins-ux IRC channel on Freenode!\n\nLinks\n\nBlue Ocean project page\n\nBlue Ocean GitHub repository","title":"Introducing Blue Ocean: a new user experience for Jenkins","tags":["blueocean","ux","pipeline"],"authors":[{"avatar":null,"blog":null,"github":"i386","html":"","id":"i386","irc":null,"linkedin":null,"name":"James Dumay","slug":"blog/author/i386","twitter":"i386"}]}},{"node":{"date":"2016-05-25T00:00:00.000Z","id":"3bdee89e-566b-52d9-a3cc-f41507becadc","slug":"/blog/2016/05/25/update-plugin-for-pipeline/","strippedHtml":"This is a guest post by Chris Price.\nChris is a software engineer at Puppet, and has been\nspending some time lately on automating performance testing using the latest\nJenkins features.\n\nIn this blog post, I’m going to attempt to provide some step-by-step notes on\nhow to refactor an existing Jenkins plugin to make it compatible with the new\nJenkins Pipeline jobs.  Before we get to the fun stuff, though, a little\nbackground.\n\nHow’d I end up here?\n\nRecently, I started working on a project to automate some performance tests for\nmy company’s products.  We use the awesome Gatling load\ntesting tool for these tests, but we’ve largely been handling the testing very\nmanually to date, due to a lack of bandwidth to get them automated in a clean,\nmaintainable, extensible way.  We have a years-old Jenkins server where we use\nthe gatling jenkins\nplugin to track the\nhistory of certain tests over time, but the setup of the Jenkins instance was\nvery delicate and not easy to reproduce, so it had fallen into a state of\ndisrepair.\n\nOver the last few days I’ve been putting some effort into getting things more\nautomated and repeatable so that we can really maximize the value that we’re\ngetting out of the performance tests.  With some encouragement from the fine\nfolks in the #jenkins IRC channel, I ended up exploring\nthe JobDSL\nplugin and the new Pipeline jobs.  Combining those two\nthings with some Puppet code to provision a Jenkins server via the\njenkins puppet module gave me\na really nice way to completely automate my Jenkins setup and get a seed job in\nplace that would create my perf testing jobs.  And the Pipeline job format is\njust an awesome fit for what I wanted to do in terms of being able to easily\nmonitor the stages of my performance tests, and to make the job definitions\nmodular so that it would be really easy to create new performance testing jobs\nwith slight variations.\n\nSo everything’s going GREAT up to this point.  I’m really happy with how it’s\nall shaping up.  But then…​ (you knew there was a \"but\" coming, right?) I\nstarted trying to figure out how to add the\nGatling Jenkins\nplugin to the Pipeline jobs, and kind of ran into a wall.\n\nAs best as I could tell from my Googling, the plugin was probably going to\nrequire some modifications in order to be able to be used with Pipeline jobs.\nHowever, I wasn’t able to find any really cohesive documentation that\ndefinitively confirmed that or explained how everything fits together.\n\nEventually, I got it all sorted out.  So, in hopes of saving the next person a\nlittle time, and encouraging plugin authors to invest the time to get their\nplugins working with Pipeline, here are some notes about what I learned.\n\nSpoiler: if you’re just interested in looking at the individual git commits that\nI made on may way to getting the plugin working with Pipeline, have a look at\nthis github\nbranch.\n\nCreating a pipeline step\n\nThe main task that the Gatling plugin performs is to archive Gatling reports\nafter a run.  I figured that the end game for this exercise was that I was going\nto end up with a Pipeline \"step\" that I could include in my Pipeline scripts, to\ntrigger the archiving of the reports.  So my first thought was to look for an\nexisting plugin / Pipeline \"step\" that was doing something roughly similar, so\nthat I could use it as a model.  The Pipeline \"Snippet Generator\" feature\n(create a pipeline job, scroll down to the \"Definition\" section of its\nconfiguration, and check the \"Snippet Generator\" checkbox) is really helpful for\nfiguring out stuff like this; it is automatically populated with all of the\nsteps that are valid on your server (based on which plugins you have installed),\nso you can use it to verify whether or not your custom \"step\" is recognized, and\nalso to look at examples of existing steps.\n\nLooking through the list of existing steps, I figured that the archive step\nwas pretty likely to be similar to what I needed for the gatling plugin:\n\nSo, I started poking around to see what magic it was that made that archive\nstep show up there.  There are some mentions of this in the\npipeline-plugin\nDEVGUIDE.md and the\nworkflow-step-api-plugin\nREADME.md, but the real breakthrough for me was finding the definition of the\narchive step in the workflow-basic-steps-plugin source\ncode.\n\nWith that as an example, I was able to start poking at getting a\ngatlingArchive step to show up in the Snippet Generator.  The first thing that\nI needed to do was to update the gatling-plugin project’s pom.xml to depend\non a recent enough version of Jenkins, as well as specify dependencies on the\nappropriate pipeline\nplugins\n\nOnce that was out of the way, I noticed that the archive step had some tests\nwritten for it, using what looks to be a pretty awesome test API for pipeline\njobs and plugins.  Based on those archive\ntests,\nI added\na\nskeleton for a test for the gatlingArchive step that I was about to write.\n\nThen, I moved on to\nactually\ncreating the step.  The meat of the code was this:\n\npublic class GatlingArchiverStep extends AbstractStepImpl {\n    @DataBoundConstructor\n    public GatlingArchiverStep() {}\n\n    @Extension\n    public static class DescriptorImpl extends AbstractStepDescriptorImpl {\n        public DescriptorImpl() { super(GatlingArchiverStepExecution.class); }\n\n        @Override\n        public String getFunctionName() {\n            return \"gatlingArchive\";\n        }\n\n        @NonNull\n        @Override\n        public String getDisplayName() {\n            return \"Archive Gatling reports\";\n        }\n    }\n}\n\nNote that in that commit I also added a config.jelly file.  This is how you\ndefine the UI for your step, which will show up in the Snippet Generator.  In\nthe case of this Gatling step there’s really not much to configure, so my\nconfig.jelly is basically empty.\n\nWith that (and the rest of the code from that commit) in place, I was able to\nfire up the development Jenkins server (via mvn hpi:run, and note that you\nneed to go into the \"Manage Plugins\" screen on your development server and\ninstall the Pipeline plugin once before any of this will work) and visit the\nSnippet Generator to see if my step showed up in the dropdown:\n\nGREAT SUCCESS!\n\nThis step doesn’t actually do anything yet, but it’s recognized by Jenkins and\ncan be included in your pipeline scripts at that point, so, we’re on our way!\n\nThe step metastep\n\nThe step that we created above is a first-class DSL addition that can be used in\nPipeline scripts.  There’s another way to make your plugin work usable from a\nPipeline job, without making it a first-class build step.  This is by use of the\nstep\"metastep\", mentioned in the pipeline-plugin\nDEVGUIDE.\nWhen using this approach, you simply refactor your Builder or Publisher to\nextend SimpleBuildStep, and then you can reference the build step from the\nPipeline DSL using the step method.\n\nIn the Jenkins GUI, go to the config screen for a Pipeline job and click on the\nSnippet Generator checkbox.  Select 'step: General Build Step' from the\ndropdown, and then have a look at the options that appear in the 'Build Step'\ndropdown.  To compare with our previous work, let’s see what \"Archive the\nartifacts\" looks like:\n\nFrom the snippet generator we can see that it’s possible to trigger an Archive\naction with syntax like:\n\nstep([$class: 'ArtifactArchiver', artifacts: 'foo*', excludes: null])\n\nThis is the \"metastep\".  It’s a way to trigger any build action that implements\nSimpleBuildStep, without having to actually implement a real \"step\" that\nextends the Pipeline DSL like we did above.  In many cases, it might only make\nsense to do one or the other in your plugin; you probably don’t really need\nboth.\n\nFor the purposes of this tutorial, we’re going to do both.  For a couple of reasons:\n\nWhy the heck not?  :)  It’s a good demonstration of how the metastep stuff\nworks.\n\nBecause implementing the \"for realz\" step will be a lot easier if the Gatling\naction that we’re trying to call from our gatlingArchive() syntax is using the\nnewer Jenkins APIs that are required for subclasses of SimpleBuildStep.\n\nGatlingPublisher is the main build action that we’re interested in using in\nPipeline jobs.  So, with all of that in mind, here’s our next goal: get\nstep([$class: 'GatlingPublisher', …​) showing up in the Snippet Generator.\n\nThe javadocs for the SimpleBuildStep\nclass\nhave some notes on what you need to do when porting an existing Builder or\nPublisher over to implement the SimpleBuildStep interface.  In all\nlikelihood, most of what you’re going to end up doing is to replace occurrences\nof AbstractBuild with references to the Run class, and replace occurrences\nof AbstractProject with references to the Job class.  The APIs are pretty\nsimilar, so it’s not too hard to do once you understand that that’s the game.\nThere is some discussion of this in the pipeline-plugin\nDEVGUIDE.\n\nFor the Gatling plugin, my\ninitial\nefforts to port the GatlingPublisher over to implement SimpleBuildStep only\nrequired the AbstractBuild → Run refactor.\n\nAfter making these changes, I fired up the development Jenkins server, and, voila!\n\nSo, now, we can add a line like this to a Pipeline build script:\n\nstep([$class: 'GatlingPublisher', enabled: true])\n\nAnd it’ll effectively be the same as if we’d added the Gatling \"Post-Build\nAction\" to an old-school Freestyle project.\n\nWell…​ mostly.\n\nBuild Actions vs. Project Actions\n\nAt this point our modified Gatling plugin should work the same way as it always\ndid in a Freestyle build, but in a Pipeline build, it only partially works.\nSpecifically, the Gatling plugin implements two different \"Actions\" to surface\nthings in the Jenkins GUI: a \"Build\" action, which adds the Gatling icon to the\nleft sidebar in the GUI when you’re viewing an individual build in the build\nhistory of a job, and a \"Project\" action, which adds that same icon to the left\nsidebar of the GUI of the main page for a job.  The \"Project\" action also adds a\n\"floating panel\" on the main job page, which shows a graph of the historical\ndata for the Gatling runs.\n\nIn a Pipeline job, though, assuming we’ve added a call to the metastep, we’re\nonly seeing the \"Build\" actions.  Part of this is because, in the last round of\nchanges that I linked, we only modified the \"Build\" action, and not the\n\"Project\" action.  Running the metastep in a Pipeline job has no visible effect\nat all on the project/job page at this point.  So that’s what we’ll tackle next.\n\nThe key thing to know about getting \"Project\" actions working in a Pipeline job\nis that, with a Pipeline job, there is no way for Jenkins to know up front what\nsteps or actions are going to be involved in a job.  It’s only after the job\nruns once that Jenkins has a chance to introspect what all the steps were.  As\nsuch, there’s no list of Builders or Publishers that it knows about up front to\ncall getProjectAction on, like it would with a Freestyle job.\n\nThis is where\nSimpleBuildStep.LastBuildAction\ncomes into play.  This is an interface that you can add to your Build actions,\nwhich give them their own getProjectActions method that Jenkins recognizes and\nwill call when rendering the project page after the job has been run at least\nonce.\n\nSo, effectively, what we need to do is to\nget\nrid of the getProjectAction method on our Publisher class, modify the Build\naction to implement SimpleBuildStep.LastBuildAction, and encapsulate our\nProject action instances in the Build action.\n\nThe build action class now constructs an instance of the Project action and\nmakes it accessible via getProjectActions (which comes from the\nLastBuildAction interface):\n\npublic class GatlingBuildAction implements Action, SimpleBuildStep.LastBuildAction {\n    public GatlingBuildAction(Run build, List sims) {\n        this.build = build;\n        this.simulations = sims;\n\n        List projectActions = new ArrayList<>();\n        projectActions.add(new GatlingProjectAction(build.getParent()));\n        this.projectActions = projectActions;\n    }\n\n    @Override\n    public Collection getProjectActions() {\n        return this.projectActions;\n    }\n}\n\nAfter making these changes, if we run the development Jenkins server, we can see\nthat after the first successful run of the Pipeline job that calls the\nGatlingPublisher metastep, the Gatling icon indeed shows up in the sidebar on\nthe main project page, and the floating box with the graph shows up as well:\n\nMaking our DSL step do something\n\nSo at this point we’ve got the metastep syntax working from end-to-end, and\nwe’ve got a valid Pipeline DSL step ( gatlingArchive()) that we can use in our\nPipeline scripts without breaking anything…​ but our custom step doesn’t\nactually do anything.  Here’s the part where we tie it all together…​ and it’s\npretty easy!  All we need to do is to make our step \"Execution\" class\ninstantiate a Publisher and call perform on\nit.\n\nAs per the\nnotes\nin the pipeline-plugin DEVGUIDE, we can use the @StepContextParameter\nannotation to inject in the objects that we need to pass to the Publisher’s\nperform method:\n\npublic class GatlingArchiverStepExecution extends AbstractSynchronousNonBlockingStepExecution {\n\n    @StepContextParameter\n    private transient TaskListener listener;\n\n    @StepContextParameter\n    private transient FilePath ws;\n\n    @StepContextParameter\n    private transient Run build;\n\n    @StepContextParameter\n    private transient Launcher launcher;\n\n    @Override\n    protected Void run() throws Exception {\n        listener.getLogger().println(\"Running Gatling archiver step.\");\n\n        GatlingPublisher publisher = new GatlingPublisher(true);\n        publisher.perform(build, ws, launcher, listener);\n\n        return null;\n    }\n}\n\nAfter these changes, we can fire up the development Jenkins server, and hack up\nour Pipeline script to call gatlingArchive() instead of the metastep\nstep([$class: 'GatlingPublisher', enabled: true]) syntax.  One of these is\nnicer to type and read than the other, but I’ll leave that as an exercise for\nthe reader.\n\nFin\n\nWith that, our plugin now works just as well in the brave new Pipeline world as\nit did in the olden days of Freestyle builds.  I hope these notes save someone\nelse a little bit of time and googling on your way to writing (or porting) an\nawesome plugin for Jenkins Pipeline jobs!\n\nLinks\n\nJenkins Pipeline Overview\n\nPipeline Plugin Developer Guide\n\nJenkins Source Code\n\nWorkflow Step API Plugin\n\nWorkflow Basic Steps Plugin","title":"Refactoring a Jenkins plugin for compatibility with Pipeline jobs","tags":["core","pipeline","plugins"],"authors":[{"avatar":null,"blog":null,"github":"cprice404","html":"<div class=\"paragraph\">\n<p>Chris is a software engineer at Puppet, who mostly works on backend services\nfor Puppet itself, but occasionally gets to spend some time improving CI\nand automation using Jenkins.</p>\n</div>","id":"cprice404","irc":null,"linkedin":null,"name":"Chris Price","slug":"blog/author/cprice404","twitter":"cprice404"}]}},{"node":{"date":"2016-05-23T00:00:00.000Z","id":"85257451-d116-575c-b893-3c51d7386caa","slug":"/blog/2016/05/23/external-workspace-manager-plugin/","strippedHtml":"About myself\n\nMy name is Alexandru Somai.\nI’m following a major in Software Engineering at the Babes-Bolyai University of Cluj-Napoca, Romania.\nI have more than two years hands-on experience working in Software Development.\n\nI enjoy writing code in Java, Groovy and JavaScript.\nThe technologies and frameworks that I’m most familiar with are: Spring Framework, Spring Security, Hibernate,\nJMS, Web Services, JUnit, TestNG, Mockito.\nAs build tools and continuous integration, I’m using Maven and Jenkins.\nI’m a passionate software developer who is always learning, always looking for new challenges.\nI want to start contributing to the open source community and Google Summer of Code is a starting point for me.\n\nProject summary\n\nCurrently, Jenkins’ build workspace may become very large in size due to the fact that some compilers generate\nvery large volumes of data.\nThe existing plugins that share the workspace across builds are able to do this by copying the files from\none workspace to another, process which is inefficient.\nA solution is to have a Jenkins plugin that is able to manage and reuse the same workspace between multiple builds.\n\nAs part of the Google Summer of Code 2016 I will be working on\nthe External Workspace Manager plugin.\nMy mentors for this project are Oleg Nenashev\nand Martin d’Anjou.\nThis plugin aims to provide an external workspace management system.\nIt should facilitate workspace share and reuse across multiple Jenkins jobs.\nIt should eliminate the need to copy, archive or move files.\nThe plugin will be written for Pipeline jobs.\n\nUsage\n\nPrerequisites\n\nMultiple physical disks accessible from controller.\n\nThe same physical disks must be accessible from Jenkins Nodes (renamed to Agents in Jenkins 2.0).\n\nIn the Jenkins global configuration, define a disk pool (or many) that will contain the physical disks.\n\nIn each Node configuration, define the mounting point from the current node to each physical disk.\n\nThe following diagram gives you an overview of how an External Workspace Manager configuration may look like:\n\nExample one\n\nLet’s assume that we have one Jenkins job. In this job, we want to use the same workspace on multiple Jenkins nodes.\nOur pipeline code may look like this:\n\nstage ('Stage 1. Allocate workspace')\ndef extWorkspace = exwsAllocate id: 'diskpool1'\n\nnode ('linux') {\n    exws (extWorkspace) {\n        stage('Stage 2. Build on the build server')\n        git url: '...'\n        sh 'mvn clean install'\n    }\n}\n\nnode ('test') {\n    exws (extWorkspace) {\n        stage('Stage 3. Run tests on a test machine')\n        sh 'mvn test'\n    }\n}\n\nNote: The stage() steps are optional from the External Workspace Manager plugin perspective.\n\nStage 1. Allocate workspace\n\nThe exwsAllocate step selects a disk from diskpool1\n(default behavior: the disk with the most available size).\nOn that disk, let’s say disk1, it allocates a directory.\nThe computed directory path is: /physicalPathOnDisk/$JOB_NAME/$BUILD_NUMBER.\n\nFor example, Let’s assume that the $JOB_NAME is integration and the $BUILD_NUMBER is 14.\nThen, the resulting path is: /jenkins-project/disk1/integration/14.\n\nStage 2. Build on the build server\n\nAll the nodes labeled linux must have access to the disks defined in the disk pool.\nIn the Jenkins Node configurations we have defined the local paths that are the mounting points to each disk.\n\nThe exws step concatenates the node’s local path with the path returned by the exwsAllocate step.\nIn our case, the node labeled linux has its local path to disk1 defined as: /linux-node/disk1/.\nSo, the complete workspace path is: /linux-node/disk1/jenkins-project/disk1/integration/14.\n\nStage 3. Run tests on a test machine\n\nFurther, we want to run our tests on a different node, but we want to reuse the previously created workspace.\n\nIn the node labeled test we have defined the local path to disk1 as: /test-node/disk1/.\nBy applying the exws step, our tests will be able to run in the same workspace as the build.\nTherefore, the path is: /test-node/disk1/jenkins-project/disk1/integration/14.\n\nExample two\n\nLet’s assume that we have two Jenkins jobs, one called upstream and the other one called downstream.\nIn the upstream job, we clone the repository and build the project, and in the downstream job we run the tests.\nIn the downstream job we don’t want to clone and re-build the project, we need to use the same\nworkspace created in the upstream job.\nWe have to be able to do so without copying the workspace content from one location to another.\n\nThe pipeline code in the upstream job is the following:\n\nstage ('Stage 1. Allocate workspace in the upstream job')\ndef extWorkspace = exwsAllocate id: 'diskpool1'\n\nnode ('linux') {\n    exws (extWorkspace) {\n        stage('Stage 2. Build in the upstream job')\n           git url: '...'\n           sh 'mvn clean install'\n    }\n}\n\nAnd the downstream 's pipeline code is:\n\nstage ('Stage 3. Allocate workspace in the downstream job')\ndef extWorkspace = exwsAllocate id: 'diskpool1', upstream: 'upstream'\n\nnode ('test') {\n    exws (extWorkspace) {\n        stage('Stage 4. Run tests in the downstream job')\n        sh 'mvn test'\n    }\n}\n\nStage 1. Allocate workspace in the upstream job\n\nThe functionality is the same as in example one - stage 1.\nIn our case, the allocated directory on the physical disk is: /jenkins-project/disk1/upstream/14.\n\nStage 2. Build in the upstream job\n\nSame functionality as example one - stage 2.\nThe final workspace path is: /linux-node/disk1/jenkins-project/disk1/upstream/14.\n\nStage 3. Allocate workspace in the downstream job\n\nBy passing the upstream parameter to the exwsAllocate step,\nit selects the most recent stable upstream workspace (default behavior).\nThe workspace path pattern is like this: /physicalPathOnDisk/$UPSTREAM_NAME/$MOST_RECENT_STABLE_BUILD.\nLet’s assume that the last stable build number is 12, then the resulting path is:\n/jenkins-project/disk1/upstream/12.\n\nStage 4. Run tests in the downstream job\n\nThe exws step concatenates the node’s local path with the path returned by the exwsAllocate step in stage 3.\nIn this scenario, the complete path for running tests is: /test-node/disk1/jenkins-project/disk1/upstream/12.\nIt will reuse the workspace defined in the upstream job.\n\nAdditional details\n\nYou may find the complete project proposal, along with the design details, features, more examples and use cases,\nimplementation ideas and milestones in the design document.\nThe plugin repository will be available on GitHub.\n\nA prototype version of the plugin should be available in late June and the releasable version in late August.\nI will be holding plugin functionality demos within the community.\n\nI do appreciate any feedback.\nYou may add comments in the design document.\nIf you are interested to have a verbal conversation, feel free to join our regular meetings on Mondays at\n12:00 PM UTC\non the Jenkins hangout.\nI will be posting updates from time to time about the plugin status on the\nJenkins developers mailing list.\n\nLinks\n\nDesign document\n\nGSoC program\n\nJenkins GSoC Page\n\nProject repository","title":"GSoC Project Intro: External Workspace Manager Plugin","tags":["pipeline","plugins","gsoc"],"authors":[{"avatar":null,"blog":null,"github":"alexsomai","html":"","id":"alexsomai","irc":null,"linkedin":null,"name":"Alexandru Somai","slug":"blog/author/alexsomai","twitter":"alex_somai"}]}},{"node":{"date":"2016-05-18T00:00:00.000Z","id":"3098cab4-cd65-52ec-a4a9-2b36e2ac20d7","slug":"/blog/2016/05/18/announcing-azure-partnership/","strippedHtml":"I am pleased to announce that we have partnered with Microsoft to migrate and\npower the Jenkins project’s infrastructure with\nMicrosoft Azure. The partnership comes\nat an important time, after the recent launch of Jenkins 2.0,\nJenkins users are more readily adopting Pipeline as\nCode and many other plugins at an increasing rate, elevating the importance of\nJenkins infrastructure to the overall success of the project. That strong and\ncontinued growth has brought new demands to our infrastructure’s design and\nimplementation, requiring the next step in its evolution. This partnership helps\nus grow with the rest of the project by unifying our existing infrastructure\nunder one comprehensive, modern and scalable platform.\n\nIn March we\ndiscussed\nthe potential partnership in our regularly scheduled\nproject\nmeeting,\nhighlighting some of the infrastructure challenges that we face:\n\nCurrently we have infrastructure in four different locations, with four\ndifferent infrastructure providers, each with their own APIs and tools for\nmanaging resources, each with varying capabilities and capacities.\n\nProject infrastructure is managed by a team of volunteers, operating\nmore than 15 different services and managing a number of additional external\nservices.\n\nOur current download/mirror network, while geographically distributed, is\nrelatively primitive and its implementation prevents us from using more modern\ndistribution best practices.\n\nIn essence, five years of tremendous growth for Jenkins has outpaced our\norganically grown, unnecessarily complex, project infrastructure. Migrating to\nAzure simplifies and improves our infrastructure in a dramatic way that would\nnot be possible without a comprehensive platform consisting of: compute, CDN,\nstorage and data-store services. Our partnership covers, at minimum, the next\nthree years of the project’s infrastructure needs, giving us a great home for\nthe future.\n\nAzure also enables a couple of projects that I\nhave long been dreaming of providing to Jenkins users and contributors:\n\nEnd-to-end TLS encrypted distribution of Jenkins packages, plugins and\nmetadata via the Azure CDN.\n\nMore complete build/test/release support and capacity on\nci.jenkins.io for plugin developers using\nAzure\nContainer Service and generic VMs.\n\nThe Jenkins infrastructure is all open\nsource which means  all of our Docker containers, Puppet code and many of our\ntools are all available on GitHub. Not\nonly can you watch the migration process to Azure as it happens, but I also\ninvite you to participate in making our project’s infrastructure better (join\nus in the #jenkins-infra channel on Freenode or our\nmailing list).\n\nSuffice it to say, I’m very excited about the bright [blue] future for the\nJenkins project and the infrastructure that powers it!","title":"Partnering with Microsoft to run Jenkins infrastructure on Azure","tags":["azure","infra","infrastructure"],"authors":[{"avatar":{"childImageSharp":null},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"blog/author/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-05-17T00:00:00.000Z","id":"f5d8714c-54dc-5d02-9311-d7a814cce01e","slug":"/blog/2016/05/17/state-of-jam/","strippedHtml":"Recently, the Jenkins project announced the release of\nJenkins 2.0, a first major release\nafter 10 years and 655 weekly releases. This has been a major milestone for\nJenkins and its growing community of developers, testers, designers and other\nusers in the software delivery process.\n\nWith its rising popularity and wide adoption, the Jenkins community continues to\ngrow and evolve into the millions. Jenkins community meetup activity has risen\nto an all time high since the first Jenkins meetup which was established on\nAugust 23 2010, in San Francisco.\n\nOver the last six months the number of\nJenkins Area Meetup (JAM) Groups has\ngrown from 5 to 30, with coverage in Asia, North America, South America and\nEurope.  That’s an average growth of 4 new JAMs per month.\n\nAs of today, there are over 4,100 Jenkins fans within the Jenkins meetup\ncommunity.  This is the result of contributions from community JAM leaders who\nhave volunteered their time to provide a platform for learning, sharing and\nnetworking all things Jenkins within their local communities.\n\nFor anyone who has not organized a meetup before, there are many moving parts\nthat have to come together at a specific location, date and time. This process\ntakes significant effort to methodically plan out. From planning the food and\nbeverages to securing speaker(s), a venue, audio/visual setup, technical\nlogistics and of course promoting the meetup. It does takes a level of passion\nand effort to make it all happen.\n\nMany THANKS to the 55 JAM leaders, who share this passion - they have\nsuccessfully organized over 41 meetups within the past six months in North\nAmerica, South America and Europe. That’s about 6 meetups a month!\n\nThere are still plenty of opportunities to be a JAM organizer. If there is not a\nJAM near you, we’d love to hear from\nyou! Here’s\nhow you can get\nstarted.","title":"The State of Jenkins Area Meetups (JAM)","tags":["meetup","JAM","event"],"authors":[{"avatar":{"childImageSharp":null},"blog":null,"github":"alyssat","html":"<div class=\"paragraph\">\n<p>Member of the <a href=\"/sigs/advocacy-and-outreach/\">Jenkins Advocacy and Outreach SIG</a>.\nAlyssa drives and manages Jenkins participation in community events and conferences like <a href=\"https://fosdem.org/\">FOSDEM</a>, <a href=\"https://www.socallinuxexpo.org/\">SCaLE</a>, <a href=\"https://events.linuxfoundation.org/cdcon/\">cdCON</a>, and <a href=\"https://events19.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2019/\">KubeCon</a>.\nShe is also responsible for Marketing &amp; Community Programs at <a href=\"https://cloudbees.com\">CloudBees, Inc.</a></p>\n</div>","id":"alyssat","irc":null,"linkedin":null,"name":"Alyssa Tong","slug":"blog/author/alyssat","twitter":null}]}},{"node":{"date":"2016-05-12T00:00:00.000Z","id":"16aeac16-1357-5d74-85da-84ef71ddbaa2","slug":"/blog/2016/05/12/sf-jam-jenkins-and-azure/","strippedHtml":"A few weeks ago, my colleague Brian Dawson\nand I were invited to present on\nScaling Jenkins for\nContinuous Delivery with Microsoft Azure in Microsoft’s\nReactor space. Azure is Microsoft’s\npublic cloud offering and one of the many tools available to Jenkins users for\nadding elastic compute capacity, among other things, to their build/test/deploy\ninfrastructure. While our presentations are applicable to practically\nany cloud-based Jenkins environment, Thiago Almeida and Oguz Pastirmaci from\nMicrosoft were also on-hand and presented some interesting Azure-specific\nofferings like\nAzure\nContainer Service with Jenkins.\n\nWhile we do not have video from the meetup, Brian and I did record\na\nsession with Thiago and Oguz for Channel9\nwhich covers much of the same content:\n\nTo kick-off the meetup we asked attendees a few polling questions and\nreceived very telling responses:\n\nHow big is your Development/IT organization?\n\nWhat is your role?\n\nBy show of hands do you practice CI/CD/DevOps/etc?\n\nAt what scale (tooling and practice)?\n\nThe responses indicated that the majority of attendees were from small to medium\norganizations where they practiced Continuous Delivery across multiple teams. A\nnotable 25% or greater attendees considered themselves \"fullstack\" or\nparticipating in all of the roles of Developer, QA, and Operations. Interesting\nwhen paired with the high number (~80%) of those who practice CD.  This is\nlikely because modern teams, with mature CD practices, tend to blur the\ntraditional lines of Developer, QA and Operations. However, In my experience,\nwhile this is often the case for small to medium companies in large\norganizations team members tend to fall into the traditional roles, with CD\nproviding the practice and platform to unify teams across roles.\n\n— Brian Dawson\n\nAfter gauging the audience, Thiago and Brian reviewed Continuous Delivery (CD)\nand implementing it at scale. They highlighted the fact that CD is being rapidly\nadopted across teams and organizations, providing the ability: to deliver a demonstrably\nhigher quality product, shipping more rapidly than before, and to keep team members happier.\n\nHowever, when organizations fail to properly support CD as they scale, they run\ninto issues such as: developers acting as administrators at the cost of\nproductivity, potential lack of security and/or exposure of IP and difficulty in\nsharing best practices across teams.\n\nThiago then highlighted that properly scaling CD practices in the organization\nalong with the infrastructure itself can alleviate these issues, and discussed\nthe benefits of scaling CD to on cloud platforms to provide \"CD-as-a-Service.\"\n\nOverall I found the \"theory\" discussion to be on point, continuous delivery is\nnot just a technology nor a people problem. Successful organizations scale their\nprocesses and tooling together.\n\nThe slides from our respective presentations are linked below:\n\n(Brian) Scaling Jenkins for Continuous Delivery (.pdf)\n\n(Tyler) Scaling Jenkins with Azure (.pdf)\n\nI hope you join us at future\nSan Francisco\nJAM s!","title":"SF JAM Report: Scaling Jenkins for Continuous Delivery with Azure","tags":["jam","azure","meetup"],"authors":[{"avatar":{"childImageSharp":null},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"blog/author/rtyler","twitter":"agentdero"}]}}]}},"pageContext":{"limit":8,"skip":400,"numPages":100,"currentPage":51}},
    "staticQueryHashes": ["3649515864"]}