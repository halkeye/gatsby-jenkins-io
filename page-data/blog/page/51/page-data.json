{
    "componentChunkName": "component---src-templates-blog-list-template-js",
    "path": "/blog/page/51",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2016-06-29T00:00:00.000Z","id":"72dc398a-0c3a-5b36-949e-7687e6ecbefc","slug":"/blog/2016/06/29/from-freestyle-to-pipeline/","strippedHtml":"This is a guest post by R. Tyler Croy, who is a\nlong-time contributor to Jenkins and the primary contact for Jenkins project\ninfrastructure. He is also a Jenkins Evangelist at\nCloudBees, Inc.\n\nFor ages I have used the \"Build After\" feature in Jenkins to cobble together\nwhat one might refer to as a \"pipeline\" of sorts. The Jenkins project itself, a\nmajor consumer of Jenkins, has used these daisy-chained Freestyle jobs to drive\na myriad of delivery pipelines in our infrastructure.\n\nOne such \"pipeline\" helped drive the complex process of generating the pretty\nblue charts on\nstats.jenkins.io.\nThis statistics generation process primarily performs two major tasks, on rather\nlarge sets of data:\n\nGenerate aggregate monthly \"census data.\"\n\nProcess the census data and create trend charts\n\nThe chained jobs allowed us to resume the independent stages of the pipeline,\nand allowed us to run different stages on different hardware (different\ncapabilities) as needed. Below is a diagram of what this looked like:\n\nThe infra_generate_monthly_json would run periodically creating the\naggregated census data, which would then be picked up by infra_census_push\nwhose sole responsibility was to take census data and publish it to the\nnecessary hosts inside the project’s infrastructure.\n\nThe second, semi-independent, \"pipeline\" would also run periodically. The\ninfra_statistics job’s responsibility was to use the census data, pushed\nearlier by infra_census_push, to generate the myriad of pretty blue charts\nbefore triggering the\ninfra_checkout_stats job which would make sure stats.jenkins.io was\nproperly updated.\n\nSuffice it to say, this \"pipeline\" had grown organically over a period time when\nmore advanced tools weren’t quite available.\n\nWhen we migrated to newer infrastructure for\nci.jenkins.io earlier this year I took the\nopportunity to do some cleaning up. Instead of migrating jobs verbatim, I pruned\nstale jobs and refactored a number of others into proper\nPipelines, statistics generation being an obvious\ntarget!\n\nOur requirements for statistics generation, in their most basic form, are:\n\nEnable a sequence of dependent tasks to be executed as a logical group (a\npipeline)\n\nEnable executing those dependent tasks on various pieces of infrastructure\nwhich support different requirements\n\nActually generate those pretty blue charts\n\nIf you wish to skip ahead, you can jump straight to the\nJenkinsfile\nwhich implements our new Pipeline.\n\nThe first iteration of the Jenkinsfile simply defined the conceptual stages we\nwould need:\n\nnode {\n    stage 'Sync raw data and census files'\n\n    stage 'Process raw logs'\n\n    stage 'Generate census data'\n\n    stage 'Generate stats'\n\n    stage 'Publish census'\n\n    stage 'Publish stats'\n}\n\nHow exciting! Although not terrifically useful. When I began actually\nimplementing the first couple stages, I noticed that the Pipeline might sync\ndozens of gigabytes of data every time it ran on a new agent in the cluster.\nWhile this problem will soon be solved by the\nExternal\nWorkspace Manager plugin, which is currently being developed. Until it’s ready,\nI chose to mitigate the issue by pinning the execution to a consistent agent.\n\n/* `census` is a node label for a single machine, ideally, which will be\n * consistently used for processing usage statistics and generating census data\n */\nnode('census && docker') {\n    /* .. */\n}\n\nRestricting a workload which previously used multiple agents to a single one\nintroduced the next challenge. As an infrastructure administrator, technically\nspeaking, I could just install all the system dependencies that I want on this\none special Jenkins agent. But what kind of example would that be setting!\n\nThe statistics generation process requires:\n\nJDK8\n\nGroovy\n\nA running MongoDB instance\n\nFortunately, with Pipeline we have a couple of useful features at our disposal:\ntool auto-installers and the\nCloudBees\nDocker Pipeline plugin.\n\nTool Auto-Installers\n\nTool Auto-Installers are exposed in Pipeline through the tool step and on\nci.jenkins.io we already had JDK8 and Groovy\navailable. This meant that the Jenkinsfile would invoke tool and Pipeline\nwould automatically install the desired tool on the agent executing the current\nPipeline steps.\n\nThe tool step does not modify the PATH environment variable, so it’s usually\nused in conjunction with the withEnv step, for example:\n\nnode('census && docker') {\n    /* .. */\n\n    def javaHome = tool(name: 'jdk8')\n    def groovyHome = tool(name: 'groovy')\n\n    /* Set up environment variables for re-using our auto-installed tools */\n    def customEnv = [\n        \"PATH+JDK=${javaHome}/bin\",\n        \"PATH+GROOVY=${groovyHome}/bin\",\n        \"JAVA_HOME=${javaHome}\",\n    ]\n\n    /* use our auto-installed tools */\n    withEnv(customEnv) {\n        sh 'java --version'\n    }\n\n    /* .. */\n}\n\nCloudBees Docker Pipeline plugin\n\nSatisfying the MongoDB dependency would still be tricky. If I caved in and installed\nMongoDB on a single unicorn agent in the cluster, what could I say the next time\nsomebody asked for a special, one-off, piece of software installed on our\nJenkins build agents?\n\nAfter doing my usual complaining and whining, I discovered that the CloudBees\nDocker Pipeline plugin provides the ability to run containers inside of a\nJenkinsfile. To make things even better, there are\nofficial MongoDB docker images readily\navailable on DockerHub!\n\nThis feature requires that the machine has a running Docker daemon which is\naccessible to the user running the Jenkins agent. After that, running a\ncontainer in the background is easy, for example:\n\nnode('census && docker') {\n    /* .. */\n\n    /* Run MongoDB in the background, mapping its port 27017 to our host's port\n     * 27017 so our script can talk to it, then execute our Groovy script with\n     * tools from our `customEnv`\n     */\n    docker.image('mongo:2').withRun('-p 27017:27017') { container ->\n        withEnv(customEnv) {\n            sh \"groovy parseUsage.groovy --logs ${usagestats_dir} --output ${census_dir} --incremental\"\n        }\n    }\n\n    /* .. */\n}\n\nThe beauty, to me, of this example is that you can pass a\nclosure to withRun which will\nexecute while the container is running. When the closure is finished executin,\njust the sh step in this case, the container is destroyed.\n\nWith that system requirement satisfied, the rest of the stages of the Pipeline\nfell into place. We now have a single source of truth, the\nJenkinsfile,\nfor the sequence of dependent tasks which need to be executed, accounting for\nvariations in systems requirements, and it actually generates\nthose pretty\nblue charts!\n\nOf course, a nice added bonus is the beautiful visualization of our\nnew Pipeline!\n\nLinks\n\nPipeline documentation\n\nCloudBees Docker Pipeline plugin documentation\n\nLive statistics Pipeline","title":"Migrating from chained Freestyle jobs to Pipelines","tags":["pipeline","infra"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-06-21T00:00:00.000Z","id":"719def0e-69a3-5e1c-b789-b2d19d5834dd","slug":"/blog/2016/06/21/gsoc-midterm-presentations-ann/","strippedHtml":"As you probably know, on this year Jenkins projects participates in\nGoogle Summer of Code 2016.\nYou can find more information about the accepted projects on the GSoC subproject page and in the\nJenkins Developer mailing list.\n\nOn this week GSoC students are going to present their projects as a part of mid-term evaluation,\nwhich covers one month of community bonding and one month of coding.\n\nWe would like to invite Jenkins developers to attend these meetings.\nThere are two additional months of coding ahead for successful students, so any feedback from Jenkins contributors and users will be appreciated.\n\nMeeting #1 - June 23, 7:00 PM UTC - 9:00 PM UTC\n\nSupport Core plugin improvements by Minudika Malshan\n\nIntro blogpost\n\nExternal Workspace Manager by Alex Somai\n\nIntro blogpost\n\nPlugin documentation publishing to jenkins.io by Cynthia Anyango\n\nIntro blogpost\n\nQ&A session\n\nMeeting link\n\nMeeting #2 - June 24, 8AM UTC - 9 AM UTC\n\nJenkins WebUI: Improving Job Creation/Configuration by Samat Davletshin\n\nIntro blogpost\n\nQ&A session\n\nMeeting link\n\nBoth meetings will be conducted and recorded via Hangouts on Air.\nThe recorded sessions will be made public after the meetup.\nThe agenda may change a bit.\n\nLinks\n\nMid-term presentations announcement on Jenkins Developer mailing list\n\nJenkins GSoC 2016 Wiki Page\n\nJenkins project page on the GSoC2016 website","title":"GSoC: Mid-term presentations by students on June 23 and 24","tags":["core","gsoc","plugin","general"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8c8e8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/bf8e1/oleg_nenashev.png","srcSet":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/914ee/oleg_nenashev.png 32w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/1c9ce/oleg_nenashev.png 64w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/bf8e1/oleg_nenashev.png 128w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/acb7c/oleg_nenashev.png 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/ef6ff/oleg_nenashev.webp 32w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/8257c/oleg_nenashev.webp 64w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/6766a/oleg_nenashev.webp 128w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/22bfc/oleg_nenashev.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"https://oleg-nenashev.github.io/","github":"oleg-nenashev","html":"<div class=\"paragraph\">\n<p>Jenkins core maintainer and board member, open source software and open hardware advocate, TOC chair in the Continuous Delivery Foundation.\nOleg started using Hudson for Hardware/Embedded projects in 2008 and became an active Jenkins contributor in 2012.\nNowadays he maintains [Jenkinsfile Runner](<a href=\"https://github.com/jenkinsci/jenkinsfile-runner/\" class=\"bare\">https://github.com/jenkinsci/jenkinsfile-runner/</a>),\ncontributes to several Jenkins <a href=\"/sigs\">SIGs</a> and outreach programs (<a href=\"/projects/gsoc\">Google Summer of Code</a>, <a href=\"/events/hacktoberfest\">Hacktoberfest</a>)\nand organizes <a href=\"/projects/jam/\">Jenkins meetups</a> in Switzerland and Russia.\nOleg works on open source programs and [Keptn](<a href=\"https://keptn.sh/\" class=\"bare\">https://keptn.sh/</a>) at the [Dynatrace](<a href=\"https://dynatrace.com\" class=\"bare\">https://dynatrace.com</a>), Open Source Program Office.</p>\n</div>","id":"oleg_nenashev","irc":"oleg_nenashev","linkedin":"onenashev","name":"Oleg Nenashev","slug":"/blog/authors/oleg_nenashev","twitter":"oleg_nenashev"}]}},{"node":{"date":"2016-06-16T00:00:00.000Z","id":"9d5b4fb7-1151-5470-985c-045b0dd79455","slug":"/blog/2016/06/16/parallel-test-executor-plugin/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nIn this blog post, I’ll show you how to speed up your pipeline by using the\nParallel Test Executor Plugin.\n\nSo much to do, so little time…​\n\nIn my career, I’ve helped many teams move to continuous integration and delivery. One problem\nwe always encounter is how to run all the tests needed to ensure high-quality\nchanges while still keeping pipeline times reasonable and changes flowing\nsmoothly. More tests mean greater confidence, but also longer wait times.\nBuild systems may or may not support running tests in parallel, but they still only use one\nmachine even while other lab machines sit idle. In these cases, parallelizing\ntest execution across multiple machines is a great way to speed up pipelines.\nThe Parallel Test Executor plugin lets us leverage Jenkins do just that with no\ndisruption to the rest of the build system.\n\nSerial Test Execution\n\nFor this post, I’ll be running a pipeline based on the\nJenkins Git Plugin. I’ve modified\nthe Jenkinsfile from that project to allow us to compare execution times to our\nlater changes, and I’ve truncated the \"mvn\" utility method since it remains\nunchanged.  You can find the original file\nhere.\n\nnode {\n  stage 'Checkout'\n  checkout scm\n\n  stage 'Build'\n\n  /* Call the Maven build without tests. */\n  mvn \"clean install -DskipTests\"\n\n  stage 'Test'\n  runTests()\n\n  /* Save Results. */\n  stage 'Results'\n\n  /* Archive the build artifacts */\n  archive includes: 'target/*.hpi,target/*.jpi'\n}\n\nvoid runTests(def args) {\n  /* Call the Maven build with tests. */\n  mvn \"install -Dmaven.test.failure.ignore=true\"\n\n  /* Archive the test results */\n  junit '**/target/surefire-reports/TEST-*.xml'\n}\n\n/* Run Maven */\nvoid mvn(def args) { /* ... */ }\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit 'https://github.com/jenkinsci/git-plugin.git'.\n\nThis is a Maven project, so the Jenkinsfile is pretty simple.\nI’ve split the Maven build into separate “Build” and “Test”\nstages. Maven doesn’t support this split very well, it wants to run all\nthe steps of the lifecycle in order every time. So, I have to call Maven twice:\nfirst using the “skipTests” property to do only build steps in the first call,\nand then a second time with out that property to run tests.\n\nOn my quad-core machine, executing this pipeline takes about 13 minutes and 30\nseconds.  Of that time, it takes 13 minutes to run about 2.7 thousand tests in\nserial.\n\nParallel Test Execution\n\nThis looks like an ideal project for parallel test execution: a short build\nfollowed by a large number of serially executed tests that consume the most of\nthe pipeline time. There are a number of things I could try to speed this up.\nFor example, I could modify test harness to look for ways to parallelize\nthe test execution on this single machine. Or I could try speed up the tests\nthemselves. Both of those can be time-consuming and both risk destabilizing the\ntests. I’d need to know more about the project to do it well.\n\nI’ll avoid that risk by using Jenkins and the\nParallel Test Executor Plugin to\nparallelize the tests across multiple nodes instead. This will isolate the tests\nfrom each other, while still giving us speed gains from parallel execution.\n\nThe plugin reads the list of tests from the results archived in the previous execution of this\njob and splits that list into a specified number of sublists. I can then use\nthose sublists to execute the tests in parallel, passing a different sublist to\neach node.\n\nLet’s look at how this changes the pipeline:\n\nnode { /* ...unchanged... */ }\n\nvoid runTests(def args) {\n  /* Request the test groupings.  Based on previous test results. */\n  /* see https://wiki.jenkins.io/display/JENKINS/Parallel+Test+Executor+Plugin and demo on github\n  /* Using arbitrary parallelism of 4 and \"generateInclusions\" feature added in v1.8. */\n  def splits = splitTests parallelism: [$class: 'CountDrivenParallelism', size: 4], generateInclusions: true\n\n  /* Create dictionary to hold set of parallel test executions. */\n  def testGroups = [:]\n\n  for (int i = 0; i }. */\n    /*     includes = whether list specifies tests to include (true) or tests to exclude (false). */\n    /*     list = list of tests for inclusion or exclusion. */\n    /* The list of inclusions is constructed based on results gathered from */\n    /* the previous successfully completed job. One additional record will exclude */\n    /* all known tests to run any tests not seen during the previous run.  */\n    testGroups[\"split-${i}\"] = {  // example, \"split3\"\n      node {\n        checkout scm\n\n        /* Clean each test node to start. */\n        mvn 'clean'\n\n        def mavenInstall = 'install -DMaven.test.failure.ignore=true'\n\n        /* Write includesFile or excludesFile for tests.  Split record provided by splitTests. */\n        /* Tell Maven to read the appropriate file. */\n        if (split.includes) {\n          writeFile file: \"target/parallel-test-includes-${i}.txt\", text: split.list.join(\"\\n\")\n          mavenInstall += \" -Dsurefire.includesFile=target/parallel-test-includes-${i}.txt\"\n        } else {\n          writeFile file: \"target/parallel-test-excludes-${i}.txt\", text: split.list.join(\"\\n\")\n          mavenInstall += \" -Dsurefire.excludesFile=target/parallel-test-excludes-${i}.txt\"\n        }\n\n        /* Call the Maven build with tests. */\n        mvn mavenInstall\n\n        /* Archive the test results */\n        junit '**/target/surefire-reports/TEST-*.xml'\n      }\n    }\n  }\n  parallel testGroups\n}\n\n/* Run Maven */\nvoid mvn(def args) { /* ... */ }\n\nThat’s it!  The change is significant but it is all encapsulated in this one\nmethod in the Jenkinsfile.\n\nGreat (ish) Success!\n\nHere’s the results for the new pipeline with parallel test execution:\n\nThe tests ran almost twice as fast, without changes outside pipeline.  Great!\n\nHowever, I used 4 test executors, so why am I not seeing a 4x? improvement.\nA quick review of the logs shows the problem: A small number of tests are taking up\nto 5 minutes each to complete! This is actually good news. It means that I\nshould be able to see further improvement in pipeline throughput just by refactoring\nthose few long running tests into smaller parts.\n\nConclusion\n\nWhile I would like to have seen closer to a 4x improvement to match to number\nof executors, 2x is still perfectly respectable. If I were working on a group of projects\nwith similar pipelines, I’d be completely comfortable reusing these same changes\non my other project and I’d expect to similar improvement without any disruption to\nother tools or processes.\n\nLinks\n\nhttps://wiki.jenkins.io/display/JENKINS/Parallel+Test+Executor+Plugin","title":"Faster Pipelines with the Parallel Test Executor Plugin","tags":["tutorial","pipeline","plugins"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2016-06-15T00:00:00.000Z","id":"92e6efae-588c-5a80-886a-8fe7822dcea3","slug":"/blog/2016/06/15/jenkins-pipeline-scalability/","strippedHtml":"This is a guest post by Damien\nCoraboeuf, Jenkins project contributor and Continuous Delivery consultant.\n\nImplementing a CI/CD solution based on Jenkins has become very easy. Dealing\nwith hundreds of jobs? Not so much. Having to scale to thousands of jobs?\nNow this is a real challenge.\n\nThis is the story of a journey to get out of the jungle of jobs…​\n\nStart of the journey\n\nAt the beginning of the journey there were several projects using roughly the same\ntechnologies. Those projects had several\nbranches, for maintenance of releases, for new features.\n\nIn turn, each of those branches had to be carefully built, deployed on different\nplatforms and versions, promoted so they could be tested for functionalities,\nperformances and security, and then promoted again for actual delivery.\n\nAdditionally, we had to offer the test teams the means to deploy any version of\ntheir choice on any supported platform in order to carry out some manual tests.\n\nThis represented, for each branch, around 20 jobs. Multiply this by the number of\nbranches and projects, and there you are: more than two years after the start\nof the story, we had more than 3500 jobs.\n\n3500 jobs. Half a dozen people to manage them all…​\n\nPreparing the journey\n\nHow did we deal with this load?\n\nWe were lucky enough to have several assets:\n\ntime - we had time to design a solution before the scaling went really out of\ncontrol\n\nforecast - we knew that the scaling would occur and we were not taken by\nsurprise\n\ntooling - the Jenkins Job DSL\nwas available, efficient and well documented\n\nWe also knew that, in order to scale, we’d have to provide a solution with the\nfollowing characteristics:\n\nself-service - we could not have a team of 6 people become a bottleneck for\nenabling CI/CD in projects\n\nsecurity - the solution had to be secure enough in order for it to be used by\nremote developers we never met and didn’t know\n\nsimplicity - enabling CI/CD had to be simple so that people having\nnever heard of it could still use it\n\nextensibility - no solution is a one-size-fits-all and must be flexible\nenough to allow for corner cases\n\nAll the mechanisms described in this article are available through the\nJenkins Seed plugin.\n\nCreating pipelines using the Job DSL and embedding the scripts in the code was\nsimple enough. But what about branching? We needed a mechanism to allow the\ncreation of pipelines per branch, by downloading the associated DSL and to\nrun it in a dedicated folder.\n\nBut then, all those projects, all those branches, they were mostly using the\nsame pipelines, give or take a few configurable items. Going this way would\nhave lead to a terrible duplication of code, transforming a job maintenance\nnightmare into a code maintenance nightmare.\n\nPipeline as configuration\n\nOur trick was to transform this vision of \"pipeline as code\" into a \"pipeline\nas configuration\":\n\nby maintaining well documented and tested \"pipeline libraries\"\n\nby asking projects to describe their pipeline not as code, but as property\nfiles which would:\n\ndefine the name and version of the DSL pipeline library to use\n\nuse the rest of the property file to configure the pipeline library, using\nas many sensible default values as possible\n\nPiloting the pipeline from the SCM\n\nOnce this was done, the only remaining trick was to automate the creation,\nupdate, start and deletion of the pipelines using SCM events. By enabling SCM\nhooks (in GitHub, BitBucket or even in Subversion), we could:\n\nautomatically create a pipeline for a new branch\n\nregenerate a pipeline when the branch’s pipeline description was modified\n\nstart the pipeline on any other commit on the branch\n\nremove the pipeline when the branch was deleted\n\nOnce a project wants to go in our ecosystem, the Jenkins team \"seeds\" the\nproject into Jenkins, by running a job and giving a few parameters.\n\nIt will create a folder for the project and grant proper authorisations, using\nActive Directory group names based on the project name.\n\nThe hook for the project must be registered into the SCM and you’re up and\nrunning.\n\nConfiguration and code\n\nMixing the use of strong pipeline libraries configured by properties and the\ndirect use of the Jenkins Job DSL is still possible. The Seed plugin\nsupports all kinds of combinations:\n\nuse of pipeline libraries only - this can even be enforced\n\nuse a DSL script which can in turn use some classes and methods defined in\na pipeline library\n\nuse of a Job DSL script only\n\nUsually, we tried to have a maximum reuse, through only pipeline libraries, for\nmost of our projects, but in other circumstances, we were less strict and\nallowed some teams to develop their own pipeline script.\n\nEnd of the journey\n\nIn the end, what did we achieve?\n\nSelf service ✔︎\n\nPipeline automation from SCM - no intervention from the Jenkins team but for\nthe initial bootstrapping\n\nGetting a project on board of this system can be done in a few minutes only\n\nSecurity ✔︎\n\nProject level authorisations\n\nNo code execution on the controller\n\nSimplicity ✔︎\n\nProperty files\n\nExtensibility ✔︎\n\nPipeline libraries\n\nDirect job DSL still possible\n\nSeed and Pipeline plugin\n\nNow, what about the Pipeline plugin? Both\nthis plugin and the Seed plugin have common functionalities:\n\nWhat we have found in our journey is that having a \"pipeline as configuration\"\nwas the easiest and most secure way to get a lot of projects on board, with\ndevelopers not knowing Jenkins and even less the DSL.\n\nThe outcome of the two plugins is different:\n\none pipeline job for the Pipeline plugin\n\na list of orchestrated jobs for the Seed plugin\n\nIf time allows, it would be probably a good idea to find a way to integrate the\nfunctionalities of the Seed plugin into the pipeline framework, and to keep\nwhat makes the strength of the Seed plugin:\n\npipeline as configuration\n\nreuseable pipeline libraries, versioned and tested\n\nLinks\n\nYou can find additional information about the Seed plugin and its usage at the\nfollowing links:\n\nthe Seed plugin itself\n\nJUC London, June 2015\n\nBruJUG Brussels, March 2016","title":"Jenkins Pipeline Scalability in the Enterprise","tags":["jenkins","scalability","dsl"],"authors":[{"avatar":null,"blog":null,"github":"dcoraboeuf","html":"<div class=\"paragraph\">\n<p>I&#8217;ve started many years ago in the Java development before switching\nprogressively toward continuous delivery aspects.  I&#8217;m now a consultant\nimplementing CD solutions based on Jenkins. Implementation of the Pipeline\nas Code principles have allowed one of my clients to be able to manage more\nthan 3000 jobs, using a self service approach based on the Seed plugin.</p>\n</div>\n<div class=\"paragraph\">\n<p>I&#8217;m also a contributor for some Jenkins plugins and the author of the\nOntrack application, which allows the monitoring of continuous delivery\npipelines.</p>\n</div>","id":"dcoraboeuf","irc":null,"linkedin":null,"name":"Damien Coraboeuf","slug":"/blog/authors/dcoraboeuf","twitter":"DamienCoraboeuf"}]}},{"node":{"date":"2016-06-14T00:00:00.000Z","id":"0e5d650b-c2ea-591f-9965-564c0e2df2c3","slug":"/blog/2016/06/14/gsoc-jenkins-support-core-plugin-improvements/","strippedHtml":"About me\n\nI am Minudika Malshan, an undergraduate student in Computer Science and Engineering from University of Moratuwa, Sri Lanka.\n\nAs a person who is passionate in open source software development and seeking for new knowledge and experience, I am willing to give my contribution for this project.\n\nLinkedIn | Twitter\n\nAbstract\n\nThe Support-Core Plugin provides the basic infrastructure for generating \"bundles\" of support information with Jenkins.\nThere are two kinds of bundles.\n\nAutomatic bundles: Bundles which are generated and get saved in $JENKINS_HOME/support once per hour starting 15 seconds after Jenkins starts the plugin.\nThe automatic bundles are retained using an exponential aging strategy. Therefore it’s possible to have a bunch of them over the entire lifetime after the plugin installing the plugin.\n\nOn demand bundles: These bundles are generated from the root \"Support\" action.\n\nHowever current support-core plugin is not much user friendly. The object of this project is to make it more user friendly by adding some features which make a sophisticated environment for the user who use support plugin.\n\nIn this project scope, there are three features and improvements we are going to consider.\n\nEase the bundles management by the administrator ( JENKINS-33090)\n\nAdding an option to anonymize customer labels (strings created by the user such as name of a job, folder, view, agent, and template etc). ( JENKINS-33091)\n\nAllowing user to create an issue and submit a bundle into the OSS tracker using the support-core plugin. ( JENKINS-21670)\n\nArnaud Héritier and Steven Christou are guiding me through the project as my mentors.\n\nTasks and Deliverables\n\nEase the bundles management by the administrator.\n\nUnder this task, the following functions are going to be implemented.\n\nListing bundles stored on the jenkins instance with their details.\n\nAllowing user to download each bundle.\n\nAllowing user to delete each bundle or all bundles.\n\nAllowing user to browse the content of each bundle.\n\nAutomatically purging old bundles.\n\nEnabling user to create an issue and submit a bundle into the OSS tracker\n\nWhen a Jenkins user sees an issue, he/she commonly contacts his support contacts (Jenkins instance admins) and then Jenkins admins troubleshoot the issue.\nThe objective of this task is to implement a feature which enables the user to report an issue to a admin through support core plugin.\n\nWhen creating bundles to attach with the ticket, it is important to protect the privacy of the user who creates the ticket. When considering doing that, anonymizing user created labels (texts) comes to the front.\n\nAdding  an option to anonymize customer labels\n\nThe following functions will be implemented under this taks.\n\nCreating randomized tokens for labels created by users.\n\nProducing a mapping for those labels.\n\nSubstituting encoded labels into all the files included in the support bundle.\n\nWhen creating randomized tokens, it would be much useful and effective if we can create those tokens in a way they make sense to humans. (i.e. readable to humans). For that, I am hoping to use a suitable java library to create human friendly random tokens. One of such libraries is wordnet-random-name.\n\nHowever in order to substitute randomized tokens, all files included in the bundle should be read. This can become inefficient when bundle consists of large number of files.  Therefore it’s important to follow an optimized method for this task.\n\nReferences\n\nInitial proposal of the project\n\nProject repository","title":"GSoC Project Intro: Support Core Plugin Improvements","tags":["core","gsoc","plugin","support-core"],"authors":[{"avatar":null,"blog":null,"github":"minudika","html":"","id":"minudika","irc":null,"linkedin":null,"name":"Minudika Malshan","slug":"/blog/authors/minudika","twitter":"minudika"}]}},{"node":{"date":"2016-06-14T00:00:00.000Z","id":"06cd0ff0-b842-5785-b6b9-ec7c24f8ea95","slug":"/blog/2016/06/14/jenkins-world-agenda/","strippedHtml":"Join us in Santa Clara, California on September 13-15, 2016!\n\nWe are excited to announce the Jenkins\nWorld agenda is now live. There will be 50+ sessions, keynotes, training,\ncertifications and workshops. Here are a few highlights of what you can expect:\n\nHigh level topics\n\nContinuous delivery\n\nDevOps\n\nMicroservices architectures\n\nTesting\n\nAutomation tools\n\nPlugin development\n\nPipeline\n\nBest practices\n\nAnd much more\n\nAdditionally, Jenkins World offers great opportunities for hands-on learning,\nexploring and networking:\n\nPlugin Development Workshop\n\nDue to its popularity in previous years, we are bringing back the plugin\ndevelopment workshop. This workshop will introduce developers to the Jenkins\nplugin ecosystem and terminology. The goal is to provide a cursory overview of\nthe resources available to Jenkins plugin developers. Armed with this\ninformation, Jenkins developers can learn how to navigate the project and\ncodebase to find answers to their questions.\n\nBirds of a Feather Sessions\n\nBoFs, as they are usually known, will be a new addition to Jenkins World this\nyear. Sessions will be curated on various technical topics from DevOps to how\nenterprises are integrating Jenkins in their environment. Discussions will be\nlead by the industry’s brightest minds who have an influence in shaping the\nfuture of Jenkins.\n\nAsk the Experts\n\nGot a Jenkins question that’s been keeping you up at night? Need to bounce ideas\noff somebody? Or you just need someone to fix your Jenkins issue? This is your chance\nto get connected with the Jenkins Experts. Experts will be on hand to help with\nall your Jenkins needs on Sept 14th & 15th.\n\nPrepare for Jenkins Certification\n\nThe objective of this session is to help you assess your level of readiness for\nthe certification exam - either the Certified Jenkins Engineer (CJE/open source)\ncertification or the Certified CloudBees Jenkins Platform Engineer\n(CCJPE/CloudBees-specific) certification. After an overview about the\ncertification program, a Jenkins expert from CloudBees will walk you through the\nvarious sections of the exam, highlighting the important things to controller ahead\nof time, not only from a pure knowledge perspective but also in terms of\npractical experience. This will be an interactive session.\n\nHope to see you at Jenkins World 2016!\n\nDon’t miss out on\nSuper\nEarly Bird Rate $399. Price goes up after July 1.\n\nLinks\n\nStart a JAM in your city if there isn’t one already.\n\nBecome a JAM member.\n\nBecome an online JAM member\n\nBe a JAM speaker or sponsor. Let us know jenkinsci-jam@googlegroups.com\n\nBecome a Jenkins project contributor","title":"Jenkins World Agenda is Live!","tags":["event"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#281818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg","srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/8d248/alyssat.jpg 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/c004c/alyssat.jpg 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/9e67b/alyssat.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/22924/alyssat.webp 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/89767/alyssat.webp 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/40d97/alyssat.webp 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/5028e/alyssat.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":166}}},"blog":null,"github":"alyssat","html":"<div class=\"paragraph\">\n<p>Member of the <a href=\"/sigs/advocacy-and-outreach/\">Jenkins Advocacy and Outreach SIG</a>.\nAlyssa drives and manages Jenkins participation in community events and conferences like <a href=\"https://fosdem.org/\">FOSDEM</a>, <a href=\"https://www.socallinuxexpo.org/\">SCaLE</a>, <a href=\"https://events.linuxfoundation.org/cdcon/\">cdCON</a>, and <a href=\"https://events19.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2019/\">KubeCon</a>.\nShe is also responsible for Marketing &amp; Community Programs at <a href=\"https://cloudbees.com\">CloudBees, Inc.</a></p>\n</div>","id":"alyssat","irc":null,"linkedin":null,"name":"Alyssa Tong","slug":"/blog/authors/alyssat","twitter":null}]}},{"node":{"date":"2016-06-13T00:00:00.000Z","id":"169b8859-fdce-5c99-a2ba-d10df55f6894","slug":"/blog/2016/06/13/gsoc-usage-stats-analysis/","strippedHtml":"About myself\n\nHello, my name is Payal Priyadarshini.  I am pursing my major in Computer\nScience & Engineering at the Indian Institute of Technology Kharagpur, India.  I\nam very proficient in writing code in Python, C++, Java and currently getting\nfamiliar and hopefully good in Groovy too.\n\nI have internship experiences in renowned institutions like Google and VMware\nwhere I worked with some exciting technologies for example Knowledge Graphs,\nBigTable, SPARQL, RDF in Google. I am a passionate computer science student who\nis always interested in learning and looking for new challenges and\ntechnologies.That’s how I came across to Google Summer of Code where I am\nworking on some exciting data mining problems which you are going to encounter\nbelow in this blog.\n\nProject Overview\n\nJenkins has collected anonymous usage information of more than 100,000\ninstallations which includes set of plugins and their versions etc and also\nrelease history information of the upgrades. This data collection can be used\nfor various data mining experiments. The main goal of this project is to perform\nvarious analysis and studies over the available dataset to discover trends\nin data usage. This project will help us to learn more about the Jenkins\nusage by solving various problems, such as:\n\nPlugin versions installation trends, will let us know about the versions installation behaviour of a given plugin.\n\nSpotting downgrades, which will warn us that something is wrong with the version from which downgrading was performed.\n\nCorrelating what users are saying (community rating) with what users are doing (upgrades/downgrades).\n\nDistribution of cluster size, where clusters represents jobs, nodes count which approximates the size of installation.\n\nFinding set of plugins which are likely to be used together, will setup pillar for plugin recommendation system.\n\nAs a part of the Google Summer of Code 2016, I will be working on the above\nmentioned problems. My mentors for the project are Kohsuke Kawaguchi and Daniel Beck. Some analyses has already been done over this\ndata but those are outdated as charts can be more clearer and interactive. This project aims to improvise existing\nstatistics and generating new ones discussed above.\n\nUse Cases\n\nThis project covers wide-range of the use-cases that has been derived from the\nproblems mentioned above.\n\nUse Case 1: Upgrade/Downgrade Analysis\n\nUnderstanding the trend in upgrades and downgrades have lots of utilities, some\nof them have already been explained earlier which includes measuring the\npopularity, spotting downgrades, giving warning about the wrong versions quickly\netc.\n\nUse Case 1.1: Plugin versions installation trends\n\nHere we are analysing the trend in the different version installations for a\ngiven plugin. This use-case will help us to know about:\n\nTrend in the upgrade to the latest version released for a given plugin.\n\nTrend in the popularity decrement of the previous versions after new version release.\n\nFind the most popular plugin version at any given point of time.\n\nUse Case 1.2: Spotting dowgrades\n\nHere we are interested to know, how many installations are downgraded from any\ngiven version to previously used version. Far fetched goal of this analysis is\nto give warning when something goes wrong with the new version release, which\ncan be sensed using downgrades performed by users. This analysis can be\naccomplished by studying the monotonic property of the version number vs.\ntimestamp graph for a given plugin.\n\nUse Case 1.3: Correlation with the perceived quality of Jenkins release\n\nTo correlate what users are saying to what users are doing, we have community\nratings which tells us about the ratings and reviews of the releases and has\nfollowing parameters:\n\nUsed the release on production site w/o major issues.\n\nDon’t recommend to other.\n\nTried but rolled it back to the previous version.\n\nFirst parameters can be calculated from the Jenkins usage data and third\nparameter is basically spotting downgrades(use case 1.2). But the second\nparameter is basically an expression which is not possible to calculate. This\nanalysis is just to get a subjective idea about the correlation.\n\nUse Case 2: Plugin Recommendation System\n\nThis section involves setting up ground work for the plugin recommendation\nsystem. The idea is to find out the set of plugins which are most likely to be\nused together. Here we will be following both content based filtering as well as\ncollaborative filtering approach.\n\nCollaborative Filtering\n\nThis approach is based upon analysing large amount of information on\ninstallation’s behaviours and activities. We have implicit form of the data\nabout the plugins, that is for every install ids, we know the set of plugins\ninstalled. We can use this information to construct plugin usage graph where\nnodes are the plugins and the edges between them is the number of installations\nin which both plugins are installed together.\n\nContent-based Filtering\n\nThis method is based on a properties or the content of the item for example\nrecommending items that are similar to the those that a user liked in the past\nor examining in the present based upon some properties. Here, we are utilizing\nJenkins\nplugin dependency graph to learn about the properties of a plugin. This graph\ntells us about dependent plugins on a given plugin as well as its dependencies\non others. Here is an example to show, how this graph is use for content based\nfiletring, suppose if a user is using “CloudBees Cloud Connector”, then we can\nrecommend them for “CloudBees Registration Plugin” as both plugins are dependent\non “CloudBees Credentials Plugin”.\n\nAdditional Details\n\nYou may find the complete project proposal along with the detailed design of the\nuse-cases with their implementation details here in the\ndesign\ndocument.\n\nA complete version of the use-case 1: Upgrade & Downgrade Analysis should be\navailable in late June and basic version of plugin recommendation system will be\navailable in late July.\n\nI do appreciate any kind of feedback and suggestions.  You may add comments in\nthe\ndesign\ndoc.  I will be posting updates about the statistics generation status on the\njenkins-dev mailing\nlist and jenkins-infra mailing list.\n\nLinks:\n\nDesign Doc\n\nGoogle Summer of Code\n\nGithub infra-stats\n\nJenkins statistics\n\nJenkins Plugin Dependency Graph\n\nGithub GSoC Jenkins Usage Statistics Analysis","title":"GSoC Project Intro: Usage Statistics Analysis","tags":["usage-statistics","gsoc"],"authors":[{"avatar":null,"blog":null,"github":"payal94","html":"","id":"payal94","irc":null,"linkedin":null,"name":"Payal Priyadarshini","slug":"/blog/authors/payal94","twitter":null}]}},{"node":{"date":"2016-06-13T00:00:00.000Z","id":"29d1cef2-5ba3-567a-b9dc-18a27c3802aa","slug":"/blog/2016/06/13/june-jenkins-events/","strippedHtml":"It is hard to believe that the first half of 2016 is almost over and summer is\njust around the corner.  As usual, there are plenty of educational Jenkins\nevents planned for this month. Below lists what’s happening in your neck of the\nwoods:\n\nOnline JAM\n\nJune 14: Plugin Development - Basics\n\nNorth America JAMs\n\nJune 14: Pipeline in a Windows Environment - Boston, Massachusetts\n\nJune 15: Open Source Jenkins 2.0, What’s New? - Washington, DC\n\nJune 22: Continuously Deploying Containers with Jenkins Pipeline to a Docker Swarm Cluster - Seattle, Washington\n\nEurope JAM\n\nJune 14: Jenkins 2.0 - London, United Kingdom\n\nJune 22: Pipeline As Code - Toulouse, France\n\nLinks\n\nStart a JAM in your city if there isn’t one already.\n\nBecome a JAM member\n\nBecome an online JAM member\n\nSpeak or sponsor at a JAM. Contact us at jenkinsci-jam@googlegroups.com\n\nTake advantage of the super-early-bird price to Jenkins World 2016\n\nBecome a Jenkins project contributor","title":"Upcoming June Jenkins Events","tags":["events","jam"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#281818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg","srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/8d248/alyssat.jpg 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/c004c/alyssat.jpg 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/9e67b/alyssat.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/22924/alyssat.webp 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/89767/alyssat.webp 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/40d97/alyssat.webp 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/5028e/alyssat.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":166}}},"blog":null,"github":"alyssat","html":"<div class=\"paragraph\">\n<p>Member of the <a href=\"/sigs/advocacy-and-outreach/\">Jenkins Advocacy and Outreach SIG</a>.\nAlyssa drives and manages Jenkins participation in community events and conferences like <a href=\"https://fosdem.org/\">FOSDEM</a>, <a href=\"https://www.socallinuxexpo.org/\">SCaLE</a>, <a href=\"https://events.linuxfoundation.org/cdcon/\">cdCON</a>, and <a href=\"https://events19.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2019/\">KubeCon</a>.\nShe is also responsible for Marketing &amp; Community Programs at <a href=\"https://cloudbees.com\">CloudBees, Inc.</a></p>\n</div>","id":"alyssat","irc":null,"linkedin":null,"name":"Alyssa Tong","slug":"/blog/authors/alyssat","twitter":null}]}}]}},"pageContext":{"limit":8,"skip":400,"numPages":101,"currentPage":51}},
    "staticQueryHashes": ["3649515864"]}