{
    "componentChunkName": "component---src-templates-blog-list-template-js",
    "path": "/blog/page/51",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2016-06-13T00:00:00.000Z","id":"169b8859-fdce-5c99-a2ba-d10df55f6894","slug":"/blog/2016/06/13/gsoc-usage-stats-analysis/","strippedHtml":"About myself\n\nHello, my name is Payal Priyadarshini.  I am pursing my major in Computer\nScience & Engineering at the Indian Institute of Technology Kharagpur, India.  I\nam very proficient in writing code in Python, C++, Java and currently getting\nfamiliar and hopefully good in Groovy too.\n\nI have internship experiences in renowned institutions like Google and VMware\nwhere I worked with some exciting technologies for example Knowledge Graphs,\nBigTable, SPARQL, RDF in Google. I am a passionate computer science student who\nis always interested in learning and looking for new challenges and\ntechnologies.That’s how I came across to Google Summer of Code where I am\nworking on some exciting data mining problems which you are going to encounter\nbelow in this blog.\n\nProject Overview\n\nJenkins has collected anonymous usage information of more than 100,000\ninstallations which includes set of plugins and their versions etc and also\nrelease history information of the upgrades. This data collection can be used\nfor various data mining experiments. The main goal of this project is to perform\nvarious analysis and studies over the available dataset to discover trends\nin data usage. This project will help us to learn more about the Jenkins\nusage by solving various problems, such as:\n\nPlugin versions installation trends, will let us know about the versions installation behaviour of a given plugin.\n\nSpotting downgrades, which will warn us that something is wrong with the version from which downgrading was performed.\n\nCorrelating what users are saying (community rating) with what users are doing (upgrades/downgrades).\n\nDistribution of cluster size, where clusters represents jobs, nodes count which approximates the size of installation.\n\nFinding set of plugins which are likely to be used together, will setup pillar for plugin recommendation system.\n\nAs a part of the Google Summer of Code 2016, I will be working on the above\nmentioned problems. My mentors for the project are Kohsuke Kawaguchi and Daniel Beck. Some analyses has already been done over this\ndata but those are outdated as charts can be more clearer and interactive. This project aims to improvise existing\nstatistics and generating new ones discussed above.\n\nUse Cases\n\nThis project covers wide-range of the use-cases that has been derived from the\nproblems mentioned above.\n\nUse Case 1: Upgrade/Downgrade Analysis\n\nUnderstanding the trend in upgrades and downgrades have lots of utilities, some\nof them have already been explained earlier which includes measuring the\npopularity, spotting downgrades, giving warning about the wrong versions quickly\netc.\n\nUse Case 1.1: Plugin versions installation trends\n\nHere we are analysing the trend in the different version installations for a\ngiven plugin. This use-case will help us to know about:\n\nTrend in the upgrade to the latest version released for a given plugin.\n\nTrend in the popularity decrement of the previous versions after new version release.\n\nFind the most popular plugin version at any given point of time.\n\nUse Case 1.2: Spotting dowgrades\n\nHere we are interested to know, how many installations are downgraded from any\ngiven version to previously used version. Far fetched goal of this analysis is\nto give warning when something goes wrong with the new version release, which\ncan be sensed using downgrades performed by users. This analysis can be\naccomplished by studying the monotonic property of the version number vs.\ntimestamp graph for a given plugin.\n\nUse Case 1.3: Correlation with the perceived quality of Jenkins release\n\nTo correlate what users are saying to what users are doing, we have community\nratings which tells us about the ratings and reviews of the releases and has\nfollowing parameters:\n\nUsed the release on production site w/o major issues.\n\nDon’t recommend to other.\n\nTried but rolled it back to the previous version.\n\nFirst parameters can be calculated from the Jenkins usage data and third\nparameter is basically spotting downgrades(use case 1.2). But the second\nparameter is basically an expression which is not possible to calculate. This\nanalysis is just to get a subjective idea about the correlation.\n\nUse Case 2: Plugin Recommendation System\n\nThis section involves setting up ground work for the plugin recommendation\nsystem. The idea is to find out the set of plugins which are most likely to be\nused together. Here we will be following both content based filtering as well as\ncollaborative filtering approach.\n\nCollaborative Filtering\n\nThis approach is based upon analysing large amount of information on\ninstallation’s behaviours and activities. We have implicit form of the data\nabout the plugins, that is for every install ids, we know the set of plugins\ninstalled. We can use this information to construct plugin usage graph where\nnodes are the plugins and the edges between them is the number of installations\nin which both plugins are installed together.\n\nContent-based Filtering\n\nThis method is based on a properties or the content of the item for example\nrecommending items that are similar to the those that a user liked in the past\nor examining in the present based upon some properties. Here, we are utilizing\nJenkins\nplugin dependency graph to learn about the properties of a plugin. This graph\ntells us about dependent plugins on a given plugin as well as its dependencies\non others. Here is an example to show, how this graph is use for content based\nfiletring, suppose if a user is using “CloudBees Cloud Connector”, then we can\nrecommend them for “CloudBees Registration Plugin” as both plugins are dependent\non “CloudBees Credentials Plugin”.\n\nAdditional Details\n\nYou may find the complete project proposal along with the detailed design of the\nuse-cases with their implementation details here in the\ndesign\ndocument.\n\nA complete version of the use-case 1: Upgrade & Downgrade Analysis should be\navailable in late June and basic version of plugin recommendation system will be\navailable in late July.\n\nI do appreciate any kind of feedback and suggestions.  You may add comments in\nthe\ndesign\ndoc.  I will be posting updates about the statistics generation status on the\njenkins-dev mailing\nlist and jenkins-infra mailing list.\n\nLinks:\n\nDesign Doc\n\nGoogle Summer of Code\n\nGithub infra-stats\n\nJenkins statistics\n\nJenkins Plugin Dependency Graph\n\nGithub GSoC Jenkins Usage Statistics Analysis","title":"GSoC Project Intro: Usage Statistics Analysis","tags":["usage-statistics","gsoc"],"authors":[{"avatar":null,"blog":null,"github":"payal94","html":"","id":"payal94","irc":null,"linkedin":null,"name":"Payal Priyadarshini","slug":"/blog/authors/payal94","twitter":null}]}},{"node":{"date":"2016-06-10T00:00:00.000Z","id":"14ea0208-9b66-5e17-a4c1-d9b43525e9f4","slug":"/blog/2016/06/10/save-costs-with-ec2-spot-fleet/","strippedHtml":"This is a guest post by Aleksei Besogonov, Senior Software Developer at\nAmazon Web Services.\n\nEarlier this year, we published a case study on how\nLyft has used Amazon EC2 Spot instances to save 75% on their continuous delivery\ninfrastructure costs by simply changing four lines of code. Several other EC2 customers like Mozilla have\nalso reduced costs of their\ncontinuous integration, deployment and testing pipelines by up to 90% on Spot instances. You can view\nthe current savings on Spot instances over EC2 On-demand instances using the\nSpot Bid Advisor :\n\nAWS Spot instances are spare EC2 instances that you can bid on. While your Spot instances may be\nterminated when EC2’s spare capacity declines, you can automatically replenish these instances and\nmaintain your target capacity using\nEC2 Spot fleets. As each\ninstance type and Availability Zone provides an alternative capacity pool, you can select multiple\nsuch pools to launch the lowest priced instances currently available by launching a Spot\nfleet on the Amazon EC2 Spot Requests console\nor using the AWS CLI/SDK tools.\n\nIn this walkthrough, we’ll show you how to configure Jenkins to automatically scale a fleet of Spot\ninstances up or down depending on the number jobs to be completed.\n\nRequest an Amazon EC2 Spot fleet\n\nTo get started, login to Amazon EC2 console, and click on Spot Requests\nin the left hand navigation pane. Alternatively, you can directly login to\nAmazon EC2 Spot Requests console. Then click on the\nRequest Spot Instances button at the top of the dashboard.\n\nIn the Spot instance launch wizard, select the Request & Maintain option to request a Spot fleet that automatically\nprovisions the most cost-effective EC2 Spot instances, and replenishes them if interrupted. Enter an initial\ntarget capacity, choose an AMI, and select multiple instance types to automatically provision the lowest priced\ninstances available.\n\nOn the next page, ensure that you have selected a key pair, complete the launch wizard, and note the Spot\nfleet request ID.\n\nAmazon EC2 Spot fleet automates finding the lowest priced instances for you, and enables your Jenkins cluster\nto maintain the required capacity; so, you don’t need any bidding algorithms to provision the optimal Spot\ninstances over time.\n\nConfigure Jenkins\n\nInstall the Plugin\n\nFrom the Jenkins dashboard, select Manage Jenkins, and then click Manage Plugins. On the Available tab,\nsearch for and select the EC2 Fleet Jenkins Plugin. Then click the Install button.\n\nAfter the plugin installation is completed, select Manage Jenkins from the Jenkins dashboard, and\nclick Configure System. In the Cloud section, select Amazon Spot Fleet to add a new Cloud.\n\nConfigure AWS Credentials\n\nNext, we will configure the AWS and agent node credentials. Click the Add button next to AWS Credentials,\nselect Jenkins, and enter your AWS Access Key, secret, and ID.\n\nNext, click the Add button in the Spot fleet launcher to configure your agents with an SSH key.\nSelect Jenkins, and enter the username and private key (from the key pair you configured in your Spot fleet request)\nas shown below.\n\nConfirm that the AWS and SSH credentials you just added are selected. Then choose the region, and the Spot fleet\nrequest ID from the drop-down. You can also enter the maximum idle time before your cluster automatically scales\ndown, and the maximum cluster size that it can scale up to.\n\nSubmit Jobs and View Status\n\nAfter you have finished the previous step, you can view the EC2 Fleet Status in the left hand navigation pane on\nthe Jenkins dashboard. Now, as you submit more jobs, Jenkins will automatically scale your Spot fleet to add more\nnodes. You can view these new nodes executing jobs under the Build Executor Status.\nAfter the jobs are done, if the nodes remain free for the specified idle time (configured in the previous step),\nthen Jenkins releases the nodes, automatically scaling down your Spot fleet nodes.\n\nBuild faster and cheaper\n\nIf you have a story to share about your team or product, or have a question to ask, do leave a comment\nfor us; we’d love to connect with you!","title":"Save up to 90% of CI cost on AWS with Jenkins and EC2 Spot Fleet","tags":["aws","plugins","ec2"],"authors":[{"avatar":null,"blog":null,"github":"Cyberax","html":"","id":"cyberax","irc":null,"linkedin":null,"name":"Aleksei Besogonov","slug":"/blog/authors/cyberax","twitter":null}]}},{"node":{"date":"2016-06-01T00:00:00.000Z","id":"2ec37d75-53e9-562c-9bbb-de2a06ecbafd","slug":"/blog/2016/06/01/gsoc-automatic-plugin-documentation/","strippedHtml":"About me\n\nI am Cynthia Anyango from Nairobi, Kenya. I am a second year student at Maseno\nUniversity. I am currently specializing on Ruby on Rails and trying to learn\nPython. I recently started contributing to Open source projects.My major\ncontribution was at Mozilla, where I worked with the QA for Cloud services. I did\nmanual and automated tests for various cloud services. I wrote documentation\ntoo. Above that, I am competent and I am always passionate about what I get my\nhands on.\n\nProject summary\n\nCurrently Jenkins plugin documentation is being stored in Confluence. Sometimes\nthe documentation is scattered and outdated. In order to improve the situation we\nwould like to follow the documentation-as-code approach and to put docs to\nplugin repositories and then publish them on the project website using the\nawestruct engine. The project aims an implementation of a documentation\ncontinuous deployment flow powered by Jenkins and Pipeline Plugin.\n\nThe idea is to automatically pull in the README and other docs from GitHub, show\nchangelogs with versions and releases dates. I will be designing file templates\nthat will contain most of the  docs information that will be required from\nplugin developers. Initially the files will be written in\nAsciiDoc. Plugin developers will get a chance to\nreview the templates. The templates will be prototyped by various plugin\ndevelopers.\n\nThe docs that will be automatically pulled from github and will be published on\nJenkins.io under the Documentation section.\n\nMy mentors are R.Tyler and\nBaptiste Mathus\n\nI hope to achieve this by 25th June when we will be having our mid-term\nevaluations.\n\nI will update more on the progress.\n\nLinks\n\nGsoc Page\n\nJenkins Gsoc Page\n\nMailing List discussion on Jenkins-Developers\n\nMy blog on Medium","title":"GSOC Project Intro: Automatic Plugin Documentation","tags":["gsoc","plugins"],"authors":[{"avatar":null,"blog":null,"github":"anyangocynthia","html":"","id":"cynthia","irc":null,"linkedin":null,"name":"Cynthia Anyango","slug":"/blog/authors/cynthia","twitter":"annyanngo"}]}},{"node":{"date":"2016-05-31T00:00:00.000Z","id":"ad068db4-bb27-52c3-9916-bb8b132c6e53","slug":"/blog/2016/05/31/pipeline-snippetizer/","strippedHtml":"Those of you updating the Pipeline Groovy plugin\nto 2.3 or later will notice a change to the appearance of the configuration form.\nThe Snippet Generator tool is no longer a checkbox enabled inside the configuration page.\nRather, there is a link Pipeline Syntax which opens a separate page with several options.\n(The link appears in the project’s sidebar; Jenkins 2 users will not see the sidebar from the configuration screen,\nso as of 2.4 there is also a link beneath the Pipeline definition.)\n\nSnippet Generator continues to be available for learning the available\nPipeline steps and creating sample calls given various configuration options.\nThe new page also offers clearer links to static reference documentation, online\nPipeline documentation resources, and an IntelliJ IDEA code completion file\n(Eclipse support is unfinished).\n\nOne motivation for this change\n( JENKINS-31831) was to\ngive these resources more visual space and more prominence.  But another\nconsideration was that people using multibranch projects or organization folders\nshould be able to use Snippet Generator when setting up the project, before\nany code is committed.\n\nThose using\nPipeline\nMultibranch plugin or organization folder plugins should upgrade to 2.4 or\nlater to see these improvements as well.","title":"New display of Pipeline’s \"snippet generator\"","tags":["pipeline"],"authors":[{"avatar":null,"blog":null,"github":"jglick","html":"<div class=\"paragraph\">\n<p>Jesse has been developing Jenkins core and plugins for years.\nHe is the coauthor with Kohsuke of the core infrastructure of the Pipeline system.</p>\n</div>","id":"jglick","irc":null,"linkedin":null,"name":"Jesse Glick","slug":"/blog/authors/jglick","twitter":"tyvole"}]}},{"node":{"date":"2016-05-26T00:00:00.000Z","id":"4a650522-ee39-5465-887f-963c348a9754","slug":"/blog/2016/05/26/gsoc-jenkins-web-ui-project/","strippedHtml":"About me\n\nMy name is Samat Davletshin and I am from HSE University from Moscow, Russia. I\ninterned at Intel and Yandex, and cofounded a startup\nproject where I personally developed front-end and back-end of the website.\n\nI am excited to participate in GSoC with Jenkins this summer as a chanсe to make\na positive change for thousands of users as well as to learn from great mentors.\n\nAbstract\n\nAlthough powerful, Jenkins new job creation and configuration process may be non\nobvious and time consuming. This can be improved by making UI more intuitive,\nconcise, and functional. I plan to achieve this by creating a simpler new job\ncreation, configuration process focused on essential elements, and embedding new\nfunctionality.\n\nMy mentors are Kirill Merkushev and\nMichael Neale\n\nDeliverables\n\nNew job creation\n\nNew job name validation\n\nInitially, job validation was unresponsive, job creation was still allowed with\nan invalid name, and some allowed characters even crashed Jenkins. Happily, two\nof this problems were fixed in recent improvements and I plan add only a real\ntime name check for invalid characters.\n\nPopup window\n\nJenkins has a lot of windows reloads that may time consuming. The creation of\nnew job is a simple process requiring only job name and job type. This way UI\nmay be improved by reducing page reloads and putting new job creation interface\nin a dialog window. Such popup would likely consist of three steps of\nimplementation: rendering a dialog window, receiving JSON with job types,\nsending a POST request to create the job.\n\nConfiguration page\n\nChanging help information\n\nAs reported by some users, it would be useful to have the functionality to\nchange help information. Installation administrators would be able to change the\nhelp info and choose editing rights for other users. That would likely require a\ncreation of extension points and a plugin using them. I also would like to\ninclude the ability to style the help information using markdown as shown above.\n\n[Optional] The functionality is extended to creation of crowd sourced \"wiki like\" documentation\n\nAs in\nlocalization\nplugin the changes are gathered and applied beyond installation of a particular\nuser.\n\nMore intuitive configuration page.\n\nPursuing to solve this  issue\n\nAlthough there are a lot improvements in new configuration page, there is always\na room for improvements. An advanced job still has a very complicated and hard\nto read configuration page. It is still open to discussion, but I may approach\nit by better division of configuration parts such as an accordion based\nnavigation.\n\nHome page\n\n[Optional] Removing \"My Views\" page\n\n\"My Views\" page may unnecessary complicate essential sidepanel navigation. Since\nit contains very small functionality, the functions may be moved to the home\npage and the whole page may be removed. That may be implemented by adding icons\nto \"My Views\" tabs. Additionally, the standard view creation page can create\neither of the types\n\n[Optional] Reducing number of UI elements\n\nThe home page may contain some UI elements that are not essential and rarely\nused. This way elements \"enable auto refresh\", “edit description”, “icon sizes”,\n”legend”, “RSS” may be removed from home page and placed under \"Manage Jenkins\"\nor an upper menu. It is also possible to create new extension points to support\nnew UI elements through plugins.\n\nCredentials store page\n\n[Optional] Grouping credentials and their domains\n\nCredentials page has too many reloads and requires many clicks to get to a\nrequired credentials page. That may be improved by removing the last page and\nshowing credentials under domains.\n\nCurrent progress\n\nBy May 25th I learned about the structure and tools of Jenkins and started\nworking on the first project:\n\nI started with New Job Name validation first. Luckily, in last updates the\nchanges of recena there\nwere implemented all of the changes I proposed except real time check on name\nvalidity. Here I proposed the change which fixes it by\nsending GET request on keyup event in addition to blur.\n\nI also made a New Job Popup with using existing interface.\n\nView the current\npop-up progress\n\nI used Remodal library for popup and put\nthere\nexisting\nNew Job container. Surprisingly, it was fully functional right away. On the GIF\nyou can see that popup receives all job types and then successfully submits the\npost form creating a new job. I think that could be a good first step. Further I\ncan start changing the window itself.\n\nLinks\n\nInitial proposal of the project\n\nThe project discussion on mailing list\n\nJenkins GSoC Page\n\nProject repository","title":"GSoC Project Intro: Improving Job Creation/Configuration","tags":["core","gsoc"],"authors":[{"avatar":null,"blog":null,"github":"samatdav","html":"","id":"samatdav","irc":null,"linkedin":null,"name":"Samat Davletshin","slug":"/blog/authors/samatdav","twitter":null}]}},{"node":{"date":"2016-05-26T00:00:00.000Z","id":"bbd7c8d9-ad20-56ae-b97b-834547d227f7","slug":"/blog/2016/05/26/introducing-blue-ocean/","strippedHtml":"In recent years developers have become rapidly attracted to tools that are not\nonly functional but are designed to fit into their workflow seamlessly and are\na joy to use. This shift represents a higher standard of design and user\nexperience that Jenkins needs to rise to meet.\n\nWe are excited to share and invite the community to join us on a project we’ve\nbeen thinking about over the last few months called Blue Ocean.\n\nBlue Ocean is a project that rethinks the user experience of Jenkins, modelling\nand presenting the process of software delivery by surfacing information that’s\nimportant to development teams with as few clicks as possible, while still\nstaying true to the extensibility that is core to Jenkins.\n\nWhile this project is in the alpha stage of development, the intent is that\nJenkins users can install Blue Ocean side-by-side with the Jenkins Classic UI\nvia a plugin.\n\nNot all the features listed on this blog are complete but we will be hard at\nwork over the next few months preparing Blue Ocean for general use. We intend\nto provide regular updates on this blog as progress is made.\n\nBlue Ocean is open source today\nand we invite you to give us feedback and to contribute to the project.\n\nBlue Ocean will provide development teams:\n\nNew modern user experience\n\nThe UI aims to improve clarity, reduce clutter and navigational depth to make\nthe user experience very concise. A modern visual design gives developers much\nneeded relief throughout their daily usage and screens respond instantly to\nchanges on the server making manual page refreshes a thing of the past.\n\nAdvanced Pipeline visualisations with built-in failure diagnosis\n\nPipelines are visualised on screen along with the\nsteps and logs to allow simplified comprehension of the continuous delivery\npipeline – from the simple to the most sophisticated scenarios.\n\nScrolling through 10,000 line log files is a thing of the past. Blue Ocean\nbreaks down your log per step and calls out where your build failed.\n\nBranch and Pull Request awareness\n\nModern pipelines make use of multiple Git branches, and Blue Ocean is designed\nwith this in mind. Drop a Jenkinsfile into your Git\nrepository that defines your pipeline and Jenkins will automatically discover\nand start automating any  Branches and validating Pull Requests.\n\nJenkins will report the status of your pipeline right inside Github or\nBitbucket on all your commits, branches or pull requests.\n\nPersonalised View\n\nFavourite any pipelines, branches or pull requests and see them appear on your\npersonalised dashboard. Intelligence is being built into the dashboard. Jobs\nthat need your attention, say a Pipeline waiting for approval or a failing job\nthat you have recently changed, appear on the top of the dashboard.\n\nYou can read more about Blue Ocean and its goals on the\nproject page and developers should watch the\nDevelopers list for more information.\n\nFor Jenkins developers and plugin authors:\n\nJenkins Design “Language”\n\nThe Jenkins Design Language (JDL) is a set of standardised React components and\na style guide that help developers create plugins that retain the look and feel\nof Blue Ocean in an effortless way. We will be publishing more on the JDL,\nincluding the style guide and developer documentation, over the next few weeks.\n\nModern JavaScript toolchain\n\nThe Jenkins plugin tool chain has been extended so that developers can use\nES6,\nReact, NPM\nin their plugins without endless yak-shaving. Jenkins\njs-modules are already in use in\nJenkins today, and this builds on this, using the same tooling.\n\nClient side Extension points\n\nClient Side plugins use Jenkins plugin infrastructure. The Blue Ocean libraries\nbuilt on ES6 and React.js provide an extensible client side component model\nthat looks familiar to developers who have built Jenkins plugins before. Client\nside extension points can help isolate failure, so one bad plugin doesn’t take\na whole page down.\n\nServer Sent Events\n\nServer Sent Events\n(SSE) allow plugin developers to tap into changes of state on the server and make\ntheir UI update in real time ( watch this for a\ndemo).\n\nTo make Blue Ocean a success, we’re asking for help and support from Jenkins\ndevelopers and plugin authors. Please join in our Blue Ocean discussions on the\nJenkins Developer\nmailing list and the #jenkins-ux IRC channel on Freenode!\n\nLinks\n\nBlue Ocean project page\n\nBlue Ocean GitHub repository","title":"Introducing Blue Ocean: a new user experience for Jenkins","tags":["blueocean","ux","pipeline"],"authors":[{"avatar":null,"blog":null,"github":"i386","html":"","id":"i386","irc":null,"linkedin":null,"name":"James Dumay","slug":"/blog/authors/i386","twitter":"i386"}]}},{"node":{"date":"2016-05-25T00:00:00.000Z","id":"3bdee89e-566b-52d9-a3cc-f41507becadc","slug":"/blog/2016/05/25/update-plugin-for-pipeline/","strippedHtml":"This is a guest post by Chris Price.\nChris is a software engineer at Puppet, and has been\nspending some time lately on automating performance testing using the latest\nJenkins features.\n\nIn this blog post, I’m going to attempt to provide some step-by-step notes on\nhow to refactor an existing Jenkins plugin to make it compatible with the new\nJenkins Pipeline jobs.  Before we get to the fun stuff, though, a little\nbackground.\n\nHow’d I end up here?\n\nRecently, I started working on a project to automate some performance tests for\nmy company’s products.  We use the awesome Gatling load\ntesting tool for these tests, but we’ve largely been handling the testing very\nmanually to date, due to a lack of bandwidth to get them automated in a clean,\nmaintainable, extensible way.  We have a years-old Jenkins server where we use\nthe gatling jenkins\nplugin to track the\nhistory of certain tests over time, but the setup of the Jenkins instance was\nvery delicate and not easy to reproduce, so it had fallen into a state of\ndisrepair.\n\nOver the last few days I’ve been putting some effort into getting things more\nautomated and repeatable so that we can really maximize the value that we’re\ngetting out of the performance tests.  With some encouragement from the fine\nfolks in the #jenkins IRC channel, I ended up exploring\nthe JobDSL\nplugin and the new Pipeline jobs.  Combining those two\nthings with some Puppet code to provision a Jenkins server via the\njenkins puppet module gave me\na really nice way to completely automate my Jenkins setup and get a seed job in\nplace that would create my perf testing jobs.  And the Pipeline job format is\njust an awesome fit for what I wanted to do in terms of being able to easily\nmonitor the stages of my performance tests, and to make the job definitions\nmodular so that it would be really easy to create new performance testing jobs\nwith slight variations.\n\nSo everything’s going GREAT up to this point.  I’m really happy with how it’s\nall shaping up.  But then…​ (you knew there was a \"but\" coming, right?) I\nstarted trying to figure out how to add the\nGatling Jenkins\nplugin to the Pipeline jobs, and kind of ran into a wall.\n\nAs best as I could tell from my Googling, the plugin was probably going to\nrequire some modifications in order to be able to be used with Pipeline jobs.\nHowever, I wasn’t able to find any really cohesive documentation that\ndefinitively confirmed that or explained how everything fits together.\n\nEventually, I got it all sorted out.  So, in hopes of saving the next person a\nlittle time, and encouraging plugin authors to invest the time to get their\nplugins working with Pipeline, here are some notes about what I learned.\n\nSpoiler: if you’re just interested in looking at the individual git commits that\nI made on may way to getting the plugin working with Pipeline, have a look at\nthis github\nbranch.\n\nCreating a pipeline step\n\nThe main task that the Gatling plugin performs is to archive Gatling reports\nafter a run.  I figured that the end game for this exercise was that I was going\nto end up with a Pipeline \"step\" that I could include in my Pipeline scripts, to\ntrigger the archiving of the reports.  So my first thought was to look for an\nexisting plugin / Pipeline \"step\" that was doing something roughly similar, so\nthat I could use it as a model.  The Pipeline \"Snippet Generator\" feature\n(create a pipeline job, scroll down to the \"Definition\" section of its\nconfiguration, and check the \"Snippet Generator\" checkbox) is really helpful for\nfiguring out stuff like this; it is automatically populated with all of the\nsteps that are valid on your server (based on which plugins you have installed),\nso you can use it to verify whether or not your custom \"step\" is recognized, and\nalso to look at examples of existing steps.\n\nLooking through the list of existing steps, I figured that the archive step\nwas pretty likely to be similar to what I needed for the gatling plugin:\n\nSo, I started poking around to see what magic it was that made that archive\nstep show up there.  There are some mentions of this in the\npipeline-plugin\nDEVGUIDE.md and the\nworkflow-step-api-plugin\nREADME.md, but the real breakthrough for me was finding the definition of the\narchive step in the workflow-basic-steps-plugin source\ncode.\n\nWith that as an example, I was able to start poking at getting a\ngatlingArchive step to show up in the Snippet Generator.  The first thing that\nI needed to do was to update the gatling-plugin project’s pom.xml to depend\non a recent enough version of Jenkins, as well as specify dependencies on the\nappropriate pipeline\nplugins\n\nOnce that was out of the way, I noticed that the archive step had some tests\nwritten for it, using what looks to be a pretty awesome test API for pipeline\njobs and plugins.  Based on those archive\ntests,\nI added\na\nskeleton for a test for the gatlingArchive step that I was about to write.\n\nThen, I moved on to\nactually\ncreating the step.  The meat of the code was this:\n\npublic class GatlingArchiverStep extends AbstractStepImpl {\n    @DataBoundConstructor\n    public GatlingArchiverStep() {}\n\n    @Extension\n    public static class DescriptorImpl extends AbstractStepDescriptorImpl {\n        public DescriptorImpl() { super(GatlingArchiverStepExecution.class); }\n\n        @Override\n        public String getFunctionName() {\n            return \"gatlingArchive\";\n        }\n\n        @NonNull\n        @Override\n        public String getDisplayName() {\n            return \"Archive Gatling reports\";\n        }\n    }\n}\n\nNote that in that commit I also added a config.jelly file.  This is how you\ndefine the UI for your step, which will show up in the Snippet Generator.  In\nthe case of this Gatling step there’s really not much to configure, so my\nconfig.jelly is basically empty.\n\nWith that (and the rest of the code from that commit) in place, I was able to\nfire up the development Jenkins server (via mvn hpi:run, and note that you\nneed to go into the \"Manage Plugins\" screen on your development server and\ninstall the Pipeline plugin once before any of this will work) and visit the\nSnippet Generator to see if my step showed up in the dropdown:\n\nGREAT SUCCESS!\n\nThis step doesn’t actually do anything yet, but it’s recognized by Jenkins and\ncan be included in your pipeline scripts at that point, so, we’re on our way!\n\nThe step metastep\n\nThe step that we created above is a first-class DSL addition that can be used in\nPipeline scripts.  There’s another way to make your plugin work usable from a\nPipeline job, without making it a first-class build step.  This is by use of the\nstep\"metastep\", mentioned in the pipeline-plugin\nDEVGUIDE.\nWhen using this approach, you simply refactor your Builder or Publisher to\nextend SimpleBuildStep, and then you can reference the build step from the\nPipeline DSL using the step method.\n\nIn the Jenkins GUI, go to the config screen for a Pipeline job and click on the\nSnippet Generator checkbox.  Select 'step: General Build Step' from the\ndropdown, and then have a look at the options that appear in the 'Build Step'\ndropdown.  To compare with our previous work, let’s see what \"Archive the\nartifacts\" looks like:\n\nFrom the snippet generator we can see that it’s possible to trigger an Archive\naction with syntax like:\n\nstep([$class: 'ArtifactArchiver', artifacts: 'foo*', excludes: null])\n\nThis is the \"metastep\".  It’s a way to trigger any build action that implements\nSimpleBuildStep, without having to actually implement a real \"step\" that\nextends the Pipeline DSL like we did above.  In many cases, it might only make\nsense to do one or the other in your plugin; you probably don’t really need\nboth.\n\nFor the purposes of this tutorial, we’re going to do both.  For a couple of reasons:\n\nWhy the heck not?  :)  It’s a good demonstration of how the metastep stuff\nworks.\n\nBecause implementing the \"for realz\" step will be a lot easier if the Gatling\naction that we’re trying to call from our gatlingArchive() syntax is using the\nnewer Jenkins APIs that are required for subclasses of SimpleBuildStep.\n\nGatlingPublisher is the main build action that we’re interested in using in\nPipeline jobs.  So, with all of that in mind, here’s our next goal: get\nstep([$class: 'GatlingPublisher', …​) showing up in the Snippet Generator.\n\nThe javadocs for the SimpleBuildStep\nclass\nhave some notes on what you need to do when porting an existing Builder or\nPublisher over to implement the SimpleBuildStep interface.  In all\nlikelihood, most of what you’re going to end up doing is to replace occurrences\nof AbstractBuild with references to the Run class, and replace occurrences\nof AbstractProject with references to the Job class.  The APIs are pretty\nsimilar, so it’s not too hard to do once you understand that that’s the game.\nThere is some discussion of this in the pipeline-plugin\nDEVGUIDE.\n\nFor the Gatling plugin, my\ninitial\nefforts to port the GatlingPublisher over to implement SimpleBuildStep only\nrequired the AbstractBuild → Run refactor.\n\nAfter making these changes, I fired up the development Jenkins server, and, voila!\n\nSo, now, we can add a line like this to a Pipeline build script:\n\nstep([$class: 'GatlingPublisher', enabled: true])\n\nAnd it’ll effectively be the same as if we’d added the Gatling \"Post-Build\nAction\" to an old-school Freestyle project.\n\nWell…​ mostly.\n\nBuild Actions vs. Project Actions\n\nAt this point our modified Gatling plugin should work the same way as it always\ndid in a Freestyle build, but in a Pipeline build, it only partially works.\nSpecifically, the Gatling plugin implements two different \"Actions\" to surface\nthings in the Jenkins GUI: a \"Build\" action, which adds the Gatling icon to the\nleft sidebar in the GUI when you’re viewing an individual build in the build\nhistory of a job, and a \"Project\" action, which adds that same icon to the left\nsidebar of the GUI of the main page for a job.  The \"Project\" action also adds a\n\"floating panel\" on the main job page, which shows a graph of the historical\ndata for the Gatling runs.\n\nIn a Pipeline job, though, assuming we’ve added a call to the metastep, we’re\nonly seeing the \"Build\" actions.  Part of this is because, in the last round of\nchanges that I linked, we only modified the \"Build\" action, and not the\n\"Project\" action.  Running the metastep in a Pipeline job has no visible effect\nat all on the project/job page at this point.  So that’s what we’ll tackle next.\n\nThe key thing to know about getting \"Project\" actions working in a Pipeline job\nis that, with a Pipeline job, there is no way for Jenkins to know up front what\nsteps or actions are going to be involved in a job.  It’s only after the job\nruns once that Jenkins has a chance to introspect what all the steps were.  As\nsuch, there’s no list of Builders or Publishers that it knows about up front to\ncall getProjectAction on, like it would with a Freestyle job.\n\nThis is where\nSimpleBuildStep.LastBuildAction\ncomes into play.  This is an interface that you can add to your Build actions,\nwhich give them their own getProjectActions method that Jenkins recognizes and\nwill call when rendering the project page after the job has been run at least\nonce.\n\nSo, effectively, what we need to do is to\nget\nrid of the getProjectAction method on our Publisher class, modify the Build\naction to implement SimpleBuildStep.LastBuildAction, and encapsulate our\nProject action instances in the Build action.\n\nThe build action class now constructs an instance of the Project action and\nmakes it accessible via getProjectActions (which comes from the\nLastBuildAction interface):\n\npublic class GatlingBuildAction implements Action, SimpleBuildStep.LastBuildAction {\n    public GatlingBuildAction(Run build, List sims) {\n        this.build = build;\n        this.simulations = sims;\n\n        List projectActions = new ArrayList<>();\n        projectActions.add(new GatlingProjectAction(build.getParent()));\n        this.projectActions = projectActions;\n    }\n\n    @Override\n    public Collection getProjectActions() {\n        return this.projectActions;\n    }\n}\n\nAfter making these changes, if we run the development Jenkins server, we can see\nthat after the first successful run of the Pipeline job that calls the\nGatlingPublisher metastep, the Gatling icon indeed shows up in the sidebar on\nthe main project page, and the floating box with the graph shows up as well:\n\nMaking our DSL step do something\n\nSo at this point we’ve got the metastep syntax working from end-to-end, and\nwe’ve got a valid Pipeline DSL step ( gatlingArchive()) that we can use in our\nPipeline scripts without breaking anything…​ but our custom step doesn’t\nactually do anything.  Here’s the part where we tie it all together…​ and it’s\npretty easy!  All we need to do is to make our step \"Execution\" class\ninstantiate a Publisher and call perform on\nit.\n\nAs per the\nnotes\nin the pipeline-plugin DEVGUIDE, we can use the @StepContextParameter\nannotation to inject in the objects that we need to pass to the Publisher’s\nperform method:\n\npublic class GatlingArchiverStepExecution extends AbstractSynchronousNonBlockingStepExecution {\n\n    @StepContextParameter\n    private transient TaskListener listener;\n\n    @StepContextParameter\n    private transient FilePath ws;\n\n    @StepContextParameter\n    private transient Run build;\n\n    @StepContextParameter\n    private transient Launcher launcher;\n\n    @Override\n    protected Void run() throws Exception {\n        listener.getLogger().println(\"Running Gatling archiver step.\");\n\n        GatlingPublisher publisher = new GatlingPublisher(true);\n        publisher.perform(build, ws, launcher, listener);\n\n        return null;\n    }\n}\n\nAfter these changes, we can fire up the development Jenkins server, and hack up\nour Pipeline script to call gatlingArchive() instead of the metastep\nstep([$class: 'GatlingPublisher', enabled: true]) syntax.  One of these is\nnicer to type and read than the other, but I’ll leave that as an exercise for\nthe reader.\n\nFin\n\nWith that, our plugin now works just as well in the brave new Pipeline world as\nit did in the olden days of Freestyle builds.  I hope these notes save someone\nelse a little bit of time and googling on your way to writing (or porting) an\nawesome plugin for Jenkins Pipeline jobs!\n\nLinks\n\nJenkins Pipeline Overview\n\nPipeline Plugin Developer Guide\n\nJenkins Source Code\n\nWorkflow Step API Plugin\n\nWorkflow Basic Steps Plugin","title":"Refactoring a Jenkins plugin for compatibility with Pipeline jobs","tags":["core","pipeline","plugins"],"authors":[{"avatar":null,"blog":null,"github":"cprice404","html":"<div class=\"paragraph\">\n<p>Chris is a software engineer at Puppet, who mostly works on backend services\nfor Puppet itself, but occasionally gets to spend some time improving CI\nand automation using Jenkins.</p>\n</div>","id":"cprice404","irc":null,"linkedin":null,"name":"Chris Price","slug":"/blog/authors/cprice404","twitter":"cprice404"}]}},{"node":{"date":"2016-05-23T00:00:00.000Z","id":"85257451-d116-575c-b893-3c51d7386caa","slug":"/blog/2016/05/23/external-workspace-manager-plugin/","strippedHtml":"About myself\n\nMy name is Alexandru Somai.\nI’m following a major in Software Engineering at the Babes-Bolyai University of Cluj-Napoca, Romania.\nI have more than two years hands-on experience working in Software Development.\n\nI enjoy writing code in Java, Groovy and JavaScript.\nThe technologies and frameworks that I’m most familiar with are: Spring Framework, Spring Security, Hibernate,\nJMS, Web Services, JUnit, TestNG, Mockito.\nAs build tools and continuous integration, I’m using Maven and Jenkins.\nI’m a passionate software developer who is always learning, always looking for new challenges.\nI want to start contributing to the open source community and Google Summer of Code is a starting point for me.\n\nProject summary\n\nCurrently, Jenkins’ build workspace may become very large in size due to the fact that some compilers generate\nvery large volumes of data.\nThe existing plugins that share the workspace across builds are able to do this by copying the files from\none workspace to another, process which is inefficient.\nA solution is to have a Jenkins plugin that is able to manage and reuse the same workspace between multiple builds.\n\nAs part of the Google Summer of Code 2016 I will be working on\nthe External Workspace Manager plugin.\nMy mentors for this project are Oleg Nenashev\nand Martin d’Anjou.\nThis plugin aims to provide an external workspace management system.\nIt should facilitate workspace share and reuse across multiple Jenkins jobs.\nIt should eliminate the need to copy, archive or move files.\nThe plugin will be written for Pipeline jobs.\n\nUsage\n\nPrerequisites\n\nMultiple physical disks accessible from controller.\n\nThe same physical disks must be accessible from Jenkins Nodes (renamed to Agents in Jenkins 2.0).\n\nIn the Jenkins global configuration, define a disk pool (or many) that will contain the physical disks.\n\nIn each Node configuration, define the mounting point from the current node to each physical disk.\n\nThe following diagram gives you an overview of how an External Workspace Manager configuration may look like:\n\nExample one\n\nLet’s assume that we have one Jenkins job. In this job, we want to use the same workspace on multiple Jenkins nodes.\nOur pipeline code may look like this:\n\nstage ('Stage 1. Allocate workspace')\ndef extWorkspace = exwsAllocate id: 'diskpool1'\n\nnode ('linux') {\n    exws (extWorkspace) {\n        stage('Stage 2. Build on the build server')\n        git url: '...'\n        sh 'mvn clean install'\n    }\n}\n\nnode ('test') {\n    exws (extWorkspace) {\n        stage('Stage 3. Run tests on a test machine')\n        sh 'mvn test'\n    }\n}\n\nNote: The stage() steps are optional from the External Workspace Manager plugin perspective.\n\nStage 1. Allocate workspace\n\nThe exwsAllocate step selects a disk from diskpool1\n(default behavior: the disk with the most available size).\nOn that disk, let’s say disk1, it allocates a directory.\nThe computed directory path is: /physicalPathOnDisk/$JOB_NAME/$BUILD_NUMBER.\n\nFor example, Let’s assume that the $JOB_NAME is integration and the $BUILD_NUMBER is 14.\nThen, the resulting path is: /jenkins-project/disk1/integration/14.\n\nStage 2. Build on the build server\n\nAll the nodes labeled linux must have access to the disks defined in the disk pool.\nIn the Jenkins Node configurations we have defined the local paths that are the mounting points to each disk.\n\nThe exws step concatenates the node’s local path with the path returned by the exwsAllocate step.\nIn our case, the node labeled linux has its local path to disk1 defined as: /linux-node/disk1/.\nSo, the complete workspace path is: /linux-node/disk1/jenkins-project/disk1/integration/14.\n\nStage 3. Run tests on a test machine\n\nFurther, we want to run our tests on a different node, but we want to reuse the previously created workspace.\n\nIn the node labeled test we have defined the local path to disk1 as: /test-node/disk1/.\nBy applying the exws step, our tests will be able to run in the same workspace as the build.\nTherefore, the path is: /test-node/disk1/jenkins-project/disk1/integration/14.\n\nExample two\n\nLet’s assume that we have two Jenkins jobs, one called upstream and the other one called downstream.\nIn the upstream job, we clone the repository and build the project, and in the downstream job we run the tests.\nIn the downstream job we don’t want to clone and re-build the project, we need to use the same\nworkspace created in the upstream job.\nWe have to be able to do so without copying the workspace content from one location to another.\n\nThe pipeline code in the upstream job is the following:\n\nstage ('Stage 1. Allocate workspace in the upstream job')\ndef extWorkspace = exwsAllocate id: 'diskpool1'\n\nnode ('linux') {\n    exws (extWorkspace) {\n        stage('Stage 2. Build in the upstream job')\n           git url: '...'\n           sh 'mvn clean install'\n    }\n}\n\nAnd the downstream 's pipeline code is:\n\nstage ('Stage 3. Allocate workspace in the downstream job')\ndef extWorkspace = exwsAllocate id: 'diskpool1', upstream: 'upstream'\n\nnode ('test') {\n    exws (extWorkspace) {\n        stage('Stage 4. Run tests in the downstream job')\n        sh 'mvn test'\n    }\n}\n\nStage 1. Allocate workspace in the upstream job\n\nThe functionality is the same as in example one - stage 1.\nIn our case, the allocated directory on the physical disk is: /jenkins-project/disk1/upstream/14.\n\nStage 2. Build in the upstream job\n\nSame functionality as example one - stage 2.\nThe final workspace path is: /linux-node/disk1/jenkins-project/disk1/upstream/14.\n\nStage 3. Allocate workspace in the downstream job\n\nBy passing the upstream parameter to the exwsAllocate step,\nit selects the most recent stable upstream workspace (default behavior).\nThe workspace path pattern is like this: /physicalPathOnDisk/$UPSTREAM_NAME/$MOST_RECENT_STABLE_BUILD.\nLet’s assume that the last stable build number is 12, then the resulting path is:\n/jenkins-project/disk1/upstream/12.\n\nStage 4. Run tests in the downstream job\n\nThe exws step concatenates the node’s local path with the path returned by the exwsAllocate step in stage 3.\nIn this scenario, the complete path for running tests is: /test-node/disk1/jenkins-project/disk1/upstream/12.\nIt will reuse the workspace defined in the upstream job.\n\nAdditional details\n\nYou may find the complete project proposal, along with the design details, features, more examples and use cases,\nimplementation ideas and milestones in the design document.\nThe plugin repository will be available on GitHub.\n\nA prototype version of the plugin should be available in late June and the releasable version in late August.\nI will be holding plugin functionality demos within the community.\n\nI do appreciate any feedback.\nYou may add comments in the design document.\nIf you are interested to have a verbal conversation, feel free to join our regular meetings on Mondays at\n12:00 PM UTC\non the Jenkins hangout.\nI will be posting updates from time to time about the plugin status on the\nJenkins developers mailing list.\n\nLinks\n\nDesign document\n\nGSoC program\n\nJenkins GSoC Page\n\nProject repository","title":"GSoC Project Intro: External Workspace Manager Plugin","tags":["pipeline","plugins","gsoc"],"authors":[{"avatar":null,"blog":null,"github":"alexsomai","html":"","id":"alexsomai","irc":null,"linkedin":null,"name":"Alexandru Somai","slug":"/blog/authors/alexsomai","twitter":"alex_somai"}]}}]}},"pageContext":{"limit":8,"skip":400,"numPages":100,"currentPage":51}},
    "staticQueryHashes": ["3649515864"]}