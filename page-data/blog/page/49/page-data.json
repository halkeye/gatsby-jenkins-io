{
    "componentChunkName": "component---src-templates-blog-list-template-js",
    "path": "/blog/page/49",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2016-08-26T00:00:00.000Z","id":"ddf1a677-43d3-5209-a6e7-d331d574df38","slug":"/blog/2016/08/26/ask-the-experts-jenkins-world/","strippedHtml":"Our events officer Alyssa has been working for\nthe past several weeks to organize the \"Open Source Hub\" at\nJenkins World 2016. The Hub\nis a location on the expo floor where contributors to the Jenkins project can hang\nout, share demos and help Jenkins users via the \"Ask the Experts\" program. Thus\nfar we have a great list of experts who have volunteered to help staff the\nbooth, which includes many frequent contributors, JAM\norganizers and board members.\n\nA few of the friendly folks you will see at Jenkins World are:\n\nPaul Allen -\nP4 Plugin\nmaintainer and Pipeline contributor.\n\nR Tyler Croy -\nJenkins infrastructure maintainer and\nboard member.\n\nJesse Glick - Pipeline\nmaintainer and long-time contributor to Jenkins\ncore.\n\nEddú Meléndez Gonzales - Organizer for\nthe Lima (Perú)\nJenkins Area Meetup and contributor to Spring.\n\nJon Hermansen - Organizer for the\nLos Angeles\nJenkins Area Meetup, developer and Pipeline user.\n\nOwen Mehegan -\nGitLab plugin\ncontributor, release engineer and copy editor for jenkins.io.\n\nOleg Nenashev -\nGoogle Summer of Code organizer, maintainer of multiple\nplugins and St.\nPetersburg Jenkins Area Meetup organizer.\n\nChristopher Orr - Maintainer of multiple\nAndroid-related plugins, including the\nAndroid\nEmulator plugin and contributor to numerous projects behind the scenes of\nJenkins.\n\nCasey Vega - Organizer for the\nLos Angeles\nJenkins Area Meetup and release engineer at Verizon Digital Media.\n\nMark Waite - Maintainer of the\nGit plugin and\ncontributor to a number of other Git-related plugins.\n\nDean Yu - Long-time contributor, board member\nand release engineer at Shutterfly.\n\nI hope that this list isn’t exhaustive! If you are an active member of the\nJenkins community and/or a contributor, consider taking part in the \"Ask the\nExperts\" program. It’s a great opportunity to bond with other contributors and\ntalk with fellow users at Jenkins World.\n\nYou will be able to find us in the expo hall under the \"Open Source Hub\" sign;\nplease stop by at Jenkins World to say hello, pick up stickers and to ask\nquestions!\n\nRegister for Jenkins World in\nSeptember with the code JWFOSS for a 20% discount off your pass.","title":"Ask the Experts at Jenkins World 2016","tags":["event","jenkinsworld","jenkinsworld2016"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-08-24T00:00:00.000Z","id":"9b375bbf-3fa2-5403-8c8a-8d26e0d78938","slug":"/blog/2016/08/24/jenkins-world-2016-festivities/","strippedHtml":"At Jenkins World 2016 on\nSeptember 14-15, stop by the \"Open Source Hub\", located in the Partner Expo\nhall at the Santa Clara Convention Center in Santa Clara, CA. The Open Source\nHub will have many Jenkins contributors, committers, JAM leaders, and\nofficers from\nthe governance board under one roof, so there will be plenty of knowledge and\ntalents on hand to share. We hope you’ll join in on the festivities.\n\nAsk the Experts\n\nThe setup that is waiting for you: white boards, monitors and lots of brain\npower to help answer those Jenkins questions that have been keeping you up at\nnight.  Jenkins experts can help with beginner questions to the more advanced\nones. All you need to do is bring your laptop and your questions; the experts\nwill help answer them!\n\nRegister for Jenkins World in\nSeptember with the code JWFOSS for a 20% discount off your pass.\n\nLive Demos\n\nSometimes seeing is believing, there will be plenty of demos in the \"Open\nSource Hub\" during the lunch hours on Wednesday September 14th, and Thursday\nSeptember 15th in the expo hall. Jenkins experts will be show-casing their\nfavorite Jenkins features, plugins and projects. Grab your lunch, take a seat\nin the open source theater to learn about:\n\nPipelines for Building and Deploying Android Apps by Android Emulator\nplugin maintainer Chris Orr\n\nGit Plugin - Large Repos, Submodule Authentication, and more by Git plugin\nmaintainer Mark Waite\n\nDocker and Pipeline by Jenkins infrastructure contributor\nR Tyler Croy\n\nExtending Pipeline with Libraries by Pipeline plugin maintainer\nJesse Glick\n\nBlue Ocean in Action by Blue Ocean contributor\nKeith Zantow\n\nExternal Workspace Manager plugin for Pipeline by\nGoogle Summer of Code student\nAlexandru Somai\n\nAnd many more\n\nJenkins Mural\n\nJenkins World participants will take part in the realization of a giant\ncollaborative mural painting with the\nCommitStrip team.  Thomas, the writer and\nEtienne, the cartoonist, teamed up with a few Jenkins contributors to design a\n5m x 2m mmural which will be drawn live! Brushes and colors will be\navailable for all attendees who wish to help paint this one of a kind piece of\nJenkins art.\n\nSticker Swap\n\nJenkins World attendees will have a chance to swap stickers. There will be a\ntable where attendees are welcome to place/take stickers. Bring your cool\nstickers to share with others and take stickers that interest you.\n\nAfter Dark Reception Sponsored by CloudBees\n\nAfter Dark reception will be from 6-8pm on Wed Sept 14 in the Partner Expo.\nEnjoy cocktails, appetizers, mingle, and dance to a live band. A big THANK\nYOU\ngoes out to CloudBees for their generous contributions! See you at After Dark!\n\nContributor Summit - Tuesday, September 13\n\nIf Blue Ocean, Pipeline and Storage Pluggability sounds interesting to you,\njoin the interactive discussions surrounding these topics. The Jenkins project\nis also looking to hear use-cases, war stories, and pain points. The objective\nof the summit is to work towards improving the Jeknins project.\nSeats are limited.\n\nDon’t forget to register; I look forward to\nseeing you at the conference!\n\nLinks\n\nJenkins World 2016\n\nAcknowledgements\n\nSpecial thanks to CloudBees as the premier\nsponsor and BlazeMeter, Microsoft, Red\nHat and all the other sponsors who have made this event possible.","title":"Jenkins World 2016 Festivities","tags":["event","jenkinsworld","jenkinsworld2016"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#281818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg","srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/8d248/alyssat.jpg 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/c004c/alyssat.jpg 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/9e67b/alyssat.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/22924/alyssat.webp 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/89767/alyssat.webp 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/40d97/alyssat.webp 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/5028e/alyssat.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":166}}},"blog":null,"github":"alyssat","html":"<div class=\"paragraph\">\n<p>Member of the <a href=\"/sigs/advocacy-and-outreach/\">Jenkins Advocacy and Outreach SIG</a>.\nAlyssa drives and manages Jenkins participation in community events and conferences like <a href=\"https://fosdem.org/\">FOSDEM</a>, <a href=\"https://www.socallinuxexpo.org/\">SCaLE</a>, <a href=\"https://events.linuxfoundation.org/cdcon/\">cdCON</a>, and <a href=\"https://events19.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2019/\">KubeCon</a>.\nShe is also responsible for Marketing &amp; Community Programs at <a href=\"https://cloudbees.com\">CloudBees, Inc.</a></p>\n</div>","id":"alyssat","irc":null,"linkedin":null,"name":"Alyssa Tong","slug":"/blog/authors/alyssat","twitter":null}]}},{"node":{"date":"2016-08-22T00:00:00.000Z","id":"5a42a05c-41f1-5105-866c-97942f6d788d","slug":"/blog/2016/08/22/ewm-stable-release/","strippedHtml":"This blog post is the last one from the series of\nGoogle Summer of Code 2016, External Workspace Manager Plugin project.\nThe previous posts are:\n\nIntroductory blog post\n\nAlpha release announcement\n\nBeta release announcement\n\nIn this post I would like to announce the 1.0.0 release of the External Workspace Manager Plugin version to the main\nupdate center.\n\nHere’s a highlight of the available features:\n\nWorkspace share and reuse across multiple jobs, running on different nodes\n\nAutomatic workspace cleanup\n\nProvide custom workspace path on the disk\n\nDisk Pool restrictions\n\nFlexible Disk allocation strategies\n\nAll the above are detailed, with usage examples, on the plugin’s\ndocumentation page.\n\nFuture work\n\nCurrently, there is work in progress for the workspace browsing feature (see pull request\n#37).\nAfterwards, I’m planning to integrate fingerprints, so that the user can view a specific workspace in which\nother jobs was used.\nA particular feature that would be nice to have is to integrate the plugin with at least one disk provider\n(e.g. Amazon EBS, Google Cloud Storage).\n\nMany other features and improvements are still to come, they are grouped in the phase 3 EPIC:\nJENKINS-37543.\nThe plugin’s repository is on GitHub.\nIf you’d like to come up with new features or ideas, contributions are very welcome.\n\nClosing\n\nThis was a Google Summer of Code 2016 project.\nA summary of the contributions that I’ve made to the Jenkins project during this time may be found\nhere.\nIt was a great experience, from which I learned a lot, and I’d wish I could repeat it every year.\n\nI’d like to thank to my mentors, Oleg Nenashev and\nMartin d’Anjou for all their support, good advices and help they gave me.\nAlso, thanks to the Jenkins contributors with which I have interacted and helped me during this period.\n\nIf you have any issues in setting up or using the plugin, please feel free to ask me on the plugin’s Gitter\nchat.\nAny feedback is welcome, and you may provide it either on the Gitter chat, or on\nJira by using the external-workspace-manager-plugin component.\n\nLinks\n\nProject repository\n\nWork product page\n\nGSoC page\n\nJenkins GSoC Page","title":"GSoC: External Workspace Manager for Pipeline is released","tags":["pipeline","plugins","external-workspace-manager","gsoc"],"authors":[{"avatar":null,"blog":null,"github":"alexsomai","html":"","id":"alexsomai","irc":null,"linkedin":null,"name":"Alexandru Somai","slug":"/blog/authors/alexsomai","twitter":"alex_somai"}]}},{"node":{"date":"2016-08-17T00:00:00.000Z","id":"a1ee11d9-c611-5f2a-a517-91048345fcf6","slug":"/blog/2016/08/17/jenkins-world-speaker-blog-aquilent/","strippedHtml":"This is a guest post by Jenkins World speaker Neil Hunt, Senior DevOps Architect at Aquilent.\n\nIn smaller companies with a handful of apps and fewer silos, implementing CD\npipelines to support these apps is fairly straightforward using one of the many\ndelivery orchestration tools available today. There is likely a constrained\ntool set to support - not an abundance of flavors of applications and security\npractices - and generally fewer cooks in the kitchen. But in a larger\norganization, I have found that in the past, there were seemingly endless\nunique requirements and mountains to climb to reach this level of automation on\neach new project.\n\nNeil will be presenting more\nof this concept at Jenkins World in\nSeptember, register with the code JWFOSS for a 20% discount off your pass.\n\nEnter the Jenkins Pipeline plugin. My recently departed former company, a large\nfinancial services organization with a 600+ person IT organization and 150+\napplication portfolio, set out to implement continuous delivery\nenterprise-wide. After considering several pipeline orchestration tools, we\ndetermined the Pipeline plugin (at the time called Workflow) to be the superior\nsolution for our company. Pipeline has continued Jenkins' legacy of presenting\nan extensible platform with just the right set of features to allow\norganizations to scale its capabilities as they see fit, and do so rapidly. As\nearly adopters of Pipeline with a protracted set of requirements, we used it\nboth to accelerate the pace of onboarding new projects and to reduce the\nongoing feature delivery time of our applications.\n\nIn my presentation at Jenkins World, I will demonstrate the methods we used to\nenable this. A few examples:\n\nWe leveraged the Pipeline Remote File Loader plugin to write shared common\ncode and sought and received community enhancements to these functions.\n\nJenkinsfile, loading a shared AWS utilities function library\n\nawsUtils.groovy, snippets of some AWS functions\n\nWe migrated from EC2 agents to Docker-based agents running on Amazon’s\nElastic Container Service, allowing us to spin up new executors in seconds\nand for teams to own their own executor definitions.\n\nPipeline run #1 using standard EC2 executors, spinning up EC2 instance for each\nnode; Pipeline run #2 using shared ECS cluster with near-instant instantiation\nof a Docker agent in the cluster for each node.\n\nWe also created a Pipeline Library of common pipelines, enabling projects\nthat fit certain models to use ready-made end-to-end pipelines. Some\nexamples:\n\nMaven JAR Pipeline: Pipeline that clones git repository, builds JAR file\nfrom pom.xml, deploys to Artifactory, and runs maven release plugin to\nincrement next version\n\nAnuglar.JS Pipeline: Pipeline that executes a grunt and bower build, then\nruns S3 sync to Amazon S3 bucket in Dev, then Stage, then Prod buckets.\n\nPentaho Reports Pipeline: Pipeline that clones git repository, constructs\nzip file, and executes Pentaho Business Intelligence Platform CLI to import new\nset of reports in Dev, Stage, then Prod servers.\n\nPerhaps most critically, a shout-out to the saving grace of this quest for our\nsecurity and ops teams: the manual 'input' step! While the ambition of\ncontinuous delivery is to have as few of these as possible, this was the\nsingle-most pivotal feature in convincing others of Pipeline’s viability, since\nnow any step of the delivery process could be gate-checked by an LDAP-enabled\npermission group. Were it not for the availability of this step, we may still\nbe living in the world of: \"This seems like a great tool for development, but\nwe will have a segregated process for production deployments.\" Instead, we had\na pipeline full of many 'input' steps at first, and then used the data we\ncollected around the longest delays to bring management focus to them and unite\neveryone around the goal of strategically removing them, one by one.\n\nGoing forward, having recently joined Aquilent’s Cloud Solutions Architecture\nteam, I’ll be working with our project teams here to further mature the use of\nthese Pipeline plugin features as we move towards continuous delivery. Already,\nwe have migrated several components of our healthcare.gov project to Pipeline.\nThe team has been able to consolidate several Jenkins jobs into a single,\nvisible delivery pipeline, to maintain the lifecycle of the pipeline with our\napplication code base in our SCM, and to more easily integrate with our\nexternal tools.\n\nDue to functional shortcomings in the early adoption stages of the Pipeline\nplugin and the ever-present political challenges of shifting organizational\npolicy, this has been and continues to be far from a bruise-free journey. But\nwe plodded through many of these issues to bring this to fruition and\nultimately reduced the number of manual steps in some pipelines from 12 down to\n1 and brought our 20+ Jenkins-minute pipelines to only six minutes after months\nof iteration. I hope you’ll join this session at Jenkins World and learn about\nour challenges and successes in achieving the promise of continuous delivery at\nenterprise scale.","title":"Continuously Delivering Continuous Delivery Pipelines","tags":["event","jenkinsworld","jenkinsworld2016"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"","id":"hinman","irc":null,"linkedin":null,"name":"Hannah Inman","slug":"/blog/authors/hinman","twitter":null}]}},{"node":{"date":"2016-08-11T00:00:00.000Z","id":"9a0e4574-a766-5c76-b7d9-8fc1d1fbcde3","slug":"/blog/2016/08/11/speaker-blog-edx-jenkins-world/","strippedHtml":"This is a guest post by Ben Patterson, Engineering Manager at\nedX.\n\nPicking a pear from a basket is straightforward when you can hold it in your hand, feel its weight, perhaps give a gentle squeeze, observe its color and look more closely at any bruises. If the only information we had was a photograph from one angle, we’d have to do some educated guessing.\n\nAs developers, we don’t get a photograph; we get a green checkmark or a red x. We use that to decide whether or not we need to switch gears and go back to a pull request we submitted recently. At edX, we take advantage of some Jenkins features that could give us more granularity on GitHub pull requests, and make that decision less of a guessing game.\n\nMultiple contexts reporting back when they’re available\n\nPull requests on our platform are evaluated from several angles: static code analysis including linting and security audits, javascript unit tests, python unit tests, acceptance tests and accessibility tests. Using an elixir of plugins, including the GitHub Pull Request Builder Plugin, we put more direct feedback into the hands of the contributor so s/he can quickly decide how much digging is going to be needed.\n\nFor example, if I made adjustments to my branch and know more requirements are coming, then I may not be as worried about passing the linter; however, if my unit tests have failed, I likely have a problem I need to address regardless of when the new requirements arrive. Timing is important as well. Splitting out the contexts means we can run tests in parallel and report results faster.\n\nDevelopers can re-run specific contexts\n\nOccasionally the feedback mechanism fails. It is oftentimes a flaky condition in a test or in test setup. (Solving flakiness is a different discussion I’m sidestepping. Accept the fact that the system fails for purposes of this blog entry.) Engineers are armed with the power of re-running specific contexts, also available through the PR plugin. A developer can say “jenkins run bokchoy” to re-run the acceptance tests, for example. A developer can also re-run everything with “jenkins run all”. These phrases are set through the GitHub Pull Request Builder configuration.\n\nMore granular data is easier to find for our Tools team\n\nSplitting the contexts has also given us important data points for our Tools team to help in highlighting things like flaky tests, time to feedback and other metrics that help the org prioritize what’s important. We use this with a log aggregator (in our case, Splunk) to produce valuable reports such as this one.\n\nI could go on! The short answer here is we have an intuitive way of divvying up our tests, not only for optimizing the overall amount of time it takes to get build results, but also to make the experience more user-friendly to developers.\n\nBen will be presenting more on this topic at\nJenkins World in September,\nregister with the code JWFOSS for a 20% discount off your pass.","title":"Using Jenkins for Disparate Feedback on GitHub","tags":["event","jenkinsworld","jenkinsworld2016"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"","id":"hinman","irc":null,"linkedin":null,"name":"Hannah Inman","slug":"/blog/authors/hinman","twitter":null}]}},{"node":{"date":"2016-08-10T00:00:00.000Z","id":"4f2eb2b7-1bb1-501e-8b2c-1b8c4ac3e6e3","slug":"/blog/2016/08/10/rails-cd-with-pipeline/","strippedHtml":"This is a guest post by R. Tyler Croy, who is a\nlong-time contributor to Jenkins and the primary contact for Jenkins project\ninfrastructure. He is also a Jenkins Evangelist at\nCloudBees, Inc.\n\nWhen the Ruby on Rails framework debuted it\nchanged the industry in two noteworthy ways: it created a trend of opinionated web\napplication frameworks ( Django,\nPlay, Grails) and it\nalso strongly encouraged thousands of developers to embrace test-driven\ndevelopment along with many other modern best practices (source control, dependency\nmanagement, etc). Because Ruby, the language underneath Rails, is interpreted\ninstead of compiled there isn’t a \"build\" per se but rather tens, if not\nhundreds, of tests, linters and scans which are run to ensure the application’s\nquality. With the rise in popularity of Rails, the popularity of application\nhosting services with easy-to-use deployment tools like Heroku or\nEngine Yard rose too.\n\nThis combination of good test coverage and easily automated deployments\nmakes Rails easy to continuously deliver with Jenkins. In this post we’ll cover\ntesting non-trivial Rails applications with Jenkins\nPipeline and, as an added bonus, we will add security scanning via\nBrakeman and the\nBrakeman\nplugin.\n\nTopics\n\nPreparing the app\n\nPreparing Jenkins\n\nWriting the Pipeline\n\nSecurity scanning\n\nDeploying the good stuff\n\nWrap up\n\nFor this demonstration, I used Ruby Central 's\ncfp-app :\n\nA Ruby on Rails application that lets you manage your conference’s call for\nproposal (CFP), program and schedule. It was written by Ruby Central to run the\nCFPs for RailsConf and RubyConf.\n\nI chose this Rails app, not only because it’s a sizable application with lots\nof tests, but it’s actually the application we used to collect talk proposals\nfor the \" Community Tracks\" at this\nyear’s Jenkins World. For the most part,\ncfp-app is a standard Rails application. It uses\nPostgreSQL for its database,\nRSpec for its tests and\nRuby 2.3.x as its runtime.\n\nIf you prefer to just to look at the code, skip straight to the\nJenkinsfile.\n\nPreparing the app\n\nFor most Rails applications there are few, if any, changes needed to enable\ncontinuous delivery with Jenkins. In the case of\ncfp-app, I added two gems to get\nthe most optimal integration into Jenkins:\n\nci_reporter, for test report\nintegration\n\nbrakeman, for security scanning.\n\nAdding these was simple, I just needed to update the Gemfile and the\nRakefile in the root of the repository to contain:\n\nGemfile\n\n# .. snip ..\ngroup :test do\n  # RSpec, etc\n  gem 'ci_reporter'\n  gem 'ci_reporter_rspec'\n  gem \"brakeman\", :require => false\nend\n\nRakefile\n\n# .. snip ..\nrequire 'ci/reporter/rake/rspec'\n# Make sure we setup ci_reporter before executing our RSpec examples\ntask :spec => 'ci:setup:rspec'\n\nPreparing Jenkins\n\nWith the cfp-app project set up, next on the list is to ensure that Jenkins itself\nis ready. Generally I suggest running the latest LTS of\nJenkins; for this demonstration I used Jenkins 2.7.1 with the following\nplugins:\n\nPipeline plugin\n\nBrakeman plugin\n\nCloudBees\nDocker Pipeline plugin\n\nI also used the\nGitHub\nOrganization Folder plugin to automatically create pipeline items in my\nJenkins instance; that isn’t required for the demo, but it’s pretty cool to see\nrepositories and branches with a Jenkinsfile automatically show up in\nJenkins, so I recommend it!\n\nIn addition to the plugins listed above, I also needed at least one\nJenkins agent with the Docker daemon installed and\nrunning on it. I label these agents with \"docker\" to make it easier to assign\nDocker-based workloads to them in the future.\n\nAny Linux-based machine with Docker installed will work, in my case I was\nprovisioning on-demand agents with the\nAzure\nplugin which, like the\nEC2 plugin,\nhelps keep my test costs down.\n\nIf you’re using Amazon Web Services, you might also be interested in\nthis blog post from\nearlier this year unveiling the\nEC2\nFleet plugin for working with EC2 Spot Fleets.\n\nWriting the Pipeline\n\nTo make sense of the various things that the Jenkinsfile needs to do, I find\nit easier to start by simply defining the stages of my pipeline. This helps me\nthink of, in broad terms, what order of operations my pipeline should have.\nFor example:\n\n/* Assign our work to an agent labelled 'docker' */\nnode('docker') {\n    stage 'Prepare Container'\n    stage 'Install Gems'\n    stage 'Prepare Database'\n    stage 'Invoke Rake'\n    stage 'Security scan'\n    stage 'Deploy'\n}\n\nAs mentioned previously, this Jenkinsfile is going to rely heavily on the\nCloudBees\nDocker Pipeline plugin. The plugin provides two very important features:\n\nAbility to execute steps inside of a running Docker container\n\nAbility to run a container in the \"background.\"\n\nLike most Rails applications, one can effectively test the application with two\ncommands: bundle install followed by bundle exec rake. I already had some\nDocker images prepared with RVM and Ruby 2.3.0 installed,\nwhich ensures a common and consistent starting point:\n\nnode('docker') {\n    // .. 'stage' steps removed\n    docker.image('rtyler/rvm:2.3.0').inside { (1)\nrvm 'bundle install' (2)\nrvm 'bundle exec rake'\n    } (3)\n}\n\n1\nRun the named container. The inside method can take optional additional flags for the docker run command.\n\n2\nExecute our shell commands using our tiny sh step wrapper\nrvm . This ensures that the shell code is executed in the correct RVM environment.\n\n3\nWhen the closure completes, the container will be destroyed.\n\nUnfortunately, with this application, the bundle exec rake command will fail\nif PostgreSQL isn’t available when the process starts. This is where the\nsecond important feature of the CloudBees Docker Pipeline plugin comes\ninto effect: the ability to run a container in the \"background.\"\n\nnode('docker') {\n    // .. 'stage' steps removed\n    /* Pull the latest `postgres` container and run it in the background */\n    docker.image('postgres').withRun { container -> (1)\necho \"PostgreSQL running in container ${container.id}\" (2)\n} (3)\n}\n\n1\nRun the container, effectively docker run postgres\n\n2\nAny number of steps can go inside the closure\n\n3\nWhen the closure completes, the container will be destroyed.\n\nRunning the tests\n\nCombining these two snippets of Jenkins Pipeline is, in my opinion, where the\npower of the DSL\nshines:\n\nnode('docker') {\n    docker.image('postgres').withRun { container ->\n        docker.image('rtyler/rvm:2.3.0').inside(\"--link=${container.id}:postgres\") { (1)\nstage 'Install Gems'\n            rvm \"bundle install\"\n\n            stage 'Invoke Rake'\n            withEnv(['DATABASE_URL=postgres://postgres@postgres:5432/']) { (2)\nrvm \"bundle exec rake\"\n            }\n            junit 'spec/reports/*.xml' (3)\n}\n    }\n}\n\n1\nBy passing the --link argument, the Docker daemon will allow the RVM container to talk to the PostgreSQL container under the host name 'postgres'.\n\n2\nUse the withEnv step to set environment variables for everything that is in the closure. In this case, the cfp-app DB scaffolding will look for the DATABASE_URL variable to override the DB host/user/dbname defaults.\n\n3\nArchive the test reports generated by ci_reporter so that Jenkins can display test reports and trend analysis.\n\nWith this done, the basics are in place to consistently run the tests for\ncfp-app in fresh Docker containers for each execution of the pipeline.\n\nSecurity scanning\n\nUsing Brakeman, the security scanner for Ruby\non Rails, is almost trivially easy inside of Jenkins Pipeline, thanks to the\nBrakeman\nplugin which implements the publishBrakeman step.\n\nBuilding off our example above, we can implement the \"Security scan\" stage:\n\nnode('docker') {\n    /* --8 (1)\npublishBrakeman 'brakeman-output.tabs' (2)\n/* --8\n\n1\nRun the Brakeman security scanner for Rails and store the output for later in brakeman-output.tabs\n\n2\nArchive the reports generated by Brakeman so that Jenkins can display detailed reports with trend analysis.\n\nAs of this writing, there is work in progress\n( JENKINS-31202) to\nrender trend graphs from plugins like Brakeman on a pipeline project’s main\npage.\n\nDeploying the good stuff\n\nOnce the tests and security scanning are all working properly, we can start to\nset up the deployment stage. Jenkins Pipeline provides the variable\ncurrentBuild which we can use to determine whether our pipeline has been\nsuccessful thus far or not. This allows us to add the logic to only deploy when\neverything is passing, as we would expect:\n\nnode('docker') {\n    /* --8 (1)\nsh './deploy.sh' (2)\n}\n    else {\n        mail subject: \"Something is wrong with ${env.JOB_NAME} ${env.BUILD_ID}\",\n                  to: 'nobody@example.com',\n                body: 'You should fix it'\n    }\n    /* --8\n\n1\ncurrentBuild has the result property which would be 'SUCCESS', 'FAILED', 'UNSTABLE', 'ABORTED'\n\n2\nOnly if currentBuild.result is successful should we bother invoking our deployment script (e.g. git push heroku master)\n\nWrap up\n\nI have gratuitously commented the full\nJenkinsfile\nwhich I hope is a useful summation of the work outlined above. Having worked\non a number of Rails applications in the past, the consistency provided by\nDocker and Jenkins Pipeline above would have definitely improved those\nprojects' delivery times. There is still room for improvement however, which\nis left as an exercise for the reader. Such as: preparing new containers with\nall their\ndependencies\nbuilt-in instead of installing them at run-time. Or utilizing the parallel\nstep for executing RSpec across multiple Jenkins agents simultaneously.\n\nThe beautiful thing about defining your continuous delivery, and continuous\nsecurity, pipeline in code is that you can continue to iterate on it!","title":"Continuous Security for Rails apps with Pipeline and Brakeman","tags":["tutorial","ruby","pipeline","rails","brakeman","continuousdelivery"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-08-09T00:00:00.000Z","id":"e7eb5984-a870-5548-985b-25b9210fb2a0","slug":"/blog/2016/08/09/ewm-beta-version/","strippedHtml":"This blog post is a continuation of the External Workspace Manager Plugin related posts, starting with\nthe introductory blog post, and followed by\nthe alpha version release announcement.\n\nAs the title suggests, the beta version of the External Workspace Manager Plugin was launched!\nThis means that it’s available only in the\nExperimental Plugins Update Center.\n\nTake care when installing plugins from the Experimental Update Center, since they may change in\nbackward-incompatible ways.\nIt’s advisable not to use it for Jenkins production environments.\n\nThe plugin’s repository is on GitHub.\nThe complete plugin’s documentation can be accessed\nhere.\n\nWhat’s new\n\nBellow is a summary of the features added so far, since the alpha version.\n\nMultiple upstream run selection strategies\n\nIt has support for the\nRun Selector Plugin (which is still in beta),\nso you can provide different run selection strategies when allocating a disk from the upstream job.\n\nLet’s suppose that we have an upstream job that clones the repository and builds the project:\n\ndef extWorkspace = exwsAllocate 'diskpool1'\n\nnode ('linux') {\n    exws (extWorkspace) {\n        checkout scm\n        sh 'mvn clean install -DskipTests'\n    }\n}\n\nIn the downstream job, we run the tests on a different node, but we reuse the same workspace as the previous job:\n\ndef run = selectRun 'upstream'\ndef extWorkspace = exwsAllocate selectedRun: run\n\nnode ('test') {\n    exws (extWorkspace) {\n        sh 'mvn test'\n    }\n}\n\nThe selectRun in this example selects the last stable build from the upstream job.\nBut, we can be more explicit, and select a specific build number from the upstream job.\n\ndef run = selectRun 'upstream',\n selector: [$class: 'SpecificRunSelector', buildNumber: UPSTREAM_BUILD_NUMBER]\ndef extWorkspace = exwsAllocate selectedRun: run\n// ...\n\nWhen the selectedRun parameter is given to the exwsAllocate step, it will allocate the same workspace that was\nused by that run.\n\nThe Run Selector Plugin has several run selection strategies that are briefly explained\nhere.\n\nAutomatic workspace cleanup\n\nProvides an automatic workspace cleanup by integrating the\nWorkspace Cleanup Plugin.\nFor example, if we need to delete the workspace only if the build has failed, we can do the following:\n\ndef extWorkspace = exwsAllocate diskPoolId: 'diskpool1'\n\nnode ('linux') {\n    exws (extWorkspace) {\n        try {\n            checkout scm\n            sh 'mvn clean install'\n        } catch (e) {\n            currentBuild.result = 'FAILURE'\n            throw e\n        } finally {\n            step ([$class: 'WsCleanup', cleanWhenFailure: false])\n        }\n    }\n}\n\nMore workspace cleanup examples can be found at this\nlink.\n\nCustom workspace path\n\nAllows the user to specify a custom workspace path to be used when allocating workspace on the disk.\nThe plugin offers two alternatives for doing this:\n\nby defining a global workspace template for each Disk Pool\n\nThis can be defined in the Jenkins global config, External Workspace Definitions section.\n\nby defining a custom workspace path in the Pipeline script\n\nWe can use the Pipeline DSL to compute the workspace path.\nThen we pass this path as input parameter to the exwsAllocate step.\n\ndef customPath = \"${env.JOB_NAME}/${PULL_REQUEST_NUMBER}/${env.BUILD_NUMBER}\"\ndef extWorkspace = exwsAllocate diskPoolId: 'diskpool1', path: customPath\n// ...\n\nFor more details see the afferent\ndocumentation page.\n\nDisk Pool restrictions\n\nThe plugin comes with Disk Pool restriction strategies.\nIt does this by using the restriction capabilities provided by the\nJob Restrictions Plugin.\n\nFor example, we can restrict a Disk Pool to be allocated only if the Jenkins job in which it’s allocated was triggered\nby a specific user:\n\nOr, we can restrict the Disk Pool to be allocated only for those jobs whose name matches a well defined pattern:\n\nWhat’s next\n\nCurrently there is ongoing work for providing flexible disk allocation strategies.\nThe user will be able to define a default disk allocation strategy in the Jenkins global config.\nSo for example, we want to select the disk with the most usable space as default allocation strategy:\n\nIf needed, this allocation strategy may be overridden in the Pipeline code.\nLet’s suppose that for a specific job, we want to allocate the disk with the highest read speed.\n\ndef extWorkspace = exwsAllocate diskPoolId: 'diskpool1', strategy: fastestRead()\n// ...\n\nWhen this feature is completed, the plugin will enter a final testing phase.\nIf all goes to plan, a stable version should be released in about two weeks.\n\nIf you have any issues in setting up or using the plugin, please feel free to ask me on the plugin’s Gitter\nchat.\nAny feedback is welcome, and you may provide it either on the Gitter chat, or on\nJira by using the external-workspace-manager-plugin component.\n\nLinks\n\nProject repository\n\nProject intro blog post\n\nAlpha version announcement\n\nGSoC page\n\nJenkins GSoC Page","title":"GSoC: External Workspace Manager for Pipeline. Beta release is available","tags":["pipeline","plugins","external-workspace-manager","gsoc"],"authors":[{"avatar":null,"blog":null,"github":"alexsomai","html":"","id":"alexsomai","irc":null,"linkedin":null,"name":"Alexandru Somai","slug":"/blog/authors/alexsomai","twitter":"alex_somai"}]}},{"node":{"date":"2016-08-08T00:00:00.000Z","id":"649a2e8e-4f2f-56eb-99f1-8897de882a49","slug":"/blog/2016/08/08/docker-pipeline-environments/","strippedHtml":"This is a guest post by Michael Neale, long time open\nsource developer and contributor to the Blue Ocean\nproject.\n\nIf you are running parts of your pipeline on Linux, possibly the easiest way to\nget a clean reusable environment is to use:\nCloudBees\nDocker Pipeline plugin.\n\nIn this short post I wanted to show how you can avoid installing stuff on the agents, and have per project, or even per branch, customized build environments.\nYour environment, as well as your pipeline is defined and versioned alongside your code.\n\nI wanted to use the Blue Ocean project as an\nexample of a\nproject that uses the CloudBees Docker Pipeline plugin.\n\nEnvironment and Pipeline for JavaScript components\n\nThe Blue Ocean project has a few moving parts, one of\nwhich is called the \"Jenkins Design Language\".  This is a grab bag of re-usable\nCSS, HTML, style rules, icons and JavaScript components (using React.js) that\nprovide the look and feel for Blue Ocean.\n\nJavaScript and Web Development being what it is in 2016, many utilities are\nneed to assemble a web app.  This includes npm and all that it needs, less.js\nto convert Less to CSS, Babel to \"transpile\" versions of JavaScript to other\ntypes of JavaScript (don’t ask) and more.\n\nWe could spend time installling nodejs/npm on the agents, but why not just use\nthe official off the shelf docker image\nfrom Docker Hub?\n\nThe only thing that has to be installed and run on the build agents is the Jenkins agent, and a docker daemon.\n\nA simple pipeline using this approach would be:\n\nnode {\n        stage \"Prepare environment\"\n          checkout scm\n          docker.image('node').inside {\n            stage \"Checkout and build deps\"\n                sh \"npm install\"\n\n            stage \"Test and validate\"\n                sh \"npm install gulp-cli && ./node_modules/.bin/gulp\"\n          }\n}\n\nThis uses the stock \"official\" Node.js image from the Docker Hub, but doesn’t let us customize much about the environment.\n\nCustomising the environment, without installing bits on the agent\n\nBeing the forward looking and lazy person that I am, I didn’t want to have to\ngo and fish around for a Docker image every time a developer wanted something\nspecial installed.\n\nInstead, I put a Dockerfile in the root of the repo, alongside the Jenkinsfile :\n\nThe contents of the Dockerfile can then define the exact environment needed\nto build the project.  Sure enough, shortly after this, someone came along\nsaying they wanted to use Flow from Facebook (A\ntypechecker for JavaScript).  This required an additional native component to\nwork (via apt-get install).\n\nThis was achieved via a\npull\nrequest to both the Jenkinsfile and the Dockerfile at the same time.\n\nSo now our environment is defined by a Dockerfile with the following contents:\n\n# Lets not just use any old version but pick one\nFROM node:5.11.1\n\n# This is needed for flow, and the weirdos that built it in ocaml:\nRUN apt-get update && apt-get install -y libelf1\n\nRUN useradd jenkins --shell /bin/bash --create-home\nUSER jenkins\n\nThe Jenkinsfile pipeline now has the following contents:\n\nnode {\n    stage \"Prepare environment\"\n        checkout scm\n        def environment  = docker.build 'cloudbees-node'\n\n        environment.inside {\n            stage \"Checkout and build deps\"\n                sh \"npm install\"\n\n            stage \"Validate types\"\n                sh \"./node_modules/.bin/flow\"\n\n            stage \"Test and validate\"\n                sh \"npm install gulp-cli && ./node_modules/.bin/gulp\"\n                junit 'reports/**/*.xml'\n        }\n\n    stage \"Cleanup\"\n        deleteDir()\n}\n\nEven hip JavaScript tools can emit that weird XML format that test\nreporters can use, e.g. the junit result archiver.\n\nThe main change is that we have docker.build being called to produce the\nenvironment which is then used.  Running docker build is essentially a\n\"no-op\" if the image has already been built on the agent before.\n\nWhat’s it like to drive?\n\nWell, using Blue Ocean, to build Blue Ocean, yields a pipeline that visually\nlooks like this (a recent run I screen capped):\n\nThis creates a pipeline that developers can tweak on a pull-request basis,\nalong with any changes to the environment needed to support it, without having\nto install any packages on the agent.\n\nWhy not use docker commands directly?\n\nYou could of course just use shell commands to do things with Docker directly,\nhowever, Jenkins Pipeline keeps track of Docker images used in a Dockerfile\nvia the \"Docker Fingerprints\" link (which is good, should that image need to\nchange due to a security patch).\n\nLinks\n\nThe project used as as an example is here\n\nThe pipeline is defined by the Jenkinsfile\n\nThe environment is defined by the Dockerfile\n\nRead more on Docker Pipeline","title":"Don't install software, define your environment with Docker and Pipeline","tags":["pipeline","plugins","blueocean","ux","javascript","nodejs"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/19e71/michaelneale.jpg","srcSet":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/77b35/michaelneale.jpg 32w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/d4a57/michaelneale.jpg 64w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/19e71/michaelneale.jpg 128w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/ef6ff/michaelneale.webp 32w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/8257c/michaelneale.webp 64w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/6766a/michaelneale.webp 128w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"michaelneale","html":"<div class=\"paragraph\">\n<p>Michael is a CD enthusiast with a interest in User Experience.\nHe is a co-founder of CloudBees and a long time OSS developer, and can often be found\nlurking around the jenkins-dev mailing list or #jenkins on irc (same nick as twitter name).\nBefore CloudBees he worked at Red Hat.</p>\n</div>","id":"michaelneale","irc":null,"linkedin":null,"name":"Michael Neale","slug":"/blog/authors/michaelneale","twitter":"michaelneale"}]}}]}},"pageContext":{"limit":8,"skip":384,"numPages":101,"currentPage":49}},
    "staticQueryHashes": ["3649515864"]}