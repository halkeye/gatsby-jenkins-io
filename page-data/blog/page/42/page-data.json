{
    "componentChunkName": "component---src-templates-blog-list-template-js",
    "path": "/blog/page/42",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2017-02-07T00:00:00.000Z","id":"3d993b8f-320e-5137-8e5b-be2cc83d0195","slug":"/blog/2017/02/07/gsoc2017-announcement/","strippedHtml":"On behalf of the GSoC Org Admin team I am happy to announce that we are going to apply to\nGoogle Summer of Code (GSoC) again this year.\nIn GSoC high-profile students work in open-source projects for several months under mentorship of organization members.\n\nWe are looking for mentors and project ideas.\nSo yes, we are looking for you :)\n\nConditions\n\nAs a mentor, you will be asked to:\n\nlead the project in the area of their interest\n\nactively participate in the project during student selection, community bonding and coding phases (March - August)\n\nwork in teams of 2+ mentors per 1 each student\n\ndedicate a consistent and significant amount of time, especially during the coding phase ( ~5 hours per week in the team of two mentors)\n\nMentorship does not require strong expertise in Jenkins plugin development.\nThe main objective is to guide students and to get them involved into the Jenkins community.\nIf your mentor team requires any specific expertise, GSoC org admins will do their best in order to find advisors.\n\nWhat do you get?\n\nA student, who works within the area of your interest on full-time for several months\n\nJoint projects with Jenkins experts, lots of fun and ability to study something together\n\nLimited edition of swags from Google and Jenkins project\n\nMaybe: Participation in GSoC Mentor Summit in California with expense coverage (depends on project results and per-project quotas)\n\nRequirements\n\nYou are:\n\npassionate about Jenkins\n\ninterested in being a mentor or advisor\n\nready to dedicate time && have no major unavailability periods planned to this summer\n\nWe expect mentors to be available by email during 75% of working days in the May-August timeframe\n\nYour project idea is:\n\nabout code (though it may and likely should include some documentation and testing work)\n\nabout Jenkins (plugins, core, infrastructure, etc.)\n\npotentially doable by a student in 3-4 months\n\nHow to apply\n\nIf you are interested, drop the Email to the Jenkins Developer mailing list with the GSoC2017 prefix.\n\nBriefly describe your project idea (a couple of sentences) and required qualifications from students. Examples: GSoC2016, GSoC2017 - current project ideas\n\nIf you already have a co-mentor(s), please mention them\n\nHaving several project ideas is fine. Having no specific ideas is also fine.\n\nDisclaimer: We cannot guarantee that all projects happen, it depends on student application results and the number of project slots.\n\nLinks\n\nGoogle Summer of Code page\n\nJenkins GSoC Page\n\nGSoC2016 project ideas\n\nGSoC2016 page (project results and more info)","title":"Google Summer Of Code 2017: Call for mentors","tags":["gsoc","events","general","gsoc2017"],"authors":[]}},{"node":{"date":"2017-02-06T00:00:00.000Z","id":"46885786-902f-55cb-b263-cae8df8f0611","slug":"/blog/2017/02/06/scm-api-2-take2/","strippedHtml":"In January we\nannounced the release of SCM API 2.0.\nAfter the original release was published we identified four new high-impact\nissues.  We decided to remove the new versions of the plugins from the update\ncenter until those issues could be resolved. The issues have now been resolved\nand the plugins are now available from the update center.\n\nSummary for busy Jenkins Administrators\n\nUpgrading should make multi-branch projects much better.  When you are ready to\nupgrade you must ensure that you upgrade all the required plugins.  If you miss\nsome, just upgrade them and restart to fix the issue. And of course, it’s\nalways a good idea to take a backup of your JENKINS_HOME before upgrading any\nplugins.\n\nIn the list below, version numbers in bold indicate a change from the\noriginal version in the\noriginal announcement\n\nFolders Plugin\n\n5.17 or newer\n\nSCM API Plugin\n\n2.0.2 or newer\n\nBranch API Plugin\n\n2.0.2 or newer\n\nGit Plugin\n\nThis depends on the exact release line of the Git plugin that you are using.\n\nFollowing the 2.6.x release line: 2.6.4 or newer\n\nFollowing the 3.0.x release line ( recommended): 3.0.4 or newer\n\nMercurial Plugin\n\n1.58 or newer\n\nGitHub Branch Source Plugin\n\n2.0.1 or newer\n\nBitBucket Branch Source Plugin\n\n2.0.2 or newer\n\nGitHub Organization Folders Plugin\n\n1.6\n\nPipeline Multibranch Plugin\n\n2.12 or newer\n\nIf you are using the Blue Ocean plugin\n\nBlue Ocean Plugin\n\n1.0.0-b22 or newer\n\nOther plugins that may require updating:\n\nGitHub API Plugin\n\n1.84 or newer\n\nGitHub Plugin\n\n1.25.0 or newer\n\nIf you upgrade to Branch API 2.0.x and you have either the GitHub Branch Source or the BitBucket Branch Source plugins and you do not upgrade those instances to the 2.0.x line then your Jenkins instance will fail to start-up correctly.\n\nThe solution is just to upgrade the GitHub Branch Source or the BitBucket Branch Source plugin (as appropriate) to the 2.0.x line.\n\nAfter an upgrade you will see the data migration warning (see the screenshot in\nJENKINS-41608 for an\nexample) this is normal and expected.  The unreadable data will be removed by\nthe next scan / index or can be removed manually using the Discard Unreadable\nData button.  The warning will disappear on the next restart after the\nunreadable data has been removed.\n\nPlease update to the versions listed above. If you want to know more about the\nissues and how they were resolved, see the next section.\n\nAnalysis of the issues\n\nThe issues described below are resolved with these plugin releases:\n\nFolders Plugin: 5.17\n\nSCM API Plugin: 2.0.2\n\nBranch API Plugin: 2.0.2\n\nGit Plugin: Either 2.6.4 or 3.0.4\n\nGitHub Branch Source Plugin: 2.0.1\n\nBitBucket Branch Source Plugin: 2.0.2\n\nPipeline Multibranch Plugin: 2.12\n\nJENKINS-41121: GitHub Branch Source upgrade can cause a lot of rebuilds :\n\nMigration of GitHub branches from 1.x to 2.x resulted in a change of the\nimplementation class used to identify branches.  Some other other bugs in\nBranch API had been fixed and the combined effect resulted in a rebuild of all\nGitHub Branches (not PRs) after an upgrade to GitHub Branch Source Plugin\n2.0.0.  This rebuild was referred to as a \"build storm\".\n\nResolution:\n\nThe SCM API plugin was enhanced to add an extension point that allows for a second round of data migration when upgrading.\n\nThe second round of data migration allows plugins implementing the SCM API contract to fix implementation class issues in context.\n\nThe Branch API plugin was enhanced to use this new extension point.\n\nThe GitHub Branch Source plugin was enhanced to provide an implementation of this extension point.\n\nJENKINS-41255: Upgrading from a navigator that did not assign consistent source ids to a version that does assign consistent source ids causes a build storm on first scan :\n\nThe GitHub Branch Source and BitBucket Branch Source plugins in 1.x were not\nassigning consistent IDs to multi-branch projects discovered in an Organization\nFolder.  Both plugins were fixed in 2.0.0 to assign consistent IDs as a change\nof ID would result in a rebuild of all projects.  What was missed is that the\nvery first scan of an Organization Folder after an upgrade will change the\nrandomly assigned ID assigned by the 1.x plugins into the consistent ID\nassigned by the 2.0.0 plugins and consequently trigger a rebuild of all\nbranches. This rebuild was referred to as a \"build storm\".\n\nResolution:\n\nThe Branch API plugin was enhanced to detect the case where a branch source has\nbeen changed but the change is only changing the ID.  When such changes are\nidentified, the downstream references of the ID are all updated which will\nprevent a build storm.\n\nJENKINS-41313: On first index after upgrade to 2.0.0 all open PRs are rebuilt :\n\nThe BitBucket Branch Source 1.x did not store all the information about PRs\nthat is required by the SCM API 2.0.x model.  This could well have resulted in\nsubtle effects when manually triggering a rebuild of a merge PR if the PR’s\ntarget branch has been modified after the PR branch was first detected by\nJenkins. Consequently, as the information is required, BitBucket Branch Source\nplugin 2.0.0 populated the information with dummy values which would force the\ncorrect information to be retrieved.  The side-effect is that all PR branches\nwould be rebuilt.\n\nResolution:\n\nThe changes in SCM API 2.0.2 introduced to resolve JENKINS-41121 provided a path to resolve this issue without causing a rebuild of all PR branches.\n\nThe BitBucket Branch Source plugin was enhanced to provide an implementation of the new SCM API extension point that connects to BitBucket and retrieves the missing information.\n\nJENKINS-41124: Can’t get a human readable job name anymore :\n\nDuring initial testing of the Branch API 2.0.0 release an issue was identified\nwith how Organization Folders handled unusual names.  None of the existing\nimplementations of the SCMNavigator API could generate such unusual names due\nto form validation on GitHub / BitBucket replacing unusual characters with -\nwhen creating a repository.\n\nIt would be irresponsible to rely on external services sanitizing their input\ndata for the correct operation of Organization Folders.  Consequently, in\nBranch API 2.0.0 the names were all transformed into URL safe names, with the\noriginal URLs still resolving to the original projects so that any existing\nsaved links would remain functional.\n\nQuite a number of people objected to this change of URL scheme.\n\nResolution:\n\nThere has been a convention in Jenkins that the on-disk storage structure for\njobs mirrors the URL structure. This is only a convention and there is nothing specific in the code that\nmandates following the convention.\n\nThe Folders Plugin was enhanced to allow for computed folders (where the item\nnames are provided by an external source) to provide a strategy to use when\ngenerating the on-disk storage names as well as the URL component names for\nthe folder’s child items.\n\nThe Branch API plugin was enhanced to use this new strategy for name transformation.\n\nThe net effect of this change is that the URLs remain the same as for 1.x but\nthe on-disk storage uses transformed names that are future proofed against\nany new SCMNavigator implementations where the backing service allows names\nthat are problematic to use as filesystem directory names.\n\nSide-effect:\n\nThe Branch API 2.0.0 approach handled the transformation of names by renaming the items using the Jenkins Item rename API.\n\nThe Branch API 2.0.2 approach does not rename the child items as it is only the on-disk storage location that is moved.\n\nThis means that the Jenkins Item rename API cannot be used.\n\nAt this time, the only known side-effect is in the Job Configuration History plugin.\nThe configuration history of each child item will still be tracked going\nforward after the upgrade.  The pre-upgrade configuration history is also\nretained.  Because the Jenkins Item rename API cannot be used to flag the\nconfiguration file location change, there is no association between the\npre-upgrade history chain and the post-upgrade history chain.","title":"SCM API 2.0 Release Take 2","tags":["development","plugins"],"authors":[]}},{"node":{"date":"2017-02-03T00:00:00.000Z","id":"e40d65de-fedc-5955-acd7-b2f63eafaf7f","slug":"/blog/2017/02/03/blueocean-devlog-feb/","strippedHtml":"With only a couple of months left before\nBlue Ocean\n1.0, which is planned for the end of March, I have\nbeen\nhighlighting\nsome of the good work being finished up by the developers hacking on Blue\nOcean.\n\nThis week was a grab bag of important behind-the-scenes features and finalising\nthe preview of the editor. The merge of the SCM API changes also made it in.\nThe editor has the new sheets style of editing (there will be blogs and more on\nthis in the next few weeks):\n\nSome highlights:\n\nFix to async loading of resources like translations, so screens don’t\n\"flash\" when they are loaded (i18n improvement)\n\nLinks in notifications can be configured to point to classic or\nBlue Ocean screens\n\nTime reporting works better when browser clock is out of sync with\nserver\n\nSECURITY-380 was backported into a small fix for those that aren’t\nrunning the latest LTS (but you should ideally be running it)\n\nSCM API changes finally landed - this will be in beta 22 which should\nhit the update centers soon. This should make things work better with\nGitHub rate limits.\n\nBeta 21 was released\n\nThe editor reached \"preview\" release state ready for use with the newly\nannounced Declarative Pipeline stuff.\n\nAlso, a reference to Australian pop culture had to be removed, sadly.\n\nUp Next:\n\nSome cosmetic changes around headers to make it much nicer and clearer\n\nFavorite improvements\n\nGitHub Org-based Pipeline creation\n\nEditor available in the general update center\n\nBeta 22 with SCM improvements and no more GitHub rate limit hassles\n\nMany fixes\n\nImprovements to the Acceptance Test Harness to reduce the number of false-positives.\n\nEnjoy!\n\nIf you’re interested in helping to make Blue Ocean a great user experience for\nJenkins, please join the Blue Ocean development team on\nGitter!","title":"Blue Ocean Dev Log: February Week #1","tags":["blueocean"],"authors":[]}},{"node":{"date":"2017-02-03T00:00:00.000Z","id":"eba78b86-bbf2-5022-a3c9-27efc09a20e1","slug":"/blog/2017/02/03/declarative-pipeline-ga/","strippedHtml":"This is a guest post by\nPatrick Wolf,\nDirector of Product Management at\nCloudBees\nand contributor to\nthe Jenkins project.\n\nI am very excited to announce the addition of\nDeclarative Pipeline syntax\n1.0 to\nJenkins Pipeline.\nWe think this new syntax will enable everyone involved in DevOps, regardless of expertise,\nto participate in the continuous delivery process. Whether creating, editing or reviewing\na pipeline, having a straightforward structure helps to understand and predict the\nflow of the pipeline and provides a common foundation across all pipelines.\n\nPipeline as Code\n\nPipeline as Code was one of the pillars of the Jenkins 2.0 release and an\nessential part of implementing continuous delivery (CD). Defining all of the\nstages of an application’s CD pipeline within a Jenkinsfile and checking it\ninto the repository with the application code provides all of the benefits\ninherent in source control management (SCM):\n\nRetain history of all changes to Pipeline\n\nRollback to a previous Pipeline version\n\nView diffs and merge changes to the Pipeline\n\nTest new Pipeline steps in branches\n\nRun the same Pipeline on a different Jenkins server\n\nGetting Started with Declarative Pipeline\n\nWe recommend people begin using it for all their Pipeline definitions in Jenkins.\nThe plugin has been available for use and testing starting with the 0.1 release that was debuted at\nJenkins World\nin September. Since then, it has already been installed in over 5,000 Jenkins\nenvironments.\n\nIf you haven’t tried Pipeline or have considered Pipeline in the past, I\nbelieve this new syntax is much more approachable with an easier adoption curve\nto quickly realize all of the benefits of Pipeline as Code. In addition, the\npre-defined structure of Declarative makes it possible to create and edit\nPipelines with a graphical user interface (GUI). The Blue Ocean team is\nactively working on a\nVisual Pipeline Editor\nwhich will be included in an upcoming release.\n\nIf you have already begun using Pipelines in Jenkins, I believe that this new\nalternative syntax can help expand that usage.\n\nThe original syntax for defining Pipelines in Jenkins is a Groovy DSL that\nallows most of the features of full\nimperative programming.\n\nThis syntax is still fully supported and is now\nreferred to as \"Scripted Pipeline Syntax\" to distinguish it from \"Declarative\nPipeline Syntax.\" Both use the same underlying execution engine in Jenkins and\nboth will generate the same results in\nPipeline Stage View\nor Blue Ocean visualizations. All existing\nPipeline steps,\nGlobal Variables, and\nShared Libraries\ncan be used in either. You can now create more cookie-cutter Pipelines and\nextend the power of Pipeline to all users regardless of Groovy expertise.\n\nDeclarative Pipeline Features\n\nSyntax Checking\n\nImmediate runtime syntax checking with explicit error messages.\n\nAPI endpoint for linting a Jenkinsfile.\n\nCLI command to lint a Jenkinsfile.\n\nDocker Pipeline integration\n\nRun all stages in a single container.\n\nRun each stage in a different container.\n\nEasy configuration\n\nQuickly define parameters for your Pipeline.\n\nQuickly define environment variables and credentials for your Pipeline.\n\nQuickly define options (such as timeout, retry, build discarding) for your Pipeline.\n\nRound trip editing with the Visual Pipeline Editor (watch for preview release shortly).\n\nConditional actions\n\nSend notifications or take actions depending upon success or failure.\n\nSkip stages based on branches, environment, or other Boolean expression.\nrelease shortly)\n\nWhere Can I Learn More?\n\nBe on the lookout for future blog posts detailing specific examples of\nscenarios or features in Declarative Pipeline. Andrew Bayer, one of the primary\ndevelopers behind Declarative Pipeline, will be presenting at\nFOSDEM\nin Brussels, Belgium this weekend. We have also scheduled an online\nJenkins Meetup (JAM)\nlater this month to demo the features of Declarative Pipeline and give a sneak\npeek at the upcoming Blue Ocean Pipeline Editor.\n\nIn the meantime, all the\nPipeline documentation\nhas been updated to incorporate a\nGuided Tour,\nand a\nSyntax Reference\nwith numerous examples to help you get on your way to using Pipeline.  Simply\nupgrade to the latest version, 2.5 or later of the Pipeline in Jenkins to\nenable all of these great features.","title":"Declarative Pipeline Syntax 1.0 is now available","tags":["pipeline","blueocean"],"authors":[]}},{"node":{"date":"2017-02-01T00:00:00.000Z","id":"aa509c7b-3f24-5cce-8519-dda84cd1233e","slug":"/blog/2017/02/01/pipeline-scalability-best-practice/","strippedHtml":"This is a guest post by Sam Van Oort,\nSoftware Engineer at CloudBees and contributor to\nthe Jenkins project.\n\nToday I’m going to show you best practices to write scalable and robust Jenkins Pipelines. This is drawn from a\ncombination of work with the internals of Pipeline and observations with large-scale users.\n\nPipeline code works beautifully for its intended role of automating\nbuild/test/deploy/administer tasks.  As it is pressed into more complex roles\nand unanticipated uses, some users hit issues.  In these cases, applying the\nbest practices can make the difference between:\n\nA single controller running\nhundreds\nof concurrent builds on low end hardware (4 CPU cores and 4 GB of\nheap)\n\nRunning a couple dozen builds and bringing a controller to its knees or\ncrashing it…​even with 16+ CPU cores and 20+ GB of heap!\n\nThis has been seen in the wild.\n\nFundamentals\n\nTo understand Pipeline behavior you must understand a few points about\nhow it executes.\n\nExcept for the steps themselves, all of the Pipeline logic, the Groovy conditionals, loops, etc execute on the controller. Whether simple or complex! Even inside a node block!\n\nSteps may use executors to do work where appropriate, but each\nstep has a small on-controller overhead too.\n\nPipeline code is written as Groovy but the execution model is\nradically transformed at compile-time to Continuation Passing Style\n(CPS).\n\nThis transformation provides valuable safety and durability\nguarantees for Pipelines, but it comes with trade-offs:\n\nSteps can invoke Java and execute fast and efficiently, but Groovy\nis much slower to run than normal.\n\nGroovy logic requires far more memory, because an object-based\nsyntax/block tree is kept in memory.\n\nPipelines persist the program and its state frequently to be able to\nsurvive failure of the controller.\n\nFrom these we arrive at a set of best practices to make pipelines more\neffective.\n\nBest Practices For Pipeline Code\n\nThink of Pipeline code as glue: just enough Groovy code to connect\ntogether the Pipeline steps and integrate tools, and no more.\n\nThis makes code easier to maintain, more robust against bugs, and\nreduces load on controllers.\n\nKeep it simple: limit the amount of complex logic embedded in the\nPipeline itself (similarly to a shell script) and avoid treating it as a\ngeneral-purpose programming language.\n\nPipeline restricts all variables to Serializable types, so keeping\nPipeline logic simple helps avoid a NotSerializableException - see\nappendix at the bottom.\n\nUse @NonCPS -annotated functions for slightly more complex work.\nThis means more involved processing, logic, and transformations. This\nlets you leverage additional Groovy & functional features for more\npowerful, concise, and performant code.\n\nThis still runs on controllers so be mindful of complexity, but is much\nfaster than native Pipeline code because it doesn’t provide durability\nand uses a faster execution model. Still, be mindful of the CPU cost and\noffload to executors for complex work (see below).\n\n@NonCPS functions can use a much broader subset of the Groovy\nlanguage, such as iterators and functional features, which makes them\nmore terse and fast to write.\n\n@NonCPS functions should not use Pipeline steps internally, however\nyou can store the result of a Pipeline step to a variable and use it\nthat as the input to a @NonCPS function.\n\nGotcha: It’s not guaranteed that use of a step will generate an\nerror (there is an open RFE to implement that), but you should not rely\non that behavior. You may see improper handling of exceptions, in\nparticular.\n\nWhile normal Pipeline is restricted to serializable local variables\n(see appendix at bottom), @NonCPS functions can use more complex,\nnonserializable types internally (for example regex matchers, etc). Parameters\nand return types should still be Serializable, however.\n\nGotcha: improper usages are not guaranteed to raise an error with\nnormal Pipeline (optimizations may mask the issue), but it is unsafe to\nrely on this behavior.\n\nPrefer external scripts/tools for complex or CPU-expensive\nprocessing rather than Groovy language features. This offloads work\nfrom the controller to external executors, allowing for easy scale-out of\nhardware resources. It is also generally easier to test because these\ncomponents can be tested in isolation without the full on-controller\nexecution environment.\n\nMany software vendors will provide easy command-line clients for\ntheir tools in various programming languages. These are often robust,\nperformant, and easy to use. Plugins offer another option (see below).\n\nShell or batch steps are often the easiest way to integrate these\ntools, which can be written in any language. For example: sh “java -jar\nclient.jar $endPointUrl $inputData” for a Java client, or sh “python\njiraClient.py $issueId $someParam” for a Python client.\n\nGotcha: especially avoid Pipeline XML or JSON parsing using Groovy’s XmlSlurper and JsonSlurper!  Strongly prefer command-line tools or scripts.\n\nThe Groovy implementations are complex and as a result more brittle in Pipeline use.\n\nXmlSlurper and JsonSlurper can carry a high memory and CPU cost in pipelines\n\nxmllint and xmlstartlet are command-line tools offering XML extraction via xpath\n\njq offers the same functionality for JSON\n\nThese extraction tools may be coupled to curl or wget for fetching information from an HTTP API\n\nExamples of other places to use command-line tools:\n\nTemplating large files\n\nNontrivial integration with external APIs (for bigger vendors,\nconsider a Jenkins plugin if a quality offering exists)\n\nSimulations/complex calculations\n\nBusiness logic\n\nConsider existing plugins for external integrations. Jenkins has a\nwealth of plugins, especially for source control, artifact management,\ndeployment systems, and systems automation. These can greatly reduce the\namount of Pipeline code to maintain. Well-written plugins may be\nfaster and more robust than Pipeline equivalents.\n\nConsider both plugins and command-line clients (above) — one may be\neasier than the other.\n\nPlugins may be of widely varying quality. Look at the number of installations and how frequently and recently updates appear in the changelog. Poorly-maintained plugins\nwith limited installations may actually be worse than writing a little\ncustom Pipeline code.\n\nAs a last resort, if there is a good-quality plugin that is not\nPipeline-enabled, it is fairly easy to write a Pipeline wrapper to\nintegrate it or write a custom step that will invoke it.\n\nAssume things will go wrong: don’t rely on workspaces being clean\nof the remnants from previous executions, clean explicitly where needed.\nMake use of timeouts and retry steps (that’s what they’re there for).\n\nWithin a git repository, git clean -fdx is a good way to\naccomplish this and reduces the amount of SCM cloning\n\nDO use parameterized Pipelines and variables to make your Pipeline\nscripts more reusable. Passing in parameters is especially helpful for\nhandling different environments and should be preferred to applying\nconditional lookup logic; however, try to limit parameterized pipelines invoking each other.\n\nTry to limit business logic embedded in Pipelines. To some extent\nthis is inevitable, but try to focus on tasks to complete instead,\nbecause this yields more maintainable, reusable, and often more\nperformant Pipeline code.\n\nOne code smell that points to a problem is many hard-coded\nconstants. Consider taking advantage of the options above to refactor\ncode for better composability.\n\nFor complex cases, consider using Jenkins integration options\n(plugins, Jenkins API calls, invoking input steps externally) to offload\nimplementation of more complex business rules to an external system if\nthey fit more naturally there.\n\nPlease, think of these as guidelines, not strict rules – Jenkins\nPipeline provides a great deal of power and flexibility, and it’s there\nto be used.\n\nBreaking enough of these rules at scale can cause controllers to fail by\nplacing an unsustainable load on them.\n\nFor additional guidance, I also recommend\nthis\nJenkins World talk\non how to engineer Pipelines for speed and performance:\n\nAppendix: Serializable vs. Non-Serializable Types:\n\nTo assist with Pipeline development, here are common serializable and\nnon-serializable types, to assist with deciding if your logic can be CPS\nor should be in a @NonCPS function to avoid issues.\n\nCommon Serializable Types (safe everywhere):\n\nAll primitive types and their object wrappers: byte, boolean, int,\ndouble, short, char\n\nStrings\n\nenums\n\nArrays of serializable types\n\nArrayLists and normal Groovy Lists\n\nSets: HashSet\n\nMaps: normal Groovy Map, HashMap, TreeMap\n\nExceptions\n\nURLs\n\nDates\n\nRegex Patterns (compiled patterns)\n\nCommon non-Serializable Types (only safe in @NonCPS functions):\n\nIterators: this is a common problem. You need to use C-style loop, i.e.\nfor(int i=0; i\n\nRegex Matchers (you can use the\nbuilt-in functions in String, etc, just not the Matcher itself)\n\nImportant: JsonObject, JsonSlurper, etc in Groovy 2+ (used in some 2.x+\nversions of Jenkins).\n\nThis is due to an internal implementation change — earlier versions may serialize.","title":"Best Practices for Scalable Pipeline Code","tags":["pipeline","performance","scalability"],"authors":[]}},{"node":{"date":"2017-02-01T00:00:00.000Z","id":"b68e4a15-e617-5570-bc2d-5ff59160e388","slug":"/blog/2017/02/01/security-updates/","strippedHtml":"We just released security updates to Jenkins, versions 2.44 and 2.32.2, that fix a high severity and several medium and low severity issues.\n\nFor an overview of what was fixed, see the security advisory.\nFor an overview on the possible impact of these changes on upgrading Jenkins LTS, see our LTS upgrade guide.\nI strongly recommend you read these documents, as there are a few possible side effects of these fixes.\n\nSubscribe to the jenkinsci-advisories mailing list to receive important notifications related to Jenkins security.","title":"Security updates for Jenkins core","tags":["core","security"],"authors":[]}},{"node":{"date":"2017-01-27T00:00:00.000Z","id":"47fad7c8-e5e2-5ee5-bca8-79356edc804b","slug":"/blog/2017/01/27/blueocean-dev-log-jan4/","strippedHtml":"As we get closer to\nBlue Ocean\n1.0, which is planned for the end of March, I have\nstarted\nhighlighting\nsome of the good stuff that has been going on. This week was 10 steps forward, and about 1.5 backwards…​\n\nThere were two releases this week, b19 and b20. Unfortunately, b20 had to\nbe released shortly after b19 hit the Update Center as an incompatible API\nchange in a 3rd party plugin was discovered.\n\nRegardless, the latest b20 has a lot of important improvements, and some\nvery nice new features.\n\nA first cut of the \"Create Pipeline\" UX, seen above, allowing you to create Git\nbased Multibranch Pipelines like you have never seen before.\n\nHandling network disconnections from the browser to server (eg server\nrestart, network etc) gracefully with a nice UI.\n\nMore precise time information for steps and running Pipelines.\n\nMore information when a Pipeline is blocked on infrastructure, such as when\nthe Pipeline is waiting for an agent to become available.\n\nFixed a really embarrassing typo (a prize if you spot it).\n\nTest reports now include stdout and stderr\n\nBetter support for parallel visualisation, such as when a parallel step exists outside of a stage.\n\nThe Visual Editor also had another release, with the \"sheets\" visual component\nand better validation.\n\nCreation\n\nCurrently this is hidden behind a\nfeature toggle,\nto access append?blueCreate to the URL in you browser, and then press the\n\"New Pipeline\" button. Currently it lets you quickly create a Pipeline from\nGit, add credentials, etc, in a very nice UX. More SCM types are being added to\nsupport this.\n\nReconnect/disconnect\n\nAs Blue Ocean is a very \"live\" style of UX, if your network becomes\nunavailable, or the server is restarted, it is good to know in case you\nwere staring at the screen waiting for something to happen (don’t you have\nanything better to do??). When this happens, now you get a polite message,\nand then when the connection is restored, even if you are waiting for a\nPipeline run to finish, it will then notice this, and refresh things for\nyou:\n\nNote the opacity changes to make it clear even if you don’t see the little\nmessage. Very nice addition for those of us who work on a train far to often.\n\nUp next\n\nWhat is up next:\n\nSCM Api changes should land, making things much better for users of\nGitHub, Bitbucket, and many more.\n\nCreating Pipelines from GitHub (including automatic discovery).\n\nLots of fixes and enhancements in the Pipeline from all over the place\n\nMore ATH [ 1 ] coverage against regressions\n\nMore Visual Editor releases as Declarative Pipeline reaches version 1.0\n\nImprovements to i18n\n\nThere was also a couple of \"alternative beta\" releases in the \"Experimental\nUpdate Center\" to help test the new SCM API improvements for better use of\nGitHub APIs (based on\nthis branch)\nI do not recommend trying this branch unless you know what you are doing,\nas this will migrate some data, but help testing it would be appreciated!\n\nEnjoy!\n\nIf you’re interested in helping to make Blue Ocean a great user experience for\nJenkins, please join the Blue Ocean development team on\nGitter!\n\n1. Acceptance Test Harness","title":"Blue Ocean Dev Log: January Week #4","tags":["blueocean"],"authors":[]}},{"node":{"date":"2017-01-20T00:00:00.000Z","id":"4d9875e3-c0ca-5b68-8c88-921f07767cc4","slug":"/blog/2017/01/20/blueocean-dev-log-jan2/","strippedHtml":"As we get closer to\nBlue Ocean\n1.0, which is planned for the end of March, I have started\nhighlighting\nsome of the good stuff that has been going on, and this week was a very busy week.\n\nA new Blue Ocean beta ( b18) was released with:\n\nParametrized pipelines are now supported!\n\ni18n improvements\n\nBetter support for matrix and the evil (yet somehow still used) Maven project type (don’t use it!)\n\nSSE fixes for IE and Edge browsers\n\nAn alpha release of the Visual Editor for Jenkinsfiles on top of\nDeclarative Pipeline\nhas snuck into the \"experimental\" update center. Andrew will be talking\nabout Declarative Pipelines at\nFOSDEM next week.\n\nParametrized Pipelines\n\nYou would know this if you followed\nThorsten’s twitter account.\n\nThat twitter account is mostly pics of Thorsten in running gear, but\noccasionally he announces new features as they land.\n\nWhen you run a pipeline that requires parameters, it will popup a dialog\nlike this no matter where you run it from. Most input types are supported\n(similar to input), with a planned extension point for custom input types.\n\nEditor\n\nA very-very early version of the\nBlue Ocean Pipeline Editor plugin\nthat will set your hair on fire of the editor is in the experimental update\ncenter.\n\nDeclarative pipelines are still not at version 1.0 status, but will be\nshortly. This editor allows you to roundtrip Jenkinsfiles written in this\nway, so they can be edited as text, or visually. The steps available are\ndiscovered form the installed plugins. One to watch.\n\nSo, what’s next?\n\nCreation of Git Pipelines, and likely GitHub too.\n\nShow parallel branches that aren’t in a stage visually\n\nShow stderr/out in test reports\n\nShow more information when Jenkins is \"busy\", such as when agents are coming online, in the Pipeline view\n\nEnjoy!\n\nIf you’re interested in helping to make Blue Ocean a great user experience for\nJenkins, please join the Blue Ocean development team on\nGitter!","title":"Blue Ocean Dev Log: January Week #3","tags":["blueocean"],"authors":[]}}]}},"pageContext":{"limit":8,"skip":328,"numPages":100,"currentPage":42}},
    "staticQueryHashes": ["3649515864"]}