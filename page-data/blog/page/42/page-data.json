{
    "componentChunkName": "component---src-templates-blog-list-template-js",
    "path": "/blog/page/42",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2017-02-15T00:00:00.000Z","id":"65c5d55a-edbc-5f25-8301-7393341de88e","slug":"/blog/2017/02/15/pipeline-editor-preview/","strippedHtml":"Back in September 2016 we announced the availability of the Blue Ocean beta\nand the forthcoming Visual Pipeline Editor. We are happy to announce that you can try\nthe Pipeline Editor preview release today.\n\nWhat is it?\n\nThe Visual Pipeline Editor is the simplest way for anyone wanting to get started with\ncreating Pipelines in Jenkins. It’s also a great way for advanced Jenkins users\nto start adopting pipeline. It allows developers to break up their pipeline into different\n stages and parallelize tasks that can occur at the same time - graphically.\n The rest is up to you.\n\nA pipeline you create visually will produce a Declarative Pipeline Jenkinsfile for you and\n the Jenkinsfile is stored within a Git repository where it is versioned with your application code.\n\nIf you are not sure what a Jenkins Pipeline or a Jenkinsfile is, why not check out the new guided tour to learn more about it?\n\nWhat are we doing next?\n\nWe are working hard to provide feature parity between the Declarative Pipeline syntax and the visual editor. The next phase is to integrate the editor into Blue Ocean so that you don’t have to leave the UI and commit the Jenkinsfile to your repository to complete authoring your pipeline.\n\nIn Blue Ocean, you will be able to edit a Jenkinsfile\nfor a branch directly from within the user interface using the Visual Pipeline Editor. When you are done authoring your pipeline, the pipeline definition will be saved back to your repository as a Jenkinsfile. You can edit the Pipeline again using the Visual Editor or from your favorite text editor.\n\nWe are hoping to deliver this level of integration into Blue Ocean and the\nVisual Pipeline Editor over the next few months, so be sure to check regularly for updates in\nthe Jenkins plugin manager.\n\nGet the Preview\n\nThe Visual Pipeline Editor is available in preview today.\n\nTo try it out today:\n\nInstall the Blue Ocean beta and Blue Ocean Pipeline Editor from the Jenkins plugin manager\n\nClick on the Open Blue Ocean button and then the Pipeline Editor in the main navigation\n\nWe are looking forward to your feedback to help make the Visual Pipeline Editor\nthe easiest way to get started with Jenkins Pipeline. To report bugs or to\nrequest features please follow the instructions on the project page.\n\nAnd don’t forget to join us on our Gitter community chat\n- drop by and say hello!","title":"Say Hello to the Blue Ocean Pipeline Editor","tags":["blueocean","editor","declarative","pipeline"],"authors":[{"avatar":null,"blog":null,"github":"i386","html":"","id":"i386","irc":null,"linkedin":null,"name":"James Dumay","slug":"/blog/authors/i386","twitter":"i386"}]}},{"node":{"date":"2017-02-10T00:00:00.000Z","id":"1055263e-a6f2-5108-b391-a1511ae33556","slug":"/blog/2017/02/10/blueocean-devlog-feb2/","strippedHtml":"We’re counting down the weeks until\nBlue Ocean\n1.0, which is planned for the end of March. If you hadn’t picked up on the hint\nin my\nprevious post,\nmost of the Blue Ocean development team is in Australia, where it is currently\nthe middle of summer. As I write this it is about 1000 degrees outside.\nEmergency measures such as air-conditioning and beer have been deployed in\norder to continue Blue Ocean development.\n\nThis week featured a new beta with the\nSCM API\nchanges; many bug fixes, and some version bumps went out in beta 22. We also\ngot some fresh new designs coming soon, though not in time for beta 22.\n\nSome development highlights:\n\nBeta 22 went out featuring the new\nSCM API\nwith better use of GitHub API rate limits.\n\nA fix for publishing of\nServer Side Events\nthat made one CPU spin up to 100% was fixed (not good unless you want to heat up\nyour room)\n\nSome new refinements to the design merged to the master branch (see images below).\n\nBeta 22 featured the 1.0 version of\nDeclarative Pipeline\n\nAn\nAustralian translation\nwas added; really critical stuff, I know..\n\nThe Acceptance Test Harness (ATH) was stabilised a bit and it now covers\ncreating Pipelines from Git, which we talked about in\nlate January.\n\nThe Visual Pipeline Editor was released to the main Update Center\nas a preview release, ready to play with!\n\nSome small performance improvements\n\nI’m looking forward to those fancy new designs making their way into an\nupcoming release too.\n\nLovely! Hopefully you see more green than I do…​\n\nAnyways, up next for Blue Ocean:\n\nCreation of Pipelines from GitHub, including auto-discovery of new Pipelines.\n\nCloser to a \"release candidate\"\n\nWorking on filtering the activity view for \"per branch\" views\n\nBetter reporting of durations of stages, steps, and runs\n\nEnjoy!\n\nIf you’re interested in helping to make Blue Ocean a great user experience for\nJenkins, please join the Blue Ocean development team on\nGitter!","title":"Blue Ocean Dev Log: February Week #2","tags":["blueocean"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/19e71/michaelneale.jpg","srcSet":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/77b35/michaelneale.jpg 32w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/d4a57/michaelneale.jpg 64w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/19e71/michaelneale.jpg 128w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/ef6ff/michaelneale.webp 32w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/8257c/michaelneale.webp 64w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/6766a/michaelneale.webp 128w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"michaelneale","html":"<div class=\"paragraph\">\n<p>Michael is a CD enthusiast with a interest in User Experience.\nHe is a co-founder of CloudBees and a long time OSS developer, and can often be found\nlurking around the jenkins-dev mailing list or #jenkins on irc (same nick as twitter name).\nBefore CloudBees he worked at Red Hat.</p>\n</div>","id":"michaelneale","irc":null,"linkedin":null,"name":"Michael Neale","slug":"/blog/authors/michaelneale","twitter":"michaelneale"}]}},{"node":{"date":"2017-02-10T00:00:00.000Z","id":"06a04f0b-7823-5a11-8c3a-d385a336b68c","slug":"/blog/2017/02/10/declarative-html-publisher/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nDeclare Your Pipelines!\nDeclarative Pipeline 1.0 is here!\nThis is the second post in a series showing some of the cool features of\nDeclarative Pipeline.\n\nIn the\nprevious blog post,\nwe created a simple Declarative Pipeline.\nIn this blog post, we’ll go back and look at the Scripted Pipeline for the\nPublishing HTML Reports in Pipeline blog post.\nWe’ll convert that Pipeline to Declarative syntax (including properties), go\ninto more detail on the post section, and then we’ll use the agent\ndirective to switch our Pipeline to run in Docker.\n\nSetup\n\nFor this post, I’m going to use the\nblog/add-declarative/html\nbranch of\nmy fork of the\nhermann project.\nI’ve set up a Multibranch Pipeline and pointed it at my repository\nthe same as did it previous post.\nAlso the same as before, I’ve set this Pipeline’s Git configuration to\nautomatically \"Clean after checkout\".\n\nThis time we already have a Pipeline checked in.\nI’ll run it a few times to get a baseline.\n\nConverting to Declarative\n\nLet’s start by converting the Scripted Pipeline straight to Declarative.\n\n// Declarative //\npipeline {\n  agent any // <1> (2)\noptions {\n    // Keep the 10 most recent builds\n    buildDiscarder(logRotator(numToKeepStr:'10')) (3)\n}\n  stages {\n    stage ('Build') { (4)\nsteps {\n        // install required gems\n        sh 'bundle install'\n\n        // build and run tests with coverage\n        sh 'bundle exec rake build spec'\n\n        // Archive the built artifacts\n        archive includes: 'pkg/*.gem'\n\n        // publish html\n        publishHTML target: [\n            allowMissing: false,\n            alwaysLinkToLastBuild: false,\n            keepAll: true,\n            reportDir: 'coverage',\n            reportFiles: 'index.html',\n            reportName: 'RCov Report'\n          ]\n      }\n    }\n  }\n}\n// Scripted //\nproperties([[$class: 'BuildDiscarderProperty',\n                strategy: [$class: 'LogRotator', numToKeepStr: '10']]]) (3)\n\nnode { (1)\nstage ('Build') { (4)\n\n// Checkout\n    checkout scm (2)\n\n// install required gems\n    sh 'bundle install'\n\n    // build and run tests with coverage\n    sh 'bundle exec rake build spec'\n\n    // Archive the built artifacts\n    archive includes: 'pkg/*.gem'\n\n    // publish html\n    publishHTML [\n        allowMissing: false,\n        alwaysLinkToLastBuild: false,\n        keepAll: true,\n        reportDir: 'coverage',\n        reportFiles: 'index.html',\n        reportName: 'RCov Report'\n      ]\n\n  }\n}\n\n1\nSelect where to run this Pipeline, in this case \"any\" agent, regardless of label.\n\n2\nDeclarative automatically performs a checkout of source code on the agent,\nwhereas Scripted Pipeline users must explicitly call checkout scm.\n\n3\nSet the Pipeline option to preserve the ten most recent runs.\nThis overrides the default behavior from the Multibranch parent of this Pipeline.\n\n4\nRun the \"Build\" stage.\n\nNow that we have this Pipeline in Declarative form, let’s take a minute to do a\nlittle clean up.  We’ll split out the bundle actions a little more and move\nsteps into logically grouped stages.  Rather than having one monolithic \"Build\"\nstage, we’ll have details for each stage.  As long as we’re prettying things\nup, let’s switch to using Blue Ocean to view our\nbuilds, as well.\n\n// Declarative //\npipeline {\n  agent any\n  options {\n    // Keep the 10 most recent builds\n    buildDiscarder(logRotator(numToKeepStr:'10'))\n  }\n  stages {\n    stage ('Install') {\n      steps {\n        // install required gems\n        sh 'bundle install'\n      }\n    }\n    stage ('Build') {\n      steps {\n        // build\n        sh 'bundle exec rake build'\n\n        // Archive the built artifacts\n        archive includes: 'pkg/*.gem'\n      }\n    }\n    stage ('Test') {\n      steps {\n        // run tests with coverage\n        sh 'bundle exec rake spec'\n\n        // publish html\n        publishHTML target: [\n            allowMissing: false,\n            alwaysLinkToLastBuild: false,\n            keepAll: true,\n            reportDir: 'coverage',\n            reportFiles: 'index.html',\n            reportName: 'RCov Report'\n          ]\n      }\n    }\n  }\n}\n// Scripted //\n\nUsing post sections\n\nThis looks pretty good, but if we think about it\nthe archive and publishHTML steps are really post-stage actions.\nThey should only occur when the rest of their stage succeeds.\nAs our Pipeline gets more complex we might need to add actions that always happen\neven if a stage or the Pipeline as a whole fail.\n\nIn Scripted Pipeline, we would use try-catch-finally,\nbut we cannot do that in Declarative.\nOne of the defining features of the Declarative Pipeline\nis that it does not allow script-based control structures\nsuch as for loops, if-then-else blocks, or try-catch-finally blocks.\nOf course, internally Step implementations can still contain whatever conditional logic they want,\nbut the Declarative Pipeline cannot.\n\nInstead of free-form conditional logic,\nDeclarative Pipeline provides a set of Pipeline-specific controls:\nwhen directives, which we’ll look at in\na later blog post in this series, control whether to execute the steps in a stage,\nand\npost sections\ncontrol which actions to take based on result of a single stage\nor a whole Pipeline. post supports a number of\nrun conditions,\nincluding always (execute no matter what) and changed\n(execute when the result differs from previous run).\nWe’ll use success to run archive and publishHTML when their respective stages complete.\nWe’ll also use an always block with a placeholder for sending notifications,\nwhich I’ll implement in the next blog post.\n\n// Declarative //\npipeline {\n  agent any\n  options {\n    // Only keep the 10 most recent builds\n    buildDiscarder(logRotator(numToKeepStr:'10'))\n  }\n  stages {\n    stage ('Install') {\n      steps {\n        // install required gems\n        sh 'bundle install'\n      }\n    }\n    stage ('Build') {\n      steps {\n        // build\n        sh 'bundle exec rake build'\n      }\n\n      post {\n        success {\n          // Archive the built artifacts\n          archive includes: 'pkg/*.gem'\n        }\n      }\n    }\n    stage ('Test') {\n      steps {\n        // run tests with coverage\n        sh 'bundle exec rake spec'\n      }\n\n      post {\n        success {\n          // publish html\n          publishHTML target: [\n              allowMissing: false,\n              alwaysLinkToLastBuild: false,\n              keepAll: true,\n              reportDir: 'coverage',\n              reportFiles: 'index.html',\n              reportName: 'RCov Report'\n            ]\n        }\n      }\n    }\n  }\n  post {\n    always {\n      echo \"Send notifications for result: ${currentBuild.result}\"\n    }\n  }\n}\n// Scripted //\n\nSwitching agent to run in Docker\n\nagent can actually accept\nseveral other parameters instead of any.\nWe could filter on label \"some-label\", for example,\nwhich would be the equivalent of node ('some-label') in Scripted Pipeline.\nHowever, agent also lets us just as easily switch to using a Docker container,\nwhich replaces a more complicated set of changes in Scripted Pipeline:\n\npipeline {\n  agent {\n    // Use docker container\n    docker {\n      image 'ruby:2.3'\n    }\n  }\n  /* ... unchanged ... */\n}\n\nIf I needed to, I could add a label filter under docker\nto select a node to host the Docker container.\nI already have Docker available on all my agents, so I don’t need label -\nthis works as is.\nAs you can see below, the Docker container spins up at the start of the run\nand the pipeline runs inside it.  Simple!\n\nConclusion\n\nAt first glance, the Declarative Pipeline’s removal of control structures seems\nlike it would be too constrictive.  However, it replaces those structures with\nfacilities like the post section, that give us reasonable control over the\nflow our our Pipeline while still improving readability and maintainability.\nIn the next blog post, we’ll add notifications to this pipeline\nand look at how to use Shared Libraries with Declarative\nPipeline to share code and keep Pipelines easy to understand.\n\nLinks\n\nDeclarative Pipeline plugin\n\nDeclarative Pipeline Syntax Reference\n\nPipeline source for this post","title":"Declarative Pipeline: Publishing HTML Reports","tags":["tutorial","pipeline","declarative","plugins","ruby"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2017-02-08T00:00:00.000Z","id":"fe83f4ec-fd69-5daf-b26d-80c88853da21","slug":"/blog/2017/02/08/jenkins-datadog-plugin/","strippedHtml":"This is a guest post by Emily Chang, Technical Author at Datadog. A modified version of this article was originally posted on the\nDatadog blog.\n\nIf you’re using Jenkins to continuously integrate changes into your projects, it’s helpful to be able to quickly identify build failures and assess their impact on other components of your stack.\n\nDatadog’s plugin helps users monitor and alert on the performance of their Jenkins builds, right alongside the rest of their infrastructure and applications.\n\nAs shown in the out-of-the-box dashboard below, the Datadog plugin provides a bird’s-eye view of job history and trends. You can use Datadog to:\n\nSet alerts for important build failures\n\nIdentify trends in build durations\n\nCorrelate Jenkins events with performance metrics from other parts of your infrastructure in order to identify and resolve issues\n\nTrack Jenkins build status in real-time\n\nOnce you install the Datadog plugin, Jenkins activities (when a build starts, fails, or succeeds) will start appearing in your Datadog event stream. You will also see what percentage of builds failed within the same job, so that you can quickly spot which jobs are experiencing a higher rate of failure than others.\n\nRemember to blacklist any jobs you don’t want to track by indicating them in your plugin configuration.\n\nDatadog’s out-of-the-box Jenkins dashboard includes a status widget that displays the count of all jobs that have run in the past day, grouped by success or failure. To explore further, you can also click on the widget to view the individual jobs that have failed or succeeded in the past day.\n\nThe dashboard also displays the proportion of successful vs. failed builds, along with the total number of job runs completed over the past four hours.\n\nDatadog enables you to correlate Jenkins events with application performance metrics to investigate the root cause of an issue. For example, the screenshot below shows that average CPU on the app servers increased sharply after a Jenkins build was completed and deployed (indicated by the pink bar). Your team can use this information as a starting point to investigate if code changes in the corresponding release may be causing issues.\n\nVisualize job duration metrics\n\nEvery time a build is completed, the plugin collects the build duration as a metric that you can aggregate by job name or any other tag, and graph over time. In the screenshot below, we can view the average job durations in the past four hours, sorted in decreasing order:\n\nYou can also graph and visualize trends in build durations for each job by using Datadog’s robust_trend() linear regression function, as shown in the screenshot below. This graph indicates which jobs' durations are trending longer over time, so that you can investigate if there appears to be a problem. If you’re experimenting with changes to your CI pipeline, consulting this graph can help you track the effects of those changes over time.\n\nUse tags to monitor your Jenkins jobs\n\nTags add custom dimensions to your monitoring, so you can focus on what’s important to you right now.\n\nEvery Jenkins event, metric, and service check is auto-tagged with job, result, and branch (if applicable). You can also enable the optional node tag in the plugin settings.\n\nAs of version 0.5.0, the plugin supports custom tags. This update was developed by one of our open source contributors, Mads Nielsen. Many thanks to Mads for helping us implement this feature!\n\nYou can create custom tags for the name of the application you’re building, your particular team name (e.g. team=licorice), or any other info that matters to you. For example, if you have multiple jobs that perform nightly builds, you might want to create a descriptive tag that distinguishes them from other types of jobs.\n\nAs shown in the configuration settings above, you can add custom tags, formatted as key=value, in two ways:\n\nin a text file (saved in the workspace for the job)\n\nin a list of properties in the text box\n\nSet up the Datadog plugin\n\nThe Datadog plugin requires Jenkins 1.580.1 or newer.\n\nIn Jenkins, navigate to Manage Jenkins > Manage Plugins.\n\nSearch for Datadog Plugin and check the box to install it.\n\nIn Jenkins, go to Manage Jenkins > Configure System.\n\nScroll down to the Datadog Plugin section, and paste your API key in the text box. You can copy this from the API Keys page of your Datadog account. Click Test Key to confirm that the plugin recognizes your API key.\n\nSave your changes, and you’re all set!\n\nGet started\n\nIf you’re already using Datadog, you can start monitoring Jenkins jobs by following the instructions here to download the Datadog plugin. If you’re not using Datadog yet, here’s a 14-day free trial.","title":"Monitor Jenkins jobs with the Datadog plugin","tags":["plugins","monitoring"],"authors":[{"avatar":null,"blog":null,"github":"echang26","html":"","id":"echang26","irc":null,"linkedin":null,"name":"Emily Chang","slug":"/blog/authors/echang26","twitter":null}]}},{"node":{"date":"2017-02-07T00:00:00.000Z","id":"df12ba62-13a1-5a1b-88f3-afc58f167e79","slug":"/blog/2017/02/07/declarative-maven-project/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nDeclare Your Pipelines!\nDeclarative Pipeline 1.0 is here!\nThis is first in a series of blog posts that will show some of the cool features of\nDeclarative Pipeline.\nFor several of these posts, I’ll be revisiting some of my\nprevious posts\non using various plugins with (Scripted) Pipeline,\nand seeing how those are implemented in Declarative Pipeline.\n\nTo start though, let’s get familiar with the basic structure of a Declarative Pipeline\nby creating a simple Pipeline for a Maven-based Java project - the\nJenkins JUnit plugin.\nWe’ll create a minimal Declarative Pipeline,\nadd the settings needed to install Maven and the JDK,\nand finally we’ll actually run Maven to build the plugin.\n\nSet up\n\nWith Declarative, it is still possible to run Pipelines edited directly in the\nJenkins web UI, but one of the key features of \"Pipeline as Code\" is\nchecking-in and being able to track changes.  For this post, I’m going to use\nthe\nblog/add-declarative-pipeline\nbranch of\nmy fork of the JUnit plugin.\nI’m going to set up a Multi-branch Pipeline and point it at my repository.\n\nI’ve also set this Pipeline’s Git configuration to automatically \"clean after\ncheckout\" and to only keep the ten most recent runs.\n\nWriting a Minimal Pipeline\n\nAs has been said before, Declarative Pipeline provides a more structured,\n\"opinionated\" way to create Pipelines. I’m going to start by creating a minimal\nDeclarative Pipeline and adding it to my branch.  Below is a minimal Pipeline\n(with annotations) that just prints a message:\n\n// Declarative //\npipeline { (1)\nagent any // <2> (3)\nstages { (4)\nstage('Build') { (5)\nsteps { (6)\necho 'This is a minimal pipeline.' (7)\n}\n        }\n    }\n}\n// Scripted //\nnode { (2)\ncheckout scm (3)\nstage ('Build') { (5)\necho 'This is a minimal pipeline.' (6)\n}\n}\n\n1\nAll Declarative Pipelines start with a pipeline section.\n\n2\nSelect where to run this Pipeline, in this case \"any\" agent, regardless of label.\n\n3\nDeclarative automatically performs a checkout of source code on the agent,\nwhereas Scripted Pipeline users must explicitly call checkout scm,\n\n4\nA Declarative Pipeline is defined as a series of stages.\n\n5\nRun the \"Build\" stage.\n\n6\nEach stage in a Declarative Pipeline runs a series of steps.\n\n7\nRun the echo step to print a message in the Console Output.\n\nIf you are familiar with Scripted Pipeline, you can toggle the above\nDeclarative code sample to show the Scripted equivalent.\n\nOnce I add the Pipeline above to my Jenkinsfile and run \"Branch Indexing\", my\nJenkins will pick it up and run run it.  We see that the Declarative Pipeline\nhas added stage called \"Declarative: Checkout SCM\":\n\nThis a \"dynamic stage\", one of several the kinds that Declarative Pipeline adds\nas needed for clearer reporting.  In this case, it is a stage in which the\nDeclarative Pipeline automatically checkouts out source code on the agent.\n\nAs you can see above, we didn’t have to tell it do any of this,\n\nConsole Output\n\n[Pipeline] node\nRunning on osx_mbp in /Users/bitwiseman/jenkins/agents/osx_mbp/workspace/blog_add-declarative-pipeline\n[Pipeline] {\n[Pipeline] stage\n[Pipeline] { (Declarative: Checkout SCM)\n[Pipeline] checkout\nCloning the remote Git repository\n{ ... truncated 20 lines ... }\n[Pipeline] }\n[Pipeline] // stage\n[Pipeline] stage\n[Pipeline] { (Build)\n[Pipeline] echo\nThis is a minimal pipeline\n[Pipeline] }\n[Pipeline] // stage\n[Pipeline] }\n[Pipeline] // node\n[Pipeline] End of Pipeline\nFinished: SUCCESS\n\nDeclarative Pipeline syntax is a little more verbose than the equivalent Scripted Pipeline,\nbut the added detail gives a clearer, more consistent view of what the Pipeline is supposed to do.\nIt also gives us a structure into which we can add more configuration details about this Pipeline.\n\nAdding Tools to Pipeline\n\nThe next thing we’ll add in this Pipeline is a tools section to let us use\nMaven.  The tools section is one of several sections we can add under\npipeline, which affect the configuration of the rest of the Pipeline.  (We’ll\nlook at the others, including agent, in later posts.) Each tool entry will\nmake whatever settings changes, such as updating PATH or other environment\nvariables, to make the named tool available in the current pipeline.  It will\nalso automatically install the named tool if that tool is configured to do so\nunder \"Managing Jenkins\" → \"Global Tool Configuration\".\n\n// Declarative //\npipeline {\n    agent any\n    tools { (1)\nmaven 'Maven 3.3.9' (2)\njdk 'jdk8' (3)\n}\n    stages {\n        stage ('Initialize') {\n            steps {\n                sh '''\n                    echo \"PATH = ${PATH}\"\n                    echo \"M2_HOME = ${M2_HOME}\"\n                ''' (4)\n}\n        }\n\n        stage ('Build') {\n            steps {\n                echo 'This is a minimal pipeline.'\n            }\n        }\n    }\n}\n// Scripted Not Defined //\n\n1\ntools section for adding tool settings.\n\n2\nConfigure this Pipeline to use the Maven version matching \"Maven 3.3.9\"\n(configured in \"Managing Jenkins\" → \"Global Tool Configuration\").\n\n3\nConfigure this Pipeline to use the Maven version matching \"jdk8\"\n(configured in \"Managing Jenkins\" → \"Global Tool Configuration\").\n\n4\nThese will show the values of PATH and M2_HOME environment variables.\n\nWhen we run this updated Pipeline the same way we ran the first, we see that\nthe Declarative Pipeline has added another stage called \"Declarative: Tool\nInstall\": In the console output, we see that during this particular stage \"Maven 3.3.9\" gets installed,\nand the PATH and M2_HOME environment variables are set:\n\nConsole Output\n\n{ ... truncated lines ... }\n[Pipeline] // stage\n[Pipeline] stage\n[Pipeline] { (Declarative: Tool Install)\n[Pipeline] tool\nUnpacking https://repo.maven.apache.org/maven2/org/apache/maven/apache-maven/3.3.9/apache-maven-3.3.9-bin.zip\nto /Users/bitwiseman/jenkins/agents/osx_mbp/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.3.9\non osx_mbp\n[Pipeline] envVarsForTool\n[Pipeline] tool\n[Pipeline] envVarsForTool\n[Pipeline] }\n[Pipeline] // stage\n{ ... }\nPATH = /Library/Java/JavaVirtualMachines/jdk1.8.0_92.jdk/Contents/Home/bin:/Users/bitwiseman/jenkins/agents/osx_mbp/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.3.9/bin:...\nM2_HOME = /Users/bitwiseman/jenkins/agents/osx_mbp/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.3.9\n{ ... }\n\nRunning a Maven Build\n\nFinally, running a Maven build is trivial.  The tools section already added\nMaven and JDK8 to the PATH, all we need to do is call mvn install.  It\nwould be nice if I could split the build and the tests into separate stages,\nbut Maven is famous for not liking when people do that, so I’ll leave it alone\nfor now.\n\nInstead, let’s load up the results of the build using the JUnit plugin,\nhowever the version that was just built, sorry.\n\n// Declarative //\npipeline {\n    agent any\n    tools {\n        maven 'Maven 3.3.9'\n        jdk 'jdk8'\n    }\n    stages {\n        stage ('Initialize') {\n            steps {\n                sh '''\n                    echo \"PATH = ${PATH}\"\n                    echo \"M2_HOME = ${M2_HOME}\"\n                '''\n            }\n        }\n\n        stage ('Build') {\n            steps {\n                sh 'mvn -Dmaven.test.failure.ignore=true install' (1)\n}\n            post {\n                success {\n                    junit 'target/surefire-reports/**/*.xml' (2)\n}\n            }\n        }\n    }\n}\n// Scripted //\nnode {\n    checkout scm\n\n    String jdktool = tool name: \"jdk8\", type: 'hudson.model.JDK'\n    def mvnHome = tool name: 'mvn'\n\n    /* Set JAVA_HOME, and special PATH variables. */\n    List javaEnv = [\n        \"PATH+MVN=${jdktool}/bin:${mvnHome}/bin\",\n        \"M2_HOME=${mvnHome}\",\n        \"JAVA_HOME=${jdktool}\"\n    ]\n\n    withEnv(javaEnv) {\n    stage ('Initialize') {\n        sh '''\n            echo \"PATH = ${PATH}\"\n            echo \"M2_HOME = ${M2_HOME}\"\n        '''\n    }\n    stage ('Build') {\n        try {\n            sh 'mvn -Dmaven.test.failure.ignore=true install'\n        } catch (e) {\n            currentBuild.result = 'FAILURE'\n        }\n    }\n    stage ('Post') {\n        if (currentBuild.result == null || currentBuild.result == 'SUCCESS') {\n            junit 'target/surefire-reports/**/*.xml' (2)\n}\n    }\n}\n\n1\nCall mvn, the version configured by the tools section will be first on the path.\n\n2\nIf the maven build succeeded, archive the JUnit test reports for display in the Jenkins web UI.\nWe’ll discuss the\npost section in detail in the next blog post.\n\nIf you are familiar with Scripted Pipeline, you can toggle the above\nDeclarative code sample to show the Scripted equivalent.\n\nBelow is the console output for this last revision:\n\nConsole Output\n\n{ ... truncated lines ... }\n+ mvn install\n[INFO] Scanning for projects...\n[WARNING] The POM for org.jenkins-ci.tools:maven-hpi-plugin:jar:1.119 is missing, no dependency information available\n[WARNING] Failed to build parent project for org.jenkins-ci.plugins:junit:hpi:1.20-SNAPSHOT\n[INFO]\n[INFO] ------------------------------------------------------------------------\n[INFO] Building JUnit Plugin 1.20-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO]\n[INFO] --- maven-hpi-plugin:1.119:validate (default-validate) @ junit ---\n[INFO]\n[INFO] --- maven-enforcer-plugin:1.3.1:display-info (display-info) @ junit ---\n[INFO] Maven Version: 3.3.9\n[INFO] JDK Version: 1.8.0_92 normalized as: 1.8.0-92\n[INFO] OS Info: Arch: x86_64 Family: mac Name: mac os x Version: 10.12.3\n[INFO]\n{ ... }\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 03:25 min\n[INFO] Finished at: 2017-02-06T22:43:41-08:00\n[INFO] Final Memory: 84M/1265M\n[INFO] ------------------------------------------------------------------------\n\nConclusion\n\nThe new Declarative syntax is a significant step forward for Jenkins Pipeline.\nIt trades some verbosity and constraints for much greater clarity and\nmaintainability.  In the coming weeks, I’ll be adding new blog posts\ndemonstrating various features of the Declarative syntax along with some recent\nJenkins Pipeline improvements.\n\nLinks\n\nDeclarative Pipeline\n\nDeclarative Pipeline Syntax Reference\n\nJenkins JUnit plugin","title":"Declarative Pipeline for Maven Projects","tags":["tutorial","pipeline","declarative","maven","java"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2017-02-07T00:00:00.000Z","id":"3d993b8f-320e-5137-8e5b-be2cc83d0195","slug":"/blog/2017/02/07/gsoc2017-announcement/","strippedHtml":"On behalf of the GSoC Org Admin team I am happy to announce that we are going to apply to\nGoogle Summer of Code (GSoC) again this year.\nIn GSoC high-profile students work in open-source projects for several months under mentorship of organization members.\n\nWe are looking for mentors and project ideas.\nSo yes, we are looking for you :)\n\nConditions\n\nAs a mentor, you will be asked to:\n\nlead the project in the area of their interest\n\nactively participate in the project during student selection, community bonding and coding phases (March - August)\n\nwork in teams of 2+ mentors per 1 each student\n\ndedicate a consistent and significant amount of time, especially during the coding phase ( ~5 hours per week in the team of two mentors)\n\nMentorship does not require strong expertise in Jenkins plugin development.\nThe main objective is to guide students and to get them involved into the Jenkins community.\nIf your mentor team requires any specific expertise, GSoC org admins will do their best in order to find advisors.\n\nWhat do you get?\n\nA student, who works within the area of your interest on full-time for several months\n\nJoint projects with Jenkins experts, lots of fun and ability to study something together\n\nLimited edition of swags from Google and Jenkins project\n\nMaybe: Participation in GSoC Mentor Summit in California with expense coverage (depends on project results and per-project quotas)\n\nRequirements\n\nYou are:\n\npassionate about Jenkins\n\ninterested in being a mentor or advisor\n\nready to dedicate time && have no major unavailability periods planned to this summer\n\nWe expect mentors to be available by email during 75% of working days in the May-August timeframe\n\nYour project idea is:\n\nabout code (though it may and likely should include some documentation and testing work)\n\nabout Jenkins (plugins, core, infrastructure, etc.)\n\npotentially doable by a student in 3-4 months\n\nHow to apply\n\nIf you are interested, drop the Email to the Jenkins Developer mailing list with the GSoC2017 prefix.\n\nBriefly describe your project idea (a couple of sentences) and required qualifications from students. Examples: GSoC2016, GSoC2017 - current project ideas\n\nIf you already have a co-mentor(s), please mention them\n\nHaving several project ideas is fine. Having no specific ideas is also fine.\n\nDisclaimer: We cannot guarantee that all projects happen, it depends on student application results and the number of project slots.\n\nLinks\n\nGoogle Summer of Code page\n\nJenkins GSoC Page\n\nGSoC2016 project ideas\n\nGSoC2016 page (project results and more info)","title":"Google Summer Of Code 2017: Call for mentors","tags":["gsoc","events","general","gsoc2017"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8c8e8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/bf8e1/oleg_nenashev.png","srcSet":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/914ee/oleg_nenashev.png 32w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/1c9ce/oleg_nenashev.png 64w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/bf8e1/oleg_nenashev.png 128w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/acb7c/oleg_nenashev.png 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/ef6ff/oleg_nenashev.webp 32w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/8257c/oleg_nenashev.webp 64w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/6766a/oleg_nenashev.webp 128w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/22bfc/oleg_nenashev.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"https://oleg-nenashev.github.io/","github":"oleg-nenashev","html":"<div class=\"paragraph\">\n<p>Jenkins core maintainer and board member.\nOleg started using Hudson for Hardware/Embedded projects in 2008 and became an active Jenkins contributor in 2012.\nNowadays he leads several Jenkins <a href=\"/sigs\">SIGs</a>, outreach programs (<a href=\"/projects/gsoc\">Google Summer of Code</a>, <a href=\"/events/hacktoberfest\">Hacktoberfest</a>) and <a href=\"/projects/jam/\">Jenkins meetups</a> in Switzerland and Russia.\nOleg works for <a href=\"https://www.cloudbees.com/\">CloudBees</a> and focuses on key projects in the community.</p>\n</div>","id":"oleg_nenashev","irc":"oleg_nenashev","linkedin":"onenashev","name":"Oleg Nenashev","slug":"/blog/authors/oleg_nenashev","twitter":"oleg_nenashev"}]}},{"node":{"date":"2017-02-06T00:00:00.000Z","id":"46885786-902f-55cb-b263-cae8df8f0611","slug":"/blog/2017/02/06/scm-api-2-take2/","strippedHtml":"In January we\nannounced the release of SCM API 2.0.\nAfter the original release was published we identified four new high-impact\nissues.  We decided to remove the new versions of the plugins from the update\ncenter until those issues could be resolved. The issues have now been resolved\nand the plugins are now available from the update center.\n\nSummary for busy Jenkins Administrators\n\nUpgrading should make multi-branch projects much better.  When you are ready to\nupgrade you must ensure that you upgrade all the required plugins.  If you miss\nsome, just upgrade them and restart to fix the issue. And of course, it’s\nalways a good idea to take a backup of your JENKINS_HOME before upgrading any\nplugins.\n\nIn the list below, version numbers in bold indicate a change from the\noriginal version in the\noriginal announcement\n\nFolders Plugin\n\n5.17 or newer\n\nSCM API Plugin\n\n2.0.2 or newer\n\nBranch API Plugin\n\n2.0.2 or newer\n\nGit Plugin\n\nThis depends on the exact release line of the Git plugin that you are using.\n\nFollowing the 2.6.x release line: 2.6.4 or newer\n\nFollowing the 3.0.x release line ( recommended): 3.0.4 or newer\n\nMercurial Plugin\n\n1.58 or newer\n\nGitHub Branch Source Plugin\n\n2.0.1 or newer\n\nBitBucket Branch Source Plugin\n\n2.0.2 or newer\n\nGitHub Organization Folders Plugin\n\n1.6\n\nPipeline Multibranch Plugin\n\n2.12 or newer\n\nIf you are using the Blue Ocean plugin\n\nBlue Ocean Plugin\n\n1.0.0-b22 or newer\n\nOther plugins that may require updating:\n\nGitHub API Plugin\n\n1.84 or newer\n\nGitHub Plugin\n\n1.25.0 or newer\n\nIf you upgrade to Branch API 2.0.x and you have either the GitHub Branch Source or the BitBucket Branch Source plugins and you do not upgrade those instances to the 2.0.x line then your Jenkins instance will fail to start-up correctly.\n\nThe solution is just to upgrade the GitHub Branch Source or the BitBucket Branch Source plugin (as appropriate) to the 2.0.x line.\n\nAfter an upgrade you will see the data migration warning (see the screenshot in\nJENKINS-41608 for an\nexample) this is normal and expected.  The unreadable data will be removed by\nthe next scan / index or can be removed manually using the Discard Unreadable\nData button.  The warning will disappear on the next restart after the\nunreadable data has been removed.\n\nPlease update to the versions listed above. If you want to know more about the\nissues and how they were resolved, see the next section.\n\nAnalysis of the issues\n\nThe issues described below are resolved with these plugin releases:\n\nFolders Plugin: 5.17\n\nSCM API Plugin: 2.0.2\n\nBranch API Plugin: 2.0.2\n\nGit Plugin: Either 2.6.4 or 3.0.4\n\nGitHub Branch Source Plugin: 2.0.1\n\nBitBucket Branch Source Plugin: 2.0.2\n\nPipeline Multibranch Plugin: 2.12\n\nJENKINS-41121: GitHub Branch Source upgrade can cause a lot of rebuilds :\n\nMigration of GitHub branches from 1.x to 2.x resulted in a change of the\nimplementation class used to identify branches.  Some other other bugs in\nBranch API had been fixed and the combined effect resulted in a rebuild of all\nGitHub Branches (not PRs) after an upgrade to GitHub Branch Source Plugin\n2.0.0.  This rebuild was referred to as a \"build storm\".\n\nResolution:\n\nThe SCM API plugin was enhanced to add an extension point that allows for a second round of data migration when upgrading.\n\nThe second round of data migration allows plugins implementing the SCM API contract to fix implementation class issues in context.\n\nThe Branch API plugin was enhanced to use this new extension point.\n\nThe GitHub Branch Source plugin was enhanced to provide an implementation of this extension point.\n\nJENKINS-41255: Upgrading from a navigator that did not assign consistent source ids to a version that does assign consistent source ids causes a build storm on first scan :\n\nThe GitHub Branch Source and BitBucket Branch Source plugins in 1.x were not\nassigning consistent IDs to multi-branch projects discovered in an Organization\nFolder.  Both plugins were fixed in 2.0.0 to assign consistent IDs as a change\nof ID would result in a rebuild of all projects.  What was missed is that the\nvery first scan of an Organization Folder after an upgrade will change the\nrandomly assigned ID assigned by the 1.x plugins into the consistent ID\nassigned by the 2.0.0 plugins and consequently trigger a rebuild of all\nbranches. This rebuild was referred to as a \"build storm\".\n\nResolution:\n\nThe Branch API plugin was enhanced to detect the case where a branch source has\nbeen changed but the change is only changing the ID.  When such changes are\nidentified, the downstream references of the ID are all updated which will\nprevent a build storm.\n\nJENKINS-41313: On first index after upgrade to 2.0.0 all open PRs are rebuilt :\n\nThe BitBucket Branch Source 1.x did not store all the information about PRs\nthat is required by the SCM API 2.0.x model.  This could well have resulted in\nsubtle effects when manually triggering a rebuild of a merge PR if the PR’s\ntarget branch has been modified after the PR branch was first detected by\nJenkins. Consequently, as the information is required, BitBucket Branch Source\nplugin 2.0.0 populated the information with dummy values which would force the\ncorrect information to be retrieved.  The side-effect is that all PR branches\nwould be rebuilt.\n\nResolution:\n\nThe changes in SCM API 2.0.2 introduced to resolve JENKINS-41121 provided a path to resolve this issue without causing a rebuild of all PR branches.\n\nThe BitBucket Branch Source plugin was enhanced to provide an implementation of the new SCM API extension point that connects to BitBucket and retrieves the missing information.\n\nJENKINS-41124: Can’t get a human readable job name anymore :\n\nDuring initial testing of the Branch API 2.0.0 release an issue was identified\nwith how Organization Folders handled unusual names.  None of the existing\nimplementations of the SCMNavigator API could generate such unusual names due\nto form validation on GitHub / BitBucket replacing unusual characters with -\nwhen creating a repository.\n\nIt would be irresponsible to rely on external services sanitizing their input\ndata for the correct operation of Organization Folders.  Consequently, in\nBranch API 2.0.0 the names were all transformed into URL safe names, with the\noriginal URLs still resolving to the original projects so that any existing\nsaved links would remain functional.\n\nQuite a number of people objected to this change of URL scheme.\n\nResolution:\n\nThere has been a convention in Jenkins that the on-disk storage structure for\njobs mirrors the URL structure. This is only a convention and there is nothing specific in the code that\nmandates following the convention.\n\nThe Folders Plugin was enhanced to allow for computed folders (where the item\nnames are provided by an external source) to provide a strategy to use when\ngenerating the on-disk storage names as well as the URL component names for\nthe folder’s child items.\n\nThe Branch API plugin was enhanced to use this new strategy for name transformation.\n\nThe net effect of this change is that the URLs remain the same as for 1.x but\nthe on-disk storage uses transformed names that are future proofed against\nany new SCMNavigator implementations where the backing service allows names\nthat are problematic to use as filesystem directory names.\n\nSide-effect:\n\nThe Branch API 2.0.0 approach handled the transformation of names by renaming the items using the Jenkins Item rename API.\n\nThe Branch API 2.0.2 approach does not rename the child items as it is only the on-disk storage location that is moved.\n\nThis means that the Jenkins Item rename API cannot be used.\n\nAt this time, the only known side-effect is in the Job Configuration History plugin.\nThe configuration history of each child item will still be tracked going\nforward after the upgrade.  The pre-upgrade configuration history is also\nretained.  Because the Jenkins Item rename API cannot be used to flag the\nconfiguration file location change, there is no association between the\npre-upgrade history chain and the post-upgrade history chain.","title":"SCM API 2.0 Release Take 2","tags":["development","plugins"],"authors":[{"avatar":null,"blog":null,"github":"stephenc","html":"","id":"stephenc","irc":null,"linkedin":null,"name":"Stephen Connolly","slug":"/blog/authors/stephenc","twitter":"connolly_s"}]}},{"node":{"date":"2017-02-03T00:00:00.000Z","id":"e40d65de-fedc-5955-acd7-b2f63eafaf7f","slug":"/blog/2017/02/03/blueocean-devlog-feb/","strippedHtml":"With only a couple of months left before\nBlue Ocean\n1.0, which is planned for the end of March, I have\nbeen\nhighlighting\nsome of the good work being finished up by the developers hacking on Blue\nOcean.\n\nThis week was a grab bag of important behind-the-scenes features and finalising\nthe preview of the editor. The merge of the SCM API changes also made it in.\nThe editor has the new sheets style of editing (there will be blogs and more on\nthis in the next few weeks):\n\nSome highlights:\n\nFix to async loading of resources like translations, so screens don’t\n\"flash\" when they are loaded (i18n improvement)\n\nLinks in notifications can be configured to point to classic or\nBlue Ocean screens\n\nTime reporting works better when browser clock is out of sync with\nserver\n\nSECURITY-380 was backported into a small fix for those that aren’t\nrunning the latest LTS (but you should ideally be running it)\n\nSCM API changes finally landed - this will be in beta 22 which should\nhit the update centers soon. This should make things work better with\nGitHub rate limits.\n\nBeta 21 was released\n\nThe editor reached \"preview\" release state ready for use with the newly\nannounced Declarative Pipeline stuff.\n\nAlso, a reference to Australian pop culture had to be removed, sadly.\n\nUp Next:\n\nSome cosmetic changes around headers to make it much nicer and clearer\n\nFavorite improvements\n\nGitHub Org-based Pipeline creation\n\nEditor available in the general update center\n\nBeta 22 with SCM improvements and no more GitHub rate limit hassles\n\nMany fixes\n\nImprovements to the Acceptance Test Harness to reduce the number of false-positives.\n\nEnjoy!\n\nIf you’re interested in helping to make Blue Ocean a great user experience for\nJenkins, please join the Blue Ocean development team on\nGitter!","title":"Blue Ocean Dev Log: February Week #1","tags":["blueocean"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/19e71/michaelneale.jpg","srcSet":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/77b35/michaelneale.jpg 32w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/d4a57/michaelneale.jpg 64w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/19e71/michaelneale.jpg 128w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/ef6ff/michaelneale.webp 32w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/8257c/michaelneale.webp 64w,\n/gatsby-jenkins-io/static/75c8520897a1db139d524965f5bb7ccc/6766a/michaelneale.webp 128w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"michaelneale","html":"<div class=\"paragraph\">\n<p>Michael is a CD enthusiast with a interest in User Experience.\nHe is a co-founder of CloudBees and a long time OSS developer, and can often be found\nlurking around the jenkins-dev mailing list or #jenkins on irc (same nick as twitter name).\nBefore CloudBees he worked at Red Hat.</p>\n</div>","id":"michaelneale","irc":null,"linkedin":null,"name":"Michael Neale","slug":"/blog/authors/michaelneale","twitter":"michaelneale"}]}}]}},"pageContext":{"limit":8,"skip":328,"numPages":100,"currentPage":42}},
    "staticQueryHashes": ["3649515864"]}