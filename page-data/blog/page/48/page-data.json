{
    "componentChunkName": "component---src-templates-blog-list-template-js",
    "path": "/blog/page/48",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2016-08-29T00:00:00.000Z","id":"48307a4d-711a-56d1-885f-e9d1945fa4d5","slug":"/blog/2016/08/29/sauce-pipeline/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nTesting web applications across multiple browsers on different platforms can be challenging even for smaller applications.\nWith Jenkins and the\nSauce OnDemand Plugin,\nyou can wrangle that complexity by defining your Pipeline as Code.\n\nPipeline ♥ UI Testing, Too\n\nI recently started looking for a way to do browser UI testing for an open-source JavaScript project to which I contribute.\nThe project is targeted primarily at\nNode.js\nbut we’re committed to maintaining browser-client compatibility as well.\nThat means we should run tests on a matrix of browsers.\nSauce Labs\nhas an \"open-sauce\" program that provides free test instances to open-source projects.\nI decided to try using the\nSauce OnDemand Plugin\nand\nNightwatch.js\nto run Selenium tests on a sample project first, before trying a full-blown suite of tests.\n\nStarting from Framework\n\nI started off by following Sauce Labs' instructions on\n\" Setting up Sauce Labs with Jenkins\"\nas far as I could.\nI installed the\nJUnit and\nSauce OnDemand\nplugins, created an account with Sauce Labs, and\nadded my Sauce Labs credentials to Jenkins.\nFrom there I started to get a little lost.\nI’m new to Selenium and I had trouble understanding how to translate the instructions to my situation.\nI needed a working example that I could play with.\n\nHappily, there’s a whole range of sample projects in\n\" saucelabs-sample-test-frameworks\"\non GitHub, which show how to integrate Sauce Labs with various test frameworks, including Nightwatch.js.\nI forked the Nightwatch.js sample to\nbitwiseman/JS-Nightwatch.js\nand set to writing my Jenkinsfile.\nBetween the sample and the Sauce Labs instructions,\nI was able to write a pipeline that ran five tests on one browser via\nSauce Connect :\n\nnode {\n    stage \"Build\"\n    checkout scm\n\n    sh 'npm install' (1)\n\nstage \"Test\"\n    sauce('f0a6b8ad-ce30-4cba-bf9a-95afbc470a8a') { (2)\nsauceconnect(options: '', useGeneratedTunnelIdentifier: false, verboseLogging: false) { (3)\nsh './node_modules/.bin/nightwatch -e chrome --test tests/guineaPig.js || true' (4)\njunit 'reports/**' (5)\nstep([$class: 'SauceOnDemandTestPublisher']) (6)\n}\n    }\n}\n\n1\nInstall dependencies\n\n2\nUse my\npreviously added sauce credentials\n\n3\nStart up the\nSauce Connect\ntunnel to Sauce Labs\n\n4\nRun Nightwatch.js\n\n5\nUse JUnit to track results and show a trend graph\n\n6\nLink result details from Sauce Labs\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit url:'https://github.com/bitwiseman/JS-Nightwatch.js', branch: 'sauce-pipeline'.\n\nI ran this job a few times to get the JUnit report to show a trend graph.\n\nThis sample app generates the SauceOnDemandSessionID for each test, enabling the Jenkins Sauce OnDemand Plugin’s result publisher to link results to details Sauce Labs captured during the run.\n\nAdding Platforms\n\nNext I wanted to add a few more platforms to my matrix.\nThis would require changing both the test framework configuration and the pipeline.\nI’d need to add new named combinations of platform, browser, and browser version (called \"environments\") to the Nightwatch.js configuration file,\nand modify the pipeline to run tests in those new environments.\n\nThis is a perfect example of the power of pipeline as code.\nIf I were working with a separately configured pipeline,\nI’d have to make the change to the test framework, then change the pipeline manually.\nWith my pipeline checked in as code,\nI could change both in one commit,\npreventing errors resulting from pipeline configurations going out of sync from the rest of the project.\n\nI added three new environments to nightwatch.json :\n\n\"test_settings\" : {\n  \"default\": { /*----8 <----*/ },\n  \"chrome\": { /*----8 <----*/ },\n\n  \"firefox\": {\n    \"desiredCapabilities\": {\n      \"platform\": \"linux\",\n      \"browserName\": \"firefox\",\n      \"version\": \"latest\"\n    }\n  },\n  \"ie\": {\n    \"desiredCapabilities\": {\n      \"platform\": \"Windows 10\",\n      \"browserName\": \"internet explorer\",\n      \"version\": \"latest\"\n    }\n  },\n  \"edge\": {\n    \"desiredCapabilities\": {\n      \"platform\": \"Windows 10\",\n      \"browserName\": \"MicrosoftEdge\",\n      \"version\": \"latest\"\n    }\n  }\n}\n\nAnd I modified my Jenkinsfile to call them:\n\n//----8 (1)\n'chrome',\n        'firefox',\n        'ie',\n        'edge'\n    ].join(',')\n    // Run selenium tests using Nightwatch.js\n    sh \"./node_modules/.bin/nightwatch -e ${configs} --test tests/guineaPig.js\" (2)\n} //----8\n\n1\nUsing an array to improve readability and make it easy to add more platforms later.\n\n2\nChanged from single-quoted string to double-quoted to support variable substitution.\n\nTest frameworks have bugs too. Nightwatch.js (v0.9.8) generates incomplete JUnit files,\nreporting results without enough information in them to distinguish between platforms.\nI implemented a fix for it and\nsubmitted a PR to Nightwatch.js.\nThis blog shows output with that fix applied locally.\n\nAs expected, Jenkins picked up the new pipeline and ran Nightwatch.js on four platforms.\nSauce Labs of course recorded the results and correctly linked them into this build.\nNightwatch.js was already configured to use multiple worker threads to run tests against those platforms in parallel, and\nmy Sauce Labs account supported running them all at the same time,\nletting me cover four configurations in less that twice the time,\nand that added time was most due to individual new environments taking longer to complete.\nWhen I move to the actual project, this will let me run broad acceptance passes quickly.\n\nConclusion: To Awesome and Beyond\n\nConsidering the complexity of the system, I was impressed with how easy it was to integrate Jenkins with Sauce OnDemand to start testing on multiple browsers.\nThe plugin worked flawlessly with Jenkins Pipeline.\nI went ahead and ran some additional tests to show that failure reporting also behaved as expected.\n\n//----8 (1)\n//----8\n\n1\nRemoved --test filter to run all tests\n\nEpilogue: Pipeline vs. Freestyle\n\nJust for comparison here’s the final state of this job in Freestyle UI versus fully-commented pipeline code:\n\nThis includes the\nAnsiColor Plugin\nto support Nightwatch.js' default ANSI color output.\n\nFreestyle\n\nPipeline\n\nnode {\n    stage \"Build\"\n    checkout scm\n\n    // Install dependencies\n    sh 'npm install'\n\n    stage \"Test\"\n\n    // Add sauce credentials\n    sauce('f0a6b8ad-ce30-4cba-bf9a-95afbc470a8a') {\n        // Start sauce connect\n        sauceconnect(options: '', useGeneratedTunnelIdentifier: false, verboseLogging: false) {\n\n            // List of browser configs we'll be testing against.\n            def platform_configs = [\n                'chrome',\n                'firefox',\n                'ie',\n                'edge'\n            ].join(',')\n\n            // Nightwatch.js supports color ouput, so wrap this step for ansi color\n            wrap([$class: 'AnsiColorBuildWrapper', 'colorMapName': 'XTerm']) {\n\n                // Run selenium tests using Nightwatch.js\n                // Ignore error codes. The junit publisher will cover setting build status.\n                sh \"./node_modules/.bin/nightwatch -e ${platform_configs} || true\"\n            }\n\n            junit 'reports/**'\n\n            step([$class: 'SauceOnDemandTestPublisher'])\n        }\n    }\n}\n\nThis pipeline expects to be run from a Jenkinsfile in SCM.\nTo copy and paste it directly into a Jenkins Pipeline job, replace the checkout scm step with\ngit url:'https://github.com/bitwiseman/JS-Nightwatch.js', branch: 'sauce-pipeline'.\n\nNot only is the pipeline as code more compact,\nit also allows for comments to further clarify what is being done.\nAnd as I noted earlier,\nchanges to this pipeline code are committed the same as changes to the rest of the project,\nkeeping everything synchronized, reviewable, and testable at any commit.\nIn fact, you can view the full set of commits for this blog post in the\nblog/sauce-pipeline\nbranch of the\nbitwiseman/JS-Nightwatch.js\nrepository.\n\nLinks\n\nSauce OnDemand Plugin\n\nbitwiseman/JS-Nightwatch.js\n\nsaucelabs-sample-test-frameworks","title":"Browser-testing with Sauce OnDemand and Pipeline","tags":["tutorial","pipeline","plugins","saucelabs","selenium","nightwatch"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2016-08-29T00:00:00.000Z","id":"95ef7134-7327-5121-a920-cbff69688c23","slug":"/blog/2016/08/29/jenkins-world-speaker-blog-goodgame/","strippedHtml":"This is a guest post by Jenkins World speaker David Hinske, Release\nEngineer at Goodgame Studios.\n\nHey there, my name is David Hinske and I work at Goodgame Studios (GGS), a game\ndevelopment company in Hamburg, Germany. As Release Engineer in a company with\nseveral development teams, it comes in handy using several Jenkins instances.\nWhile this approach works fine in our company and gives the developers a lot of\nfreedom, we came across some long-term problems concerning maintenance and\nstandards. These problems were mostly caused by misconfiguration or non-use of\nplugins. With “configuration as code” in mind, I took the approach to apply\nstatic code analysis with the help of SonarQube, a platform to manage code\nquality, for all of our Jenkins job configurations.\n\nAs a small centralized team, we were looking for an easy way to control the\nhealth of our growing Jenkins infrastructure. With considering “configuration\nas code“, I developed a simple extension of SonarQube, to manage the quality\nand usage of all spawned Jenkins instances. The given SonarQube features (like\ncustomized rules/metrics, quality profiles and dashboards) allow us and the\ndevelopment teams to analyze and measure the quality of all created jobs in our\ncompany. Even though Jenkins configuration analysis cannot cover all\nSonarQube’s axes of code quality, I think there is still potential for\nconventions/standards, duplications, complexity, potential bugs\n(misconfiguration) and design and architecture.\n\nThe results of this analysis can be used by all people working with Jenkins. To\nachieve this, I developed a simple extension of SonarQube, containing\neverything which is needed to hook up our SonarQube with our Jenkins\nenvironment. The implementation contains a new basic-language “Jenkins“ and an\ninitial set of rules.\n\nOf course the needs depend strongly on the way Jenkins is being used, so not\nevery rule implemented might be useful for every team, but this applies to all\ntypes of code analysis. The main inspirations for the rules were developer\nfeedback and some articles found in the web. The different ways Jenkins can be\nconfigured provides the potential for many more rules. With this new approach\nof quality analysis, we can enforce best practices like:\n\nPolling must die (Better to triggerb uilds from pushes than poll the\nrepository every x minutes).\n\nUse Log Rotator (Not using log-rotator can result in disk space problems on\nthe controller).\n\nUse agents/labels (Jobs should be defined where to run).\n\nDon’t build on the controller (In larger systems, don’t build on the controller).\n\nEnforce plugin usage (For example: Timestamp, Mask-Passwords).\n\nNaming sanity (Limit project names to a sane (e.g. alphanumeric) character\nset).\n\nAnalyze Groovy Scripts (For example: Prevent System.exit(0) in System Groovy\nScripts).\n\nBesides taking control of all configuration of any Jenkins instance we want,\nthere is also room for additional metrics, like measuring the amount and\ndifferent types of jobs (Freestyle/Maven etc…​) to get an overview about the\ngeneral load of the Jenkins instance. A more sophisticated idea is to measure\ncomplexity of jobs and even pipelines. As code, jobs configuration gets harder\nto understand the more steps are involved. On the one hand scripts, conditions\nand many parameters can negatively influence the readability, especially if you\nhave external dependencies (like scripts) in different locations. On the other\nhand, pipelines can also grow very complex when many jobs are involved and\nchained for execution. It will be very interesting for us to see where and why\ntoo complex pipelines are being created.\n\nOn visualization we rely on the data and its interpretation of SonarQube, which\noffers a big bandwidth of widgets. Everybody can use and customize the\ndashboards. Our centralized team for example has a separate dashboard where we\ncan get a quick overview over all instances.\n\nThe problem of \"growing\" Jenkins with maintenance problems is not new.\nEspecially when you have many developers involved, including with the access to\ncreate jobs and pipelines themselves, an analysis like this SonarQube plugin\nprovides can be useful for anyone who wants to keep their Jenkins in shape.\nCustomization and standards are playing a big role in this scenario. This blog\npost surely is not an advertisement for my developed plugin, it is more about\nthe crazy idea of using static code analysis for Jenkins job configuration. I\nhaven’t seen anything like it so far and I feel that there might be some\npotential behind this idea.\n\nJoin me at my Enforcing Jenkins Best Practices session at the 2016 Jenkins\nWorld to hear more!\n\nDavid will be\npresenting\nmore of this concept at\nJenkins World in September.\nRegister with the code JWFOSS for 20% off your full conference pass.","title":"Enforcing Jenkins Best Practices","tags":["event","jenkinsworld","jenkinsworld2016"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"","id":"hinman","irc":null,"linkedin":null,"name":"Hannah Inman","slug":"/blog/authors/hinman","twitter":null}]}},{"node":{"date":"2016-08-26T00:00:00.000Z","id":"ddf1a677-43d3-5209-a6e7-d331d574df38","slug":"/blog/2016/08/26/ask-the-experts-jenkins-world/","strippedHtml":"Our events officer Alyssa has been working for\nthe past several weeks to organize the \"Open Source Hub\" at\nJenkins World 2016. The Hub\nis a location on the expo floor where contributors to the Jenkins project can hang\nout, share demos and help Jenkins users via the \"Ask the Experts\" program. Thus\nfar we have a great list of experts who have volunteered to help staff the\nbooth, which includes many frequent contributors, JAM\norganizers and board members.\n\nA few of the friendly folks you will see at Jenkins World are:\n\nPaul Allen -\nP4 Plugin\nmaintainer and Pipeline contributor.\n\nR Tyler Croy -\nJenkins infrastructure maintainer and\nboard member.\n\nJesse Glick - Pipeline\nmaintainer and long-time contributor to Jenkins\ncore.\n\nEddú Meléndez Gonzales - Organizer for\nthe Lima (Perú)\nJenkins Area Meetup and contributor to Spring.\n\nJon Hermansen - Organizer for the\nLos Angeles\nJenkins Area Meetup, developer and Pipeline user.\n\nOwen Mehegan -\nGitLab plugin\ncontributor, release engineer and copy editor for jenkins.io.\n\nOleg Nenashev -\nGoogle Summer of Code organizer, maintainer of multiple\nplugins and St.\nPetersburg Jenkins Area Meetup organizer.\n\nChristopher Orr - Maintainer of multiple\nAndroid-related plugins, including the\nAndroid\nEmulator plugin and contributor to numerous projects behind the scenes of\nJenkins.\n\nCasey Vega - Organizer for the\nLos Angeles\nJenkins Area Meetup and release engineer at Verizon Digital Media.\n\nMark Waite - Maintainer of the\nGit plugin and\ncontributor to a number of other Git-related plugins.\n\nDean Yu - Long-time contributor, board member\nand release engineer at Shutterfly.\n\nI hope that this list isn’t exhaustive! If you are an active member of the\nJenkins community and/or a contributor, consider taking part in the \"Ask the\nExperts\" program. It’s a great opportunity to bond with other contributors and\ntalk with fellow users at Jenkins World.\n\nYou will be able to find us in the expo hall under the \"Open Source Hub\" sign;\nplease stop by at Jenkins World to say hello, pick up stickers and to ask\nquestions!\n\nRegister for Jenkins World in\nSeptember with the code JWFOSS for a 20% discount off your pass.","title":"Ask the Experts at Jenkins World 2016","tags":["event","jenkinsworld","jenkinsworld2016"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-08-24T00:00:00.000Z","id":"9b375bbf-3fa2-5403-8c8a-8d26e0d78938","slug":"/blog/2016/08/24/jenkins-world-2016-festivities/","strippedHtml":"At Jenkins World 2016 on\nSeptember 14-15, stop by the \"Open Source Hub\", located in the Partner Expo\nhall at the Santa Clara Convention Center in Santa Clara, CA. The Open Source\nHub will have many Jenkins contributors, committers, JAM leaders, and\nofficers from\nthe governance board under one roof, so there will be plenty of knowledge and\ntalents on hand to share. We hope you’ll join in on the festivities.\n\nAsk the Experts\n\nThe setup that is waiting for you: white boards, monitors and lots of brain\npower to help answer those Jenkins questions that have been keeping you up at\nnight.  Jenkins experts can help with beginner questions to the more advanced\nones. All you need to do is bring your laptop and your questions; the experts\nwill help answer them!\n\nRegister for Jenkins World in\nSeptember with the code JWFOSS for a 20% discount off your pass.\n\nLive Demos\n\nSometimes seeing is believing, there will be plenty of demos in the \"Open\nSource Hub\" during the lunch hours on Wednesday September 14th, and Thursday\nSeptember 15th in the expo hall. Jenkins experts will be show-casing their\nfavorite Jenkins features, plugins and projects. Grab your lunch, take a seat\nin the open source theater to learn about:\n\nPipelines for Building and Deploying Android Apps by Android Emulator\nplugin maintainer Chris Orr\n\nGit Plugin - Large Repos, Submodule Authentication, and more by Git plugin\nmaintainer Mark Waite\n\nDocker and Pipeline by Jenkins infrastructure contributor\nR Tyler Croy\n\nExtending Pipeline with Libraries by Pipeline plugin maintainer\nJesse Glick\n\nBlue Ocean in Action by Blue Ocean contributor\nKeith Zantow\n\nExternal Workspace Manager plugin for Pipeline by\nGoogle Summer of Code student\nAlexandru Somai\n\nAnd many more\n\nJenkins Mural\n\nJenkins World participants will take part in the realization of a giant\ncollaborative mural painting with the\nCommitStrip team.  Thomas, the writer and\nEtienne, the cartoonist, teamed up with a few Jenkins contributors to design a\n5m x 2m mmural which will be drawn live! Brushes and colors will be\navailable for all attendees who wish to help paint this one of a kind piece of\nJenkins art.\n\nSticker Swap\n\nJenkins World attendees will have a chance to swap stickers. There will be a\ntable where attendees are welcome to place/take stickers. Bring your cool\nstickers to share with others and take stickers that interest you.\n\nAfter Dark Reception Sponsored by CloudBees\n\nAfter Dark reception will be from 6-8pm on Wed Sept 14 in the Partner Expo.\nEnjoy cocktails, appetizers, mingle, and dance to a live band. A big THANK\nYOU\ngoes out to CloudBees for their generous contributions! See you at After Dark!\n\nContributor Summit - Tuesday, September 13\n\nIf Blue Ocean, Pipeline and Storage Pluggability sounds interesting to you,\njoin the interactive discussions surrounding these topics. The Jenkins project\nis also looking to hear use-cases, war stories, and pain points. The objective\nof the summit is to work towards improving the Jeknins project.\nSeats are limited.\n\nDon’t forget to register; I look forward to\nseeing you at the conference!\n\nLinks\n\nJenkins World 2016\n\nAcknowledgements\n\nSpecial thanks to CloudBees as the premier\nsponsor and BlazeMeter, Microsoft, Red\nHat and all the other sponsors who have made this event possible.","title":"Jenkins World 2016 Festivities","tags":["event","jenkinsworld","jenkinsworld2016"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#281818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg","srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/8d248/alyssat.jpg 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/c004c/alyssat.jpg 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/9e67b/alyssat.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/22924/alyssat.webp 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/89767/alyssat.webp 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/40d97/alyssat.webp 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/5028e/alyssat.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":166}}},"blog":null,"github":"alyssat","html":"<div class=\"paragraph\">\n<p>Member of the <a href=\"/sigs/advocacy-and-outreach/\">Jenkins Advocacy and Outreach SIG</a>.\nAlyssa drives and manages Jenkins participation in community events and conferences like <a href=\"https://fosdem.org/\">FOSDEM</a>, <a href=\"https://www.socallinuxexpo.org/\">SCaLE</a>, <a href=\"https://events.linuxfoundation.org/cdcon/\">cdCON</a>, and <a href=\"https://events19.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2019/\">KubeCon</a>.\nShe is also responsible for Marketing &amp; Community Programs at <a href=\"https://cloudbees.com\">CloudBees, Inc.</a></p>\n</div>","id":"alyssat","irc":null,"linkedin":null,"name":"Alyssa Tong","slug":"/blog/authors/alyssat","twitter":null}]}},{"node":{"date":"2016-08-22T00:00:00.000Z","id":"5a42a05c-41f1-5105-866c-97942f6d788d","slug":"/blog/2016/08/22/ewm-stable-release/","strippedHtml":"This blog post is the last one from the series of\nGoogle Summer of Code 2016, External Workspace Manager Plugin project.\nThe previous posts are:\n\nIntroductory blog post\n\nAlpha release announcement\n\nBeta release announcement\n\nIn this post I would like to announce the 1.0.0 release of the External Workspace Manager Plugin version to the main\nupdate center.\n\nHere’s a highlight of the available features:\n\nWorkspace share and reuse across multiple jobs, running on different nodes\n\nAutomatic workspace cleanup\n\nProvide custom workspace path on the disk\n\nDisk Pool restrictions\n\nFlexible Disk allocation strategies\n\nAll the above are detailed, with usage examples, on the plugin’s\ndocumentation page.\n\nFuture work\n\nCurrently, there is work in progress for the workspace browsing feature (see pull request\n#37).\nAfterwards, I’m planning to integrate fingerprints, so that the user can view a specific workspace in which\nother jobs was used.\nA particular feature that would be nice to have is to integrate the plugin with at least one disk provider\n(e.g. Amazon EBS, Google Cloud Storage).\n\nMany other features and improvements are still to come, they are grouped in the phase 3 EPIC:\nJENKINS-37543.\nThe plugin’s repository is on GitHub.\nIf you’d like to come up with new features or ideas, contributions are very welcome.\n\nClosing\n\nThis was a Google Summer of Code 2016 project.\nA summary of the contributions that I’ve made to the Jenkins project during this time may be found\nhere.\nIt was a great experience, from which I learned a lot, and I’d wish I could repeat it every year.\n\nI’d like to thank to my mentors, Oleg Nenashev and\nMartin d’Anjou for all their support, good advices and help they gave me.\nAlso, thanks to the Jenkins contributors with which I have interacted and helped me during this period.\n\nIf you have any issues in setting up or using the plugin, please feel free to ask me on the plugin’s Gitter\nchat.\nAny feedback is welcome, and you may provide it either on the Gitter chat, or on\nJira by using the external-workspace-manager-plugin component.\n\nLinks\n\nProject repository\n\nWork product page\n\nGSoC page\n\nJenkins GSoC Page","title":"GSoC: External Workspace Manager for Pipeline is released","tags":["pipeline","plugins","external-workspace-manager","gsoc"],"authors":[{"avatar":null,"blog":null,"github":"alexsomai","html":"","id":"alexsomai","irc":null,"linkedin":null,"name":"Alexandru Somai","slug":"/blog/authors/alexsomai","twitter":"alex_somai"}]}},{"node":{"date":"2016-08-17T00:00:00.000Z","id":"a1ee11d9-c611-5f2a-a517-91048345fcf6","slug":"/blog/2016/08/17/jenkins-world-speaker-blog-aquilent/","strippedHtml":"This is a guest post by Jenkins World speaker Neil Hunt, Senior DevOps Architect at Aquilent.\n\nIn smaller companies with a handful of apps and fewer silos, implementing CD\npipelines to support these apps is fairly straightforward using one of the many\ndelivery orchestration tools available today. There is likely a constrained\ntool set to support - not an abundance of flavors of applications and security\npractices - and generally fewer cooks in the kitchen. But in a larger\norganization, I have found that in the past, there were seemingly endless\nunique requirements and mountains to climb to reach this level of automation on\neach new project.\n\nNeil will be presenting more\nof this concept at Jenkins World in\nSeptember, register with the code JWFOSS for a 20% discount off your pass.\n\nEnter the Jenkins Pipeline plugin. My recently departed former company, a large\nfinancial services organization with a 600+ person IT organization and 150+\napplication portfolio, set out to implement continuous delivery\nenterprise-wide. After considering several pipeline orchestration tools, we\ndetermined the Pipeline plugin (at the time called Workflow) to be the superior\nsolution for our company. Pipeline has continued Jenkins' legacy of presenting\nan extensible platform with just the right set of features to allow\norganizations to scale its capabilities as they see fit, and do so rapidly. As\nearly adopters of Pipeline with a protracted set of requirements, we used it\nboth to accelerate the pace of onboarding new projects and to reduce the\nongoing feature delivery time of our applications.\n\nIn my presentation at Jenkins World, I will demonstrate the methods we used to\nenable this. A few examples:\n\nWe leveraged the Pipeline Remote File Loader plugin to write shared common\ncode and sought and received community enhancements to these functions.\n\nJenkinsfile, loading a shared AWS utilities function library\n\nawsUtils.groovy, snippets of some AWS functions\n\nWe migrated from EC2 agents to Docker-based agents running on Amazon’s\nElastic Container Service, allowing us to spin up new executors in seconds\nand for teams to own their own executor definitions.\n\nPipeline run #1 using standard EC2 executors, spinning up EC2 instance for each\nnode; Pipeline run #2 using shared ECS cluster with near-instant instantiation\nof a Docker agent in the cluster for each node.\n\nWe also created a Pipeline Library of common pipelines, enabling projects\nthat fit certain models to use ready-made end-to-end pipelines. Some\nexamples:\n\nMaven JAR Pipeline: Pipeline that clones git repository, builds JAR file\nfrom pom.xml, deploys to Artifactory, and runs maven release plugin to\nincrement next version\n\nAnuglar.JS Pipeline: Pipeline that executes a grunt and bower build, then\nruns S3 sync to Amazon S3 bucket in Dev, then Stage, then Prod buckets.\n\nPentaho Reports Pipeline: Pipeline that clones git repository, constructs\nzip file, and executes Pentaho Business Intelligence Platform CLI to import new\nset of reports in Dev, Stage, then Prod servers.\n\nPerhaps most critically, a shout-out to the saving grace of this quest for our\nsecurity and ops teams: the manual 'input' step! While the ambition of\ncontinuous delivery is to have as few of these as possible, this was the\nsingle-most pivotal feature in convincing others of Pipeline’s viability, since\nnow any step of the delivery process could be gate-checked by an LDAP-enabled\npermission group. Were it not for the availability of this step, we may still\nbe living in the world of: \"This seems like a great tool for development, but\nwe will have a segregated process for production deployments.\" Instead, we had\na pipeline full of many 'input' steps at first, and then used the data we\ncollected around the longest delays to bring management focus to them and unite\neveryone around the goal of strategically removing them, one by one.\n\nGoing forward, having recently joined Aquilent’s Cloud Solutions Architecture\nteam, I’ll be working with our project teams here to further mature the use of\nthese Pipeline plugin features as we move towards continuous delivery. Already,\nwe have migrated several components of our healthcare.gov project to Pipeline.\nThe team has been able to consolidate several Jenkins jobs into a single,\nvisible delivery pipeline, to maintain the lifecycle of the pipeline with our\napplication code base in our SCM, and to more easily integrate with our\nexternal tools.\n\nDue to functional shortcomings in the early adoption stages of the Pipeline\nplugin and the ever-present political challenges of shifting organizational\npolicy, this has been and continues to be far from a bruise-free journey. But\nwe plodded through many of these issues to bring this to fruition and\nultimately reduced the number of manual steps in some pipelines from 12 down to\n1 and brought our 20+ Jenkins-minute pipelines to only six minutes after months\nof iteration. I hope you’ll join this session at Jenkins World and learn about\nour challenges and successes in achieving the promise of continuous delivery at\nenterprise scale.","title":"Continuously Delivering Continuous Delivery Pipelines","tags":["event","jenkinsworld","jenkinsworld2016"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"","id":"hinman","irc":null,"linkedin":null,"name":"Hannah Inman","slug":"/blog/authors/hinman","twitter":null}]}},{"node":{"date":"2016-08-11T00:00:00.000Z","id":"9a0e4574-a766-5c76-b7d9-8fc1d1fbcde3","slug":"/blog/2016/08/11/speaker-blog-edx-jenkins-world/","strippedHtml":"This is a guest post by Ben Patterson, Engineering Manager at\nedX.\n\nPicking a pear from a basket is straightforward when you can hold it in your hand, feel its weight, perhaps give a gentle squeeze, observe its color and look more closely at any bruises. If the only information we had was a photograph from one angle, we’d have to do some educated guessing.\n\nAs developers, we don’t get a photograph; we get a green checkmark or a red x. We use that to decide whether or not we need to switch gears and go back to a pull request we submitted recently. At edX, we take advantage of some Jenkins features that could give us more granularity on GitHub pull requests, and make that decision less of a guessing game.\n\nMultiple contexts reporting back when they’re available\n\nPull requests on our platform are evaluated from several angles: static code analysis including linting and security audits, javascript unit tests, python unit tests, acceptance tests and accessibility tests. Using an elixir of plugins, including the GitHub Pull Request Builder Plugin, we put more direct feedback into the hands of the contributor so s/he can quickly decide how much digging is going to be needed.\n\nFor example, if I made adjustments to my branch and know more requirements are coming, then I may not be as worried about passing the linter; however, if my unit tests have failed, I likely have a problem I need to address regardless of when the new requirements arrive. Timing is important as well. Splitting out the contexts means we can run tests in parallel and report results faster.\n\nDevelopers can re-run specific contexts\n\nOccasionally the feedback mechanism fails. It is oftentimes a flaky condition in a test or in test setup. (Solving flakiness is a different discussion I’m sidestepping. Accept the fact that the system fails for purposes of this blog entry.) Engineers are armed with the power of re-running specific contexts, also available through the PR plugin. A developer can say “jenkins run bokchoy” to re-run the acceptance tests, for example. A developer can also re-run everything with “jenkins run all”. These phrases are set through the GitHub Pull Request Builder configuration.\n\nMore granular data is easier to find for our Tools team\n\nSplitting the contexts has also given us important data points for our Tools team to help in highlighting things like flaky tests, time to feedback and other metrics that help the org prioritize what’s important. We use this with a log aggregator (in our case, Splunk) to produce valuable reports such as this one.\n\nI could go on! The short answer here is we have an intuitive way of divvying up our tests, not only for optimizing the overall amount of time it takes to get build results, but also to make the experience more user-friendly to developers.\n\nBen will be presenting more on this topic at\nJenkins World in September,\nregister with the code JWFOSS for a 20% discount off your pass.","title":"Using Jenkins for Disparate Feedback on GitHub","tags":["event","jenkinsworld","jenkinsworld2016"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"","id":"hinman","irc":null,"linkedin":null,"name":"Hannah Inman","slug":"/blog/authors/hinman","twitter":null}]}},{"node":{"date":"2016-08-10T00:00:00.000Z","id":"4f2eb2b7-1bb1-501e-8b2c-1b8c4ac3e6e3","slug":"/blog/2016/08/10/rails-cd-with-pipeline/","strippedHtml":"This is a guest post by R. Tyler Croy, who is a\nlong-time contributor to Jenkins and the primary contact for Jenkins project\ninfrastructure. He is also a Jenkins Evangelist at\nCloudBees, Inc.\n\nWhen the Ruby on Rails framework debuted it\nchanged the industry in two noteworthy ways: it created a trend of opinionated web\napplication frameworks ( Django,\nPlay, Grails) and it\nalso strongly encouraged thousands of developers to embrace test-driven\ndevelopment along with many other modern best practices (source control, dependency\nmanagement, etc). Because Ruby, the language underneath Rails, is interpreted\ninstead of compiled there isn’t a \"build\" per se but rather tens, if not\nhundreds, of tests, linters and scans which are run to ensure the application’s\nquality. With the rise in popularity of Rails, the popularity of application\nhosting services with easy-to-use deployment tools like Heroku or\nEngine Yard rose too.\n\nThis combination of good test coverage and easily automated deployments\nmakes Rails easy to continuously deliver with Jenkins. In this post we’ll cover\ntesting non-trivial Rails applications with Jenkins\nPipeline and, as an added bonus, we will add security scanning via\nBrakeman and the\nBrakeman\nplugin.\n\nTopics\n\nPreparing the app\n\nPreparing Jenkins\n\nWriting the Pipeline\n\nSecurity scanning\n\nDeploying the good stuff\n\nWrap up\n\nFor this demonstration, I used Ruby Central 's\ncfp-app :\n\nA Ruby on Rails application that lets you manage your conference’s call for\nproposal (CFP), program and schedule. It was written by Ruby Central to run the\nCFPs for RailsConf and RubyConf.\n\nI chose this Rails app, not only because it’s a sizable application with lots\nof tests, but it’s actually the application we used to collect talk proposals\nfor the \" Community Tracks\" at this\nyear’s Jenkins World. For the most part,\ncfp-app is a standard Rails application. It uses\nPostgreSQL for its database,\nRSpec for its tests and\nRuby 2.3.x as its runtime.\n\nIf you prefer to just to look at the code, skip straight to the\nJenkinsfile.\n\nPreparing the app\n\nFor most Rails applications there are few, if any, changes needed to enable\ncontinuous delivery with Jenkins. In the case of\ncfp-app, I added two gems to get\nthe most optimal integration into Jenkins:\n\nci_reporter, for test report\nintegration\n\nbrakeman, for security scanning.\n\nAdding these was simple, I just needed to update the Gemfile and the\nRakefile in the root of the repository to contain:\n\nGemfile\n\n# .. snip ..\ngroup :test do\n  # RSpec, etc\n  gem 'ci_reporter'\n  gem 'ci_reporter_rspec'\n  gem \"brakeman\", :require => false\nend\n\nRakefile\n\n# .. snip ..\nrequire 'ci/reporter/rake/rspec'\n# Make sure we setup ci_reporter before executing our RSpec examples\ntask :spec => 'ci:setup:rspec'\n\nPreparing Jenkins\n\nWith the cfp-app project set up, next on the list is to ensure that Jenkins itself\nis ready. Generally I suggest running the latest LTS of\nJenkins; for this demonstration I used Jenkins 2.7.1 with the following\nplugins:\n\nPipeline plugin\n\nBrakeman plugin\n\nCloudBees\nDocker Pipeline plugin\n\nI also used the\nGitHub\nOrganization Folder plugin to automatically create pipeline items in my\nJenkins instance; that isn’t required for the demo, but it’s pretty cool to see\nrepositories and branches with a Jenkinsfile automatically show up in\nJenkins, so I recommend it!\n\nIn addition to the plugins listed above, I also needed at least one\nJenkins agent with the Docker daemon installed and\nrunning on it. I label these agents with \"docker\" to make it easier to assign\nDocker-based workloads to them in the future.\n\nAny Linux-based machine with Docker installed will work, in my case I was\nprovisioning on-demand agents with the\nAzure\nplugin which, like the\nEC2 plugin,\nhelps keep my test costs down.\n\nIf you’re using Amazon Web Services, you might also be interested in\nthis blog post from\nearlier this year unveiling the\nEC2\nFleet plugin for working with EC2 Spot Fleets.\n\nWriting the Pipeline\n\nTo make sense of the various things that the Jenkinsfile needs to do, I find\nit easier to start by simply defining the stages of my pipeline. This helps me\nthink of, in broad terms, what order of operations my pipeline should have.\nFor example:\n\n/* Assign our work to an agent labelled 'docker' */\nnode('docker') {\n    stage 'Prepare Container'\n    stage 'Install Gems'\n    stage 'Prepare Database'\n    stage 'Invoke Rake'\n    stage 'Security scan'\n    stage 'Deploy'\n}\n\nAs mentioned previously, this Jenkinsfile is going to rely heavily on the\nCloudBees\nDocker Pipeline plugin. The plugin provides two very important features:\n\nAbility to execute steps inside of a running Docker container\n\nAbility to run a container in the \"background.\"\n\nLike most Rails applications, one can effectively test the application with two\ncommands: bundle install followed by bundle exec rake. I already had some\nDocker images prepared with RVM and Ruby 2.3.0 installed,\nwhich ensures a common and consistent starting point:\n\nnode('docker') {\n    // .. 'stage' steps removed\n    docker.image('rtyler/rvm:2.3.0').inside { (1)\nrvm 'bundle install' (2)\nrvm 'bundle exec rake'\n    } (3)\n}\n\n1\nRun the named container. The inside method can take optional additional flags for the docker run command.\n\n2\nExecute our shell commands using our tiny sh step wrapper\nrvm . This ensures that the shell code is executed in the correct RVM environment.\n\n3\nWhen the closure completes, the container will be destroyed.\n\nUnfortunately, with this application, the bundle exec rake command will fail\nif PostgreSQL isn’t available when the process starts. This is where the\nsecond important feature of the CloudBees Docker Pipeline plugin comes\ninto effect: the ability to run a container in the \"background.\"\n\nnode('docker') {\n    // .. 'stage' steps removed\n    /* Pull the latest `postgres` container and run it in the background */\n    docker.image('postgres').withRun { container -> (1)\necho \"PostgreSQL running in container ${container.id}\" (2)\n} (3)\n}\n\n1\nRun the container, effectively docker run postgres\n\n2\nAny number of steps can go inside the closure\n\n3\nWhen the closure completes, the container will be destroyed.\n\nRunning the tests\n\nCombining these two snippets of Jenkins Pipeline is, in my opinion, where the\npower of the DSL\nshines:\n\nnode('docker') {\n    docker.image('postgres').withRun { container ->\n        docker.image('rtyler/rvm:2.3.0').inside(\"--link=${container.id}:postgres\") { (1)\nstage 'Install Gems'\n            rvm \"bundle install\"\n\n            stage 'Invoke Rake'\n            withEnv(['DATABASE_URL=postgres://postgres@postgres:5432/']) { (2)\nrvm \"bundle exec rake\"\n            }\n            junit 'spec/reports/*.xml' (3)\n}\n    }\n}\n\n1\nBy passing the --link argument, the Docker daemon will allow the RVM container to talk to the PostgreSQL container under the host name 'postgres'.\n\n2\nUse the withEnv step to set environment variables for everything that is in the closure. In this case, the cfp-app DB scaffolding will look for the DATABASE_URL variable to override the DB host/user/dbname defaults.\n\n3\nArchive the test reports generated by ci_reporter so that Jenkins can display test reports and trend analysis.\n\nWith this done, the basics are in place to consistently run the tests for\ncfp-app in fresh Docker containers for each execution of the pipeline.\n\nSecurity scanning\n\nUsing Brakeman, the security scanner for Ruby\non Rails, is almost trivially easy inside of Jenkins Pipeline, thanks to the\nBrakeman\nplugin which implements the publishBrakeman step.\n\nBuilding off our example above, we can implement the \"Security scan\" stage:\n\nnode('docker') {\n    /* --8 (1)\npublishBrakeman 'brakeman-output.tabs' (2)\n/* --8\n\n1\nRun the Brakeman security scanner for Rails and store the output for later in brakeman-output.tabs\n\n2\nArchive the reports generated by Brakeman so that Jenkins can display detailed reports with trend analysis.\n\nAs of this writing, there is work in progress\n( JENKINS-31202) to\nrender trend graphs from plugins like Brakeman on a pipeline project’s main\npage.\n\nDeploying the good stuff\n\nOnce the tests and security scanning are all working properly, we can start to\nset up the deployment stage. Jenkins Pipeline provides the variable\ncurrentBuild which we can use to determine whether our pipeline has been\nsuccessful thus far or not. This allows us to add the logic to only deploy when\neverything is passing, as we would expect:\n\nnode('docker') {\n    /* --8 (1)\nsh './deploy.sh' (2)\n}\n    else {\n        mail subject: \"Something is wrong with ${env.JOB_NAME} ${env.BUILD_ID}\",\n                  to: 'nobody@example.com',\n                body: 'You should fix it'\n    }\n    /* --8\n\n1\ncurrentBuild has the result property which would be 'SUCCESS', 'FAILED', 'UNSTABLE', 'ABORTED'\n\n2\nOnly if currentBuild.result is successful should we bother invoking our deployment script (e.g. git push heroku master)\n\nWrap up\n\nI have gratuitously commented the full\nJenkinsfile\nwhich I hope is a useful summation of the work outlined above. Having worked\non a number of Rails applications in the past, the consistency provided by\nDocker and Jenkins Pipeline above would have definitely improved those\nprojects' delivery times. There is still room for improvement however, which\nis left as an exercise for the reader. Such as: preparing new containers with\nall their\ndependencies\nbuilt-in instead of installing them at run-time. Or utilizing the parallel\nstep for executing RSpec across multiple Jenkins agents simultaneously.\n\nThe beautiful thing about defining your continuous delivery, and continuous\nsecurity, pipeline in code is that you can continue to iterate on it!","title":"Continuous Security for Rails apps with Pipeline and Brakeman","tags":["tutorial","ruby","pipeline","rails","brakeman","continuousdelivery"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}}]}},"pageContext":{"limit":8,"skip":376,"numPages":100,"currentPage":48}},
    "staticQueryHashes": ["3649515864"]}