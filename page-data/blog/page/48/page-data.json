{
    "componentChunkName": "component---src-templates-blog-list-template-js",
    "path": "/blog/page/48",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2016-08-22T00:00:00.000Z","id":"5a42a05c-41f1-5105-866c-97942f6d788d","slug":"/blog/2016/08/22/ewm-stable-release/","strippedHtml":"This blog post is the last one from the series of\nGoogle Summer of Code 2016, External Workspace Manager Plugin project.\nThe previous posts are:\n\nIntroductory blog post\n\nAlpha release announcement\n\nBeta release announcement\n\nIn this post I would like to announce the 1.0.0 release of the External Workspace Manager Plugin version to the main\nupdate center.\n\nHere’s a highlight of the available features:\n\nWorkspace share and reuse across multiple jobs, running on different nodes\n\nAutomatic workspace cleanup\n\nProvide custom workspace path on the disk\n\nDisk Pool restrictions\n\nFlexible Disk allocation strategies\n\nAll the above are detailed, with usage examples, on the plugin’s\ndocumentation page.\n\nFuture work\n\nCurrently, there is work in progress for the workspace browsing feature (see pull request\n#37).\nAfterwards, I’m planning to integrate fingerprints, so that the user can view a specific workspace in which\nother jobs was used.\nA particular feature that would be nice to have is to integrate the plugin with at least one disk provider\n(e.g. Amazon EBS, Google Cloud Storage).\n\nMany other features and improvements are still to come, they are grouped in the phase 3 EPIC:\nJENKINS-37543.\nThe plugin’s repository is on GitHub.\nIf you’d like to come up with new features or ideas, contributions are very welcome.\n\nClosing\n\nThis was a Google Summer of Code 2016 project.\nA summary of the contributions that I’ve made to the Jenkins project during this time may be found\nhere.\nIt was a great experience, from which I learned a lot, and I’d wish I could repeat it every year.\n\nI’d like to thank to my mentors, Oleg Nenashev and\nMartin d’Anjou for all their support, good advices and help they gave me.\nAlso, thanks to the Jenkins contributors with which I have interacted and helped me during this period.\n\nIf you have any issues in setting up or using the plugin, please feel free to ask me on the plugin’s Gitter\nchat.\nAny feedback is welcome, and you may provide it either on the Gitter chat, or on\nJira by using the external-workspace-manager-plugin component.\n\nLinks\n\nProject repository\n\nWork product page\n\nGSoC page\n\nJenkins GSoC Page","title":"GSoC: External Workspace Manager for Pipeline is released","tags":["pipeline","plugins","external-workspace-manager","gsoc"],"authors":[]}},{"node":{"date":"2016-08-17T00:00:00.000Z","id":"a1ee11d9-c611-5f2a-a517-91048345fcf6","slug":"/blog/2016/08/17/jenkins-world-speaker-blog-aquilent/","strippedHtml":"This is a guest post by Jenkins World speaker Neil Hunt, Senior DevOps Architect at Aquilent.\n\nIn smaller companies with a handful of apps and fewer silos, implementing CD\npipelines to support these apps is fairly straightforward using one of the many\ndelivery orchestration tools available today. There is likely a constrained\ntool set to support - not an abundance of flavors of applications and security\npractices - and generally fewer cooks in the kitchen. But in a larger\norganization, I have found that in the past, there were seemingly endless\nunique requirements and mountains to climb to reach this level of automation on\neach new project.\n\nNeil will be presenting more\nof this concept at Jenkins World in\nSeptember, register with the code JWFOSS for a 20% discount off your pass.\n\nEnter the Jenkins Pipeline plugin. My recently departed former company, a large\nfinancial services organization with a 600+ person IT organization and 150+\napplication portfolio, set out to implement continuous delivery\nenterprise-wide. After considering several pipeline orchestration tools, we\ndetermined the Pipeline plugin (at the time called Workflow) to be the superior\nsolution for our company. Pipeline has continued Jenkins' legacy of presenting\nan extensible platform with just the right set of features to allow\norganizations to scale its capabilities as they see fit, and do so rapidly. As\nearly adopters of Pipeline with a protracted set of requirements, we used it\nboth to accelerate the pace of onboarding new projects and to reduce the\nongoing feature delivery time of our applications.\n\nIn my presentation at Jenkins World, I will demonstrate the methods we used to\nenable this. A few examples:\n\nWe leveraged the Pipeline Remote File Loader plugin to write shared common\ncode and sought and received community enhancements to these functions.\n\nJenkinsfile, loading a shared AWS utilities function library\n\nawsUtils.groovy, snippets of some AWS functions\n\nWe migrated from EC2 agents to Docker-based agents running on Amazon’s\nElastic Container Service, allowing us to spin up new executors in seconds\nand for teams to own their own executor definitions.\n\nPipeline run #1 using standard EC2 executors, spinning up EC2 instance for each\nnode; Pipeline run #2 using shared ECS cluster with near-instant instantiation\nof a Docker agent in the cluster for each node.\n\nWe also created a Pipeline Library of common pipelines, enabling projects\nthat fit certain models to use ready-made end-to-end pipelines. Some\nexamples:\n\nMaven JAR Pipeline: Pipeline that clones git repository, builds JAR file\nfrom pom.xml, deploys to Artifactory, and runs maven release plugin to\nincrement next version\n\nAnuglar.JS Pipeline: Pipeline that executes a grunt and bower build, then\nruns S3 sync to Amazon S3 bucket in Dev, then Stage, then Prod buckets.\n\nPentaho Reports Pipeline: Pipeline that clones git repository, constructs\nzip file, and executes Pentaho Business Intelligence Platform CLI to import new\nset of reports in Dev, Stage, then Prod servers.\n\nPerhaps most critically, a shout-out to the saving grace of this quest for our\nsecurity and ops teams: the manual 'input' step! While the ambition of\ncontinuous delivery is to have as few of these as possible, this was the\nsingle-most pivotal feature in convincing others of Pipeline’s viability, since\nnow any step of the delivery process could be gate-checked by an LDAP-enabled\npermission group. Were it not for the availability of this step, we may still\nbe living in the world of: \"This seems like a great tool for development, but\nwe will have a segregated process for production deployments.\" Instead, we had\na pipeline full of many 'input' steps at first, and then used the data we\ncollected around the longest delays to bring management focus to them and unite\neveryone around the goal of strategically removing them, one by one.\n\nGoing forward, having recently joined Aquilent’s Cloud Solutions Architecture\nteam, I’ll be working with our project teams here to further mature the use of\nthese Pipeline plugin features as we move towards continuous delivery. Already,\nwe have migrated several components of our healthcare.gov project to Pipeline.\nThe team has been able to consolidate several Jenkins jobs into a single,\nvisible delivery pipeline, to maintain the lifecycle of the pipeline with our\napplication code base in our SCM, and to more easily integrate with our\nexternal tools.\n\nDue to functional shortcomings in the early adoption stages of the Pipeline\nplugin and the ever-present political challenges of shifting organizational\npolicy, this has been and continues to be far from a bruise-free journey. But\nwe plodded through many of these issues to bring this to fruition and\nultimately reduced the number of manual steps in some pipelines from 12 down to\n1 and brought our 20+ Jenkins-minute pipelines to only six minutes after months\nof iteration. I hope you’ll join this session at Jenkins World and learn about\nour challenges and successes in achieving the promise of continuous delivery at\nenterprise scale.","title":"Continuously Delivering Continuous Delivery Pipelines","tags":["event","jenkinsworld","jenkinsworld2016"],"authors":[]}},{"node":{"date":"2016-08-11T00:00:00.000Z","id":"9a0e4574-a766-5c76-b7d9-8fc1d1fbcde3","slug":"/blog/2016/08/11/speaker-blog-edx-jenkins-world/","strippedHtml":"This is a guest post by Ben Patterson, Engineering Manager at\nedX.\n\nPicking a pear from a basket is straightforward when you can hold it in your hand, feel its weight, perhaps give a gentle squeeze, observe its color and look more closely at any bruises. If the only information we had was a photograph from one angle, we’d have to do some educated guessing.\n\nAs developers, we don’t get a photograph; we get a green checkmark or a red x. We use that to decide whether or not we need to switch gears and go back to a pull request we submitted recently. At edX, we take advantage of some Jenkins features that could give us more granularity on GitHub pull requests, and make that decision less of a guessing game.\n\nMultiple contexts reporting back when they’re available\n\nPull requests on our platform are evaluated from several angles: static code analysis including linting and security audits, javascript unit tests, python unit tests, acceptance tests and accessibility tests. Using an elixir of plugins, including the GitHub Pull Request Builder Plugin, we put more direct feedback into the hands of the contributor so s/he can quickly decide how much digging is going to be needed.\n\nFor example, if I made adjustments to my branch and know more requirements are coming, then I may not be as worried about passing the linter; however, if my unit tests have failed, I likely have a problem I need to address regardless of when the new requirements arrive. Timing is important as well. Splitting out the contexts means we can run tests in parallel and report results faster.\n\nDevelopers can re-run specific contexts\n\nOccasionally the feedback mechanism fails. It is oftentimes a flaky condition in a test or in test setup. (Solving flakiness is a different discussion I’m sidestepping. Accept the fact that the system fails for purposes of this blog entry.) Engineers are armed with the power of re-running specific contexts, also available through the PR plugin. A developer can say “jenkins run bokchoy” to re-run the acceptance tests, for example. A developer can also re-run everything with “jenkins run all”. These phrases are set through the GitHub Pull Request Builder configuration.\n\nMore granular data is easier to find for our Tools team\n\nSplitting the contexts has also given us important data points for our Tools team to help in highlighting things like flaky tests, time to feedback and other metrics that help the org prioritize what’s important. We use this with a log aggregator (in our case, Splunk) to produce valuable reports such as this one.\n\nI could go on! The short answer here is we have an intuitive way of divvying up our tests, not only for optimizing the overall amount of time it takes to get build results, but also to make the experience more user-friendly to developers.\n\nBen will be presenting more on this topic at\nJenkins World in September,\nregister with the code JWFOSS for a 20% discount off your pass.","title":"Using Jenkins for Disparate Feedback on GitHub","tags":["event","jenkinsworld","jenkinsworld2016"],"authors":[]}},{"node":{"date":"2016-08-10T00:00:00.000Z","id":"4f2eb2b7-1bb1-501e-8b2c-1b8c4ac3e6e3","slug":"/blog/2016/08/10/rails-cd-with-pipeline/","strippedHtml":"This is a guest post by R. Tyler Croy, who is a\nlong-time contributor to Jenkins and the primary contact for Jenkins project\ninfrastructure. He is also a Jenkins Evangelist at\nCloudBees, Inc.\n\nWhen the Ruby on Rails framework debuted it\nchanged the industry in two noteworthy ways: it created a trend of opinionated web\napplication frameworks ( Django,\nPlay, Grails) and it\nalso strongly encouraged thousands of developers to embrace test-driven\ndevelopment along with many other modern best practices (source control, dependency\nmanagement, etc). Because Ruby, the language underneath Rails, is interpreted\ninstead of compiled there isn’t a \"build\" per se but rather tens, if not\nhundreds, of tests, linters and scans which are run to ensure the application’s\nquality. With the rise in popularity of Rails, the popularity of application\nhosting services with easy-to-use deployment tools like Heroku or\nEngine Yard rose too.\n\nThis combination of good test coverage and easily automated deployments\nmakes Rails easy to continuously deliver with Jenkins. In this post we’ll cover\ntesting non-trivial Rails applications with Jenkins\nPipeline and, as an added bonus, we will add security scanning via\nBrakeman and the\nBrakeman\nplugin.\n\nTopics\n\nPreparing the app\n\nPreparing Jenkins\n\nWriting the Pipeline\n\nSecurity scanning\n\nDeploying the good stuff\n\nWrap up\n\nFor this demonstration, I used Ruby Central 's\ncfp-app :\n\nA Ruby on Rails application that lets you manage your conference’s call for\nproposal (CFP), program and schedule. It was written by Ruby Central to run the\nCFPs for RailsConf and RubyConf.\n\nI chose this Rails app, not only because it’s a sizable application with lots\nof tests, but it’s actually the application we used to collect talk proposals\nfor the \" Community Tracks\" at this\nyear’s Jenkins World. For the most part,\ncfp-app is a standard Rails application. It uses\nPostgreSQL for its database,\nRSpec for its tests and\nRuby 2.3.x as its runtime.\n\nIf you prefer to just to look at the code, skip straight to the\nJenkinsfile.\n\nPreparing the app\n\nFor most Rails applications there are few, if any, changes needed to enable\ncontinuous delivery with Jenkins. In the case of\ncfp-app, I added two gems to get\nthe most optimal integration into Jenkins:\n\nci_reporter, for test report\nintegration\n\nbrakeman, for security scanning.\n\nAdding these was simple, I just needed to update the Gemfile and the\nRakefile in the root of the repository to contain:\n\nGemfile\n\n# .. snip ..\ngroup :test do\n  # RSpec, etc\n  gem 'ci_reporter'\n  gem 'ci_reporter_rspec'\n  gem \"brakeman\", :require => false\nend\n\nRakefile\n\n# .. snip ..\nrequire 'ci/reporter/rake/rspec'\n# Make sure we setup ci_reporter before executing our RSpec examples\ntask :spec => 'ci:setup:rspec'\n\nPreparing Jenkins\n\nWith the cfp-app project set up, next on the list is to ensure that Jenkins itself\nis ready. Generally I suggest running the latest LTS of\nJenkins; for this demonstration I used Jenkins 2.7.1 with the following\nplugins:\n\nPipeline plugin\n\nBrakeman plugin\n\nCloudBees\nDocker Pipeline plugin\n\nI also used the\nGitHub\nOrganization Folder plugin to automatically create pipeline items in my\nJenkins instance; that isn’t required for the demo, but it’s pretty cool to see\nrepositories and branches with a Jenkinsfile automatically show up in\nJenkins, so I recommend it!\n\nIn addition to the plugins listed above, I also needed at least one\nJenkins agent with the Docker daemon installed and\nrunning on it. I label these agents with \"docker\" to make it easier to assign\nDocker-based workloads to them in the future.\n\nAny Linux-based machine with Docker installed will work, in my case I was\nprovisioning on-demand agents with the\nAzure\nplugin which, like the\nEC2 plugin,\nhelps keep my test costs down.\n\nIf you’re using Amazon Web Services, you might also be interested in\nthis blog post from\nearlier this year unveiling the\nEC2\nFleet plugin for working with EC2 Spot Fleets.\n\nWriting the Pipeline\n\nTo make sense of the various things that the Jenkinsfile needs to do, I find\nit easier to start by simply defining the stages of my pipeline. This helps me\nthink of, in broad terms, what order of operations my pipeline should have.\nFor example:\n\n/* Assign our work to an agent labelled 'docker' */\nnode('docker') {\n    stage 'Prepare Container'\n    stage 'Install Gems'\n    stage 'Prepare Database'\n    stage 'Invoke Rake'\n    stage 'Security scan'\n    stage 'Deploy'\n}\n\nAs mentioned previously, this Jenkinsfile is going to rely heavily on the\nCloudBees\nDocker Pipeline plugin. The plugin provides two very important features:\n\nAbility to execute steps inside of a running Docker container\n\nAbility to run a container in the \"background.\"\n\nLike most Rails applications, one can effectively test the application with two\ncommands: bundle install followed by bundle exec rake. I already had some\nDocker images prepared with RVM and Ruby 2.3.0 installed,\nwhich ensures a common and consistent starting point:\n\nnode('docker') {\n    // .. 'stage' steps removed\n    docker.image('rtyler/rvm:2.3.0').inside { (1)\nrvm 'bundle install' (2)\nrvm 'bundle exec rake'\n    } (3)\n}\n\n1\nRun the named container. The inside method can take optional additional flags for the docker run command.\n\n2\nExecute our shell commands using our tiny sh step wrapper\nrvm . This ensures that the shell code is executed in the correct RVM environment.\n\n3\nWhen the closure completes, the container will be destroyed.\n\nUnfortunately, with this application, the bundle exec rake command will fail\nif PostgreSQL isn’t available when the process starts. This is where the\nsecond important feature of the CloudBees Docker Pipeline plugin comes\ninto effect: the ability to run a container in the \"background.\"\n\nnode('docker') {\n    // .. 'stage' steps removed\n    /* Pull the latest `postgres` container and run it in the background */\n    docker.image('postgres').withRun { container -> (1)\necho \"PostgreSQL running in container ${container.id}\" (2)\n} (3)\n}\n\n1\nRun the container, effectively docker run postgres\n\n2\nAny number of steps can go inside the closure\n\n3\nWhen the closure completes, the container will be destroyed.\n\nRunning the tests\n\nCombining these two snippets of Jenkins Pipeline is, in my opinion, where the\npower of the DSL\nshines:\n\nnode('docker') {\n    docker.image('postgres').withRun { container ->\n        docker.image('rtyler/rvm:2.3.0').inside(\"--link=${container.id}:postgres\") { (1)\nstage 'Install Gems'\n            rvm \"bundle install\"\n\n            stage 'Invoke Rake'\n            withEnv(['DATABASE_URL=postgres://postgres@postgres:5432/']) { (2)\nrvm \"bundle exec rake\"\n            }\n            junit 'spec/reports/*.xml' (3)\n}\n    }\n}\n\n1\nBy passing the --link argument, the Docker daemon will allow the RVM container to talk to the PostgreSQL container under the host name 'postgres'.\n\n2\nUse the withEnv step to set environment variables for everything that is in the closure. In this case, the cfp-app DB scaffolding will look for the DATABASE_URL variable to override the DB host/user/dbname defaults.\n\n3\nArchive the test reports generated by ci_reporter so that Jenkins can display test reports and trend analysis.\n\nWith this done, the basics are in place to consistently run the tests for\ncfp-app in fresh Docker containers for each execution of the pipeline.\n\nSecurity scanning\n\nUsing Brakeman, the security scanner for Ruby\non Rails, is almost trivially easy inside of Jenkins Pipeline, thanks to the\nBrakeman\nplugin which implements the publishBrakeman step.\n\nBuilding off our example above, we can implement the \"Security scan\" stage:\n\nnode('docker') {\n    /* --8 (1)\npublishBrakeman 'brakeman-output.tabs' (2)\n/* --8\n\n1\nRun the Brakeman security scanner for Rails and store the output for later in brakeman-output.tabs\n\n2\nArchive the reports generated by Brakeman so that Jenkins can display detailed reports with trend analysis.\n\nAs of this writing, there is work in progress\n( JENKINS-31202) to\nrender trend graphs from plugins like Brakeman on a pipeline project’s main\npage.\n\nDeploying the good stuff\n\nOnce the tests and security scanning are all working properly, we can start to\nset up the deployment stage. Jenkins Pipeline provides the variable\ncurrentBuild which we can use to determine whether our pipeline has been\nsuccessful thus far or not. This allows us to add the logic to only deploy when\neverything is passing, as we would expect:\n\nnode('docker') {\n    /* --8 (1)\nsh './deploy.sh' (2)\n}\n    else {\n        mail subject: \"Something is wrong with ${env.JOB_NAME} ${env.BUILD_ID}\",\n                  to: 'nobody@example.com',\n                body: 'You should fix it'\n    }\n    /* --8\n\n1\ncurrentBuild has the result property which would be 'SUCCESS', 'FAILED', 'UNSTABLE', 'ABORTED'\n\n2\nOnly if currentBuild.result is successful should we bother invoking our deployment script (e.g. git push heroku master)\n\nWrap up\n\nI have gratuitously commented the full\nJenkinsfile\nwhich I hope is a useful summation of the work outlined above. Having worked\non a number of Rails applications in the past, the consistency provided by\nDocker and Jenkins Pipeline above would have definitely improved those\nprojects' delivery times. There is still room for improvement however, which\nis left as an exercise for the reader. Such as: preparing new containers with\nall their\ndependencies\nbuilt-in instead of installing them at run-time. Or utilizing the parallel\nstep for executing RSpec across multiple Jenkins agents simultaneously.\n\nThe beautiful thing about defining your continuous delivery, and continuous\nsecurity, pipeline in code is that you can continue to iterate on it!","title":"Continuous Security for Rails apps with Pipeline and Brakeman","tags":["tutorial","ruby","pipeline","rails","brakeman","continuousdelivery"],"authors":[]}},{"node":{"date":"2016-08-09T00:00:00.000Z","id":"e7eb5984-a870-5548-985b-25b9210fb2a0","slug":"/blog/2016/08/09/ewm-beta-version/","strippedHtml":"This blog post is a continuation of the External Workspace Manager Plugin related posts, starting with\nthe introductory blog post, and followed by\nthe alpha version release announcement.\n\nAs the title suggests, the beta version of the External Workspace Manager Plugin was launched!\nThis means that it’s available only in the\nExperimental Plugins Update Center.\n\nTake care when installing plugins from the Experimental Update Center, since they may change in\nbackward-incompatible ways.\nIt’s advisable not to use it for Jenkins production environments.\n\nThe plugin’s repository is on GitHub.\nThe complete plugin’s documentation can be accessed\nhere.\n\nWhat’s new\n\nBellow is a summary of the features added so far, since the alpha version.\n\nMultiple upstream run selection strategies\n\nIt has support for the\nRun Selector Plugin (which is still in beta),\nso you can provide different run selection strategies when allocating a disk from the upstream job.\n\nLet’s suppose that we have an upstream job that clones the repository and builds the project:\n\ndef extWorkspace = exwsAllocate 'diskpool1'\n\nnode ('linux') {\n    exws (extWorkspace) {\n        checkout scm\n        sh 'mvn clean install -DskipTests'\n    }\n}\n\nIn the downstream job, we run the tests on a different node, but we reuse the same workspace as the previous job:\n\ndef run = selectRun 'upstream'\ndef extWorkspace = exwsAllocate selectedRun: run\n\nnode ('test') {\n    exws (extWorkspace) {\n        sh 'mvn test'\n    }\n}\n\nThe selectRun in this example selects the last stable build from the upstream job.\nBut, we can be more explicit, and select a specific build number from the upstream job.\n\ndef run = selectRun 'upstream',\n selector: [$class: 'SpecificRunSelector', buildNumber: UPSTREAM_BUILD_NUMBER]\ndef extWorkspace = exwsAllocate selectedRun: run\n// ...\n\nWhen the selectedRun parameter is given to the exwsAllocate step, it will allocate the same workspace that was\nused by that run.\n\nThe Run Selector Plugin has several run selection strategies that are briefly explained\nhere.\n\nAutomatic workspace cleanup\n\nProvides an automatic workspace cleanup by integrating the\nWorkspace Cleanup Plugin.\nFor example, if we need to delete the workspace only if the build has failed, we can do the following:\n\ndef extWorkspace = exwsAllocate diskPoolId: 'diskpool1'\n\nnode ('linux') {\n    exws (extWorkspace) {\n        try {\n            checkout scm\n            sh 'mvn clean install'\n        } catch (e) {\n            currentBuild.result = 'FAILURE'\n            throw e\n        } finally {\n            step ([$class: 'WsCleanup', cleanWhenFailure: false])\n        }\n    }\n}\n\nMore workspace cleanup examples can be found at this\nlink.\n\nCustom workspace path\n\nAllows the user to specify a custom workspace path to be used when allocating workspace on the disk.\nThe plugin offers two alternatives for doing this:\n\nby defining a global workspace template for each Disk Pool\n\nThis can be defined in the Jenkins global config, External Workspace Definitions section.\n\nby defining a custom workspace path in the Pipeline script\n\nWe can use the Pipeline DSL to compute the workspace path.\nThen we pass this path as input parameter to the exwsAllocate step.\n\ndef customPath = \"${env.JOB_NAME}/${PULL_REQUEST_NUMBER}/${env.BUILD_NUMBER}\"\ndef extWorkspace = exwsAllocate diskPoolId: 'diskpool1', path: customPath\n// ...\n\nFor more details see the afferent\ndocumentation page.\n\nDisk Pool restrictions\n\nThe plugin comes with Disk Pool restriction strategies.\nIt does this by using the restriction capabilities provided by the\nJob Restrictions Plugin.\n\nFor example, we can restrict a Disk Pool to be allocated only if the Jenkins job in which it’s allocated was triggered\nby a specific user:\n\nOr, we can restrict the Disk Pool to be allocated only for those jobs whose name matches a well defined pattern:\n\nWhat’s next\n\nCurrently there is ongoing work for providing flexible disk allocation strategies.\nThe user will be able to define a default disk allocation strategy in the Jenkins global config.\nSo for example, we want to select the disk with the most usable space as default allocation strategy:\n\nIf needed, this allocation strategy may be overridden in the Pipeline code.\nLet’s suppose that for a specific job, we want to allocate the disk with the highest read speed.\n\ndef extWorkspace = exwsAllocate diskPoolId: 'diskpool1', strategy: fastestRead()\n// ...\n\nWhen this feature is completed, the plugin will enter a final testing phase.\nIf all goes to plan, a stable version should be released in about two weeks.\n\nIf you have any issues in setting up or using the plugin, please feel free to ask me on the plugin’s Gitter\nchat.\nAny feedback is welcome, and you may provide it either on the Gitter chat, or on\nJira by using the external-workspace-manager-plugin component.\n\nLinks\n\nProject repository\n\nProject intro blog post\n\nAlpha version announcement\n\nGSoC page\n\nJenkins GSoC Page","title":"GSoC: External Workspace Manager for Pipeline. Beta release is available","tags":["pipeline","plugins","external-workspace-manager","gsoc"],"authors":[]}},{"node":{"date":"2016-08-08T00:00:00.000Z","id":"649a2e8e-4f2f-56eb-99f1-8897de882a49","slug":"/blog/2016/08/08/docker-pipeline-environments/","strippedHtml":"This is a guest post by Michael Neale, long time open\nsource developer and contributor to the Blue Ocean\nproject.\n\nIf you are running parts of your pipeline on Linux, possibly the easiest way to\nget a clean reusable environment is to use:\nCloudBees\nDocker Pipeline plugin.\n\nIn this short post I wanted to show how you can avoid installing stuff on the agents, and have per project, or even per branch, customized build environments.\nYour environment, as well as your pipeline is defined and versioned alongside your code.\n\nI wanted to use the Blue Ocean project as an\nexample of a\nproject that uses the CloudBees Docker Pipeline plugin.\n\nEnvironment and Pipeline for JavaScript components\n\nThe Blue Ocean project has a few moving parts, one of\nwhich is called the \"Jenkins Design Language\".  This is a grab bag of re-usable\nCSS, HTML, style rules, icons and JavaScript components (using React.js) that\nprovide the look and feel for Blue Ocean.\n\nJavaScript and Web Development being what it is in 2016, many utilities are\nneed to assemble a web app.  This includes npm and all that it needs, less.js\nto convert Less to CSS, Babel to \"transpile\" versions of JavaScript to other\ntypes of JavaScript (don’t ask) and more.\n\nWe could spend time installling nodejs/npm on the agents, but why not just use\nthe official off the shelf docker image\nfrom Docker Hub?\n\nThe only thing that has to be installed and run on the build agents is the Jenkins agent, and a docker daemon.\n\nA simple pipeline using this approach would be:\n\nnode {\n        stage \"Prepare environment\"\n          checkout scm\n          docker.image('node').inside {\n            stage \"Checkout and build deps\"\n                sh \"npm install\"\n\n            stage \"Test and validate\"\n                sh \"npm install gulp-cli && ./node_modules/.bin/gulp\"\n          }\n}\n\nThis uses the stock \"official\" Node.js image from the Docker Hub, but doesn’t let us customize much about the environment.\n\nCustomising the environment, without installing bits on the agent\n\nBeing the forward looking and lazy person that I am, I didn’t want to have to\ngo and fish around for a Docker image every time a developer wanted something\nspecial installed.\n\nInstead, I put a Dockerfile in the root of the repo, alongside the Jenkinsfile :\n\nThe contents of the Dockerfile can then define the exact environment needed\nto build the project.  Sure enough, shortly after this, someone came along\nsaying they wanted to use Flow from Facebook (A\ntypechecker for JavaScript).  This required an additional native component to\nwork (via apt-get install).\n\nThis was achieved via a\npull\nrequest to both the Jenkinsfile and the Dockerfile at the same time.\n\nSo now our environment is defined by a Dockerfile with the following contents:\n\n# Lets not just use any old version but pick one\nFROM node:5.11.1\n\n# This is needed for flow, and the weirdos that built it in ocaml:\nRUN apt-get update && apt-get install -y libelf1\n\nRUN useradd jenkins --shell /bin/bash --create-home\nUSER jenkins\n\nThe Jenkinsfile pipeline now has the following contents:\n\nnode {\n    stage \"Prepare environment\"\n        checkout scm\n        def environment  = docker.build 'cloudbees-node'\n\n        environment.inside {\n            stage \"Checkout and build deps\"\n                sh \"npm install\"\n\n            stage \"Validate types\"\n                sh \"./node_modules/.bin/flow\"\n\n            stage \"Test and validate\"\n                sh \"npm install gulp-cli && ./node_modules/.bin/gulp\"\n                junit 'reports/**/*.xml'\n        }\n\n    stage \"Cleanup\"\n        deleteDir()\n}\n\nEven hip JavaScript tools can emit that weird XML format that test\nreporters can use, e.g. the junit result archiver.\n\nThe main change is that we have docker.build being called to produce the\nenvironment which is then used.  Running docker build is essentially a\n\"no-op\" if the image has already been built on the agent before.\n\nWhat’s it like to drive?\n\nWell, using Blue Ocean, to build Blue Ocean, yields a pipeline that visually\nlooks like this (a recent run I screen capped):\n\nThis creates a pipeline that developers can tweak on a pull-request basis,\nalong with any changes to the environment needed to support it, without having\nto install any packages on the agent.\n\nWhy not use docker commands directly?\n\nYou could of course just use shell commands to do things with Docker directly,\nhowever, Jenkins Pipeline keeps track of Docker images used in a Dockerfile\nvia the \"Docker Fingerprints\" link (which is good, should that image need to\nchange due to a security patch).\n\nLinks\n\nThe project used as as an example is here\n\nThe pipeline is defined by the Jenkinsfile\n\nThe environment is defined by the Dockerfile\n\nRead more on Docker Pipeline","title":"Don't install software, define your environment with Docker and Pipeline","tags":["pipeline","plugins","blueocean","ux","javascript","nodejs"],"authors":[]}},{"node":{"date":"2016-08-03T00:00:00.000Z","id":"f3311ae4-4467-5080-87f7-70b0aee702c4","slug":"/blog/2016/08/03/st-petersburg-jam-3-4-report/","strippedHtml":"I would like to write about two last Jenkins Meetups in Saint Petersburg, Russia.\n\nMeetup #3. Jenkins Administration (May 20, 2016)\n\nIn May we had a meetup about Jenkins administration techniques.\nAt this meetup we were talking about common Jenkins ecosystem components\nlike custom update centers, tool repositories and generic jobs.\n\nTalks:\n\nKirill Merkushev, Yandex, \"Juseppe. A custom Update Center for Jenkins\"\n\nPresentation (rus)\n\nKeywords: Juseppe\n\nAnna Muravieva, EMC, \"Generic jobs in Jenkins. How to build anything?\"\n\nPresentation (rus)\n\nKeywords: Generic Builds, Scripted Build Wrappers\n\nOleg Nenashev, CloudBees, \"Building Jenkins Tool infrastructures with help of Custom Tools Plugin and Docker\"\n\nPresentation (rus)\n\nKeywords: Custom Tools Plugin, Extra Tool Installers Plugin, Docker\n\nMeetup #4. IT Global Meetup (July 23, 2016)\n\nIn Saint Petersburg there is a regular gathering of local IT communities.\nThis IT Global Meetup is a full-day event, which provides an opportunity to dozens of communities and hundreds of visitors to meet at a single place.\n\nOn July 23rd our local Jenkins community participated in the eight’s global meetup.\nWe conduced 2 talks in main tracks and also had a round table in the evening.\n\nTalks:\n\nOleg Nenashev, CloudBees, \"About Jenkins 2 and future plans\"\n\nOleg provided a top-level overview about changes in Jenkins,\nshared insights about upgrading to the new Jenkins 2.7.1 LTS and talked about Jenkins plans\n\nPresentation (rus)\n\nAleksandr Tarasov, Alfa-Laboratory, \"Continuous Delivery with Jenkins: Lessons learned\"\n\nAleksandr summarized AlfaLab’s experience of Jenkins usage for Continuous Delivery in their environment.\nHe talked about the flow based on Jenkins Pipeline, JobDSL and BlueOcean prototype.\n\nPresentation (rus)\n\nAfter the talks we had a roundtable about Jenkins (~10 Jenkins experts).\nOleg provided an overview of Docker and Configuration-as-Code features available in Jenkins,\nand then we talked about common use-cases in Jenkins installations.\nWe hope to finally organize a \"Jenkins & Docker\" meetup at some point.\n\nQ&A\n\nIf you have any questions, all speakers can be contacted via\nJenkins RU Gitter Chat.\n\nLinks\n\nSt. Petersburg Meetup page (follow the events here)\n\nSt. Petersburg Meetup Twitter\n\nJenkins RU Twitter\n\nJenkins RU Gitter Chat\n\nIT Global Meetup\n\nAcknowledgments\n\nThe events have been organized with help from\nCloudBees, EMC and\norganizers of the St. Petersburg IT Global Meetup.","title":"St. Petersburg Jenkins Meetup #3 and #4 Reports","tags":["jam","jenkins_ru"],"authors":[]}},{"node":{"date":"2016-07-26T00:00:00.000Z","id":"61a711c9-9de6-5990-b29e-67f3bac971b1","slug":"/blog/2016/07/26/join-me-at-jenkinsworld/","strippedHtml":"Jenkins World, September\n13-15 at the Santa Clara Convention Center (SCCC), takes our 6th annual\ncommunity user conference to a whole new level. It will be one big party for\neverything Jenkins, from users to developers, from the community to vendors.\nThere will be more of what people always loved in past user conferences, such\nas technical sessions from users and developers, the Ask the Experts booth and\nplugin development workshop, and even more has been added, such as Jenkins\ntraining pre-conference, workshops and the opportunity to get certified for\nfree. Jenkins World is a not-to-be-missed.\n\nFor me, the best part of Jenkins World is the opportunity to meet other Jenkins\nusers and developers face-to-face. We all interact on IRC, Google Groups or\nGitHub, but when you have a chance to meet in person, the person behind the\nGitHub ID or IRC name, whose plugin you use every day, becomes a real person.\nYour motivation might be a little different from mine, but we have the breath\nin the agenda to cover everyone from new users to senior plugin developers.\n\nThis year, you’ll have more opportunities than ever before to learn about\nJenkins and continuous delivery/DevOps practices, and explore what Jenkins has\nto offer.\n\nIf you are travelling from somewhere, you might as well get a two-day Jenkins training course to be held onsite, starting Monday.\n\nOn Tuesday, you can attend your choice of workshops, which gives you more hands-on time to go deeper, including:\n\nThe DevOps Toolkit 2.0 Workshop\n\nLet’s Build a Jenkins Pipeline\n\nPreparing for Jenkins Certification\n\nIntro to Plugin Development\n\nCD and DevOps Maturity for Managers\n\nOn Wednesday, the formal conference kicks off. Throughout Wednesday and\nThursday, you can choose from sessions spread across five tracks and covering\na diverse range of topics like infrastructure as code, security, containers,\npipeline automation, best practices, scaling Jenkins and new community\ndevelopment initiatives.\n\nAt Jenkins World, you’ll be exposed to projects going on in the community such\nas Blue Ocean, a new Jenkins UX project. You can\nlearn more about Jenkins 2 - a major release for the project, and based on the\nhuge number of downloads we saw in the weeks following its introduction at the\nend of April, it was a big +1. At Jenkins World, you will be immersed in\nJenkins and community, and leave knowing that you are part of a meaningful open\nsource project that, with your involvement, can do anything!\n\nThis year there will only be one Jenkins World conference, so that everyone\ninvolved in Jenkins can get together in one place at one time and actually see\neach other. I understand that it might be a bit more difficult for Jenkins\nusers outside of the US to make it to Jenkins World, but hopefully we made the\nevent worth your visit. As the final push on the back, CloudBees has created a\nspecial international program\nfor those who are coming from outside the United States.  You’ll have\ntime to talk with all of the other Jenkins users who have made the journey from\nacross the globe, you’ll be able to attend exclusive networking events and\nmore.\n\nI hope to see you September 13th through 15th in Santa Clara at\nJenkins World in Santa Clara!\n\nRegister for Jenkins World in\nSeptember with the code JWFOSS for a 20% discount off your pass.","title":"Join me for Jenkins World 2016","tags":["event","jenkinsworld","jenkinsworld2016"],"authors":[]}}]}},"pageContext":{"limit":8,"skip":376,"numPages":100,"currentPage":48}},
    "staticQueryHashes": ["3649515864"]}