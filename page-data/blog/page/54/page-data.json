{
    "componentChunkName": "component---src-templates-blog-list-template-js",
    "path": "/blog/page/54",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2016-04-15T00:00:00.000Z","id":"fa2f2d1b-91d5-5e2e-9783-72c7059face6","slug":"/blog/2016/04/15/the-need-for-pipeline/","strippedHtml":"This is a cross-post of\nan article authored\nby Viktor Farcic on the\nCloudBees blog. Viktor is also the author\nof The DevOps 2.0 Toolkit, which\nexplores Jenkins, the Pipeline plugin, and the ecosystem\naround it in much more detail.\n\nOver the years, Jenkins has become the undisputed ruler among continuous\nintegration (CI), delivery and deployment (CD) tools. It, in a way, defined the\nCI/CD processes we use today. As a result of its leadership, many other products\nhave tried to overthrow it from its position. Among others, we got Bamboo and\nTeam City attempting to get a piece of the market. At the same time, new\nproducts emerged with a service approach (as opposed to on-premises). Some of\nthem are Travis, CircleCI and Shippable. Be that as it may, none managed to get\neven close to Jenkins' adoption. Today, depending on the source we use, Jenkins\nholds between 50-70% of the whole CI/CD tools market. The reason behind such a\nhigh percentage is its dedication to open source principles set from the very\nbeginning by Kohsuke Kawaguchi. Those same principles were the reason he forked\nJenkins from Hudson. The community behind the project, as well as commercial\nentities behind enterprise versions, are continuously improving the way it works\nand adding new features and capabilities. They are redefining not only the way\nJenkins behaves but also the CI/CD practices in a much broader sense. One of\nthose new features is the Jenkins Pipeline plugin. Before we\ndive into it, let us take a step back and discuss the reasons that led us to\ninitiate the move away from Freestyle jobs and towards the Pipeline.\n\nThe Need for Change\n\nOver time, Jenkins, like most other self-hosted CI/CD tools, tends to accumulate\na vast number of jobs. Having a lot of them causes quite an increase in\nmaintenance cost. Maintaining ten jobs is easy. It becomes a bit harder (but\nstill bearable) to manage a hundred. When the number of jobs increases to\nhundreds or even thousands, managing them becomes very tedious and time\ndemanding.\n\nIf you are not proficient with Jenkins (or other CI/CD tools) or you do not work\nfor a big project, you might think that hundreds of jobs is excessive. The truth\nis that such a number is reached over a relatively short period when teams\nare practicing continuous delivery or deployment. Let’s say that an average\nCD flow has the following set of tasks that should be run on each commit:\nbuilding, pre-deployment testing, deployment to a staging environment,\npost-deployment testing and deployment to production. That’s five groups of\ntasks that are often divided into, at least, five separate Jenkins jobs. In\nreality, there are often more than five jobs for a single CD flow, but let\nus keep it an optimistic estimate. How many different CD flows does a medium\nsized company have? With twenty, we are already reaching a three digits\nnumber. That’s quite a lot of  jobs to cope with even though the estimates\nwe used are too optimistic for all but the smallest entities.\n\nNow, imagine that we need to change all those jobs from, let’s say, Maven to\nGradle. We can choose to start modifying them through the Jenkins UI, but that\ntakes too much time. We can apply changes directly to Jenkins XML files that\nrepresent those jobs but that is too complicated and error prone. Besides,\nunless we write a script that will do the modifications for us, we would\nprobably not save much time with this approach. There are quite a few plugins\nthat can help us to apply changes to multiple jobs at once, but none of them is\ntruly successful (at least among free plugins). They all suffer from one\ndeficiency or another. The problem is not whether we have the tools to perform\nmassive changes to our jobs, but whether jobs are defined in a way that they can\nbe easily maintained.\n\nBesides the sheer number of Jenkins jobs, another critical Jenkins' pain point\nis centralization. While having everything in one location provides a lot of\nbenefits (visibility, reporting and so on), it also poses quite a few\ndifficulties. Since the emergence of agile methodologies, there’s been a huge\nmovement towards self-sufficient teams. Instead of horizontal organization with\nseparate development, testing, infrastructure, operations and other groups, more\nand more companies are moving (or already moved) towards self-sufficient teams\norganized vertically. As a result, having one centralized place that defines all\nthe CD flows becomes a liability and often impedes us from splitting teams\nvertically based on projects. Members of a team should be able to collaborate\neffectively without too much reliance on other teams or departments. Translated\nto CD needs, that means that each team should be able to define the deployment\nflow of the application they are developing.\n\nFinally, Jenkins, like many other tools, relies heavily on its UI. While that is\nwelcome and needed as a way to get a visual overview through dashboards and\nreports, it is suboptimal as a way to define the delivery and deployment flows.\nJenkins originated in an era when it was fashionable to use UIs for everything.\nIf you worked in this industry long enough you probably saw the swarm of tools\nthat rely completely on UIs, drag & drop operations and a lot of forms that\nshould be filled. As a result, we got tools that produce artifacts that cannot\nbe easily stored in a code repository and are hard to reason with when anything\nbut simple operations are to be performed. Things changed since then, and now we\nknow that many things (deployment flow being one of them) are much easier to\nexpress through code. That can be observed when, for example, we try to define a\ncomplex flow through many Jenkins jobs. When deployment complexity requires\nconditional executions and some kind of a simple intelligence that depends on\nresults of different steps, chained jobs are truly complicated and often\nimpossible to create.\n\nAll things considered, the major pain points Jenkins had until recently are as\nfollows.\n\nTendency to create a vast number of jobs\n\nRelatively hard and costly maintenance\n\nCentralization of everything\n\nLack of powerful and easy ways to specify deployment flow through code\n\nThis list is, by no means, unique to Jenkins. Other CI/CD tools have at least\none of the same problems or suffer from deficiencies that Jenkins solved a long\ntime ago. Since the focus of this article is Jenkins, I won’t dive into a\ncomparison between the CI/CD tools.\n\nLuckily, all those, and many other deficiencies are now a thing of the past.\nWith the emergence of the\nPipeline\nplugin and many others that were created on\ntop of it, Jenkins entered a new era and proved itself as a dominant player in\nthe CI/CD market. A whole new ecosystem was born, and the door was opened for\nvery exciting possibilities in the future.\n\nBefore we dive into the Jenkins Pipeline and the toolset that surrounds it, let\nus quickly go through the needs of a modern CD flow.\n\nContinuous Delivery or Deployment Flow with Jenkins\n\nWhen embarking on the CD journey for the first time, newcomers tend to think\nthat the tasks that constitute the flow are straightforward and linear. While\nthat might be true with small projects, in most cases things are much more\ncomplicated than that. You might think that the flow consists of building,\ntesting and deployment, and that the approach is linear and follows the\nall-or-nothing rule. Build invokes testing and testing invokes deployment. If\none of them fails, the developer gets a notification, fixes the problem and\ncommits the code that will initiate the repetition of the process.\n\nIn most instances, the process is far more complex. There are many tasks to run,\nand each of them might produce a failure. In some cases, a failure should only\nstop the process. However, more often than not, some additional logic should be\nexecuted as part of the after-failure cleanup. For example, what happens if\npost-deployment tests fail after a new release was deployed to production? We\ncannot just stop the flow and declare the build a failure. We might need to\nrevert to the previous release, rollback the proxy, de-register the service and\nso on. I won’t go into many examples of situations that require complex flow\nwith many tasks, conditionals that depend on results, parallel execution and so\non. Instead, I’ll share a diagram of one of the flows I worked on.\n\nSome tasks are run in one of the testing servers (yellow) while others are run\non the production cluster (blue). While any task might produce an error, in some\ncases such an outcome triggers a separate set of tasks. Some parts of the flow\nare not linear and depend on task results. Some tasks should be executed in\nparallel to improve the overall time required to run them. The list goes on and\non. Please note that this discussion is not about the best way to execute the\ndeployment flow but only a demonstration that the complexity can be, often, very\nhigh and cannot be solved by a simple chaining of Freestyle jobs. Even in cases\nwhen such chaining is possible, the maintenance cost tends to be very high.\n\nOne of the CD objectives we are unable to solve through chained jobs, or is\nproved to be difficult to implement, is conditional logic. In many cases, it is\nnot enough to simply chain jobs in a linear fashion. Often, we do not want only\nto create a job A that, once it’s finished running, executes job B, which, in\nturn, invokes job C. In real-world situations, things are more complicated than\nthat. We want to run some tasks (let’s call them job A), and, depending on the\nresult, invoke jobs B1 or B2, then run in parallel C1, C2 and C3, and, finally,\nexecute job D only when all C jobs are finished successfully. If this were a\nprogram or a script, we would have no problem accomplishing something like that,\nsince all modern programming languages allow us to employ conditional logic in a\nsimple and efficient way. Chained Jenkins jobs, created through its UI, pose\ndifficulties to create even a simple conditional logic. Truth be told, some\nplugins can help us with conditional logic. We have Conditional Build Steps,\nParameterised Trigger, Promotions and others. However, one of the major issues\nwith these plugins is configuration. It tends to be scattered across multiple\nlocations, hard to maintain and with little visibility.\n\nResource allocation needs a careful thought and is, often, more complicated than\na simple decision to run a job on a predefined agent. There are cases when agent\nshould be decided dynamically, workspace should be defined during runtime and\ncleanup depends on a result of some action.\n\nWhile a continuous deployment process means that the whole pipeline ends with\ndeployment to production, many businesses are not ready for such a goal or have\nuse-cases when it is not appropriate. Any other process with a smaller scope, be\nit continuous delivery or continuous integration, often requires some human\ninteraction. A step in the pipeline might need someone’s confirmation, a failed\nprocess might require a manual input about reasons for the failure, and so on.\nThe requirement for human interaction should be an integral part of the pipeline\nand should allow us to pause, inspect and resume the flow. At least, until we\nreach the true continuous deployment stage.\n\nThe industry is, slowly, moving towards microservices architectures. However,\nthe transformation process might take a long time to be adopted, and even more\nto be implemented. Until then, we are stuck with monolithic applications that\noften require a long time for deployment pipelines to be fully executed. It is\nnot uncommon for them to run for a couple of hours, or even days. In such cases,\nfailure of the process, or the whole node the process is running on, should not\nmean that everything needs to be repeated. We should have a mechanism to\ncontinue the flow from defined checkpoints, thus avoiding costly repetition,\npotential delays and additional costs. That is not to say that long-running\ndeployment flows are appropriate or recommended. A well-designed CD process\nshould run within minutes, if not seconds. However, such a process requires not\nonly the flow to be designed well, but also the architecture of our applications\nto be changed. Since, in many cases, that does not seem to be a viable option,\nresumable points of the flow are a time saver.\n\nAll those needs, and many others, needed to be addressed in Jenkins if it was to\ncontinue being a dominant CI/CD tool. Fortunately, developers behind the project\nunderstood those needs and, as a result, we got the Jenkins Pipeline plugin. The\nfuture of Jenkins lies in a transition from Freestyle chained jobs to a single\npipeline expressed as code. Modern delivery flows cannot be expressed and easily\nmaintained through UI drag 'n drop features, nor through chained jobs. They can\nneither be defined through YAML (Yet Another Markup Language) definitions\nproposed by some of the newer tools (which I’m not going to name). We need to go\nback to code as a primary way to define not only the applications and services\nwe are developing but almost everything else. Many other types of tools adopted\nthat approach, and it was time for us to get that option for CI/CD processes as\nwell.","title":"The Need For Jenkins Pipeline","tags":["jenkins2","pipeline"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-04-14T00:00:00.000Z","id":"c98c0308-267c-58ea-bd87-6999a646f331","slug":"/blog/2016/04/14/jenkins-world-registration-is-open/","strippedHtml":"This is a guest post by Alyssa Tong.\nAlyssa works for CloudBees, helping to organize\nJenkins community events around the\nworld.\n\nJenkins World 2016 will be the largest gathering of Jenkins users in the world. This event will bring together Jenkins experts, continuous delivery thought leaders and the ecosystem offering complementary technologies for Jenkins. Join us September 13-15, 2016 in Santa Clara, California to learn and explore, network face-to-face and help shape the next evolution of Jenkins development and solutions for DevOps.\n\nRegistration for Jenkins World 2016 is now live. Take advantage of the Super Early Bird rate of $399 (available until July 1st).\n\nAnd don’t forget, the Call for Papers will be ending on May 1st. That’s 2.5 short weeks left to get your proposal(s) in.  We anxiously await your amazing stories.","title":"Registration is Open for Jenkins World 2016!","tags":["event","jenkinsworld","jenkinsworld2016"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#281818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg","srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/8d248/alyssat.jpg 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/c004c/alyssat.jpg 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/9e67b/alyssat.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/22924/alyssat.webp 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/89767/alyssat.webp 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/40d97/alyssat.webp 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/5028e/alyssat.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":166}}},"blog":null,"github":"alyssat","html":"<div class=\"paragraph\">\n<p>Member of the <a href=\"/sigs/advocacy-and-outreach/\">Jenkins Advocacy and Outreach SIG</a>.\nAlyssa drives and manages Jenkins participation in community events and conferences like <a href=\"https://fosdem.org/\">FOSDEM</a>, <a href=\"https://www.socallinuxexpo.org/\">SCaLE</a>, <a href=\"https://events.linuxfoundation.org/cdcon/\">cdCON</a>, and <a href=\"https://events19.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2019/\">KubeCon</a>.\nShe is also responsible for Marketing &amp; Community Programs at <a href=\"https://cloudbees.com\">CloudBees, Inc.</a></p>\n</div>","id":"alyssat","irc":null,"linkedin":null,"name":"Alyssa Tong","slug":"/blog/authors/alyssat","twitter":null}]}},{"node":{"date":"2016-04-14T00:00:00.000Z","id":"44aa3a43-715d-549b-b0d2-cc5c5bd94725","slug":"/blog/2016/04/14/replay-with-pipeline/","strippedHtml":"This is a cross-post of\nan article authored by\nPipeline plugin maintainer Jesse Glick on the\nCloudBees blog.\n\nFor those of you not checking their Updates tab obsessively, Pipeline 1.14 [up\nto 2.1 now] was\nreleased\na couple of weeks ago and I wanted to highlight the major feature in this\nrelease: JENKINS-32727,\nor replay. Some folks writing \"Jenkinsfiles\" in the field had grumbled that it\nwas awkward to develop the script incrementally, especially compared to jobs\nusing inline scripts stored in the Jenkins job configuration: to try a change to\nthe script, you had to edit Jenkinsfile in SCM, commit it (perhaps to a\nbranch), and then go back to Jenkins to follow the output. Now this is a little\neasier. If you have a Pipeline build which did not proceed exactly as you\nexpected, for reasons having to do with Jenkins itself (say, inability to find &\npublish test results, as opposed to test failures you could reproduce locally),\ntry clicking the Replay link in the build’s sidebar. The quickest way to try\nthis for yourself is to run the\nstock CD demo in its\nlatest release:\n\n$ docker run --rm -p 2222:2222 -p 8080:8080 -p 8081:8081 -p 9418:9418 -ti jenkinsci/workflow-demo:1.14-3\n\nWhen you see the page Replay\n#1 , you are shown two\n(Groovy) editor boxes: one for the main\nJenkinsfile , one for a library script\nit loaded\n( servers.groovy , introduced to help demonstrate this feature). You\ncan make edits to either or both. For example, the original demo allocates a\ntemporary web application with a random name like\n9c89e9aa-6ca2-431c-a04a-6599e81827ac for the duration of the functional tests.\nPerhaps you wished to prefix the application name with tmp- to make it obvious\nto anyone encountering the Jetty index page that these\nURLs are transient. So in the second text area, find the line\n\ndef id = UUID.randomUUID().toString()\n\nand change it to read\n\ndef id = \"tmp-${UUID.randomUUID()}\"\n\nthen click Run. In\nthe new build’s log\nyou will now see\n\nReplayed #1\n\nand later something like\n\n… test -Durl=http://localhost:8081/tmp-812725bb-74c6-41dc-859e-7d9896b938c3/ …\n\nwith the improved URL format. Like the result? You will want to make it\npermanent. So jump to the [second build’s index\npage]( http://localhost:8080/job/cd/branch/master/2/) where you will see a note\nthat this build > Replayed #1 (diff) If you\nclick on diff you\nwill see:\n\n--- old/Script1\n+++ new/Script1\n@@ -8,7 +8,7 @@\n }\n\n def runWithServer(body) {\n-    def id = UUID.randomUUID().toString()\n+    def id = \"tmp-${UUID.randomUUID()}\"\n     deploy id\n     try {\n         body.call id\n\nso you can know exactly what you changed from the last-saved version. In fact if you replay #2 and change tmp to temp in the loaded script, in the diff view for #3 you will see the diff from the first build, the aggregate diff:\n\n--- old/Script1\n+++ new/Script1\n@@ -8,7 +8,7 @@\n }\n\n def runWithServer(body) {\n-    def id = UUID.randomUUID().toString()\n+    def id = \"temp-${UUID.randomUUID()}\"\n     deploy id\n     try {\n         body.call id\n\nAt this point you could touch up the patch to refer to servers.groovy\n( JENKINS-31838), git\napply it to a clone of your repository, and commit. But why go to the trouble\nof editing Groovy in the Jenkins web UI and then manually copying changes back\nto your IDE, when you could stay in your preferred development environment from\nthe start?\n\n$ git clone git://localhost/repo\nCloning into 'repo'...\nremote: Counting objects: 23, done.\nremote: Compressing objects: 100% (12/12), done.\nremote: Total 23 (delta 1), reused 0 (delta 0)\nReceiving objects: 100% (23/23), done.\nResolving deltas: 100% (1/1), done.\nChecking connectivity... done.\n$ cd repo\n$ $EDITOR servers.groovy\n# make the same edit as previously described\n$ git diff\ndiff --git a/servers.groovy b/servers.groovy\nindex 562d92e..63ea8d6 100644\n--- a/servers.groovy\n+++ b/servers.groovy\n@@ -8,7 +8,7 @@ def undeploy(id) {\n }\n\n def runWithServer(body) {\n-    def id = UUID.randomUUID().toString()\n+    def id = \"tmp-${UUID.randomUUID()}\"\n     deploy id\n     try {\n         body.call id\n$ ssh -p 2222 -o StrictHostKeyChecking=no localhost replay-pipeline cd/master -s Script1 webapp-naming\n\nUsing the replay-pipeline CLI command (in this example via\nSSH)\nyou can prepare, test, and commit changes to your Pipeline script code without\ncopying anything to or from a browser. That is all for now. Enjoy!","title":"Replay a Pipeline with script edits","tags":["jenkins2","pipeline"],"authors":[]}},{"node":{"date":"2016-04-11T00:00:00.000Z","id":"25d1b7f0-6783-53a9-aec5-f7242e8493d2","slug":"/blog/2016/04/11/gsoc2016-mentors-call/","strippedHtml":"As you probably know, Jenkins project has been accepted to\nGoogle Summer of Code 2016.\n\nDuring last month we were working with students in order to discuss their project ideas and to review their application drafts.\nThanks again to all students and mentors for your hard work during about ten office hours and dozens of other calls/chats!\n\nCurrent status\n\nWe have successfully handled the student application period\n\nWe have received a bunch of good project proposals (mentors cannot disclose the number)\n\nWe have done the preliminary filtering of applications\n\nGSoC mentors and organization admins have prepared the project slot application draft\n\nCurrently we are looking for mentors.\nWe have a minimal required number for the current project slot application plan, but additional expertise would allow us to share the load and to provide more expertise to students.\n\nIf you want to be a mentor:\n\nCheck out mentor requirements here.\n\nCheck out the project ideas\nhere.\n\nStudent application period is finished, so it is too late to propose project ideas for this year\n\nYou can join the mentor team for one of the mentioned projects\n\nHot areas: UI improvements, Fingerprints, External Workspace Manager\n\nContact Google GSoC admins via jenkinsci-gsoc-org@googlegroups.com\n\nLinks\n\nGSoC2016 page on our Wiki\n\nJenkins page on the GSoC2016 website","title":"Google Summer of Code. Call for Mentors","tags":["general","jenkins2","gsoc"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8c8e8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/bf8e1/oleg_nenashev.png","srcSet":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/914ee/oleg_nenashev.png 32w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/1c9ce/oleg_nenashev.png 64w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/bf8e1/oleg_nenashev.png 128w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/acb7c/oleg_nenashev.png 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/ef6ff/oleg_nenashev.webp 32w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/8257c/oleg_nenashev.webp 64w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/6766a/oleg_nenashev.webp 128w,\n/gatsby-jenkins-io/static/611f92ce782a36a3a1454a29c79753db/22bfc/oleg_nenashev.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"https://oleg-nenashev.github.io/","github":"oleg-nenashev","html":"<div class=\"paragraph\">\n<p>Jenkins core maintainer and board member, open source software and open hardware advocate, TOC chair in the Continuous Delivery Foundation.\nOleg started using Hudson for Hardware/Embedded projects in 2008 and became an active Jenkins contributor in 2012.\nNowadays he maintains [Jenkinsfile Runner](<a href=\"https://github.com/jenkinsci/jenkinsfile-runner/\" class=\"bare\">https://github.com/jenkinsci/jenkinsfile-runner/</a>),\ncontributes to several Jenkins <a href=\"/sigs\">SIGs</a> and outreach programs (<a href=\"/projects/gsoc\">Google Summer of Code</a>, <a href=\"/events/hacktoberfest\">Hacktoberfest</a>)\nand organizes <a href=\"/projects/jam/\">Jenkins meetups</a> in Switzerland and Russia.\nOleg works on open source programs and [Keptn](<a href=\"https://keptn.sh/\" class=\"bare\">https://keptn.sh/</a>) at the [Dynatrace](<a href=\"https://dynatrace.com\" class=\"bare\">https://dynatrace.com</a>), Open Source Program Office.</p>\n</div>","id":"oleg_nenashev","irc":"oleg_nenashev","linkedin":"onenashev","name":"Oleg Nenashev","slug":"/blog/authors/oleg_nenashev","twitter":"oleg_nenashev"}]}},{"node":{"date":"2016-04-11T00:00:00.000Z","id":"e9d4f87e-38f5-5fa5-8f99-7062f5a29099","slug":"/blog/2016/04/11/jenkins-plugins-security-update/","strippedHtml":"The Script Security Plugin and the Extra Columns Plugin were updated today to fix medium-severity security vulnerabilities. For detailed information about the security content of these updates, see the security advisory.\n\nSubscribe to the jenkinsci-advisories mailing list to receive important notifications related to Jenkins security.","title":"Security fixes in Script Security Plugin and Extra Columns Plugin","tags":["plugins","security"],"authors":[{"avatar":null,"blog":null,"github":"daniel-beck","html":"<div class=\"paragraph\">\n<p>Daniel is a Jenkins core maintainer and member of the <a href=\"/security/#team\">Jenkins security team</a>.\nHe was the inaugural Jenkins security officer from 2015 to 2021.\nHe sometimes contributes to developer documentation and project infrastructure in his spare time.</p>\n</div>","id":"daniel-beck","irc":null,"linkedin":null,"name":"Daniel Beck","slug":"/blog/authors/daniel-beck","twitter":null}]}},{"node":{"date":"2016-04-11T00:00:00.000Z","id":"71b50dae-de34-5505-957d-a9c0f8d10044","slug":"/blog/2016/04/11/run-your-api-tests-continuously-with-jenkins-and-dhc/","strippedHtml":"This is a guest post by Guillaume Laforge.\nWell known for his contribution to the Apache Groovy project,\nGuillaume is also the \"Product Ninja and Advocate\" of Restlet,\na company focusing on Web APIs:\nwith DHC (an API testing client),\nRestlet Studio (an API designer),\nAPISpark (an API platform in the cloud),\nand the Restlet Framework\nopen source project for developing APIs.\n\nModern mobile apps, single-page web sites and applications, are more and more relying on Web APIs,\nas the nexus of the interaction between the frontend and the backend services.\nWeb APIs are also central to third-party integration, when you want to share your services with others,\nor when you need to consume existing APIs to build your own solution on top of their shoulders.\n\nWith APIs being a key element of your architecture and big picture,\nit’s obviously important to assess that this API is functioning the way it should, thanks to proper testing.\nYour framework of choice, regardless of the technology stack or programming language used,\nwill hopefully offer some facilities for testing your code,\nwhether in the form of unit tests, or ideally with integration tests.\n\nCoding Web API tests\n\nFrom a code perspective, as I said, most languages and frameworks provide approaches to testing APIs built with them.\nThere’s one I wanted to highlight in particular, which is one developed with a DSL approach (Domain-Specific Language),\nusing the Apache Groovy programming language, it’s\nAccuREST.\n\nTo get started, you can have a look at the introduction,\nand the usage guide.\nIf you use the contract DSL,\nyou’ll be able to write highly readable examples of requests you want to issue against your API,\nand the assertions that you expect to be true when getting the response from that call.\nHere’s a concrete example from the documentation:\n\nGroovyDsl.make {\n    request {\n        method 'POST'\n        urlPath('/users') {\n            queryParameters {\n                parameter 'limit': 100\n                parameter 'offset': containing(\"1\")\n                parameter 'filter': \"email\"\n            }\n        }\n        headers {\n            header 'Content-Type': 'application/json'\n        }\n        body '''{ \"login\" : \"john\", \"name\": \"John The Contract\" }'''\n    }\n    response {\n        status 200\n        headers {\n            header 'Location': '/users/john'\n        }\n    }\n}\n\nNotice that the response is expected to return a status code 200 OK, and a Location header pointing at /users/john.\nIndeed, a very readable way to express the requests and responses!\n\nTooling to test your APIs\n\nFrom a tooling perspective, there are some interesting tools that can be used to test Web APIs,\nlike Paw (on Macs),\nAdvanced REST client,\nPostman or\nInsomnia.\n\nBut in this article, I’ll offer a quick look at DHC,\na handy visual tool, that you can use both manually to craft your tests and assertions,\nand whose test scenarios you can export and integrate in your build and continuous integration pipeline,\nthanks to Maven and Jenkins.\n\nAt the end of this post, you should be able to see the following reporting in your Jenkins dashboard,\nwhen visualising the resulting API test execution:\n\nIntroducing DHC\n\nDHC is a Chrome extension, that you can\ninstall from the Chrome Web Store,\nin your Chrome browser. There’s also an online service available, with some limitations.\nFor the purpose of this article, we’ll use the Chrome extension.\n\nIn the main area, you can create your request, define the URL to call, specify the various request headers or params,\nchose the method you want to use, and then, you can click the send button to issue the request.\n\nIn the left pane, that’s where you’ll be able to see your request history, create and save your project in the cloud,\nor also set context variables.\n\nThe latter is important when testing your Web API, as you’ll be able to insert variables like for example\n{localhost} for testing locally on your machine or {staging} and {prod} to run your tests in different environments.\n\nIn the bottom pane, you have access to actual raw HTTP exchange, as well as the assertions pane.\n\nAgain, a very important pane to look at! With assertions, you’ll be able to ensure that your Web API works as expected.\nFor instance, you can check the status code of the call, check the payload contains a certain element,\nby using JSON Path or XPath to go through the JSON or XML payload respectively.\n\nBeyond assertions, what’s also interesting is that you can chain requests together.\nA call request can depend on the outcome of a previous request!\nFor example, in a new request, you could pass a query parameter whose value would be the value of some element\nof the JSON payload of a previously executed request.\nAnd by combining assertions, linked requests and context variables together, you can create full-blown test scenarios,\nthat you can then save in the cloud, but also export as a JSON file.\n\nTo export that test scenario, you can click on the little export icon in the bottom left hand corner,\nand you’ll be able to select exactly what you want to export:\n\nRunning your Web API tests with Maven\n\nNow things become even more interesting, as we’ll proceed to using Maven and Jenkins!\nAs the saying goes, there’s a Maven plugin for that! For running those Web API tests in your build!\nEven if your Web API is developed in another technology than Java, you can still create a small Maven build\njust for your Web API tests.\nAnd the icing on the cake, when you configure Jenkins to run this build, as the plugin outputs JUnit-friendly test reports,\nyou’ll be able to see the details of your successful and failed tests, just like you would see JUnit’s!\n\nLet’s sketch your Maven POM:\n\n4.0.0\n\ncom.example\nmy-first-api-test\n1.2.3\n\ncom.restlet.dhc\ndhc-maven-plugin\n1.1\n\ntest\n\ntest\n\ncompanies-scenario.json\n\nrestlet-maven\nRestlet public Maven repository Release Repository\nhttps://maven.restlet.com\n\nVisualizing Web API test executions in Jenkins\n\nOnce you’ve configured your Jenkins server to launch the test goal of this Maven project,\nyou’ll be able to see nice test reports for your Web API scenarios, like in the screenshot in introduction of this article!\n\nNext, you can easily run your Web API tests when developers commit changes to the API,\nor schedule regular builds with Jenkins to monitor an online Web API.\n\nFor more information, be sure to read the tutorial on\ntesting Web APIs with DHC.\nThere are also some more resources like a\nscreencast,\nas well as the\nuser guide, if you want to learn more.\nAnd above all, happy testing!","title":"Run Your API Tests Continuously with Jenkins and DHC","tags":["development","webapis","testing"],"authors":[{"avatar":null,"blog":"https://glaforge.appspot.com/","github":"glaforge","html":"","id":"glaforge","irc":null,"linkedin":null,"name":"Guillaume Laforge","slug":"/blog/authors/glaforge","twitter":"glaforge"}]}},{"node":{"date":"2016-04-07T00:00:00.000Z","id":"e79f0919-bcdb-509d-81fc-767eb7210b3a","slug":"/blog/2016/04/07/2.0-release-candidate/","strippedHtml":"Those who fervently watch the\njenkinsci-dev@\nlist, like I do, may have caught Daniel\nBeck 's email today which quietly referenced a significant milestone on the\nroad to 2.0 which has been reached: the first 2.0 release\ncandidate is here!\n\nThe release candidate process, in short, is the final stabilization and testing\nperiod before the final release of Jenkins 2.0. If you have the\ncycles to help test, please download the release candidate and give\nus your feedback as soon as possible!\n\nThe release candidate process also means that changes targeting release after\n2.0 can start landing in the master branch, laying the groundwork 2.1 and\nbeyond.\n\nI pushed the merge to 'master'. So anything targeting 2.1+ can be now proposed\nin pull requests to that branch.\n\nAnything happening on '2.0' branch will be limited to critical fixes for the 2.0\nrelease specifically.\n\n— Daniel Beck\n\nCompared to the\n2.0 beta release, the first\nrelease candidate has a number of fixes for issues discovered in the alpha and beta\nprocess. Most notable perhaps is the stabilization of a system property which\nconfiguration management tools, like Puppet/Chef/Ansible/etc, can use to suppress\nthe user-friendly Getting Started wizard. Since users of those tools\nhave alternative means of ensuring security and correctness of their Jenkins\ninstallations, the out-of-the-box experience can be skipped.\n\nBased on our\nrough\ntimeline this gives us a couple weeks to test the release candidates and get\nready for a big exciting release of 2.0 at the end of April!","title":"Jenkins 2.0 Release Candidate available!","tags":["jenkins2"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg","srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/77b35/rtyler.jpg 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/d4a57/rtyler.jpg 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/19e71/rtyler.jpg 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/68974/rtyler.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/ef6ff/rtyler.webp 32w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/8257c/rtyler.webp 64w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/6766a/rtyler.webp 128w,\n/gatsby-jenkins-io/static/c98b6de76465e89f9f5e5b331e9abfea/22bfc/rtyler.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/authors/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-04-07T00:00:00.000Z","id":"dc2d5625-7a35-5883-9c50-152fda1c584a","slug":"/blog/2016/04/07/jenkins-community-survey-results-blog/","strippedHtml":"This is a guest post by Brian\nDawson at CloudBees, where he works as a DevOps Evangelist responsible for\ndeveloping and sharing continuous delivery and DevOps best practices. He also\nserves as the CloudBees Product Marketing Manager for Jenkins.\n\nLast fall CloudBees asked attendees at the Jenkins User Conference – US West\n(JUC), and other in the Jenkins community to take a survey.  Almost 250 people\ndid – and thanks to their input, we have results which provided interesting\ninsights into how Jenkins is being used.\n\nBack in 2012, at the time of the last community survey, 83% of respondents felt\nthat Jenkins was mission-critical. By 2015, the percentage saying that\nJenkins was mission-critical was 92%. Additionally, echoing the\nimportance of Jenkins, 89% of respondents said their use of Jenkins had\nincreased over the last year, while 11% said it had stayed the same. 0%\nsaid that it had decreased.\n\nThe trend in the industry over the last couple of years has been to adopt\ncontinuous delivery (CD), thus pushing automation further down the pipeline –\nfrom development all the way into production.  Jenkins being an automation\nengine applicable to any phase of the software delivery lifecycle, is readily\nsupporting this trend. Jenkins' extensible architecture and unparalleled plugin\necosystem enables integration with and orchestration of practically any tool in\nany phase of software delivery.\n\nThe trend towards adoption of CD is clearly reflected amongst the community: 59%\nof respondents are using Jenkins for continuous integration (CI), but an\nadditional 30% have extended CI into CD and are manually deploying code to\nproduction.  Finally, 11% are practicing continuous deployment – they have\nextended CI to CD and are deploying code automatically into production.\n\nAnother trend tied to the adoption of CD and DevOps is the frequent deployment\nof incremental releases to production. 26% of those respondents using continuous\ndelivery practices are deploying code at least once per day.  Another 37% are\ndeploying code at least once per week.\n\nIn keeping with the move to CD, 30% of survey takers are already using the\nrelatively new Pipeline plugin to automate their\nsoftware delivery pipelines.  Of those not using the Pipeline plugin, 79% plan\nto adopt it in the next 12 months.\n\nSurvey respondents are also using Jenkins for many different activities.  97% of\nsurvey takers use it for \"build\" – no surprise, since that is where Jenkins got\nits start - but 58% now also use it for their deployment.\n\nWhen the 2012 community survey was conducted, container technology was not as\nwell understood as it is today,  and many didn’t know what a “Docker” was. A\nshort four years later, 96% of survey respondents who use Linux containers are\nusing Docker.  Container technology has seen impressive adoption and arguably is\nrevolutionizing the way application infrastructure is delivered.  When coupled\nwith Jenkins as an automation engine, containers help accelerate software\ndelivery by providing rapid access to lightweight environments.  The Jenkins\ncommunity has recognized and embraced the power of containers by\nproviding plugins for Docker and Kubernetes.\n\nThe Jenkins improvements which survey respondents desired the most were\nquality/timely bug fixes, a better UI and more documentation/examples.\nInterestingly, Jenkins 2.0 - which is just about to officially launch,\nprovides UI improvements and the new Jenkins.io website\nprovides improved, centralized documentation.\n\nFinally, the respondents favorite Star Wars character was R2-D2, followed by\nObi-Wan and Darth Vader. Yoda and Han Solo also got a fair amount of votes. The\nvotes for Jar-Jar Binks and Jabba the Hutt left us puzzled. Notably, BB-8 had a\nwrite-in vote despite the fact the new Star Wars movie hadn’t been released yet.\n\nAs to where the community is headed, our prediction is that by the next Jenkins Community Survey:\n\nMore Jenkins users will have transitioned from just continuous\nintegration to continuous delivery with some evening practicing continuous\ndeployment\n\nPipeline plugin adoption and improvements will continue, leading to\npipeline-as-code becoming an essential solution for automating the software\n(and infrastructure) delivery process\n\nThere will be a significant increase in use of the Docker plugin to support\nelastic Jenkins infrastructure and continuous delivery of containers using\nsoftware development best practices\n\nBB-8 will be the next favorite Star Wars character! <3</p>\n\nSee you at Jenkins World, September 13-15, in Santa Clara, California!\nRegister now for the largest Jenkins event on the planet in 2016 – and get the Early Bird discount. The Call for Papers is still open – so submit a talk and share your knowledge with the community about Jenkins.\n\n2015 Community Survey Results (PDF)\n\nState of Jenkins Infographic (PDF)","title":"Jenkins Community Survey Results","tags":["continuousdelivery","pipeline","docker"],"authors":[{"avatar":null,"blog":null,"github":"bvdawson","html":"<div class=\"paragraph\">\n<p>DevOps dude at CloudBees.\nJenkins Marketing Manager.\nTools geek.</p>\n</div>","id":"bvdawson","irc":null,"linkedin":null,"name":"Brian Dawson","slug":"/blog/authors/bvdawson","twitter":"brianvdawson"}]}}]}},"pageContext":{"limit":8,"skip":424,"numPages":101,"currentPage":54}},
    "staticQueryHashes": ["3649515864"]}