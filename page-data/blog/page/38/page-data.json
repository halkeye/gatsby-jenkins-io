{
    "componentChunkName": "component---src-templates-blog-list-template-js",
    "path": "/blog/page/38",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2017-06-13T00:00:00.000Z","id":"502035e4-b31c-5ecd-8870-ec66f6748d36","slug":"/blog/2017/06/13/blueocean-1-1/","strippedHtml":"The Blue Ocean team are proud to announce the release of Blue Ocean 1.1.\n We’ve shipped a tonne of small improvements, features and bug fixes here that\n will make your day-to-day experience with Blue Ocean even smoother.\n\nToday is also the first time we are promoting\nour Public Roadmap.\nWe recognise that using JIRA can be a bit of a pain to track what we are working\n on at a macro level and the Public Roadmap makes it very easy for anyone to\n find out what we are working on. We’ve got some really cool stuff coming,\n so check back here soon!\n\nIt’s been an insane two months since the launch of Blue Ocean 1.0 and there\n are now officially over 10,000 teams using Blue Ocean  – so here’s a big\n “thank you” to all of you for your support.\n\nNow, lets get to the goods!\n\nFast search\n\nFor those of you who have many pipelines we’ve introduced pipeline fast search\nto the pipeline dashboard. Click the search icon to activate and just start\ntyping what you’re looking for.\n\nTrigger reasons\n\nDifferentiate at a glance between pipeline runs that have been manually\ntriggered and by who, triggered automatically by a commit or triggered by any\nother means.\n\nBlockage reasons\n\nPipelines can be blocked from execution for a variety of reasons, including\nwaiting for executors or other resources to become free. You can see from the\nPipeline Activity, Branch and Result screen why the pipeline is blocked from\nexecution.\n\nHistory jump\n\nDevelopers can quickly jump from the branches tab to the run history for a\nspecific branch. This makes it more convenient to see historical runs for the\nbranch within the Pipeline which improves the your ability to trace down\nproblems.\n\nAnalyse 1,000s of tests\n\nNow you can see more than 100 test results for a Pipeline run. This makes\nBlue Ocean practical for teams who have invested heavily in testing.\nWe’ve also dramatically improved loading times for Pipelines with large\nnumbers of tests so theres no more waiting for the test tab to load.\n\nCustom run names and descriptions\n\nDevelopers authoring Pipeline using the scripted syntax can set a custom name\nand description for Pipeline run. This feature is commonly used to name or\ndescribe a pipeline run that is meaningful in their release management workflow.\n\nFor example, a developer can set the run name to the release version\n1.1 and the description to something meaningful, like Final Release.\n\ncurrentBuild.displayName = '1.1'\ncurrentBuild.description = ‘Final Release’\n\nPerformance\n\nWe’ve been making optimisations for general page speed.\nIn Blue Ocean 1.1, plugin data was automatically sent to browser and we’ve made\na change so that this data is only sent on the request of plugins. The long and\nshort of it is that you shouldn’t notice a thing except those Blue Ocean pages\nzipping faster into your browser.\n\n48+ bug fixes\n\nThere have been a total of 48 bug improvements, with emphasis on how executing\npipelines behave, and we’ve  invested a large amount of time to improve\nautomated test coverage of Blue Ocean to ensure reliability in\nproduction settings.\n\nFor a full list of bug fixes and improvements,\nsee the JIRA.\n\nWhat are you waiting for? Try Blue Ocean 1.1 today","title":"Blue Ocean 1.1 - fast search for pipelines and much more","tags":["blueocean"],"authors":[{"avatar":null,"blog":null,"github":"i386","html":"","id":"i386","irc":null,"linkedin":null,"name":"James Dumay","slug":"/blog/author/i386","twitter":"i386"}]}},{"node":{"date":"2017-05-18T00:00:00.000Z","id":"231f2272-acd6-5bb2-beae-e3871de86c48","slug":"/blog/2017/05/18/pipeline-dev-tools/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nI’ve only been working with Pipeline for about a year.\nPipeline in and of itself has been a huge improvement over old-style Jenkins projects.\nAs a developer, it has been so great be able work with Jenkins Pipelines\nusing the same tools I use for writing any other kind of code.\n\nI’ve also found a number of tools that are super helpful specifically\nfor developing pipelines. Some were easy to find like the\nbuilt-in documentation\nand the\nSnippet Generator.\nOthers were not as obvious or were only recently released.\nIn this post, I’ll show how a few of those tools make working with Pipelines\neven better.\n\nThe Blue Ocean Pipeline Editor\n\nThe best way to start this list is with the most recent and coolest\narrival in this space: the Blue Ocean Pipeline Editor.  The editor only works\nwith Declarative Pipelines, but it brings a sleek new user experience to writing\nPipelines.  My recent screencast, released as part of the Blue Ocean Launch,\ngives good sense of how useful the editor is:\n\nCommand-line Pipeline Linter\n\nOne of the neat features of the Blue Ocean Pipeline Editor is that it does basic\nvalidation on our Declarative Pipelines before they are even committed or Run.\nThis feature is based on the\nDeclarative Pipeline Linter\nwhich can be accessed from the command-line even if you don’t have Blue Ocean\ninstalled.\n\nWhen I was working on the\nDeclarative Pipeline: Publishing HTML Reports\nblog post, I was still learning the declarative syntax and I made a lot lot of mistakes.\nGetting quick feedback about the whether my Pipeline was in a sane state made writing that blog much easier.\nI wrote a simple shell script that would run my Jenkinsfile through the Declarative Pipeline Linter.\n\npipelint.sh - Linting via HTTP POST using curl\n\n# curl (REST API)\n# User\nJENKINS_USER=bitwisenote-jenkins1\n\n# Api key from \"/me/configure\" on my Jenkins instance\nJENKINS_USER_KEY=--my secret, get your own--\n\n# Url for my local Jenkins instance.\nJENKINS_URL=http://$JENKINS_USER:$JENKINS_USER_KEY@localhost:32769 (1)\n\n# JENKINS_CRUMB is needed if your Jenkins controller has CRSF protection enabled (which it should)\nJENKINS_CRUMB=`curl \"$JENKINS_URL/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\\\":\\\",//crumb)\"`\ncurl -X POST -H $JENKINS_CRUMB -F \"jenkinsfile=\n\n1\nThis is not secure.  I’m running this locally only.\nSee Jenkins CLI for details on how to do this securely.\n\nWith this script, I was able to find the error in this this Pipeline without\nhaving to take the time to run it in Jenkins: (Can you spot the mistake?)\n\n#!groovy\n\npipeline {\n  agent any\n\n  options {\n    // Only keep the 10 most recent builds\n    buildDiscarder(logRotator(numToKeepStr:'10'))\n  }\n  stages {\n    stage ('Install') {\n      steps {\n        // install required bundles\n        sh 'bundle install'\n      }\n    }\n    stage ('Build') {\n      steps {\n        // build\n        sh 'bundle exec rake build'\n      }\n\n      post {\n        success {\n          // Archive the built artifacts\n          archive includes: 'pkg/*.gem'\n        }\n      }\n    }\n    stage ('Test') {\n      step {\n        // run tests with coverage\n        sh 'bundle exec rake spec'\n      }\n\n      post {\n        success {\n          // publish html\n          publishHTML target: [\n              allowMissing: false,\n              alwaysLinkToLastBuild: false,\n              keepAll: true,\n              reportDir: 'coverage',\n              reportFiles: 'index.html',\n              reportName: 'RCov Report'\n            ]\n        }\n      }\n    }\n  }\n  post {\n    always {\n      echo \"Send notifications for result: ${currentBuild.result}\"\n    }\n  }\n}\n\nWhen I ran my pipelint.sh script on this pipeline it reported this error:\n\n$ pipelint.sh\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100    46  100    46    0     0   3831      0 --:--:-- --:--:-- --:--:--  4181\nErrors encountered validating Jenkinsfile:\nWorkflowScript: 30: Unknown stage section \"step\". Starting with version 0.5, steps in a stage must be in a steps block. @ line 30, column 5.\n       stage ('Test') {\n       ^\n\nWorkflowScript: 30: Nothing to execute within stage \"Test\" @ line 34, column 5.\n       stage ('Test') {\n       ^\n\nDoh. I forgot the \"s\" on steps on line 35. Once I added the \"s\" and ran\npipelint.sh again, I got an all clear.\n\n$ pipelint.sh\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100    46  100    46    0     0   5610      0 --:--:-- --:--:-- --:--:--  5750\nJenkinsfile successfully validated.\n\nThis didn’t mean there weren’t other errors, but for a two second smoke test I’ll take it.\n\nReplay\n\nI love being able to use source control to track changes to my Pipelines\nright alongside the rest of the code in a project.  There are also times,\nwhen prototyping or debugging, that I need to iterate quickly on a series\nof possible Pipeline changes.\nThe Replay feature let’s me do that and see the results,\nwithout committing those changes to source control.\n\nWhen I wanted to take the previous Pipeline from agent any to using Docker via\nthe docker { …​ } directive, I used the Replay feature to test it out:\n\nSelected the previously completed run in the build history\n\nClicked \"Replay\" in the left menu\n\nMade modifications and click \"Run\". In this example, I replaced any with the docker { …​ } directive.\n\nChecked the results of changes looked good.\n\nOnce I worked any bugs out of my Pipeline,\nI used Replay to view the Pipeline for the last run and copy it back to my\nJenkinsfile and create a commit for that change.\n\nConclusion\n\nThis is far from a complete list of the tools out there for working with Pipeline.\nThere are many more and the number is growing.\nFor example, one tool  I just recently heard about and haven’t had a chance to delve into\nis the\nPipeline Unit Testing Framework,\nwhich promises the ability to test Pipelines before running them.\nIt’s been a fun year and I can’t wait to see what the next year holds for Pipeline.\n\nHow do you work with Pipeline?\nDo you have a tool that you feel has greatly improved your development experience\nwith Pipeline?  I’m interested in hear about others Jenkins user’s favorite ways\nof working with Pipeline.  Drop me a line via\nemail or on the\n#jenkins IRC channel.","title":"Pipeline Development Tools","tags":["blueocean","pipeline","tutorial"],"authors":[{"avatar":{"childImageSharp":null},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/author/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2017-05-15T00:00:00.000Z","id":"355e4b5a-0c27-5f01-94a5-7a981139c2fb","slug":"/blog/2017/05/15/kubernetes-journey-on-azure/","strippedHtml":"With the\nongoing migration to Azure,\nI would like to share my thoughts regarding one of the biggest challenges we\nhave faced thus far: orchestrating container infrastructure. Many of the\nJenkins project’s applications are run as Docker containers, making Kubernetes\na logical choice as far as running our containers, but it presents its own set\nof challenges. For example, what would the workflow from development to\nproduction look like?\n\nBefore going deeper into the challenges, let’s review the requirements we\nstarted with:\n\nGit\n\nWe found it mandatory to keep track of all the infrastructure changes in Git\nrepositories, including secrets, in order to facilitate reviewing,\nvalidation, rollback, etc of all infra changes.\n\nTests\n\nInfrastructure contributors are geographically distributed and in different\ntimezones.  Getting feedback can take time, so we heavily rely on a lot of\ntests before any changes can be merged.\n\nAutomation\n\nThe change submitter is not necessarily the person who will deploy it.\nRepetitive tasks are error prone and a waste of time.\nFor these reasons, all steps must be automated and stay as simple as possible.\n\nA high level overview of our \"infrastructure as code\" workflow would look like:\n\nInfrastructure as Code Workflow\n\n__________       _________       ______________\n  |         |      |        |      |             |\n  | Changes | ---->|  Test  |----->| Deployment  |\n  |_________|      |________|  ^   |_____________|\n                               |\n                        ______________\n                       |             |\n                       | Validation  |\n                       |_____________|\n\nWe identified two possible approaches for implementing our container\norchestration with Kubernetes:\n\nThe Jenkins Way: Jenkins is triggered by a Git commit, runs the tests, and\nafter validation, Jenkins deploys changes into production.\n\nThe Puppet Way: Jenkins is triggered by a Git commit, runs the tests, and\nafter validation, it triggers Puppet to deploy into production.\n\nLet’s discuss these two approaches in detail.\n\nThe Jenkins Way\n\nWorkflow\n\n_________________       ____________________       ______________\n  |                |      |                   |      |             |\n  |    Github:     |      |     Jenkins:      |      |   Jenkins:  |\n  | Commit trigger | ---->| Test & Validation | ---->|  Deployment |\n  |________________|      |___________________|      |_____________|\n\nIn this approach, Jenkins is used to test, validate, and deploy our Kubernetes\nconfiguration files. kubectl can be run on a directory and is idempotent.\nThis means that we can run it as often as we want: the result will not change.\nTheoretically, this is the simplest way. The only thing needed is to run\nkubectl command each time Jenkins detects changes.\n\nThe following Jenkinsfile gives an example of this workflow.\n\nJenkinsfile\n\npipeline {\n    agent any\n    stages {\n      stage('Init'){\n        steps {\n          sh 'curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl'\n        }\n      }\n      stage('Test'){\n        steps {\n          sh 'Run tests'\n        }\n      }\n      stage('Deploy'){\n        steps {\n          sh './kubectl apply -R true -f my_project'\n        }\n      }\n    }\n  }\n\nThe devil is in the details of course, and it was not as easy as it looked at\nfirst sight.\n\nOrder matters\n\nSome resources needed to be deployed before others. A workaround was to use\nnumbers as file names. But this added extra logic at file name level, for\nexample:\n\nproject/00-nginx-ingress\nproject/09-www.jenkins.io\n\nPortability\n\nThe deployment environments needed to be the same across development machines\nand the Jenkins host. Although this a well-known problem, it was not easy to\nsolve.  The more the project grew, the more our scripts needed additional tools\n( make, bats, jq gpg, etc).  The more tools we used, the more issues\nappeared because of the different versions used.\n\nAnother challenge that emerged when dealing with different environments was:\nhow should we manage environment-specific configurations (dev, prod, etc)?\nWould it be better to define different configuration files per environment?\nPerhaps, but this means code duplication, or using file templates which would require\nmore tools ( sed, jinja2, erb), and more work.\n\nThere wasn’t a golden rule we discovered, and the answer is probably somewhere in between.\n\nIn any case, the good news is that a Jenkinsfile provides an easy way to\nexecute tasks from a Docker image, and an image can contain all the necessary\ntools in our environment. We can even use different Docker images for each\nstage along the way.\n\nIn the following example, I use the my_env Docker image. It contains all the\ntools needed to test, validate, and deploy changes.\n\nJenkinsfile\n\npipeline{\n  agent {\n    docker{\n      image 'my_env:1.0'\n    }\n  }\n  options{\n    buildDiscarder(logRotator(numToKeepStr: '10'))\n    disableConcurrentBuilds()\n    timeout(time: 1, unit: 'HOURS')\n  }\n  triggers{\n    pollSCM('* * * * *')\n  }\n  stages{\n    stage('Init'){\n      steps{\n        // Init everything required to deploy our infra\n        sh 'make init'\n      }\n    }\n    stage('Test'){\n      steps{\n       // Run tests to validate changes\n       sh 'make test'\n      }\n    }\n    stage('Deploy'){\n      steps{\n       // Deploy changes in production\n       sh 'make deploy'\n      }\n    }\n  }\n  post{\n    always {\n      sh 'make notify'\n    }\n  }\n}\n\nSecret credentials\n\nManaging secrets is a big subject and brings with it many different\nrequirements which are very hard to fulfill.  For obvious reasons, we couldn’t\npublish the credentials used within the infra project.  On the other hand, we\nneeded to keep track and share them, particularly for the Jenkins node that\ndeploys our cluster.  This means that we needed a way to encrypt or decrypt\nthose credentials depending on permissions, environments, etc.  We analyzed two\ndifferent approaches to handle this:\n\nStoring secrets in a key management tool like Key Vault or Vault and use them like a Kubernetes \"secret\" type of resource.\n→ Unfortunately, these tools are not yet integrated in Kubernetes. But we may come back to this option later.\nKubernetes issue: 10439\n\nPublishing and encrypting using a public GPG key.\nThis means that everybody can encrypt credentials for the infrastructure project but only the owner of the private key can decrypt credentials.\nThis solution implies:\n\nScripting: as secrets need to be decrypted at deployment time.\n\nTemplates: as secret values will change depending on the environment.\n→ Each Jenkins node should only have the private key to decrypt secrets associated to its environment.\n\nScripting\n\nFinally, the system we had built was hard to work with.  Our initial\nJenkinsfile which only ran one kubectl command slowly become a bunch of\nscripts to accommodate for:\n\nResources needing to be updated only in some situations.\n\nSecrets needing to be encrypted/decrypted.\n\nTests needing to be run.\n\nIn the end, the amount of scripts required to deploy the Kubernetes resources\nstarted to become unwieldy and we began asking ourselves: \"aren’t we\nre-inventing the wheel?\"\n\nThe Puppet Way\n\nThe Jenkins project already uses Puppet, so we decided to look at using Puppet\nto orchestrate our container deployment with Kubernetes.\n\nWorkflow\n\n_________________       ____________________       _____________\n  |                |      |                   |      |            |\n  |    Github:     |      |     Jenkins:      |      | Puppet:    |\n  | Commit trigger | ---->| Test & Validation | ---->| Deployment |\n  |________________|      |___________________|      |____________|\n\nIn this workflow, Puppet is used to template and deploy all Kubernetes\nconfigurations files needed to orchestrate our cluster.\nPuppet is also used to automate basic kubectl operations such as 'apply' or\n'remove' for various resources based on file changes.\n\nPuppet workflow\n\n______________________\n|                     |\n|  Puppet Code:       |\n|    .                |\n|    ├── apply.pp     |\n|    ├── kubectl.pp   |\n|    ├── params.pp    |\n|    └── resources    |\n|        ├── lego.pp  |\n|        └── nginx.pp |\n|_____________________|\n          |                                        _________________________________\n          |                                       |                                |\n          |                                       |  Host: Prod orchestrator       |\n          |                                       |    /home/k8s/                  |\n          |                                       |    .                           |\n          |                                       |    └── resources               |\n          | Puppet generate workspace             |        ├── lego                |\n          └-------------------------------------->|        │   ├── configmap.yaml  |\n            Puppet apply workspaces' resources on |        │   ├── deployment.yaml |\n          ----------------------------------------|        │   └── namespace.yaml  |\n          |                                       |        └── nginx               |\n          v                                       |            ├── deployment.yaml |\n ______________                                   |            ├── namespace.yaml  |\n |     Azure:  |                                  |            └── service.yaml    |\n | K8s Cluster |                                  |________________________________|\n |_____________|\n\nThe main benefit of this approach is letting Puppet manage the environment and run\ncommon tasks. In the following example, we define a Puppet class for Datadog.\n\nPuppet class for resource Datadog\n\n# Deploy datadog resources on kubernetes cluster\n#   Class: profile::kubernetes::resources::datadog\n#\n#   This class deploy a datadog agent on each kubernetes node\n#\n#   Parameters:\n#     $apiKey:\n#       Contain datadog api key.\n#       Used in secret template\nclass profile::kubernetes::resources::datadog (\n    $apiKey = base64('encode', $::datadog_agent::api_key, 'strict')\n  ){\n  include ::stdlib\n  include profile::kubernetes::params\n  require profile::kubernetes::kubectl\n\n  file { \"${profile::kubernetes::params::resources}/datadog\":\n    ensure => 'directory',\n    owner  => $profile::kubernetes::params::user,\n  }\n\n  profile::kubernetes::apply { 'datadog/secret.yaml':\n    parameters => {\n        'apiKey' => $apiKey\n    },\n  }\n  profile::kubernetes::apply { 'datadog/daemonset.yaml':}\n  profile::kubernetes::apply { 'datadog/deployment.yaml':}\n\n  # As secrets change do not trigger pods update,\n  # we must reload pods 'manually' in order to use updated secrets.\n  # If we delete a pod defined by a daemonset,\n  # this daemonset will recreate pods automatically.\n  exec { 'Reload datadog pods':\n    path        => [\"${profile::kubernetes::params::bin}/\"],\n    command     => 'kubectl delete pods -l app=datadog',\n    refreshonly => true,\n    environment => [\"KUBECONFIG=${profile::kubernetes::params::home}/.kube/config\"] ,\n    logoutput   => true,\n    subscribe   => [\n      Exec['apply datadog/secret.yaml'],\n      Exec['apply datadog/daemonset.yaml'],\n    ],\n  }\n}\n\n→\nMore \"resources\" examples\n\nLet’s compare the Puppet way with the challenges discovered with the Jenkins\nway.\n\nOrder Matters\n\nWith Puppet, it becomes easier to define priorities as\nPuppet provides relationship meta parameters and the function 'require' (see\nalso:\nPuppet\nrelationships).\n\nIn our Datadog example, we can be sure that deployment will respect the following order:\n\ndatadog/secret.yaml -> datadog/daemonset.yaml -> datadog/deployment.yaml\n\nCurrently, our Puppet code only applies configuration when it detects file\nchanges.  It would be better to compare local files with the cluster\nconfiguration in order to trigger the required updates, but we haven’t found a\ngood way to implement this yet.\n\nPortability\n\nAs Puppet is used to configure working environments, it becomes easier to be\nsure that all tools are present and correctly configured.  It’s also easier to\nreplicate environments and run tests on them with tools like\nRSpec-puppet, Serverspec or\nVagrant.\n\nIn our Datadog example, we can also easily change the Datadog API key depending\non the environment with Hiera.\n\nSecret credentials\n\nAs we were already using Hiera GPG\nwith Puppet, we decided to continue to use it, making managing secrets for\ncontainers very simple.\n\nScripting\n\nOf course the Puppet DSL is used, and even if it seems harder at the beginning,\nPuppet simplifies a lot the management of Kubernetes configuration files.\n\nConclusion\n\nIt was much easier to bootstrap the project with a full CI workflow within\nJenkins as long as the Kubernetes project itself stays basic. But as soon as\nthe project grew, and we started deploying different applications with\ndifferent configurations per environment, it became easier to delegate\nKubernetes management to Puppet.\n\nIf you have any comments feel free to send a message to\nJenkins Infra mailing list.\n\nThanks\n\nThanks to Lindsay Vanheyste, Jean Marc Meessen, and Damien Duportal for their feedback.","title":"A journey to Kubernetes on Azure","tags":["puppet","kubernetes","docker","azure"],"authors":[{"avatar":{"childImageSharp":null},"blog":null,"github":"olblak","html":"<div class=\"paragraph\">\n<p>Olivier is the Jenkins infrastructure officer and Senior Operations Engineer at CloudBees.\nAs a regular contributor to the Jenkins infrastructure projects, he works on a wide range of tasks from services reliability to applications maintenance.</p>\n</div>","id":"olblak","irc":"olblak","linkedin":null,"name":"Olivier Vernin","slug":"/blog/author/olblak","twitter":"0lblak"}]}},{"node":{"date":"2017-05-03T00:00:00.000Z","id":"b3e6239b-7f6a-59f1-aa10-c3a3c05df1da","slug":"/blog/2017/05/03/jenkinsworld-2017-awards/","strippedHtml":"This is a guest post by Alyssa Tong, who runs\nthe Jenkins Area Meetup program and is also responsible for\nMarketing & Community Programs at CloudBees, Inc.\n\nThis year at Jenkins World 2017,\nthe Jenkins community will celebrate the Most Valuable Contributor, a Jenkins\nSecurity MVP, and the Most Valuable Advocate.\n\nThis will be the first year we are commemorating community members who have\nshown excellence through commitment, creative thinking, and contributions to\ncontinue making Jenkins a great open source automation server. Special thanks\nto CloudBees for the generous donations to make\nthis program possible.\n\nWith that said, the Jenkins\nCommunity Award nomination is currently open. Nominate your story, or that\nof a fellow contributor, for recognition at Jenkins World. Ee sure to join us at\nJenkins World 2017 in San\nFrancisco on August 28-31 to hear the winners announced.\n\nNominations will be accepted until June 16, 2017.\nNominate someone\ntoday!","title":"Jenkins World 2017 Community Awards - Open for Nominations!","tags":["event","jenkinsworld","jenkinsworld2017"],"authors":[{"avatar":{"childImageSharp":null},"blog":null,"github":"alyssat","html":"<div class=\"paragraph\">\n<p>Member of the <a href=\"/sigs/advocacy-and-outreach/\">Jenkins Advocacy and Outreach SIG</a>.\nAlyssa drives and manages Jenkins participation in community events and conferences like <a href=\"https://fosdem.org/\">FOSDEM</a>, <a href=\"https://www.socallinuxexpo.org/\">SCaLE</a>, <a href=\"https://events.linuxfoundation.org/cdcon/\">cdCON</a>, and <a href=\"https://events19.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2019/\">KubeCon</a>.\nShe is also responsible for Marketing &amp; Community Programs at <a href=\"https://cloudbees.com\">CloudBees, Inc.</a></p>\n</div>","id":"alyssat","irc":null,"linkedin":null,"name":"Alyssa Tong","slug":"/blog/author/alyssat","twitter":null}]}},{"node":{"date":"2017-04-27T00:00:00.000Z","id":"c3440bc5-c7e2-5c2b-b6e6-f923222e2902","slug":"/blog/2017/04/27/colombia/","strippedHtml":"The Jenkins project has learned that a company is trying to register \"Jenkins\" as a trademark in Colombia. This is alarming for us, and we are trying to oppose it. In order to do this effectively, we need to hear from Colombian users of Jenkins.\n\nFigure 1. South American visitors to jenkins.io for 2017\n\nThe Jenkins project owns a trademark \"Jenkins\" in the U.S., through a non-profit entity SPI Inc. According to experts on the subject citing the \"Washington Convention\", our trademark registration in the U.S. does give us some strength in the argument to oppose this. To successfully mount this argument however, we need to be able to show that Jenkins has significant usage and awareness in Colombia. Users, installations, meetups, conference talks, anything of that nature will help.\n\nThose of you with the project for a long time might recall that the name \"Jenkins\" was born because of a trademark issue with Oracle. So we are particularly sensitive to the issue is trademarks. We want to make sure the same tragedy won’t happen again.\n\nIf you know anything about the usage and the name recognition of Jenkins in Colombia, please let us know by submitting the information here . We know that Jenkins is popular in Colombia, because our website traffic shows that Colombian Jenkins users are the third most frequent visitors to jenkins.io in South America after Brazil and Argentina.\n\nThis information will be only shared with the Jenkins project board and those involved in the defense, and for the sole purpose of defending the trademark and nothing more.\n\nPlease help us spread the word. Thanks!\n\nEl proyecto Jenkins se ha enterado de que una compañía está intentando registrar \"Jenkins\" como marca registrada en Colombia. Esto es alarmante y estamos tratando de oponernos. Para hacerlo de manera efectiva, necesitamos escuchar a los usuarios colombianos de Jenkins.\n\nEl proyecto Jenkins posee una marca registrada \"Jenkins\" en los Estados Unidos, a través de una entidad sin ánimo de lucro SPI Inc. Según los expertos en la materia citando la \"Convención de Washington\", nuestro registro de marca en los EE.UU. nos da algo de fuerza para oponernos. Sin embargo, para argumentar con éxito, tenemos que ser capaces de demostrar que Jenkins tiene un uso significativo y es conocido en Colombia. Usuarios, instalaciones, encuentros, conferencias, cualquier cosa de ese tipo ayudará.\n\nAquellos que llevan mucho tiempo con el proyecto pueden recordar que el nombre \"Jenkins\" nació debido a un problema de marca con Oracle. Por lo tanto, estamos especialmente sensibles al tema de las marcas registradas. Queremos asegurarnos de que el mismo problema no vuelva a ocurrir.\n\nSi sabe algo sobre el uso y el reconocimiento del nombre Jenkins en Colombia, por favor háganoslo saber enviando la información aquí . Sabemos que Jenkins es popular en Colombia, porque nuestro sitio web de tráfico muestra que los usuarios colombianos de Jenkins son los terceros visitantes más frecuentes a jenkins.io en América del Sur después de Brasil y Argentina.\n\nEsta información sólo se compartirá con el comité de proyecto de Jenkins y los involucrados en la defensa, y con el único propósito de defender la marca y nada más.\n\nPor favor, ayúdenos a difundir la palabra. ¡Gracias!","title":"Calling for Colombian Jenkins users!","tags":["feedback","general"],"authors":[{"avatar":{"childImageSharp":null},"blog":null,"github":"kohsuke","html":"<div class=\"paragraph\">\n<p>Kohsuke is the creator of Jenkins.</p>\n</div>","id":"kohsuke","irc":null,"linkedin":null,"name":"Kohsuke Kawaguchi","slug":"/blog/author/kohsuke","twitter":"kohsukekawa"}]}},{"node":{"date":"2017-04-26T00:00:00.000Z","id":"d81f1582-c3e8-5662-bd12-92e53401c9f2","slug":"/blog/2017/04/26/security-advisory/","strippedHtml":"We just released security updates to Jenkins, versions 2.57 and 2.46.2, that fix several security vulnerabilities, including a critical one.\n\nThat critical vulnerability is an unauthenticated remote code execution via the remoting-based CLI.\nWhen I announced the fix for the previous vulnerability of this kind, I announced our plans to revisit the design of the CLI that enabled this class of vulnerabilities.\n\nSince Jenkins 2.54, we now have a new CLI implementation that isn’t based on remoting, and deprecated its remoting mode.\nDespite it being a major feature, we decided to backport it to 2.46.2, so LTS users can also disable the unsafe remoting mode while retaining almost all of the CLI’s existing functionality.\n\nFor an overview of what was fixed, see the security advisory.\nFor an overview on the possible impact of these changes on upgrading Jenkins LTS, see our LTS upgrade guide.\nI recommend you read these documents, especially if you’re using the CLI with Jenkins LTS, as there are possible side effects of these fixes.\n\nSubscribe to the jenkinsci-advisories mailing list to receive important notifications related to Jenkins security.","title":"Important security updates for Jenkins core","tags":["core","security"],"authors":[{"avatar":null,"blog":null,"github":"daniel-beck","html":"<div class=\"paragraph\">\n<p>Daniel is a Jenkins core maintainer and, as security officer, leads the <a href=\"/security/#team\">Jenkins security team</a>.\nHe sometimes contributes to developer documentation and project infrastructure.</p>\n</div>","id":"daniel-beck","irc":null,"linkedin":null,"name":"Daniel Beck","slug":"/blog/author/daniel-beck","twitter":null}]}},{"node":{"date":"2017-04-20T00:00:00.000Z","id":"1824bec7-be57-5f54-a27b-537ad8ff5a34","slug":"/blog/2017/04/20/secure-jenkins-on-azure/","strippedHtml":"This is a guest post by Claudiu Guiman and Eric Jizba,\nSoftware Engineers in the Azure DevOps team at Microsoft. If you have any questions, please email us at azdevopspub@microsoft.com.\n\nOne of the most frequently asked questions for managing a Jenkins instance is\n\"How do I make it secure?\" Like any other web application, these issues must be\nsolved:\n\nHow do I securely pass secrets between the browser and the server?\n\nHow do I hide certain parts from unauthorized users and show other parts to anonymous users?\n\nThis blog post details how to securely connect to a Jenkins instance and how to\nsetup a read-only public dashboard.  We’ll cover topics like: setting up a\nreverse proxy, blocking inbound requests to certain URLs and ports, enabling\nproject-based authorization, and making the Jenkins agents accessible through\nthe JNLP protocol.\n\nDeploy Jenkins\n\nThe simplest way to deploy a secure Jenkins instance is by using the Azure Marketplace offer. If you have an existing Jenkins instance or want to setup your instance manually, follow the steps below.\n\nSecurely log in to Jenkins\n\nAfter you’ve deployed your new virtual machine with a hosted Jenkins instance, you will notice that by default the instance listens on port 8080 using 'HTTP'. If you want to set up 'HTTPS' communication, you will need to provide an SSL certificate. Unfortunately, most certificate authorities are not cheap and other free services like Let’s Encrypt have a very small quota (about 20 certificates per week for the entire 'azure.com' subdomain). The only other option is to use a self-signed certificate, but then users must explicitly verify and mark your certificate as trusted, which is not recommended.\n\nIf you do not setup 'HTTPS' communication, the best way to make sure the sign-in credentials are not leaked due to a Man-in-the-middle attack is to only log in using SSH tunneling.\nAn SSH tunnel is an encrypted tunnel created through an SSH protocol connection, which can be used to transfer unencrypted traffic over an unsecured network. Simply run this command:\n\nLinux or Mac\n\nssh -L 8080:localhost:8080 @\n\nWindows ( using PuTTY)\n\nputty.exe -ssh -L 8080:localhost:8080 @\n\nThis command will open an SSH connection to your remote host and bind remote port 8080 to listen to requests coming from your local machine. Navigate to http://localhost:8080 on your local machine to view your Jenkins dashboard and you’ll be able to log in securely.\n\nSetup a reverse proxy\n\nNow that you can securely log in to your Jenkins instance, you should prevent people from accidentally authenticating through the public (unsecured) interface. To achieve this, you can setup a reverse proxy on the Jenkins hosting machine that will listen on a different port (80 is the best candidate) and redirect only certain requests to port 8080.\n\nSpecifically, it is recommended to block the login and the CLI requests. Some CLI versions fall back to unsecure HTTP connections if they have problems establishing the secured connection. In most cases, users don’t need the CLI and it should be enabled on an as-needed basis.\n\nInstall Nginx:\n\nsudo apt-get update\nsudo apt-get install nginx\n\nOpen the Nginx config file:\n\nsudo nano /etc/nginx/sites-enabled/default\n\nModify the file to configure Nginx to work as a reverse proxy (you’ll need to update):\n\nserver {\n    listen 80;\n    server_name;\n    # Uncomment the line bellow to change the default 403 error page\n    # error_page 403 /secure-jenkins;\n    location / {\n        proxy_set_header        Host \\$host:\\$server_port;\n        proxy_set_header        X-Real-IP \\$remote_addr;\n        proxy_set_header        X-Forwarded-For \\$proxy_add_x_forwarded_for;\n        proxy_set_header        X-Forwarded-Proto \\$scheme;\n        proxy_pass              http://localhost:8080;\n        proxy_redirect          http://localhost:8080 http://;\n        proxy_read_timeout      90;\n    }\n    #block requests to /cli\n    location /cli {\n        deny all;\n    }\n    #block requests to /login\n    location ~ /login* {\n        deny all;\n    }\n    # Uncomment the lines bellow to redirect /secure-jenkins\n    #location /secure-jenkins {\n    #  alias /usr/share/nginx/secure-jenkins;\n    #}\n}\n\nThe first section tells the Nginx server to listen to any requests that come from port 80. It also contains a commented redirect of the 403 error to a custom location (we’ll get back to this later).\n\nlisten 80;\n    server_name;\n    # error_page 403 /secure-jenkins;\n\nThe next section describes the reverse proxy configuration. This tells the Nginx server to take all incoming requests and proxy them to the Jenkins instance that is listening to port 8080 on the local network interface.\n\nlocation / {\n        proxy_set_header        Host \\$host:\\$server_port;\n        proxy_set_header        X-Real-IP \\$remote_addr;\n        proxy_set_header        X-Forwarded-For \\$proxy_add_x_forwarded_for;\n        proxy_set_header        X-Forwarded-Proto \\$scheme;\n        proxy_pass              http://localhost:8080;\n        proxy_redirect          http://localhost:8080 http://;\n        proxy_read_timeout      90;\n    }\n\nThe last section filters out specific URLs (login, cli) and denies access to them.\n\nlocation /cli {\n        deny all;\n    }\n    location ~ /login* {\n        deny all;\n    }\n\nRestart Nginx:\n\nsudo service nginx restart\n\nGo to http:// and verify you can access your Jenkins instance.\n\nVerify clicking 'login' returns a '403 Forbidden' page. If you want to customize that page, update the Nginx configuration and remove the comments around /secure-jenkins. This will redirect all 403 errors to the file /usr/share/nginx/secure-jenkins. You can add any content to that file, for example:\n\nsudo mkdir /usr/share/nginx/secure-jenkins\necho \"Access denied! Use SSH tunneling to log in to your Jenkins instance!\" | sudo tee /usr/share/nginx/secure-jenkins/index.html\n\nIf restart fails or you cannot access your instance, check the error log: cat /var/log/nginx/error.log\n\nSecure your Jenkins dashboard\n\nIf you go to http:// :8080 you’ll notice you can still\nbypass the reverse proxy and access the Jenkins instance directly through an\nunsecure channel. You can easily block all inbound requests on port 8080 on\nAzure with a\nNetwork\nSecurity Group (NSG).\n\nCreate the NSG and add it to your existing network interface or to the subnet your Azure Virtual Machine is bound to.\n\nAdd 2 inbound security rules:\n\nAllow requests to port 22 so you can SSH into the machine.\n\nAllow requests to port 80 so the reverse proxy can be reached\n\nBy default, all other external traffic will be blocked\n\nNavigate to http:// :8080 and verify you cannot connect.\n\nIf you don’t want to deploy an Azure Network Security Group, you can block port 8080 using the Uncomplicated Firewall (ufw)\n\nConfigure read-only access to your dashboard\n\nAfter installing Jenkins, the default security strategy is 'Logged-in users can do anything'. If you want to allow read-only access to anonymous users, you need to set up Matrix-based security. In this example, we’ll set up a project-based authorization matrix, so that you can make certain projects private and others public.\n\nInstall the Matrix Authorization Strategy Plugin and restart Jenkins.\n\nGo to http://localhost:8080/configureSecurity/ ('Configure Global Security' page under 'Manage Jenkins') and select 'Project-base Matrix Authorization Strategy' from the 'Authorization' options.\n\nAs an example, you can grant read-only access to anonymous users (Overall/Read, Job/Discover and Job/Read should be enough) and grant all logged in users full access in a  group called 'authenticated':\n\nConnect JNLP-based agents\n\nSince your Jenkins instance is only accessible through the reverse proxy on port 80, any Jenkins agents that use the JNLP protocol will not be able to register to the controller anymore. To overcome this problem, all agents must be in the same virtual network as the Jenkins controller and must connect using their private IP (by default, the NSG allows all internal traffic).\n\nMake sure that the Jenkins virtual machine will always be assigned the same private IP by going to the Azure Portal, opening the Network Interface of your virtual machine, opening 'IP configuration', and clicking on the configuration.\n\nMake sure the Private IP has a static assignment and restart the virtual machine if necessary.\n\nCopy the static IP Address and go to http://localhost:8080/configure ('Configure System' page under 'Manage Jenkins') and update the 'Jenkins URL' to point to that private IP ( https://10.0.0.5:8080/ in this example)\n\nNow agents can communicate through JNLP. If you want to streamline the process,\nyou can use the\nAzure VM Agents plugin,\nwhich automatically deploys agents in the same virtual network\nand connects them to the controller.","title":"Securing a Jenkins instance on Azure","tags":["azure","cloud"],"authors":[{"avatar":null,"blog":null,"github":"clguimanMSFT","html":"","id":"clguiman","irc":null,"linkedin":null,"name":"Claudiu Guiman","slug":"/blog/author/clguiman","twitter":null}]}},{"node":{"date":"2017-04-18T00:00:00.000Z","id":"714ef576-2af6-5d69-a880-a3280836f4e4","slug":"/blog/2017/04/18/continuousdelivery-devops-sonarqube/","strippedHtml":"This is a guest post by Michael Hüttermann. Michael is an expert\nin Continuous Delivery, DevOps and SCM/ALM. More information about him at huettermann.net, or\nfollow him on Twitter: @huettermann.\n\nContinuous Delivery and DevOps are well known and widely spread practices nowadays. It is commonly accepted that it\nis crucial to form great teams and define shared goals first and then choose and integrate the tools fitting best to\ngiven tasks. Often it is a mashup of lightweight tools, which are integrated to build up Continuous Delivery pipelines\nand underpin DevOps initiatives. In this blog post, we zoom in to an important part of the overall pipeline, that is the discipline\noften called Continuous Inspection, which comprises inspecting code and injecting a quality gate on that, and show how artifacts can\nbe uploaded after the quality gate was met. DevOps enabler tools covered are Jenkins, SonarQube, and Artifactory.\n\nThe Use Case\n\nYou already know that quality cannot be injected after the fact, rather it should be part of the process and product from the very beginning.\nAs a commonly used good practice, it is strongly recommended to inspect the code and make findings visible, as soon as possible.\nFor that SonarQube is a great choice. But SonarQube is not just running on any isolated\nisland, it is integrated in a Delivery Pipeline. As part of the pipeline, the code is inspected, and only if the code is fine according to defined\nrequirements, in other words: it meets the quality gates, the built artifacts are uploaded to the binary repository manager.\n\nLet’s consider the following scenario. One of the busy developers has to fix code, and checks in changes to the central\nversion control system. The day was long and the night short, and against all team commitments the developer\ndid not check the quality of the code in the local sandbox. Luckily, there is the build engine Jenkins\nwhich serves as a single point of truth, implementing the Delivery Pipeline with its native pipeline features, and as a handy coincidence\nSonarQube has support for Jenkins pipeline.\n\nThe change triggers a new run of the pipeline. Oh no! The build pipeline broke, and the change is not further processed.\nIn the following image you see that a defined quality gate was missed. The visualizing is done with Jenkins Blue Ocean.\n\nSonarQube inspection\n\nWhat is the underlying issue? We can open the SonarQube web application and drill down to the finding. In the Java code, obviously a string literal is not placed on the right side.\n\nDuring a team meeting it was decided to define this to be a Blocker, and SonarQube was configured accordingly. Furthermore, a SonarQube quality gate was created to break any build, if a blocker was identified. Let’s now quickly look into the code.\nYes, SonarQube is right, there is the issue with the following code snippet.\n\nWe do not want to discuss in detail all used tools, and also covering the complete Jenkins build job would be out of scope.\nBut the interesting extract here in regard of the inspection is the following stage defined in Jenkins pipeline DSL:\n\nconfig.xml: SonarQube inspection\n\nstage('SonarQube analysis') { (1)\nwithSonarQubeEnv('Sonar') { (2)\nsh 'mvn org.sonarsource.scanner.maven:sonar-maven-plugin:3.3.0.603:sonar ' + (3)\n'-f all/pom.xml ' +\n          '-Dsonar.projectKey=com.huettermann:all:master ' +\n          '-Dsonar.login=$SONAR_UN ' +\n          '-Dsonar.password=$SONAR_PW ' +\n          '-Dsonar.language=java ' +\n          '-Dsonar.sources=. ' +\n          '-Dsonar.tests=. ' +\n          '-Dsonar.test.inclusions=**/*Test*/** ' +\n          '-Dsonar.exclusions=**/*Test*/**'\n        }\n    }\n\n1\nThe dedicated stage for running the SonarQube analysis.\n\n2\nAllow to select the SonarQube server you want to interact with.\n\n3\nRunning and configuring the scanner, many options available, check the docs.\n\nMany options are available to integrate and configure SonarQube. Please consult the documentation for alternatives. Same applies to the other covered tools.\n\nSonarQube Quality Gate\n\nAs part of a Jenkins pipeline stage, SonarQube is configured to run and inspect the code. But this is just the first part,\nbecause we now also want to add the quality gate in order to break the build. The next stage is covering exactly that, see\nnext snippet. The pipeline is paused until the quality gate is computed, specifically the waitForQualityGate step will pause the\npipeline until SonarQube analysis is completed and returns the quality gate status. In case a quality gate was missed, the build breaks.\n\nconfig.xml: SonarQube Quality Gate\n\nstage(\"SonarQube Quality Gate\") { (1)\ntimeout(time: 1, unit: 'HOURS') { (2)\ndef qg = waitForQualityGate() (3)\nif (qg.status != 'OK') {\n             error \"Pipeline aborted due to quality gate failure: ${qg.status}\"\n           }\n        }\n    }\n\n1\nThe defined quality gate stage.\n\n2\nA timeout to define when to proceed without waiting for any results for ever.\n\n3\nHere we wait for the OK. Underlying implementation is done with SonarQube’s webhooks feature.\n\nThis blog post is an appetizer, and scripts are excerpts. For more information, please consult the respective documentation, or a good book, or the great community, or ask your local expert.\n\nSince they all work in a wonderful Agile team, the next available colleague just promptly fixes the issue. After checking in\nthe fixed code, the build pipeline runs again.\n\nThe pipeline was processed successfully, including the SonarQube quality gate, and as the final step, the packaged and tested artifact was\ndeployed to Artifactory. There are a couple of different flexible ways how to upload the artifacts,\nthe one we use here is using an upload spec to actually collect and upload the artifact which was built at the very beginning of the pipeline.\nAlso meta information are published to Artifactory, since it is the context which matters and thus we can add valuable labels to the artifact for further processing.\n\nconfig.xml: Upload to Artifactory\n\nstage ('Distribute binaries') { (1)\ndef SERVER_ID = '4711' (2)\ndef server = Artifactory.server SERVER_ID\n    def uploadSpec = (3)\"\"\"\n    {\n    \"files\": [\n        {\n            \"pattern\": \"all/target/all-(*).war\",\n            \"target\": \"libs-snapshots-local/com/huettermann/web/{1}/\"\n        }\n      ]\n    }\n    \"\"\"\n    def buildInfo = Artifactory.newBuildInfo() (4)\nbuildInfo.env.capture = true (5)\nbuildInfo=server.upload(uploadSpec) (6)\nserver.publishBuildInfo(buildInfo) (7)\n}\n\n1\nThe stage responsible for uploading the binary.\n\n2\nThe server can be defined Jenkins wide, or as part of the build step, as done here.\n\n3\nIn the upload spec, in JSON format, we define what to deploy to which target, in a fine-grained way.\n\n4\nThe build info contains meta information attached to the artifact.\n\n5\nWe want to capture environmental data.\n\n6\nUpload of artifact, according to upload spec.\n\n7\nBuild info are published as well.\n\nNow let’s see check that the binary was deployed to Artifactory, successfully. As part of the context information, also a reference to the\nproducing Jenkins build job is available for better traceability.\n\nSummary\n\nIn this blog post, we’ve discovered tips and tricks to integrate Jenkins with SonarQube, how to define\nJenkins stages with the Jenkins pipeline DSL, how those stages are visualized with Jenkins Blue Ocean, and how the artifact\nwas deployed to our binary repository manager Artifactory.\nNow I wish you a lot of further fun with your great tools of choice to implement your Continuous Delivery pipelines.\n\nReferences\n\nJenkins 2\n\nSonarqube\n\nSonarqube Jenkins plugin\n\nArtifactory\n\nJenkins Artifactory plugin\n\n'DevOps for Developers', Apress, 2012\n\n'Agile ALM', Manning, 2011","title":"Delivery Pipelines, with Jenkins 2, SonarQube, and Artifactory","tags":["quality","sonarqube","jenkins","artifactory"],"authors":[{"avatar":{"childImageSharp":null},"blog":"http://huettermann.net","github":"michaelhuettermann","html":"<div class=\"paragraph\">\n<p>Michael is expert in Continuous Delivery, DevOps and SCM/ALM supporting enterprises in implementing DevOps.\nMichael is Jenkins Ambassador.</p>\n</div>","id":"michaelhuettermann","irc":null,"linkedin":null,"name":"Michael Hüttermann","slug":"/blog/author/michaelhuettermann","twitter":"huettermann"}]}}]}},"pageContext":{"limit":8,"skip":296,"numPages":100,"currentPage":38}},
    "staticQueryHashes": ["3649515864"]}