{
    "componentChunkName": "component---src-templates-blog-list-template-js",
    "path": "/blog/page/38",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2017-07-05T00:00:00.000Z","id":"fd2d8b89-b05f-5caf-a6b1-923d33960065","slug":"/blog/2017/07/05/continuousdelivery-devops-artifactory/","strippedHtml":"This is a guest post by Michael Hüttermann. Michael is an expert\nin Continuous Delivery, DevOps and SCM/ALM. More information about him at huettermann.net, or\nfollow him on Twitter: @huettermann.\n\nIn a past blog post Delivery Pipelines,\nwith Jenkins 2, SonarQube, and Artifactory, we talked about pipelines which result in binaries for development versions. Now, in this blog post, I zoom in to different parts of the\nholistic pipeline and cover the handling of possible downstream steps once you have the binaries of development versions, in our example a Java EE WAR and a Docker image (which contains the WAR).\nWe discuss basic concept of staging software, including further information about quality gates, and show example toolchains. This contribution particularly examines the staging from binaries from\ndev versions to release candidate versions and from release candidate versions to final releases from the perspective of the automation server Jenkins, integrating with the binary\nrepository manager JFrog Artifactory and the distribution management platform JFrog Bintray, and ecosystem.\n\nStaging software\n\nStaging (also often called promoting) software is the process of completely and consistently transferring a release with all its configuration items\nfrom one environment to another. This is even more true with DevOps, where you want to accelerate the cycle time (see Michael Hüttermann, DevOps for Developers (Apress, 2012), 38ff).\nFor accelerating the cycle time, meaning to bring software to production, fast and in good quality, it is crucial to have fine processes and integrated tools to streamline the\ndelivery of software. The process of staging releases consists of deploying software to different staging levels, especially different test environments.\nStaging also involves configuring the software for various environments without needing to recompile or rebuild the software. Staging is necessary\nto transport the software to production systems in high quality. Many Agile projects make great experience with implementing a staging ladder in\norder to optimize the cycle time between development software and the point when the end user is able to use the software in production.\n\nCommonly, the staging ladder is illustrated on its side, with the higher rungs being the boxes further to the right. It’s good practice not to skip any rungs during staging.\nThe central development environment packages and integrates all respective configuration items and is the base for releasing. Software is staged over different environments by\nconfiguration, without rebuilding. All changes go through the entire staging process, although defined exception routines may be in place,\nfor details see Michael Hüttermann, Agile ALM (Manning, 2012).\n\nTo make concepts clearer, this blog post covers sample tools. Please note, that there are also alternative tools available. As one example: Sonatype Nexus is also able to host the covered binaries and also offers scripting functionality.\n\nWe nowadays often talk about delivery pipelines. A pipeline is just a set of stages and transition rules between those stages. From a DevOps perspective, a pipeline bridges multiple\nfunctions in organizations, above all development and operations. A pipeline is a staging ladder. A change enters the pipeline at the beginning and leaves it at the end. The processing\ncan be triggered automatically (typical for delivery pipelines) or by a human actor (typical for special steps at overall pipelines, e.g. pulling and thus cherry-picking specific\nversions to promote them to be release candidates are final releases).\n\nPipelines often look different, because they strongly depend on requirements and basic conditions, and can contain further sub pipelines. In our scenario, we have two sub pipelines to\nmanage the promotion of continuous dev versions to release candidates and the promotion of release candidates to final release. A change typically waits at a stage for further processing\naccording to the transition rules, aligned with defined requirements to meet, which are the Quality Gates, explored next.\n\nQuality Gates\n\nQuality gates allow the software to pass through stages only if it meets their defined requirements. The next illustration shows a staging ladder with quality gates injected. You and\nother engaged developers commit code to the version control system (please, use VCS as an abbreviation, not SCM, because the latter is much more) in order to update the central test\nenvironment only if the code satisfies the defined quality requirements; for instance, the local build may need to run successfully and have all tests pass locally. Build, test, and\nmetrics should pass out of the central development environment, and then automated and manual acceptance tests are needed to pass the system test. In our case, the last quality gate\nto pass is the one from the  production mirror to production. Here, for example, specific production tests are done or relevant documents must be filled in and signed.\n\nIt’s mandatory to define the quality requirements in advance and to resist customizing them after the fact, when the software has failed. Quality gates are different at lower and\nhigher stages; the latter normally consist of a more severe or broader set of quality requirements, and they often include the requirements of the lower gates. The binary repository\nmanager must underpin corresponding quality gates, while managing the binaries, what we cover next.\n\nThis blog post illustrates typical concepts and sample toolchains. For more information, please consult the respective documentation, good books or attend top notch conferences, e.g.\nJenkins World, powered by CloudBees.\n\nBinary repository manager\n\nA central backbone of the staging ladder is the binary repository manager, e.g. JFrog Artifactory. The binary repository manager manages all binaries including the self-produced\nones (producing view) and the 3rd party ones (consuming view), across all artifact types, in our case a Java EE WAR file and a Docker image. Basic idea here is that the repo manager serves\nas a proxy, thus all developers access the repo manager, and not remote binary pools directly, e.g. Maven Central. The binary repository manager offers cross-cutting services,\ne.g. role-based access control on specific logical repositories, which may correspond to specific stages of the staging ladder.\n\nLogical repositories can be generic ones (meaning they are agnostic regarding any tools and platforms, thus you can also just upload the menu of your local canteen) or repos\nspecific to tools and platforms. In our case, we need a repository for managing the Java EE WAR files and for the Docker images. This can be achieved by\n\na generic repository (preferred for higher stages) or a repo which is aligned with the layout of the Maven build tool, and\n\na repository for managing Docker images, which serves as a Docker registry.\n\nIn our scenario, preparing the staging of artifacts includes the following ramp-up activities\n\nCreating two sets of logical repositories, inside JFrog Artifactory, where each set has a repo for the WAR file and a repo for the Docker image, and one set is for managing dev\nversions and one set is for release candidate versions.\n\nDefining and implementing processes to promote the binaries from the one set of repositories (which is for dev versions) to the other set of repositories (which is for RC versions).\nPart of the process is defining roles, and JFrog Artifactory helps you to implement role-based access control.\n\nSetting up procedures or scripts to bring binaries from one set of repositories to the other set of repositories, reproducibly. Adding meta data to binaries is important if the degree of maturity\nof the binary cannot be easily derived from the context.\n\nThe following illustration shows a JFrog Artifactory instance with the involved logical repos in place. In our simplified example, the repo promotions are supposed to go from\ndocker-local to docker-prod-local, and from libs-release-local to libs-releases-staging-local. In our use case, we promote the software in version 1.0.0.\n\nAnother type of binary repository manager is JFrog Bintray, which serves as a universal distribution platform for many technologies. JFrog Bintray can be an interesting choice\nif you have strong requirements for scalability and worldwide coverage including IP restrictions and handy features around statistics. Most of the concepts and ramp up activities\n are similar compared to JFrog Artifactory, thus I do not want to repeat them here. Bintray is used by lot of projects e.g. by Groovy, to host their deliverables in the public.\n But keep in mind that you can of course also host your release binaries in JFrog Artifactory.\n In this blog post, I’d like to introduce different options, thus we promote our release candidates to JFrog Artifactory and our releases to JFrog Bintray.\n Bintray has the concept of products, packages and versions. A product can have multiple packages and has different versions. In our example, the product has two packages, namely the Java EE WAR and\n the Docker image, and the concrete version that will be processed is 1.0.0.\n\nSome tool features covered in this blog post are available as part of commercial offerings of tool vendors. Examples include the Docker support of JFrog Artifactory or the Firehose Event API of JFrog Bintray.\nPlease consult the respective documentation for more information.\n\nNow it is time to have a deeper look at the pipelines.\n\nImplementing Pipelines\n\nOur example pipelines are implemented with Jenkins, including its Blue Ocean and declarative pipelines facilities, JFrog Artifactory and JFrog Bintray. To derive your personal\npipelines, please check your individual requirements and basic conditions to come up with the best solution for your target architecture, and consult the respective documentation for\n more information, e.g. about scripting the tools.\n\nIn case your development versions are built with Maven, and have SNAPSHOT character, you need to either rebuild the software after setting the release version, as part of\nyour pipeline, or you solely use Maven releases from the very beginning. Many projects make great experience with morphing Maven snapshot versions into\nrelease versions, as part of the pipeline, by using a dedicated Maven plugin, and externalizing it into a Jenkins shared library. This can look like the following:\n\nsl.groovy (excerpt): A Jenkins shared library, to include in Jenkins pipelines.\n\n#!/usr/bin/groovy\n    def call(args) { (1)\necho \"Calling shared library, with ${args}.\"\n       sh \"mvn com.huettermann:versionfetcher:1.0.0:release versions:set -DgenerateBackupPoms=false -f ${args}\" (2)\n}\n\n1\nWe provide a global variable/function to include it in our pipelines.\n\n2\nThe library calls a Maven plugin, which dynamically morphs the snapshot version of a Maven project to a release version.\n\nAnd including it into the pipeline is then also very straight forward:\n\npipeline.groovy (excerpt): A stage calling a Jenkins shared library.\n\nstage('Produce RC') { (1)\nreleaseVersion 'all/pom.xml' (2)\n}\n\n1\nThis stage is part of a scripted pipeline and is dedicated to morphing a Maven snapshot version into a release version, dynamically.\n\n2\nWe call the Jenkins shared library, with a parameter pointing to the Maven POM file, which can be a parent POM.\n\nYou can find the code of the underlying Maven plugin here.\n\nLet’s now discuss how to proceed for the release candidates.\n\nRelease Candidate (RC)\n\nThe pipeline to promote a dev version to a RC version does contain a couple of different stages, including stages to certify the binaries (meaning labeling it or adding context information) and stages to process the concrete promotion.\nThe following illustration shows the successful run of the promotion, for software version 1.0.0.\n\nWe utilize Jenkins Blue Ocean that is a new user experience for Jenkins based on a personalizable, modern design that allows users to graphically create, visualize and diagnose\ndelivery pipelines. Besides the new approach in general, single Blue Ocean features help to boost productivity dramatically, e.g. to provide log information at your fingertips\nand the ability to search pipelines. The stages to perform the promote are as follows starting with the  Jenkins pipeline stage for promoting the WAR file. Keep in mind that all\nscripts are parameterized, including variables for versions and Artifactory domain names, which are either injected to the pipeline run by user input or set system wide in the Jenkins admin panel,\nand the underlying call is using the JFrog command line interface, CLI in short. JFrog Artifactory\nas well as JFrog Bintray can be used and managed by scripts, based on a REST API. The JFrog CLI\nis an abstraction on top of the JFrog REST API, and we show sample usages of both.\n\npipeline.groovy (excerpt): Staging WAR file to different logical repository\n\nstage('Promote WAR') { (1)\nsteps { (2)\nsh 'jfrog rt cp --url=https://$ARTI3 --apikey=$artifactory_key --flat=true libs-release-local/com/huettermann/web/$version/ ' + (3)\n'libs-releases-staging-local/com/huettermann/web/$version/'\n       }\n    }\n\n1\nThe dedicated stage for running the promotion of the WAR file.\n\n2\nHere we have the steps which make up the stage, based on Jenkins declarative pipeline syntax.\n\n3\nCopying the WAR file, with JFrog CLI, using variables, e.g. the domain name of the Artifactory installation. Many options available, check the docs.\n\nThe second stage to explore more is the promotion of the Docker image. Here, I want to show you a different way how to achieve the goal, thus in this use case we utilize the JFrog REST API.\n\npipeline.grovvy (excerpt): Promote Docker image\n\nstage('Promote Docker Image') {\n          sh '''curl -H \"X-JFrog-Art-Api:$artifactory_key\" -X POST https://$ARTI3/api/docker/docker-local/v2/promote ''' + (1)\n'''-H \"Content-Type:application/json\" ''' + (2)\n'''-d \\'{\"targetRepo\" : \"docker-prod-local\", \"dockerRepository\" : \"michaelhuettermann/tomcat7\", \"tag\": \"\\'$version\\'\", \"copy\": true }\\' (3)\n'''\n    }\n\n1\nThe shell script to perform the staging of Docker image is based on JFrog REST API.\n\n2\nPart of parameters are sent in JSON format.\n\n3\nThe payload tells the REST API endpoint what to to, i.e. gives information about target repo and tag.\n\nOnce the binaries are promoted (and hopefully deployed and tested on respective environments before), we can promote them to become final releases, which I like to call GA.\n\nGeneral Availability (GA)\n\nIn our scenario, JFrog Bintray serves as the distribution platform to manage and provide binaries for further usage. Bintray can also serve as a Docker registry, or can just\nprovide binaries for scripted or manual download. There are again different ways how to promote binaries, in this case from the RC repos inside JFrog Artifactory to the GA storage in JFrog Bintray, and I summarize one of those possible ways. First, let’s look at the Jenkins pipeline, showed in the next illustration. The processing is on its way, currently, and we again have a list of linked stages.\n\nZooming in now to the key stages, we see that promoting the WAR file is a set of steps that utilize JFrog REST API. We download the binary from JFrog Artifactory, parameterized,\nand upload it to JFrog Bintray.\n\npipeline.groovy (excerpt): Promote WAR to Bintray\n\nstage('Promote WAR to Bintray') {\n       steps {\n          sh '''\n             curl -u michaelhuettermann:${bintray_key} -X DELETE https://api.bintray.com/packages/huettermann/meow/cat/versions/$version (1)\ncurl -u michaelhuettermann:${bintray_key} -H \"Content-Type: application/json\" -X POST https://api.bintray.com/packages/huettermann/meow/cat/$version --data \"\"\"{ \"name\": \"$version\", \"desc\": \"desc\" }\"\"\" (2)\ncurl -T \"$WORKSPACE/all-$version-GA.war\" -u michaelhuettermann:${bintray_key} -H \"X-Bintray-Package:cat\" -H \"X-Bintray-Version:$version\" https://api.bintray.com/content/huettermann/meow/ (3)\ncurl -u michaelhuettermann:${bintray_key} -H \"Content-Type: application/json\" -X POST https://api.bintray.com/content/huettermann/meow/cat/$version/publish --data '{ \"discard\": \"false\" }' (4)\n'''\n       }\n    }\n\n1\nFor testing and demo purposes, we remove the existing release version.\n\n2\nNext we create the version in Bintray, in our case the created version is 1.0.0. The value was insert by user while triggering the pipeline.\n\n3\nThe upload of the WAR file.\n\n4\nBintray needs a dedicated publish step to make the binary publicly available.\n\nProcessing the Docker image is as easy as processing the WAR. In this case, we just push the Docker image to the Docker registry, which is served by JFrog Bintray.\n\npipeline.groovy (excerpt): Promote Docker image to Bintray\n\nstage('Promote Docker Image to Bintray') { (1)\nsteps {\n          sh 'docker push $BINTRAYREGISTRY/michaelhuettermann/tomcat7:$version' (2)\n}\n    }\n\n1\nThe stage for promoting the Docker image. Please note, depending on your setup, you may add further stages, e.g. to login to your Docker registry.\n\n2\nThe Docker push of the specific version. Note, that also here all variables are parameterized.\n\nWe now have promoted the binaries and uploaded them to JFrog Bintray. The overview page of our product lists two packages: the WAR file and the Docker image. Both can be downloaded\nnow and used, the Docker image can be pulled from the JFrog Bintray Docker registry with native Docker commands.\n\nAs part of its graphical visualization capabilitites, Bintray is able to show the single layers of the uploaded Docker images.\n\nBintray can also display usage statistics, e.g. download details. Now guess where I’m sitting right now while downloading the binary?\n\nBesides providing own statistics, Bintray provides the JFrog Firehose Event API. This API streams live usage data, which in turn can be integrated or aggregated with your ecosystem.\nIn our case, we visualize the data, particularly download, upload, and delete statistics, with the ELK stack, as part of a functional monitoring initiative.\n\nCrisp, isn’t it?\n\nSummary\n\nThis closes are quick ride through the world of staging binaries, based on Jenkins. We’ve discussed concepts and example DevOps enabler tools, which can help to implement\n the concepts. Along the way, we discussed some more options how to integrate with ecosystem, e.g. releasing Maven snapshots and functional monitoring with dedicated tools.\n After this appetizer you may want to now consider to double-check your staging processes and toolchains, and maybe you find some room for further adjustments.\n\nReferences\n\n'Agile ALM', Manning, 2011\n\nBinary Repository Manager Feature Matrix\n\n'DevOps for Developers', Apress, 2012\n\nDocker\n\nELK\n\nJFrog Artifactory\n\nJFrog Bintray\n\nJFrog CLI\n\nJFrog REST API\n\nSonatype Nexus","title":"Delivery pipelines, with Jenkins 2: how to promote Java EE and Docker binaries toward production.","tags":["devops","jenkins","artifactory","bintray"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/c09ea/michaelhuettermann.jpg","srcSet":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/534e5/michaelhuettermann.jpg 32w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/99887/michaelhuettermann.jpg 64w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/c09ea/michaelhuettermann.jpg 128w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/76fd4/michaelhuettermann.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/59a6b/michaelhuettermann.webp 32w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/cbb78/michaelhuettermann.webp 64w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/96250/michaelhuettermann.webp 128w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/50511/michaelhuettermann.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":171}}},"blog":"http://huettermann.net","github":"michaelhuettermann","html":"<div class=\"paragraph\">\n<p>Michael is expert in Continuous Delivery, DevOps and SCM/ALM supporting enterprises in implementing DevOps.\nMichael is Jenkins Ambassador.</p>\n</div>","id":"michaelhuettermann","irc":null,"linkedin":null,"name":"Michael Hüttermann","slug":"/blog/authors/michaelhuettermann","twitter":"huettermann"}]}},{"node":{"date":"2017-07-03T00:00:00.000Z","id":"3eff5455-8949-570e-ae59-6f180d034c6f","slug":"/blog/2017/07/03/contributor-summit/","strippedHtml":"As in previous years, there’ll be a contributor summit at Jenkins World 2017 :\n\nLet’s talk about the future of Jenkins and how you can help shape it! The contributor summit is the place where the current and future contributors of the Jenkins project get together. This year, the theme is “working together”. Traditionally, most contributors are plugin maintainers focusing on their own plugins, but it’s important to look at Jenkins as a whole, because that’s how users experience it. There is more to the project beyond writing plugins, and even for plugin developers, there are increasing number of common libraries and modules that plugins should rely on. In short, we should work better together.\n\nA few contributors will prepare some presentations to help clarify what that means, and what all of us can do. And in the afternoon, there will be \"unconference\" sessions to brainstorm and discuss what we can do and how.\n\nWhether you are already a contributor or just thinking about becoming one, please join us for this full day free event.\n\nDetails about this year’s agenda are available on the event’s meetup page.\nAttending is free, and no Jenkins World ticket is needed, but RSVP if you’re going to attend to help us plan.\n\nSee you there!","title":"Jenkins World Contributor Summit","tags":["event","jenkinsworld","jenkinsworld2017"],"authors":[{"avatar":null,"blog":null,"github":"daniel-beck","html":"<div class=\"paragraph\">\n<p>Daniel is a Jenkins core maintainer and, as security officer, leads the <a href=\"/security/#team\">Jenkins security team</a>.\nHe sometimes contributes to developer documentation and project infrastructure.</p>\n</div>","id":"daniel-beck","irc":null,"linkedin":null,"name":"Daniel Beck","slug":"/blog/authors/daniel-beck","twitter":null}]}},{"node":{"date":"2017-06-27T00:00:00.000Z","id":"c68cdf2d-4588-5bfa-80b3-2a0c1581eedd","slug":"/blog/2017/06/27/speaker-blog-SAS-jenkins-world/","strippedHtml":"This is a guest post by Brent Laster, Senior Manager, Research and Development at\nSAS.\n\nJenkins Pipeline\nhas fundamentally changed how users can orchestrate their pipelines and workflows.\nEssentially, anything that you can do in a script or program can now be done in a Jenkinsfile or in a pipeline script created within the application.\nBut just because you can do nearly anything directly in those mechanisms doesn’t mean you necessarily should.\n\nIn some cases, it’s better to abstract the functionality out separately from your main Pipeline.\nPreviously, the main way to do this in Jenkins itself was through creating plugins.\nWith Jenkins 2 and the tight incorporation of Pipeline, we now have another approach – shared libraries.\n\nBrent will be\npresenting\nmore of this topic at Jenkins World in\nAugust, register with the code JWFOSS for a 30% discount off your pass.\n\nShared libraries\nprovide solutions for a number of situations that can be challenging or time-consuming to deal with in Pipeline.\nAmong them:\n\nProviding common routines that can be accessed across a number of pipelines or within a designated scope (more on scope later)\n\nAbstracting out complex or restricted code\n\nProviding a means to execute scripted code from calls in declarative pipelines (where scripted code is not normally allowed)\n\nSimplifying calls in a script to custom code that only differ by calling parameters\n\nTo understand how to use shared libraries in Pipeline, we first need to understand how they are constructed.\nA shared library for Jenkins consists of a source code repository with a structure like the one below:\n\nEach of the top-level directories has its own purpose.\n\nThe resources directory can have non-groovy resources that get loaded via the libraryResource step.\nThink of this as a place to store supporting data files such as json files.\n\nThe src directory uses a structure similar to the standard Java src layout.\nThis area is added to the classpath when a Pipeline that includes this shared library is executed.\n\nThe vars directory holds global variables that should be accessible from pipeline scripts.\nA corresponding.txt file can be included that defines documentation for objects here.\nIf found, this will be pulled in as part of the documentation in the Jenkins application.\n\nAlthough you might think that it would always be best to define library functions in the src structure, it actually works better in many cases to define them in the vars area.\nThe notion of a global variable may not correspond very well to a global function, but you can think of it as the function being a global value that can be pulled in and used in your pipeline.\nIn fact, to work in a declarative style pipeline, having your function in the vars area is the only option.\n\nLet’s look at a simple function that we can create for a shared library.\nIn this case, we’ll just wrap picking up the location of the Gradle installation from Jenkins and calling the corresponding executable with whatever tasks are passed in as arguments.\nThe code is below:\n\n/vars/gbuild.groovy\n\ndef call(args) {\n      sh \"${tool 'gradle3'}/bin/gradle ${args}\"\n}\n\nNotice that we are using a structured form here with the def call syntax.\nThis allows us to simply invoke the routine in our pipeline (assuming we have loaded the shared library) based on the name of the file in the vars area.\nFor example, since we named this file gbuild.groovy, then we can invoke it in our pipeline via a step like this:\n\ngbuild 'clean compileJava'\n\nSo, how do we get our shared library loaded to use in our pipeline?\nThe shared library itself is just code in the structure outlined above committed/pushed into a source code repository that Jenkins can access.\nIn our example, we’ll assume we’ve staged, committed, and pushed this code into a local Git repository on the system at /opt/git/shared-library.git.\n\nLike most other things in Jenkins, we need to first tell Jenkins where this shared library can be found and how to reference it \"globally\" so that pipelines can reference it specifically.\n\nFirst, though, we need to decide at what scope you want this shared library to be available.\nThe most common case is making it a \"global shared library\" so that all Pipelines can access it.\nHowever, we also have the option of only making shared libraries available for projects in a particular Jenkins Folder structure,\nor those in a Multibranch Pipeline, or those in a GitHub Organization pipeline project.\n\nTo keep it simple, we’ll just define ours to be globally available to all pipelines.\nDoing this is a two-step process.\nWe first tell Jenkins what we want to call the library and define some default behavior for Jenkins related to the library,\nsuch as whether we wanted it loaded implicitly for all pipelines.\nThis is done in the Global Pipeline Libraries section of the Configure System page.\n\nFor the second part, we need to tell Jenkins where the actual source repository for the shared library is located.\nSCM plugins that have been modified to understand how to work with shared libraries are called \" Modern SCM\".\nThe git plugin in one of these updated plugin, so we just supply the information in the same Configure System page.\n\nAfter configuring Jenkins so that it can find the shared library repository, we can load the shared library into our pipeline using the @Library(' ') annotation.\nSince Annotations\nare designed to annotate something that follows them,\nwe need to either include a specific import statement, or, if we want to include everything, we can use an underscore character as a placeholder.\nSo our basic step to load the library in a pipeline would be:\n\n@Library('Utilities2') _\n\nBased on this step, when Jenkins runs our Pipeline, it will first go out to the repository that holds the shared library and clone down a copy to use.\nThe log output during this part of the pipeline execution would look something like this:\n\nLoading library Utilities2@master\n > git rev-parse --is-inside-work-tree # timeout=10\nSetting origin to /opt/git/shared-libraries\n > git config remote.origin.url /opt/git/shared-libraries # timeout=10\nFetching origin...\nFetching upstream changes from origin\n > git --version # timeout=10\nusing GIT_SSH to set credentials Jenkins2 SSH\n > git fetch --tags --progress origin +refs/heads/*:refs/remotes/origin/*\n > git rev-parse master^{commit} # timeout=10\n > git rev-parse origin/master^{commit} # timeout=10\nCloning the remote Git repository\nCloning repository /opt/git/shared-libraries\n\nThen Pipeline can call our shared library gbuild function and translate it to the desired Gradle build commands.\n\nFirst time build.\nSkipping changelog.\n[Pipeline] }\n[Pipeline] // stage\n[Pipeline] stage\n[Pipeline] { (Compile)\n[Pipeline] tool\n[Pipeline] sh\n[gsummit17_lab2-4T357CUTJORMC2TIF7WW5LMRR37F7PM2QRUHXUNSRTWTTRHB3XGA]\nRunning shell script\n+ /usr/share/gradle/bin/gradle clean compileJava -x test\nStarting a Gradle Daemon (subsequent builds will be faster)\n\nThis is a very basic illustration of how using shared libraries work.\nThere is much more detail and functionality surrounding shared libraries, and extending your pipeline in general, than we can cover here.\n\nBe sure to catch my talk on\nExtending your Pipeline with Shared Libraries, Global Functions and External Code\nat Jenkins World 2017.\nAlso, watch for my new book on\nJenkins 2 Up and Running\nwhich will have a dedicated chapter on this – expected to be available later this year from O’Reilly.","title":"Extending your Pipeline with Shared Libraries, Global Functions and External Code","tags":["event","jenkinsworld","jenkinsworld2017"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"","id":"hinman","irc":null,"linkedin":null,"name":"Hannah Inman","slug":"/blog/authors/hinman","twitter":null}]}},{"node":{"date":"2017-06-26T00:00:00.000Z","id":"7b02e5d8-2c15-5100-9be1-7256902c46eb","slug":"/blog/2017/06/26/share-jenkins-world-keynote-stage/","strippedHtml":"Jenkins World is approaching fast,\nand the event staff are all busy preparing.\nI’ve decided to do something different this year as part of my keynote:\nI want to invite a few Jenkins users like you come up on stage with me.\n\nThere have been amazing developments in Jenkins over the past year.\nFor my keynote, I want highlight how the new Jenkins\n(Pipeline as code with the Jenkinsfile, no more creating jobs,\nBlue Ocean)\nis different and better than the old Jenkins (freestyle jobs, chaining jobs together, etc.).\nAll these developments have helped Jenkins users,\nand it would be more meaningful to have fellow users, like you, share their stories\nabout how recent Jenkins improvements like Pipeline and Blue Ocean have positively impacted them.\n\nIf you’re interested sharing your story, please complete\nthis form\nso that I can contact you.\nThis is a great opportunity to let\nthe rest of the world (and your boss!) hear about your accomplishments.\nYou’ll also get into Jenkins World for free and get to join me backstage.\nIf you concerns about traveling to Jenkins World,\nI’m happy to discuss helping with that as well.\n\nI look forward to hearing from you.","title":"Come Share the Jenkins World Keynote Stage with Me!","tags":["event","jenkinsworld","jenkinsworld2017"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/edb43/kohsuke.jpg","srcSet":"/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/f81fe/kohsuke.jpg 32w,\n/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/01b1b/kohsuke.jpg 64w,\n/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/edb43/kohsuke.jpg 128w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/035c3/kohsuke.webp 32w,\n/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/273f8/kohsuke.webp 64w,\n/gatsby-jenkins-io/static/dd191cfa3b1158515bff16d455e6117b/e3840/kohsuke.webp 128w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":148}}},"blog":null,"github":"kohsuke","html":"<div class=\"paragraph\">\n<p>Kohsuke is the creator of Jenkins.</p>\n</div>","id":"kohsuke","irc":null,"linkedin":null,"name":"Kohsuke Kawaguchi","slug":"/blog/authors/kohsuke","twitter":"kohsukekawa"}]}},{"node":{"date":"2017-06-14T00:00:00.000Z","id":"d17e39c6-5d60-518a-9ff6-8280ace32930","slug":"/blog/2017/06/14/jenkinsworld-awards-lastcall/","strippedHtml":"This is a guest post by Alyssa Tong, who runs\nthe Jenkins Area Meetup program and is also responsible for\nMarketing & Community Programs at CloudBees, Inc.\n\nWe have received a good number of nominations for the Jenkins World 2017 Community Awards. These nominations are indicative of the excellent work Jenkins members are doing for the betterment of Jenkins.\n\nThe deadline for nomination is this Friday, June 16.\n\nThis will be the first year we are commemorating community members who have\nshown excellence through commitment, creative thinking, and contributions to\ncontinue making Jenkins a great open source automation server. The award\ncategories includes:\n\nMost Valuable Contributor -\nThis award is presented to the Jenkins contributor who has helped move the Jenkins project forward the most through their invaluable feature contributions, bug fixes or plugin development efforts.\n\nJenkins Security MVP -\nThis award is presented to the individual most consistently providing excellent security reports or who helped secure Jenkins by fixing security issues.\n\nMost Valuable Advocate -\nThis award is presented to an individual who has helped advocate for Jenkins through organization of their local Jenkins Area Meetup.\n\nSubmit your story, or nominate someone today! Winners will be announced at Jenkins World 2017 in San Francisco on August 28-31.\n\nWe look forward to hearing about the great Jenkins work you are doing.","title":"Jenkins World 2017 Community Awards - Last Call for Nominations!","tags":["event","jenkinsworld","jenkinsworld2017"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#281818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg","srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/8d248/alyssat.jpg 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/c004c/alyssat.jpg 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/a55dc/alyssat.jpg 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/9e67b/alyssat.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/22924/alyssat.webp 32w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/89767/alyssat.webp 64w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/40d97/alyssat.webp 128w,\n/gatsby-jenkins-io/static/fee8c4b53c42d9c396735bc2b737ff84/5028e/alyssat.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":166}}},"blog":null,"github":"alyssat","html":"<div class=\"paragraph\">\n<p>Member of the <a href=\"/sigs/advocacy-and-outreach/\">Jenkins Advocacy and Outreach SIG</a>.\nAlyssa drives and manages Jenkins participation in community events and conferences like <a href=\"https://fosdem.org/\">FOSDEM</a>, <a href=\"https://www.socallinuxexpo.org/\">SCaLE</a>, <a href=\"https://events.linuxfoundation.org/cdcon/\">cdCON</a>, and <a href=\"https://events19.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2019/\">KubeCon</a>.\nShe is also responsible for Marketing &amp; Community Programs at <a href=\"https://cloudbees.com\">CloudBees, Inc.</a></p>\n</div>","id":"alyssat","irc":null,"linkedin":null,"name":"Alyssa Tong","slug":"/blog/authors/alyssat","twitter":null}]}},{"node":{"date":"2017-06-13T00:00:00.000Z","id":"502035e4-b31c-5ecd-8870-ec66f6748d36","slug":"/blog/2017/06/13/blueocean-1-1/","strippedHtml":"The Blue Ocean team are proud to announce the release of Blue Ocean 1.1.\n We’ve shipped a tonne of small improvements, features and bug fixes here that\n will make your day-to-day experience with Blue Ocean even smoother.\n\nToday is also the first time we are promoting\nour Public Roadmap.\nWe recognise that using JIRA can be a bit of a pain to track what we are working\n on at a macro level and the Public Roadmap makes it very easy for anyone to\n find out what we are working on. We’ve got some really cool stuff coming,\n so check back here soon!\n\nIt’s been an insane two months since the launch of Blue Ocean 1.0 and there\n are now officially over 10,000 teams using Blue Ocean  – so here’s a big\n “thank you” to all of you for your support.\n\nNow, lets get to the goods!\n\nFast search\n\nFor those of you who have many pipelines we’ve introduced pipeline fast search\nto the pipeline dashboard. Click the search icon to activate and just start\ntyping what you’re looking for.\n\nTrigger reasons\n\nDifferentiate at a glance between pipeline runs that have been manually\ntriggered and by who, triggered automatically by a commit or triggered by any\nother means.\n\nBlockage reasons\n\nPipelines can be blocked from execution for a variety of reasons, including\nwaiting for executors or other resources to become free. You can see from the\nPipeline Activity, Branch and Result screen why the pipeline is blocked from\nexecution.\n\nHistory jump\n\nDevelopers can quickly jump from the branches tab to the run history for a\nspecific branch. This makes it more convenient to see historical runs for the\nbranch within the Pipeline which improves the your ability to trace down\nproblems.\n\nAnalyse 1,000s of tests\n\nNow you can see more than 100 test results for a Pipeline run. This makes\nBlue Ocean practical for teams who have invested heavily in testing.\nWe’ve also dramatically improved loading times for Pipelines with large\nnumbers of tests so theres no more waiting for the test tab to load.\n\nCustom run names and descriptions\n\nDevelopers authoring Pipeline using the scripted syntax can set a custom name\nand description for Pipeline run. This feature is commonly used to name or\ndescribe a pipeline run that is meaningful in their release management workflow.\n\nFor example, a developer can set the run name to the release version\n1.1 and the description to something meaningful, like Final Release.\n\ncurrentBuild.displayName = '1.1'\ncurrentBuild.description = ‘Final Release’\n\nPerformance\n\nWe’ve been making optimisations for general page speed.\nIn Blue Ocean 1.1, plugin data was automatically sent to browser and we’ve made\na change so that this data is only sent on the request of plugins. The long and\nshort of it is that you shouldn’t notice a thing except those Blue Ocean pages\nzipping faster into your browser.\n\n48+ bug fixes\n\nThere have been a total of 48 bug improvements, with emphasis on how executing\npipelines behave, and we’ve  invested a large amount of time to improve\nautomated test coverage of Blue Ocean to ensure reliability in\nproduction settings.\n\nFor a full list of bug fixes and improvements,\nsee the JIRA.\n\nWhat are you waiting for? Try Blue Ocean 1.1 today","title":"Blue Ocean 1.1 - fast search for pipelines and much more","tags":["blueocean"],"authors":[{"avatar":null,"blog":null,"github":"i386","html":"","id":"i386","irc":null,"linkedin":null,"name":"James Dumay","slug":"/blog/authors/i386","twitter":"i386"}]}},{"node":{"date":"2017-05-18T00:00:00.000Z","id":"231f2272-acd6-5bb2-beae-e3871de86c48","slug":"/blog/2017/05/18/pipeline-dev-tools/","strippedHtml":"This is a guest post by Liam Newman,\nTechnical Evangelist at CloudBees.\n\nI’ve only been working with Pipeline for about a year.\nPipeline in and of itself has been a huge improvement over old-style Jenkins projects.\nAs a developer, it has been so great be able work with Jenkins Pipelines\nusing the same tools I use for writing any other kind of code.\n\nI’ve also found a number of tools that are super helpful specifically\nfor developing pipelines. Some were easy to find like the\nbuilt-in documentation\nand the\nSnippet Generator.\nOthers were not as obvious or were only recently released.\nIn this post, I’ll show how a few of those tools make working with Pipelines\neven better.\n\nThe Blue Ocean Pipeline Editor\n\nThe best way to start this list is with the most recent and coolest\narrival in this space: the Blue Ocean Pipeline Editor.  The editor only works\nwith Declarative Pipelines, but it brings a sleek new user experience to writing\nPipelines.  My recent screencast, released as part of the Blue Ocean Launch,\ngives good sense of how useful the editor is:\n\nCommand-line Pipeline Linter\n\nOne of the neat features of the Blue Ocean Pipeline Editor is that it does basic\nvalidation on our Declarative Pipelines before they are even committed or Run.\nThis feature is based on the\nDeclarative Pipeline Linter\nwhich can be accessed from the command-line even if you don’t have Blue Ocean\ninstalled.\n\nWhen I was working on the\nDeclarative Pipeline: Publishing HTML Reports\nblog post, I was still learning the declarative syntax and I made a lot lot of mistakes.\nGetting quick feedback about the whether my Pipeline was in a sane state made writing that blog much easier.\nI wrote a simple shell script that would run my Jenkinsfile through the Declarative Pipeline Linter.\n\npipelint.sh - Linting via HTTP POST using curl\n\n# curl (REST API)\n# User\nJENKINS_USER=bitwisenote-jenkins1\n\n# Api key from \"/me/configure\" on my Jenkins instance\nJENKINS_USER_KEY=--my secret, get your own--\n\n# Url for my local Jenkins instance.\nJENKINS_URL=http://$JENKINS_USER:$JENKINS_USER_KEY@localhost:32769 (1)\n\n# JENKINS_CRUMB is needed if your Jenkins controller has CRSF protection enabled (which it should)\nJENKINS_CRUMB=`curl \"$JENKINS_URL/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\\\":\\\",//crumb)\"`\ncurl -X POST -H $JENKINS_CRUMB -F \"jenkinsfile=\n\n1\nThis is not secure.  I’m running this locally only.\nSee Jenkins CLI for details on how to do this securely.\n\nWith this script, I was able to find the error in this this Pipeline without\nhaving to take the time to run it in Jenkins: (Can you spot the mistake?)\n\n#!groovy\n\npipeline {\n  agent any\n\n  options {\n    // Only keep the 10 most recent builds\n    buildDiscarder(logRotator(numToKeepStr:'10'))\n  }\n  stages {\n    stage ('Install') {\n      steps {\n        // install required bundles\n        sh 'bundle install'\n      }\n    }\n    stage ('Build') {\n      steps {\n        // build\n        sh 'bundle exec rake build'\n      }\n\n      post {\n        success {\n          // Archive the built artifacts\n          archive includes: 'pkg/*.gem'\n        }\n      }\n    }\n    stage ('Test') {\n      step {\n        // run tests with coverage\n        sh 'bundle exec rake spec'\n      }\n\n      post {\n        success {\n          // publish html\n          publishHTML target: [\n              allowMissing: false,\n              alwaysLinkToLastBuild: false,\n              keepAll: true,\n              reportDir: 'coverage',\n              reportFiles: 'index.html',\n              reportName: 'RCov Report'\n            ]\n        }\n      }\n    }\n  }\n  post {\n    always {\n      echo \"Send notifications for result: ${currentBuild.result}\"\n    }\n  }\n}\n\nWhen I ran my pipelint.sh script on this pipeline it reported this error:\n\n$ pipelint.sh\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100    46  100    46    0     0   3831      0 --:--:-- --:--:-- --:--:--  4181\nErrors encountered validating Jenkinsfile:\nWorkflowScript: 30: Unknown stage section \"step\". Starting with version 0.5, steps in a stage must be in a steps block. @ line 30, column 5.\n       stage ('Test') {\n       ^\n\nWorkflowScript: 30: Nothing to execute within stage \"Test\" @ line 34, column 5.\n       stage ('Test') {\n       ^\n\nDoh. I forgot the \"s\" on steps on line 35. Once I added the \"s\" and ran\npipelint.sh again, I got an all clear.\n\n$ pipelint.sh\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100    46  100    46    0     0   5610      0 --:--:-- --:--:-- --:--:--  5750\nJenkinsfile successfully validated.\n\nThis didn’t mean there weren’t other errors, but for a two second smoke test I’ll take it.\n\nReplay\n\nI love being able to use source control to track changes to my Pipelines\nright alongside the rest of the code in a project.  There are also times,\nwhen prototyping or debugging, that I need to iterate quickly on a series\nof possible Pipeline changes.\nThe Replay feature let’s me do that and see the results,\nwithout committing those changes to source control.\n\nWhen I wanted to take the previous Pipeline from agent any to using Docker via\nthe docker { …​ } directive, I used the Replay feature to test it out:\n\nSelected the previously completed run in the build history\n\nClicked \"Replay\" in the left menu\n\nMade modifications and click \"Run\". In this example, I replaced any with the docker { …​ } directive.\n\nChecked the results of changes looked good.\n\nOnce I worked any bugs out of my Pipeline,\nI used Replay to view the Pipeline for the last run and copy it back to my\nJenkinsfile and create a commit for that change.\n\nConclusion\n\nThis is far from a complete list of the tools out there for working with Pipeline.\nThere are many more and the number is growing.\nFor example, one tool  I just recently heard about and haven’t had a chance to delve into\nis the\nPipeline Unit Testing Framework,\nwhich promises the ability to test Pipelines before running them.\nIt’s been a fun year and I can’t wait to see what the next year holds for Pipeline.\n\nHow do you work with Pipeline?\nDo you have a tool that you feel has greatly improved your development experience\nwith Pipeline?  I’m interested in hear about others Jenkins user’s favorite ways\nof working with Pipeline.  Drop me a line via\nemail or on the\n#jenkins IRC channel.","title":"Pipeline Development Tools","tags":["blueocean","pipeline","tutorial"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#382818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg","srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/77b35/lnewman.jpg 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/d4a57/lnewman.jpg 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/19e71/lnewman.jpg 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/68974/lnewman.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/ef6ff/lnewman.webp 32w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/8257c/lnewman.webp 64w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/6766a/lnewman.webp 128w,\n/gatsby-jenkins-io/static/9717c5c33fe8f4903eec2f2b5a8d1532/22bfc/lnewman.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"bitwiseman","html":"<div class=\"paragraph\">\n<p>Liam started his software career as a tester, which might explain why he&#8217;s such a fan of CI/CD and Pipeline as Code.\nHe has spent the majority of his software engineering career implementing Continuous Integration systems at companies big and small.\nHe is a Jenkins project contributor and an expert in Jenkins Pipeline, both Scripted and Declarative.\nLiam currently works as a Jenkins Evangelist at <a href=\"https://cloudbees.com\">CloudBees</a>.\nWhen not at work, he enjoys testing gravity by doing Aikido.</p>\n</div>","id":"lnewman","irc":null,"linkedin":null,"name":"Liam Newman","slug":"/blog/authors/lnewman","twitter":"bitwiseman"}]}},{"node":{"date":"2017-05-15T00:00:00.000Z","id":"355e4b5a-0c27-5f01-94a5-7a981139c2fb","slug":"/blog/2017/05/15/kubernetes-journey-on-azure/","strippedHtml":"With the\nongoing migration to Azure,\nI would like to share my thoughts regarding one of the biggest challenges we\nhave faced thus far: orchestrating container infrastructure. Many of the\nJenkins project’s applications are run as Docker containers, making Kubernetes\na logical choice as far as running our containers, but it presents its own set\nof challenges. For example, what would the workflow from development to\nproduction look like?\n\nBefore going deeper into the challenges, let’s review the requirements we\nstarted with:\n\nGit\n\nWe found it mandatory to keep track of all the infrastructure changes in Git\nrepositories, including secrets, in order to facilitate reviewing,\nvalidation, rollback, etc of all infra changes.\n\nTests\n\nInfrastructure contributors are geographically distributed and in different\ntimezones.  Getting feedback can take time, so we heavily rely on a lot of\ntests before any changes can be merged.\n\nAutomation\n\nThe change submitter is not necessarily the person who will deploy it.\nRepetitive tasks are error prone and a waste of time.\nFor these reasons, all steps must be automated and stay as simple as possible.\n\nA high level overview of our \"infrastructure as code\" workflow would look like:\n\nInfrastructure as Code Workflow\n\n__________       _________       ______________\n  |         |      |        |      |             |\n  | Changes | ---->|  Test  |----->| Deployment  |\n  |_________|      |________|  ^   |_____________|\n                               |\n                        ______________\n                       |             |\n                       | Validation  |\n                       |_____________|\n\nWe identified two possible approaches for implementing our container\norchestration with Kubernetes:\n\nThe Jenkins Way: Jenkins is triggered by a Git commit, runs the tests, and\nafter validation, Jenkins deploys changes into production.\n\nThe Puppet Way: Jenkins is triggered by a Git commit, runs the tests, and\nafter validation, it triggers Puppet to deploy into production.\n\nLet’s discuss these two approaches in detail.\n\nThe Jenkins Way\n\nWorkflow\n\n_________________       ____________________       ______________\n  |                |      |                   |      |             |\n  |    Github:     |      |     Jenkins:      |      |   Jenkins:  |\n  | Commit trigger | ---->| Test & Validation | ---->|  Deployment |\n  |________________|      |___________________|      |_____________|\n\nIn this approach, Jenkins is used to test, validate, and deploy our Kubernetes\nconfiguration files. kubectl can be run on a directory and is idempotent.\nThis means that we can run it as often as we want: the result will not change.\nTheoretically, this is the simplest way. The only thing needed is to run\nkubectl command each time Jenkins detects changes.\n\nThe following Jenkinsfile gives an example of this workflow.\n\nJenkinsfile\n\npipeline {\n    agent any\n    stages {\n      stage('Init'){\n        steps {\n          sh 'curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl'\n        }\n      }\n      stage('Test'){\n        steps {\n          sh 'Run tests'\n        }\n      }\n      stage('Deploy'){\n        steps {\n          sh './kubectl apply -R true -f my_project'\n        }\n      }\n    }\n  }\n\nThe devil is in the details of course, and it was not as easy as it looked at\nfirst sight.\n\nOrder matters\n\nSome resources needed to be deployed before others. A workaround was to use\nnumbers as file names. But this added extra logic at file name level, for\nexample:\n\nproject/00-nginx-ingress\nproject/09-www.jenkins.io\n\nPortability\n\nThe deployment environments needed to be the same across development machines\nand the Jenkins host. Although this a well-known problem, it was not easy to\nsolve.  The more the project grew, the more our scripts needed additional tools\n( make, bats, jq gpg, etc).  The more tools we used, the more issues\nappeared because of the different versions used.\n\nAnother challenge that emerged when dealing with different environments was:\nhow should we manage environment-specific configurations (dev, prod, etc)?\nWould it be better to define different configuration files per environment?\nPerhaps, but this means code duplication, or using file templates which would require\nmore tools ( sed, jinja2, erb), and more work.\n\nThere wasn’t a golden rule we discovered, and the answer is probably somewhere in between.\n\nIn any case, the good news is that a Jenkinsfile provides an easy way to\nexecute tasks from a Docker image, and an image can contain all the necessary\ntools in our environment. We can even use different Docker images for each\nstage along the way.\n\nIn the following example, I use the my_env Docker image. It contains all the\ntools needed to test, validate, and deploy changes.\n\nJenkinsfile\n\npipeline{\n  agent {\n    docker{\n      image 'my_env:1.0'\n    }\n  }\n  options{\n    buildDiscarder(logRotator(numToKeepStr: '10'))\n    disableConcurrentBuilds()\n    timeout(time: 1, unit: 'HOURS')\n  }\n  triggers{\n    pollSCM('* * * * *')\n  }\n  stages{\n    stage('Init'){\n      steps{\n        // Init everything required to deploy our infra\n        sh 'make init'\n      }\n    }\n    stage('Test'){\n      steps{\n       // Run tests to validate changes\n       sh 'make test'\n      }\n    }\n    stage('Deploy'){\n      steps{\n       // Deploy changes in production\n       sh 'make deploy'\n      }\n    }\n  }\n  post{\n    always {\n      sh 'make notify'\n    }\n  }\n}\n\nSecret credentials\n\nManaging secrets is a big subject and brings with it many different\nrequirements which are very hard to fulfill.  For obvious reasons, we couldn’t\npublish the credentials used within the infra project.  On the other hand, we\nneeded to keep track and share them, particularly for the Jenkins node that\ndeploys our cluster.  This means that we needed a way to encrypt or decrypt\nthose credentials depending on permissions, environments, etc.  We analyzed two\ndifferent approaches to handle this:\n\nStoring secrets in a key management tool like Key Vault or Vault and use them like a Kubernetes \"secret\" type of resource.\n→ Unfortunately, these tools are not yet integrated in Kubernetes. But we may come back to this option later.\nKubernetes issue: 10439\n\nPublishing and encrypting using a public GPG key.\nThis means that everybody can encrypt credentials for the infrastructure project but only the owner of the private key can decrypt credentials.\nThis solution implies:\n\nScripting: as secrets need to be decrypted at deployment time.\n\nTemplates: as secret values will change depending on the environment.\n→ Each Jenkins node should only have the private key to decrypt secrets associated to its environment.\n\nScripting\n\nFinally, the system we had built was hard to work with.  Our initial\nJenkinsfile which only ran one kubectl command slowly become a bunch of\nscripts to accommodate for:\n\nResources needing to be updated only in some situations.\n\nSecrets needing to be encrypted/decrypted.\n\nTests needing to be run.\n\nIn the end, the amount of scripts required to deploy the Kubernetes resources\nstarted to become unwieldy and we began asking ourselves: \"aren’t we\nre-inventing the wheel?\"\n\nThe Puppet Way\n\nThe Jenkins project already uses Puppet, so we decided to look at using Puppet\nto orchestrate our container deployment with Kubernetes.\n\nWorkflow\n\n_________________       ____________________       _____________\n  |                |      |                   |      |            |\n  |    Github:     |      |     Jenkins:      |      | Puppet:    |\n  | Commit trigger | ---->| Test & Validation | ---->| Deployment |\n  |________________|      |___________________|      |____________|\n\nIn this workflow, Puppet is used to template and deploy all Kubernetes\nconfigurations files needed to orchestrate our cluster.\nPuppet is also used to automate basic kubectl operations such as 'apply' or\n'remove' for various resources based on file changes.\n\nPuppet workflow\n\n______________________\n|                     |\n|  Puppet Code:       |\n|    .                |\n|    ├── apply.pp     |\n|    ├── kubectl.pp   |\n|    ├── params.pp    |\n|    └── resources    |\n|        ├── lego.pp  |\n|        └── nginx.pp |\n|_____________________|\n          |                                        _________________________________\n          |                                       |                                |\n          |                                       |  Host: Prod orchestrator       |\n          |                                       |    /home/k8s/                  |\n          |                                       |    .                           |\n          |                                       |    └── resources               |\n          | Puppet generate workspace             |        ├── lego                |\n          └-------------------------------------->|        │   ├── configmap.yaml  |\n            Puppet apply workspaces' resources on |        │   ├── deployment.yaml |\n          ----------------------------------------|        │   └── namespace.yaml  |\n          |                                       |        └── nginx               |\n          v                                       |            ├── deployment.yaml |\n ______________                                   |            ├── namespace.yaml  |\n |     Azure:  |                                  |            └── service.yaml    |\n | K8s Cluster |                                  |________________________________|\n |_____________|\n\nThe main benefit of this approach is letting Puppet manage the environment and run\ncommon tasks. In the following example, we define a Puppet class for Datadog.\n\nPuppet class for resource Datadog\n\n# Deploy datadog resources on kubernetes cluster\n#   Class: profile::kubernetes::resources::datadog\n#\n#   This class deploy a datadog agent on each kubernetes node\n#\n#   Parameters:\n#     $apiKey:\n#       Contain datadog api key.\n#       Used in secret template\nclass profile::kubernetes::resources::datadog (\n    $apiKey = base64('encode', $::datadog_agent::api_key, 'strict')\n  ){\n  include ::stdlib\n  include profile::kubernetes::params\n  require profile::kubernetes::kubectl\n\n  file { \"${profile::kubernetes::params::resources}/datadog\":\n    ensure => 'directory',\n    owner  => $profile::kubernetes::params::user,\n  }\n\n  profile::kubernetes::apply { 'datadog/secret.yaml':\n    parameters => {\n        'apiKey' => $apiKey\n    },\n  }\n  profile::kubernetes::apply { 'datadog/daemonset.yaml':}\n  profile::kubernetes::apply { 'datadog/deployment.yaml':}\n\n  # As secrets change do not trigger pods update,\n  # we must reload pods 'manually' in order to use updated secrets.\n  # If we delete a pod defined by a daemonset,\n  # this daemonset will recreate pods automatically.\n  exec { 'Reload datadog pods':\n    path        => [\"${profile::kubernetes::params::bin}/\"],\n    command     => 'kubectl delete pods -l app=datadog',\n    refreshonly => true,\n    environment => [\"KUBECONFIG=${profile::kubernetes::params::home}/.kube/config\"] ,\n    logoutput   => true,\n    subscribe   => [\n      Exec['apply datadog/secret.yaml'],\n      Exec['apply datadog/daemonset.yaml'],\n    ],\n  }\n}\n\n→\nMore \"resources\" examples\n\nLet’s compare the Puppet way with the challenges discovered with the Jenkins\nway.\n\nOrder Matters\n\nWith Puppet, it becomes easier to define priorities as\nPuppet provides relationship meta parameters and the function 'require' (see\nalso:\nPuppet\nrelationships).\n\nIn our Datadog example, we can be sure that deployment will respect the following order:\n\ndatadog/secret.yaml -> datadog/daemonset.yaml -> datadog/deployment.yaml\n\nCurrently, our Puppet code only applies configuration when it detects file\nchanges.  It would be better to compare local files with the cluster\nconfiguration in order to trigger the required updates, but we haven’t found a\ngood way to implement this yet.\n\nPortability\n\nAs Puppet is used to configure working environments, it becomes easier to be\nsure that all tools are present and correctly configured.  It’s also easier to\nreplicate environments and run tests on them with tools like\nRSpec-puppet, Serverspec or\nVagrant.\n\nIn our Datadog example, we can also easily change the Datadog API key depending\non the environment with Hiera.\n\nSecret credentials\n\nAs we were already using Hiera GPG\nwith Puppet, we decided to continue to use it, making managing secrets for\ncontainers very simple.\n\nScripting\n\nOf course the Puppet DSL is used, and even if it seems harder at the beginning,\nPuppet simplifies a lot the management of Kubernetes configuration files.\n\nConclusion\n\nIt was much easier to bootstrap the project with a full CI workflow within\nJenkins as long as the Kubernetes project itself stays basic. But as soon as\nthe project grew, and we started deploying different applications with\ndifferent configurations per environment, it became easier to delegate\nKubernetes management to Puppet.\n\nIf you have any comments feel free to send a message to\nJenkins Infra mailing list.\n\nThanks\n\nThanks to Lindsay Vanheyste, Jean Marc Meessen, and Damien Duportal for their feedback.","title":"A journey to Kubernetes on Azure","tags":["puppet","kubernetes","docker","azure"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/bf8e1/olblak.png","srcSet":"/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/914ee/olblak.png 32w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/1c9ce/olblak.png 64w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/bf8e1/olblak.png 128w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/acb7c/olblak.png 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/ef6ff/olblak.webp 32w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/8257c/olblak.webp 64w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/6766a/olblak.webp 128w,\n/gatsby-jenkins-io/static/f6d731c7e61ff5c7f0ddc7f2b8a6671e/22bfc/olblak.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}}},"blog":null,"github":"olblak","html":"<div class=\"paragraph\">\n<p>Olivier is the Jenkins infrastructure officer and Senior Operations Engineer at CloudBees.\nAs a regular contributor to the Jenkins infrastructure projects, he works on a wide range of tasks from services reliability to applications maintenance.</p>\n</div>","id":"olblak","irc":"olblak","linkedin":null,"name":"Olivier Vernin","slug":"/blog/authors/olblak","twitter":"0lblak"}]}}]}},"pageContext":{"limit":8,"skip":296,"numPages":100,"currentPage":38}},
    "staticQueryHashes": ["3649515864"]}