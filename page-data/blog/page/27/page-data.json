{
    "componentChunkName": "component---src-templates-blog-list-template-js",
    "path": "/blog/page/27",
    "result": {"data":{"allBlog":{"edges":[{"node":{"date":"2018-07-13T00:00:00.000Z","id":"970120e2-6f31-5ddd-9fed-9bfcc306c79a","slug":"/blog/2018/07/13/jenkins-user-conference-china.adoc/","strippedHtml":"On June 30, 2018 in sunny Beijing, the capital of China, we welcomed over 200 attendees to Jenkins User Conference China (JUCC). This is the first JUCC in Beijing and we are overwhelmed by the interest and love for Jenkins. The conference had sessions in DevOps, Continuous Delivery, Jenkins X, Pipeline, and Container. The GreatOps community, event host, invited John Willis, a thought leader of DevOps to deliver the keynote speech. John’s topic was \"DevOps: Almost 10 years - What A  Strange Long Trip It’s Been.\" It was very insightful to learn of the history of DevOps and John’s point of view on the practice.\n\nLily Lin from Micro Focus presented, \"How to practice CI/CD for large-scale micro service based on Jenkins Pipeline.\"\n\nJames Rawlings, one of the core Jenkins X contributors traveled from the United Kingdom to present, \"Jenkins X for the future, Easy CI/CD for Kubernetes.\"\n\nAfter James’ presentation, there were many questions about Jenkins X, Jenkins users in China are very interested in Jenkins X. We all posed Jenkins \"X\" gesture.\n\nWe also invite Shuwei Hao from Alibaba, Michael Hüttermann who is the author of DevOps for Developers, Xiang Lu from CPI.\n\nMr Huaqiang Li and Xiaojie Zhao ran a workshop for help attendees master Jenkins Pipeline and Jenkins X in the cloud environment.\n\nHere are additional pictures from our event\n\nSpecial THANKS to BC who is the co-organizer of JUCC to host the main track and Alyssa and Maxwell for your help with our event.\n\nNext up, Jenkins User Conference China Shenzhen in November.\nLet’s Jenkins X and DevOps!","title":"Jenkins User Conference China Beijing Recap","tags":["event","juc"],"authors":[]}},{"node":{"date":"2018-07-10T00:00:00.000Z","id":"b0312033-b8b4-5dbc-b5a3-6c06dd49dcb5","slug":"/blog/2018/07/10/jenkins-essentials-on-aws/","strippedHtml":"Jenkins Essentials has been renamed to Jenkins Evergreen since this was written.\n\nJenkins Essentials is about providing a distribution of Jenkins in less than five minutes and five clicks.\nOne of the main ideas to make this a reality is that Jenkins will be autoconfigured with sane defaults for the environment it is running in.\n\nWe are happy to report we recently merged the change that provides this feature for AWS.\nWe use an AWS CloudFormation template to provision a working version of Jenkins Essentials, automatically configured to:\n\ndynamically provision EC2 agents, using the EC2 plugin;\n\nuse the Artifact Manager on S3 plugin, so that artifacts are not stored anymore on the controller’s file system, but directly in an S3 bucket.\n\nI recorded a short demo video last week showing the basics of this:\n\nWhile there are still many items to complete to provide a usable version for end-users, we are making steady progress towards it.\n\nYou can learn more about Jenkins Essentials from the\nGitHub repository, or join us\non our\nGitter channel.","title":"Jenkins Essentials flavor for AWS","tags":["jenkinsevergreen","evergreen"],"authors":[]}},{"node":{"date":"2018-07-05T00:00:00.000Z","id":"b1c89781-ce73-5eec-8d31-bf7f51174f63","slug":"/blog/2018/07/05/remoting-over-message-bus-alpha-release/","strippedHtml":"I am happy to announce that we have recently released an alpha version of Remoting Kafka Plugin to the Experimental Update Center. You can check the CHANGELOG to see the features included in this initial release.\n\nOverview\n\nCurrent versions of Jenkins Remoting are based on the TCP protocol. If it fails, the agent connection and the build fails as well. There are also issues with traffic prioritization and multi-agent communications, which impact Jenkins stability and scalability.\n\nRemoting Kafka Plugin is a plugin developed under Jenkins Google Summer of Code 2018. The plugin is developed to add support of a popular message queue/bus technology (Kafka) as a fault-tolerant communication layer in Jenkins. A quick introduction of the project can be found in this introduction blogpost.\n\nHow to use the plugin?\n\nThe instructions to run the plugin in alpha version are written here. Feel free to have a try and let us know your feedback on Gitter or the mailing list.\n\nLinks\n\nAlpha Changelog\n\nIntroduction Blogpost\n\nGitHub Repository\n\nProject Page\n\nPhase 1 Presentation Video\n\nPhase 1 Presentation Slides","title":"GSoC Project Update: Alpha release of Remoting Kafka Plugin","tags":["plugins","gsoc","gsoc2018","remoting","alpha-release"],"authors":[]}},{"node":{"date":"2018-07-02T00:00:00.000Z","id":"0c991121-dfec-5b10-a945-33a402cf3ddd","slug":"/blog/2018/07/02/new-api-token-system/","strippedHtml":"About API tokens\n\nJenkins API tokens are an authentication mechanism that allows a tool (script, application, etc.) to impersonate a user\nwithout providing the actual password for use with the Jenkins API or CLI.\nThis is especially useful when your security realm is based on a central directory, like Active Directory or LDAP,\nand you don’t want to store your password in scripts.\nRecent versions of Jenkins also make it easier to use the remote API when using API tokens to authenticate,\nas no CSRF tokens need to be provided even with CSRF protection enabled.\nAPI tokens are not meant to — and cannot — replace the regular password for the Jenkins UI.\n\nPrevious problems\n\nWe addressed two major problems with the existing API token system in Jenkins 2.129:\n\nFirst, reported in JENKINS-32442,\nuser accounts in Jenkins have an automatically generated API token by default.\nAs these tokens can be used to authenticate as a given user, they increase the attack surface of Jenkins.\n\nThe second problem was reported in JENKINS-32776 :\nThe tokens were previously stored on disk in an encrypted form.\nThis meant that they could be decrypted by unauthorized users by leveraging another security vulnerability,\nor obtained, for example, from improperly secured backups, and used to impersonate other users.\n\nNew approach\n\nThe main objective of this new system is to provide API tokens that are stored in a unidirectional way on the disk,\ni.e. using a hashing algorithm (in this particular case SHA-256).\n\nWhile this means that you will not be able to see the actual API tokens anymore after you’ve created them,\nseveral features were added to mitigate this potential problem:\n\nYou can have multiple active API tokens at the same time.\nIf you don’t remember an API token’s value anymore, just revoke it.\n\nYou can name your tokens to know where they are used (and rename them after creation if desired).\nWe recommend that tokens use a name that indicates where (for example the application, script, or host) where it will be used.\n\nYou can track the usage of your tokens.\nEvery token keeps a record of the number of uses and the date of the last use.\nThis will allow you to better know which tokens are really used and which are no longer actively required.\nJenkins also encourages users to rotate old API tokens by highlighting their creation date in orange after six months, and in red after twelve months.\nThe goal is to remind the user that tokens are more secure when you regenerate them often:\nThe longer a token is around, perhaps passed around in script files and stored on shared drives,\nthe greater the chance it’s going to be accessed by someone not authorized to use it.\n\nFigure 1. Token usage tracking\n\nYou can revoke API tokens.\nWhen you know that you are not using a given token anymore, you can revoke it to reduce the risk of it getting used by unauthorized users.\nSince you can have multiple API tokens, this allows fine-grained control over which scripts, hosts, or applications are allowed to use Jenkins as a given user.\n\nMigrating to new API tokens\n\nTo help administrators migrate their instances progressively, the legacy behavior is still available, while new system is also usable.\n\nOn the user configuration page, the legacy token is highlighted with a warning sign,\nexplaining that users should revoke it and generate a new one (if needed) to increase security.\n\nFigure 2. Legacy token renewal still possible\n\nNew options for administrators\n\nIn order to let administrators control the pace of migration to the new API token system,\nwe added two global configuration options in the \"Configure Global Security\" page in the brand new \"API Token\" section:\n\nAn option to disable the creation of legacy API tokens on user creation.\n\nAn option to disable the recreation of legacy API tokens by users, forcing them to only use the new, unrecoverable API tokens.\n\nBoth options are disabled by default for new installations (the safe default), while they’re enabled when Jenkins is upgraded from before 2.129.\n\nFigure 3. Security Configuration options\n\nFigure 4. Remove legacy token and disable the re-creation\n\nNew administrator warnings\n\nWhen upgrading to Jenkins 2.129, an administrative monitor informs admins about the new options described above, and recommend disabling them.\n\nAnother administrative warnings shows up if at least one user still has a legacy API token.\nIt provides central control over legacy tokens still configured in the Jenkins instance, and allows revoking them all.\n\nFigure 5. Legacy token monitoring page\n\nSummary\n\nJenkins API tokens are now much more flexible: They allow and even encourage better security practices.\nWe recommend you revoke legacy API tokens as soon as you can, and only use the newly introduced API tokens.","title":"Security Hardening: New API token system in Jenkins 2.129+","tags":["community","core","security","upgrade"],"authors":[]}},{"node":{"date":"2018-07-02T00:00:00.000Z","id":"44cd4bd6-4c8f-5fdd-8f54-c99da078882e","slug":"/blog/2018/07/02/whats-new-declarative-piepline-13x-sequential-stages/","strippedHtml":"We recently released version 1.3 of Declarative Pipelines, which includes a couple significant new features. We’re\ngoing to cover these features in separate blog posts. The next post will show the new ability to restart a completed\nPipeline run starting from a stage partway through the Pipeline, but first, let’s look at the new sequential stages\nfeature.\n\nSequential Stages\n\nIn Declarative 1.2, we added the ability to define stages to run in parallel\nas part of the Declarative syntax. Now in Declarative 1.3, we’ve added another way to specify stages nested within other\nstages, which we’re calling \"sequential stages\".\n\nRunning Multiple Stages in a Parallel Branch\n\nOne common use case is running build and tests on multiple platforms. You could already do that with parallel stages,\nbut now you can run multiple stages in each parallel branch giving you more visibility into the progress of your\nPipeline without having to check the logs to see exactly which step is currently running where, etc.\n\nYou can also\nuse stage directives, including post, when, agent, and all the others covered in the\nPipeline Syntax reference\nin your sequential stages, letting you control behavior for different parts of each parallel branch.\n\nIn the example below, we are running builds on both Windows and Linux, but only want to deploy if we’re on the master branch.\n\npipeline {\n    agent none\n\n    stages {\n        stage(\"build and deploy on Windows and Linux\") {\n            parallel {\n                stage(\"windows\") {\n                    agent {\n                        label \"windows\"\n                    }\n                    stages {\n                        stage(\"build\") {\n                            steps {\n                                bat \"run-build.bat\"\n                            }\n                        }\n                        stage(\"deploy\") {\n                            when {\n                                branch \"master\"\n                            }\n                            steps {\n                                bat \"run-deploy.bat\"\n                            }\n                        }\n                    }\n                }\n\n                stage(\"linux\") {\n                    agent {\n                        label \"linux\"\n                    }\n                    stages {\n                        stage(\"build\") {\n                            steps {\n                                sh \"./run-build.sh\"\n                            }\n                        }\n                        stage(\"deploy\") {\n                             when {\n                                 branch \"master\"\n                             }\n                             steps {\n                                sh \"./run-deploy.sh\"\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nRunning Multiple Stages with the Same agent, or environment, or options\n\nWhile the sequential stages feature was originally driven by users wanting to have multiple stages in parallel branches,\nwe’ve found that being able to group multiple stages together with the same agent, environment, when, etc has a lot\nof other uses. For example, if you are using multiple agents in your Pipeline, but would like to be sure that stages using\nthe same agent use the same workspace, you can use a parent stage with an agent directive on it, and then all the stages\ninside its stages directive will run on the same executor, in the same workspace. Another example is that until now, you\ncould only set a timeout for the entire Pipeline or an individual stage. But by using a parent stage with nested stages,\nyou can define a timeout in the parent’s options directive, and that timeout will be applied for the execution of the\nparent, including its nested stages. You may also want to conditionally control the execution of multiple stages. For example,\nyour deployment process may be spread across multiple stages, and you don’t want to run any of those stages unless you’re on\na certain branch or some other criteria is satisified. Now you can group all those related stages together in a parent\nstage, within its stages directive, and have a single when condition on that parent, rather than having to copy an\nidentical when condition to each of the relevant stages.\n\nOne of my favorite use cases is shown in the example below. In Declarative 1.2.6, we added the input directive for stages.\nThis will pause the execution of the Pipeline until a user confirms that the Pipeline should continue, using the Scripted\nPipeline input step. The input directive is evaluated before the stage enters its agent, if it has one specified, and\nbefore the stage’s when condition, if specified, is evaluated. But if you’re using a top-level agent for most of your\nstages, you’re still going to be using that agent’s executor while waiting for input, which can be a waste of resources.\nWith sequential stages, you can instead use agent none at the top-level of the Pipeline, and group the stages using a common\nagent and running before the stage with the input directive together under a parent stage with the required agent\nspecified. Then, when your Pipeline reaches the stage with input, it will no longer be using an agent’s executor.\n\npipeline {\n    agent none\n\n    stages {\n        stage(\"build and test the project\") {\n            agent {\n                docker \"our-build-tools-image\"\n            }\n            stages {\n               stage(\"build\") {\n                   steps {\n                       sh \"./build.sh\"\n                   }\n               }\n               stage(\"test\") {\n                   steps {\n                       sh \"./test.sh\"\n                   }\n               }\n            }\n            post {\n                success {\n                    stash name: \"artifacts\", includes: \"artifacts/**/*\"\n                }\n            }\n        }\n\n        stage(\"deploy the artifacts if a user confirms\") {\n            input {\n                message \"Should we deploy the project?\"\n            }\n            agent {\n                docker \"our-deploy-tools-image\"\n            }\n            steps {\n                sh \"./deploy.sh\"\n            }\n        }\n    }\n}\n\nThese are just a few example of the power of the new sequential stages feature in Declarative 1.3.\nThis new feature adds another set of significant use cases that can be handled smoothly using Declarative Pipeline.\nIn my next post, I’ll show the another highly requested feature - the new ability to restart a Pipeline run from any stage in that Pipeline.","title":"What's New in Declarative Pipeline 1.3: Sequential Stages","tags":["pipeline"],"authors":[]}},{"node":{"date":"2018-06-27T00:00:00.000Z","id":"c68773a2-1f81-56bc-af65-71e0703df409","slug":"/blog/2018/06/27/new-login-page/","strippedHtml":"This blog post gives an introduction to the new design for the login and signup forms and Jenkins is (re)starting pages introduced in Jenkins 2.128.\nThe first part of the blog post is an introduction to the new design and UX for Jenkins users.\nThe later part is talking about extensibility in a more technical manner, aimed at plugin developers.\n\nOverview\n\nThe recent changes to some core pages provide new design and UX and further dropping all external dependencies to prevent\nany possible malicious javascript introduced by third party libraries.\nTo be clear, this never was an issue with previous releases of Jenkins, but having read this article, this author believes that the article has good points and leading by example may raise awareness of data protection.\n\nThis meant to drop the usage of the jelly layout lib (aka xmlns:l=\"/lib/layout\") and as well the page decorators it\nsupported. However there is a new SimplePageDecorator extension point (discussed below) which can be used to modify the look and feel for the login and sign up page.\n\nThe following pages have given a new design:\n\nJenkins is (re)starting pages\n\nLogin\n\nSign up\n\nUX enhancement\n\nForm validation has changed to give inline feedback about data validation errors in the same form.\n\nLogin\n\nSign up\n\nThe above image shows that the validation is now done on all input fields instead of before breaking on the\nfirst error found, which should lead to fewer retry cycles.\n\nInstead of forcing the user to repeat the password, the new UX introduces the possibility to display the password in\nclear text. Further a basic password strength meter indicates password strength to the user while she enters the password.\n\nCustomizing the UI\n\nThe re-/starting screens do not support the concept of decorators very well, hence the decision to not support them for these pages.\n\nThe SimplePageDecorator is the key component for customization and uses three different files to\nallow overriding the look and feel of the login and signup pages.\n\nsimple-head.jelly\n\nsimple-header.jelly\n\nsimple-footer.jelly\n\nAll of the above SimplePageDecorator Jelly files are supported in the login page. The following snippet is a minimal excerpt\nof the login page, showing how it makes use of SimplePageDecorator.\n\nThe sign-up page only supports the simple-head.jelly:\n\nSimplePageDecorator - custom implementations\n\nHave a look at Login Theme Plugin, which allows you to\nconfigure your own custom content to be injected into the new login/sign-up page.\n\nTo allow easy customisation the decorator only implements one instance by the principal \"first-come-first-serve\".\nIf jenkins finds an extension of the SimplePageDecorator it will use the Jelly files provided by that plugin.\nOtherwise Jenkins will fall back to the default implementation.\n\n@Extension\npublic class MySimplePageDecorator extends SimplePageDecorator {\n   public String getProductName() {\n     return \"MyJenkins\";\n   }\n}\n\nThe above will take override over the default because the default implementation has a very low ordinal ( @Extension(ordinal=-9999))\nIf you have competing plugins implementing SimplePageDecorator, the implementation with the highest ordinal will be used.\n\nAs a simple example, to customize the logo we display in the login page, create a simple-head.jelly with the following content:\n\nTo customize the login page further, create a simple-header.jelly like this:\n\nWelcome to ${it.productName}!\n\nFor example, I used this technique to create a prototype of a login page for a CloudBees product I am working on:\n\nConclusion\n\nWe hope you like the recent changes to some core pages and as well the new design and UX. We further hope you feel enabled to\ncustomize the look and feel to adopt your needs with the SimplePageDecorator.","title":"New design, UX and extensibility digest for login page et. al.","tags":["core","developer","ux"],"authors":[]}},{"node":{"date":"2018-06-27T00:00:00.000Z","id":"fdf34f60-9243-5d28-b9d3-ea140833ede2","slug":"/blog/2018/06/27/lessons-java10-hackathon/","strippedHtml":"Last week I participated in the\nJenkins & Java 10 Online Hackathon.\nIt was my first Jenkins hackathon and I roped in\nJonah Graham to do some pair-programming.\nThe hackathon featured JDK Project Jigsaw committers Mandy Chung and Paul Sandoz,\nas well as Jenkins creator Kohsuke Kawaguchi.\nIt was a great opportunity for me to learn a lot about Jenkins and Java 10.\n\nWhy Java 10?\n\nWith the\nJava 8 EoL data looming,\nthe focus was on the current available version of Java, Java 10.\nJava 10 offers some nice new features and APIs, not least\nimproved docker container integration.\nWe learned from Paul of a number of projects with Java 10 migration success stories including Elasticsearch, Kafka & Netty.\n\nAt the beginning of the hackathon week, the Jenkins Pipeline feature would crash out when using Java 10.\nThis was resolved with a number of fixes including the upgrade of the\nASM library.\nThen it was nice to see things\nup and running with Java 10.\n\nGetting up & running\n\nThe first steps were to do some exploratory testing using\nJenkins with Java 10 via Docker, thanks to\nOleg for providing clear instructions.\nThis was boringly straightforward as most things worked and we only found one\nissue to report.\nNext to try to get some patches in, we needed to set-up a dev environment.\nThe live session gave us what we needed to set up a\nplugin or\ncore dev environment.\nOne open question we had was whether Jenkins has semantic versioning and\nAPI tools\nto help identify when you might be breaking backwards compatibility.\nOverall it was straightforward to get a dev environment up and running.\n\nJava 10 New APIs\n\nThe next step was to find an issue which we could help resolve.\nMany of the Java 10 issues were related to 'Illegal reflective access' from various plugins or third-party libraries.\nHowever after investigating a couple, removing these warnings required a good architectural knowledge of the plugin or core code itself.\nIn the end we decided that messing around with classloaders or attempting to upgrade version of jdom was not one for the newbies.\n\nInstead we looked at\nremoving reflection\nin cases of isAccessible calls.\nWe found the\nProcessHandle\napi very useful and a good replacement for some misuse of reflection, and even better it made the code work on Windows too.\nMandy also pointed us to look at the\nLookup api\nas possible alternate to findClass calls.\n\nMulti-Release JAR Builds\n\nUsing new APIs is all well and good but presents a problem when you want to maintain backwards compatibility with Java 8.\nHence the need for some sort of multi-jar solution -\nNicolas De loof proposed one such solution for\nmulti-release jars with Maven for this case.\n\nsun.misc.Signal\n\nThe Java Signal API is being deprecated, but so far no replacement APIs\nare available for signal handling.\nJenkins makes use of the Signal APIs so a big question for the Jigsaw team was whether this would be replaced going forward.\nKohsuke pointed out how it is important for Java to maintain this UNIX like behaviour as it shouldn’t matter to end users that Jenkins is written in Java.\nIt seems these APIs will be replaced in due course, they just\naren’t there right now.\n\nCollaboration, Collaboration, Collaboration\n\nIt was great to have the discussions with the Jigsaw team.\nThey reminded us how they need to know the Java use cases out there and how their team uses these to feed into their development process.\nIn turn, the hackathon had Jenkins community members participate, for instance\neasy-jenkins was up and running with Java 10 by the end of the week.\nThe hackathon had a great feeling of community spirit and was a reminder why collaborations with communities and also between different communities can be powerful and fun for all involved.\n\nAt the end of the week Jonah and I were both happy that we made our first Jenkins contributions (which were reviewed and merged quickly).\nThanks to all who participated and made it highly enjoyable, especially Oleg for great organization.\nI look forward to the next one!","title":"What I learned from the Jenkins & Java 10+ Hackathon","tags":["events","community","developer","java10","java11"],"authors":[]}},{"node":{"date":"2018-06-26T00:00:00.000Z","id":"40137c5d-c5ac-54d7-87d1-c1c8460bf136","slug":"/blog/2018/06/26/jenkins-essentials-at-eclipsecon-france/","strippedHtml":"Jenkins Essentials has been renamed to Jenkins Evergreen since this was written.\n\nIt’s been far too long since we posted an update on\nJenkins Essentials. While it’s not\nquite ready for users to start trying it out, we\ncontinue hacking away on all\nmanner of changes to support the safe and automatic upgrades of a running\nJenkins environment. In the meantime, Jenkins contributor\nBaptiste Mathus took some time to introduce and\ndemonstrate Jenkins Essentials at the recently held\nEclipseCon France,\n\nFrom the talk’s abstract:\n\nThe Jenkins Project is working on providing its users with a brand new,\nstrongly opinionated, and continuously delivered distribution of Jenkins:\nJenkins Essentials. Constantly self-updating, including auto-rollback, with\nan aggressive subset of verified plugins.\n\nIn this talk, we will detail how this works: how we run and upgrade Jenkins\nitself. How instances are continuously sending health data back to help\nautomated decision-making about the quality of given new release, and decide to\ngeneralize a given version of Jenkins to the whole fleet, or roll it back.\n\nWe will end giving an overview of the status of the project: how it’s managed\nin a fully open manner, from design to code and its infrastructure, and all the\nradical solutions to imagine and the upcoming challenges for the next months.\n\nI hope you enjoy the video\n\nYou can learn more about Jenkins Essentials from\nGitHub repository, or join us\non our\nGitter channel.","title":"Presenting Jenkins Essentials at EclipseCon France","tags":["jenkinsevergreen","evergreen"],"authors":[]}}]}},"pageContext":{"limit":8,"skip":208,"numPages":100,"currentPage":27}},
    "staticQueryHashes": ["3649515864"]}