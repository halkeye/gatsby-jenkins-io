{
    "componentChunkName": "component---src-templates-author-blog-list-template-js",
    "path": "/blog/authors/devmandy/",
    "result": {"data":{"author":{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#485858","images":{"fallback":{"src":"/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/19e71/devmandy.jpg","srcSet":"/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/77b35/devmandy.jpg 32w,\n/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/d4a57/devmandy.jpg 64w,\n/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/19e71/devmandy.jpg 128w,\n/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/68974/devmandy.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/ef6ff/devmandy.webp 32w,\n/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/8257c/devmandy.webp 64w,\n/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/6766a/devmandy.webp 128w,\n/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/22bfc/devmandy.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}},"publicURL":"/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/devmandy.jpeg"},"blog":null,"github":"DevMandy","html":"<div class=\"paragraph\">\n<p>Mandy Hubbard has almost 20 years of professional QA experience,\nmost of which has been spent in fast-paced startup environments driving product quality.\nShe is passionate about ensuring quality through process improvements, test automation, following CI/CD best practices and all things DevOps.\nShe is currently a software engineer/QA architect at CS Disco, an innovative startup delivering a cloud-based eDiscovery platform.</p>\n</div>","id":"devmandy","irc":null,"linkedin":null,"name":"Mandy Hubbard","slug":"/blog/authors/devmandy/","twitter":"DevMandy"},"allBlog":{"edges":[{"node":{"date":"2018-09-14T00:00:00.000Z","id":"3853be51-02ed-55b0-8b3f-edbfcfe998c7","slug":"/blog/2018/09/14/kubernetes-and-secret-agents/","strippedHtml":"At long last, the way we build and deploy software is finally changing and significantly so.\nThe days of the persnickety, prima donna build machine where monolithic applications were built, tested, and deployed are numbered.\nAnd that is a \"Good Thing (tm)\" - a consequence of how we will meet the transformation goals of our businesses.\nModern applications consist of distributed services, often with multiple microservices that are developed and deployed independent of other services.\nHowever, the only way to build these services with their own dependencies and schedules is to bake in continuous integration and delivery from the beginning.\nAnd as usual, your Jenkins platform is your friend.\n\nBut let’s take a moment and think about that in the context of microservices, especially if you’ve only used Jenkins for monolithic applications.\nYou’ll be creating a greater number of individual Jenkins jobs that each run multiple times a day.\nThis is a significant process change, and it’s important to acknowledge this and change our approach to managing Jenkins to accommodate these changes.\nIt’s well within Jenkins’ capabilities, but you will need to think a little differently, and invest to close those last-mile deployment gaps.\n\nEvolution of my Jenkins Environment\n\nOne of the biggest challenges I’ve faced as a DevOps practitioner is a long and evolving set of options to manage my Jenkins agent infrastructure.\nWith only a few large jobs you don’t really need to worry too much about your agents.\nBut when you’re orchestrating the CI/CD pipelines for dozens or even hundreds of services, optimizing efficiency and minimizing cost becomes important.\nAnd that journey has allowed me to consider and test many different Jenkins build agent architectures over the years.\nThis journey may be familiar to you as well.\n\nThese are the types of Jenkins environments I’ve run over the years.\n\nExecute all the builds on the controller.\nConcentrate all the moving parts on one instance.\n(I call this Hello Jenkins)\n\nCreate a Jenkins EC2 agent with all the required tools for building every service, and then clone it if I need to “scale” Jenkins.\n(I call this the Monster Agent.)\n\nCreate an individual Jenkins EC2 agent for each service I need to build.\n(I call this the Snowflake Agent.)\n\nRun build steps in containers.\nFor example, launching agents in containers using the\nDocker Plugin or using multi-stage Dockerfiles to encapsulate all the logic for building, testing and packaging an application.\nThey are both good first steps in container abstraction and allow you to easily copy artifacts from one container to another.\nOf course, access to a Docker engine is required for either approach, and I’ve managed my Docker host(s) for running Jenkins agents several different ways:\n\nRun the Docker engine inside my Jenkins controller container - Docker in Docker (DinD)\n\nMount the Docker socket of the host on which my Jenkins controller container runs, allowing agents to run as sibling or sidecar containers - Docker outside of Docker (DooD)\n\nConfigure a single external EC2 Docker host for the Jenkins controller to use for launching builds in containers\n\nDynamically launch agents using the EC2 plugin with an AMI that contains the Docker Engine and then run all the steps in a multi-stage Dockerfile\n\nAll these approaches were attempts to get out of the business of curating and managing Jenkins agents and infrastructure, each with their own benefits and drawbacks.\nBut recently I begin working in a new Jenkins environment - Jenkins on Kubernetes.\n\nOnce you’ve come to view Jenkins, build agents and jobs as containerized services, migrating platforms becomes much more straightforward.\nAnd total disclaimer here - I had never used Kubernetes in my life, not even for side projects - when I set out to do this.\nThat said, it was surprisingly simple to create a Kubernetes cluster in Google Cloud Platform’s (GCP) GKE, launch a Jenkins controller using a\nHelm chart and begin running build steps in Jenkins agents running in containers on my new Kubernetes cluster.\n\nLaunch agents in Kubernetes from your pipeline scripts\n\nThe focus of this post and my Jenkins World talk for 2018, is to show you how to configure Jenkins to launch agents in Kubernetes from your pipeline scripts.\nMy examples assume you are launching your agents in the same Kubernetes cluster where your Jenkins controller is running, but there are other options.\nYou’ll begin by installing the\nKubernetes plugin.\nAs a bonus, when I installed Jenkins using the latest stable chart in the default Helm repository, the Kubernetes plugin was automatically installed for me.\n\nOnce you get the Jenkins controller running on your Kubernetes cluster, there are only a few configuration steps required and then you can begin launching ephemeral build agents on Kubernetes.\n\nConfigure the Jenkins controller\n\nYou’ll first need to create a credentials set for the Jenkins controller to access the Kubernetes cluster.\nTo do this, perform the following steps:\n\nIn the Jenkins UI, click the Credentials link in the left-hand navigation pane\n\nClick the arrow next to (global) in the Stores scoped to Jenkins table (you have to hover next to the link to see the arrow)\n\nClick Add Credentials\n\nUnder Kind, specify Kubernetes Service Account\n\nLeave the scope set to Global\n\nClick OK.\n\nThat’s it! This configuration allows the Jenkins controller to use a Kubernetes service account to access the Kubernetes API.\n\nCreate a Cloud Configuration on the Jenkins controller\n\nThe next step is to create a cloud configuration for your K8s cluster.\n(When I use K8s instead of Kubernetes it’s because it is quicker to type, not just for coolness.)\n\nIn the Jenkins UI, go to Manage Jenkins → Configure System\n\nScroll down until you see Cloud settings and click the Add a new cloud box and select kubernetes\n\nThe following parameters must be set:\n\nName : - This defaults to kubernetes\n\nKubernetes URL : https://kubernetes.default - This was automatically configured from the service account.\n\nKubernetes Namespace : default - Unless you are running your controller in another namespace\n\nCredentials :  Select the Kubernetes Service Account credentials you created in the previous step\n\nJenkins URL : http:// :8080\n\nJenkins tunnel : :5555 - This is the port that is used to communicate with an agent\n\nThese were the only parameters I had to set to launch an agent in my K8s cluster.\nYou can certainly modify other parameters to tweak your environment.\n\nNow that you’ve configured your Jenkins controller so that it can access your K8s cluster, it’s time to define some pods.\nA pod is the basic building block of Kubernetes and consists of one or more containers with shared network and storage.\nEach Jenkins agent is launched as a Kubernetes pod.\nIt will always contain the default JNLP container that runs the Jenkins agent jar and any other containers you specify in the pod definition.\nThere are at least two ways to configure pod templates – in the Jenkins UI and in your pipeline script.\n\nConfigure a Pod Template in the Jenkins UI\n\nIn the Jenkins UI, go to Manage Jenkins → Configure Systems\n\nScroll down to the cloud settings you configured in the previous step\n\nClick the Add Pod Template button and select Kubernetes Pod Template\n\nEnter values for the following parameters:\n\nName :\n\nNamespace : default - unless you configured a different namespace in the previous step\n\nLabels : - this will be used to identify the agent pod from your Jenkinsfiles\n\nUsage : Select \" Use this node as much as possible\" if you would like for this pod to be your default node when no node is specified.\nSelect \" Only build jobs with label matching expressions matching this node\" to use this pod only when its label is specified in the pipeline script\n\nThe name of the pod template to inherit from - you can leave this blank.\nIt will be useful once you gain experience with this configuration, but don’t worry about it for now.\n\nContainers : The containers you want to launch inside this pod.\nThis is described in detail below.\n\nEnvVars : The environment variables you would like to inject into your pod at runtime.\nThis is described in detail below.\n\nVolumes :  Any volumes you want to mount inside your pod.\nThis is described further below.\n\nRemember that a pod consists of one or more containers that live and die together.\nThe pod must always include a JNLP container, which is configured by default if you installed the controller using the Helm Chart.\nHowever, you will want to add containers with the tool chains required to build your application.\n\nAdd Your Own Container Template\n\nIn the Jenkins UI, return to the pod template you created in the last step\n\nClick the Add Container button and select Container Template\n\nEnter values in the following fields:\n\nName :\n\nDocker image : any Docker image you’d like\nFor example, if you are building an application written in Go, you can enter 'golang:1.11-alpine3.8'\n\nLabel : Enter any label strings you’d like to use to refer to this container template in your pipeline scripts\n\nAlways pull image : - Select this option if you want the plugin to pull the image each time a pod is created.\n\nYou can leave the default values for the other parameters, but you can see that the plugin gives you fine-grained control over your pod and the individual containers that run within it.\nAny values you might set in your Kubernetes pod configuration can be set via this plugin as well.\nYou can also inject your configuration data by entering raw YAML.\nI encourage you not to get distracted by the sheer number of options you can configure in this plugin.\nYou only have to configure a small subset of them to get a working environment.\n\nYou can click the Add Environment Variable button in the container template to inject environment variables into a specific container.\nYou can click the Add Environment Variable button in the pod template to inject environment variables into all containers in the pod.\nThe following environment variables are automatically injected into the default JNLP container to allow it to connect automatically to the Jenkins controller:\n\nJENKINS_URL : Jenkins web interface url\n\nJENKINS_JNLP_URL : url for the jnlp definition of the specific agent\n\nJENKINS_SECRET : the secret key for authentication\n\nJENKINS_NAME : the name of the Jenkins agent\n\nIf you click the Add Volume button in the pod template, you’ll see several options for adding volumes to your pod.\nI use the Host Path Volume option to mount the docker socket inside the pod.\nI can then run a container with the Docker client installed and use the host Docker socket to build and push Docker images.\n\nAt this point, we’ve created a cloud configuration for our Kubernetes cluster and defined a pod consisting of one or more containers.\nNow, how do we use this to run Jenkins jobs? We simply refer to the pod and containers by label in our Jenkins pipeline script.\nWe use the label we gave to the pod in the node block and the label for the container we wish to use in the container block.\nThe examples in this post use scripted pipeline, but you can achieve the same outcome using the declarative pipeline syntax:\n\nnode('test-pod') {\n    stage('Checkout') {\n        checkout scm\n    }\n    stage('Build'){\n        container('go-agent') {\n            // This is where we build our code.\n        }\n    }\n}\n\nDefining the Pod in the Jenkinsfile\n\nConfiguring a plugin through the UI is perfectly fine in a proof of concept.\nHowever, it does not result in a software-defined infrastructure that can be versioned and stored right alongside your source code.\nLuckily, you can create the entire pod definition directly in your Jenkinsfile.\nIs there anything you can’t do in a Jenkinsfile???\n\nAny of the configuration parameters available in the UI or in the YAML definition can be added to the podTemplate and containerTemplate sections.\nIn the example below, I’ve defined a pod with two container templates.\nThe pod label is used in the node block to signify that we want to spin up an instance of this pod.\nAny steps defined directly inside the node block but not in a container block with be run in the default JNLP container.\n\nThe container block is used to signify that the steps inside the block should be run inside the container with the given label.\nI’ve defined a container template with the label 'golang', which I will use to build the Go executable that I will eventually package into a Docker image.\nIn the volumes definition, I have indicated that I want to mount the Docker socket of the host, but I still need the Docker client to interact with it using the Docker API.\nTherefore, I’ve defined a container template with the label 'docker' which uses an image with the Docker client installed.\n\npodTemplate(\n    name: 'test-pod',\n    label: 'test-pod',\n    containers: [\n        containerTemplate(name: 'golang', image: 'golang:1.9.4-alpine3.7'),\n        containerTemplate(name: 'docker', image:'trion/jenkins-docker-client'),\n    ],\n    volumes: [\n        hostPathVolume(mountPath: '/var/run/docker.sock'),\n        hostPath: '/var/run/docker.sock',\n    ],\n    {\n        //node = the pod label\n        node('test-pod'){\n            //container = the container label\n            stage('Build'){\n                container('golang'){\n                    // This is where we build our code.\n                }\n            }\n            stage('Build Docker Image'){\n                container(‘docker’){\n                    // This is where we build the Docker image\n                }\n            }\n        }\n    })\n\nIn my Docker-based pipeline scripts, I was building Docker images and pushing them to a Docker registry, and it was important to me to replicate that exactly with my new Kubernetes setup.\nOnce I accomplished that, I was ready to build my image using gcloud, the Google Cloud SDK, and push that image to the Google Container Registry in anticipation of deploying to my K8s cluster.\n\nTo do this, I specified a container template using a gcloud image and changed my docker command to a gcloud command.\nIt’s that simple!\n\npodTemplate(\n    name: 'test-pod',\n    label: 'test-pod',\n    containers: [\n        containerTemplate(name: 'golang', image: 'golang:1.9.4-alpine3.7'),\n        containerTemplate(name: 'gcloud', image:'gcr.io/cloud-builders/gcloud'),\n    ],\n    {\n        //node = the pod label\n        node('test-pod'){\n            //container = the container label\n            stage('Build'){\n                container('golang'){\n                    // This is where we build our code.\n                }\n            }\n            stage('Build Docker Image'){\n                container(‘gcloud’){\n                    //This is where we build and push our Docker image.\n                }\n            }\n        }\n    })\n\nStanding up a Jenkins controller on Kubernetes, running ephemeral agents, and building and deploying a sample application only took me a couple of hours.\nI spent another weekend really digging in to better understand the platform.\nYou can be up and running in a matter of days if you are a quick study.\nThere are a wealth of resources available on running Jenkins on Kubernetes, and I hope this blog post helps to further that knowledge.\nEven better, come to\nmy session at Jenkins World and let’s talk in person.\n\nSo, what else do you want to know?\nHit me up on Twitter.\nI might even add your questions to my Jenkins World session.\nI suppose next up is Mesos?\n\nCome meet Mandy and other Jenkins and Kubernetes experts at\nJenkins World on September 16-19th,\nregister with the code JWFOSS for a 30% discount off your pass.","title":"Jenkins and Kubernetes - Secret Agents in the Clouds","tags":["jenkinsworld","jenkinsworld2018","cloud-native","kubernetes"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#485858","images":{"fallback":{"src":"/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/19e71/devmandy.jpg","srcSet":"/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/77b35/devmandy.jpg 32w,\n/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/d4a57/devmandy.jpg 64w,\n/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/19e71/devmandy.jpg 128w,\n/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/68974/devmandy.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/ef6ff/devmandy.webp 32w,\n/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/8257c/devmandy.webp 64w,\n/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/6766a/devmandy.webp 128w,\n/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/22bfc/devmandy.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":128}},"publicURL":"/gatsby-jenkins-io/static/04dee4202a439e0bfaf32d7e1722f2b0/devmandy.jpeg"},"blog":null,"github":"DevMandy","html":"<div class=\"paragraph\">\n<p>Mandy Hubbard has almost 20 years of professional QA experience,\nmost of which has been spent in fast-paced startup environments driving product quality.\nShe is passionate about ensuring quality through process improvements, test automation, following CI/CD best practices and all things DevOps.\nShe is currently a software engineer/QA architect at CS Disco, an innovative startup delivering a cloud-based eDiscovery platform.</p>\n</div>","id":"devmandy","irc":null,"linkedin":null,"name":"Mandy Hubbard","slug":"/blog/authors/devmandy/","twitter":"DevMandy"}]}}]}},"pageContext":{"author":"devmandy","limit":8,"skip":0,"numPages":1,"currentPage":1}},
    "staticQueryHashes": ["3649515864"]}