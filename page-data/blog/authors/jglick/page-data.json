{
    "componentChunkName": "component---src-templates-author-blog-list-template-js",
    "path": "/blog/authors/jglick/",
    "result": {"data":{"author":{"avatar":null,"blog":null,"github":"jglick","html":"<div class=\"paragraph\">\n<p>Jesse has been developing Jenkins core and plugins for years.\nHe is the coauthor with Kohsuke of the core infrastructure of the Pipeline system.</p>\n</div>","id":"jglick","irc":null,"linkedin":null,"name":"Jesse Glick","slug":"/blog/authors/jglick/","twitter":"tyvole"},"allBlog":{"edges":[{"node":{"date":"2020-11-10T00:00:00.000Z","id":"4a81cf3c-4ca0-511f-9197-d77a776ede34","slug":"/blog/2020/11/10/spring-xstream/","strippedHtml":"Cleaning up technical debt is a perennial topic among Jenkins core developers,\nand one of the most visible issues is the use of obsolete and/or forked third-party libraries.\nIn a world where Dependabot is offering updates to libraries released just hours before,\nit is unpleasant to be working with dependencies that are many years old.\nSince large organizations in particular are unhappy to install software using obsolete or nonstandard versions,\nmy employer (CloudBees) gave its blessing for me to spend some time cleaning up some of the worst offenders.\n\nThe toughest nut to crack was the Acegi Security library used for authentication,\nwhich has long since been replaced by Spring Security\n(and Jenkins was also bundling a long-outdated version of some Spring Framework dependencies).\njep:227[] tracks the complicated task of updating to Spring Security\nwithout breaking the numerous plugins that interact with authentication,\nespecially those offering a Security Realm.\n\nAnother longstanding problem was the XStream library which Jenkins uses to read and write XML configuration files.\nThis had been forked long ago by what was then the Hudson project and a few fixes applied.\nUnfortunately, some of those fixes were rejected upstream as invalid (representing unsupported usage patterns),\nand the fork fell behind the upstream version.\njep:228[] describes the impact of switching to the upstream library in a more standard usage mode,\nincluding fixes to a smaller number of plugins which would otherwise be incompatible.\n\nNow that the Jenkins 2.266 weekly release includes both updates,\nit is important for both Jenkins administrators and plugin maintainers to check for actual or potential incompatibilities.\nThere are two tables listing the impact of these changes on plugins:\n\nSpring Security compatibility\n\nXStream compatibility\n\nIf you use Jenkins then it is a good idea before upgrading to take a look at these tables\nto see if you are running any plugins considered incompatible.\nIf so, try not to rely on that plugin, or find out if there is an active maintainer who could help.\nFor entries marked unknown, it would be appreciated if you could do a sanity check after upgrading\nand offer a pull request to the table page (click Edit this file) with a more informative status.\n\nIf you find a regression in a plugin, please file a bug report in Jira and link to it from the table.\nAlso please add a JEP-227 or JEP-228 label as appropriate, for ease of tracking:\n\nOpen JEP-227 issues\n\nOpen JEP-228 issues\n\nIt is a good idea to update all your plugins before upgrading Jenkins core.\nIn the case of the Spring Security update, some security realm plugins including LDAP and Active Directory must be updated in advance.\n(You can safely run the new plugin versions on Jenkins releases prior to this change.)\nOtherwise, you risk being unable to log in to Jenkins—and thus unable to update those plugins from the GUI!\nThe LDAP plugin additionally has a new version available only after the core upgrade, but there is no rush in switching to that.\n\nIf you maintain a Jenkins plugin then please check whether it is marked anything less than compatible.\nIn some cases, there are already pull requests awaiting merge.\nIn other cases, some minor aspects of the source code have been identified that could be edited to improve compatibility.\n\nWe expect to see a bit of disruption from these changes\nbut hope that in the long run they will save time for core and plugin developers\nand lead to a more secure and stable tool.\nPlease reach out on the developers’ list with any questions or suggestions.","title":"Spring and XStream updates (breaking changes!)","tags":["jenkins","core","developer"],"authors":[{"avatar":null,"blog":null,"github":"jglick","html":"<div class=\"paragraph\">\n<p>Jesse has been developing Jenkins core and plugins for years.\nHe is the coauthor with Kohsuke of the core infrastructure of the Pipeline system.</p>\n</div>","id":"jglick","irc":null,"linkedin":null,"name":"Jesse Glick","slug":"/blog/authors/jglick/","twitter":"tyvole"}]}},{"node":{"date":"2020-02-02T00:00:00.000Z","id":"bd7439b7-9ac0-59cd-a51c-c1739abb74ee","slug":"/blog/2020/02/02/web-socket/","strippedHtml":"I am happy to report that jep:222[] has landed in Jenkins weeklies,\nstarting in 2.217.\nThis improvement brings experimental WebSocket support to Jenkins,\navailable when connecting inbound agents or when running the CLI.\nThe WebSocket protocol allows bidirectional, streaming communication over an HTTP(S) port.\n\nWhile many users of Jenkins could benefit,\nimplementing this system was particularly important for CloudBees\nbecause of how CloudBees Core on modern cloud platforms\n(i.e., running on Kubernetes) configures networking.\nWhen an administrator wishes to connect an inbound (formerly known as “JNLP”) external agent to a Jenkins controller,\nsuch as a Windows virtual machine running outside the cluster and using the agent service wrapper,\nuntil now the only option was to use a special TCP port.\nThis port needed to be opened to external traffic using low-level network configuration.\nFor example, users of the nginx ingress controller\nwould need to proxy a separate external port for each Jenkins service in the cluster.\nThe instructions to do this are complex and hard to troubleshoot.\n\nUsing WebSocket, inbound agents can now be connected much more simply when a reverse proxy is present:\nif the HTTP(S) port is already serving traffic,\nmost proxies will allow WebSocket connections with no additional configuration.\nThe WebSocket mode can be enabled in agent configuration,\nand support for pod-based agents in the Kubernetes plugin is coming soon.\nYou will need an agent version 4.0 or later,\nwhich is bundled with Jenkins in the usual way (Docker images with this version are coming soon).\n\nAnother part of Jenkins that was troublesome for reverse proxy users was the CLI.\nBesides the SSH protocol on port 22, which again was a hassle to open from the outside,\nthe CLI already had the ability to use HTTP(S) transport.\nUnfortunately the trick used to implement that confused some proxies and was not very portable.\nJenkins 2.217 offers a new -webSocket CLI mode which should avoid these issues;\nagain you will need to download a new version of jenkins-cli.jar to use this mode.\n\nThe WebSocket code has been tested against a sample of Kubernetes implementations (including OpenShift),\nbut it is likely that some bugs and limitations remain,\nand scalability of agents under heavy build loads has not yet been tested.\nTreat this feature as beta quality for now and let us know how it works!","title":"WebSocket","tags":["core","remoting","agents","cli"],"authors":[{"avatar":null,"blog":null,"github":"jglick","html":"<div class=\"paragraph\">\n<p>Jesse has been developing Jenkins core and plugins for years.\nHe is the coauthor with Kohsuke of the core infrastructure of the Pipeline system.</p>\n</div>","id":"jglick","irc":null,"linkedin":null,"name":"Jesse Glick","slug":"/blog/authors/jglick/","twitter":"tyvole"}]}},{"node":{"date":"2019-02-17T00:00:00.000Z","id":"a79ea93c-0082-5a4a-a338-9fde01ae88b1","slug":"/blog/2019/02/17/remoting-cli-removed/","strippedHtml":"Close to two years ago, we announced in\nNew, safer CLI in 2.54\nthat the traditional “Remoting” operation mode of the Jenkins command-line interface\nwas being deprecated for a variety of reasons, especially its very poor security record.\nToday in Jenkins 2.165 support for this mode is finally being removed altogether,\nin both the server and bundled jenkins-cli.jar client.\nThe projected June 5th LTS release will reflect this removal,\nat which point the Jenkins project will no longer maintain this feature\nnor investigate security vulnerabilities in it.\n\nThis change makes the code in Jenkins core related to the CLI considerably simpler and more maintainable.\n(There are still two transports —HTTP(S) and SSH—but they have similar capabilities and behavior.)\nIt also reduces the “attack surface” the Jenkins security team must consider.\nAmong other issues, a compromised server could freely attack a developer’s laptop if -remoting were used.\n\nThe\n2.46.x upgrade guide\nalready urged administrators to disable Remoting mode on the server.\nThose Jenkins users who rely on the CLI for remote scripting (as opposed to the HTTP(S) REST APIs)\nwould be affected only if they were still using the -remoting CLI flag,\nsince the default has long been to use HTTP(S) mode.\n\nMost CLI features have long worked fine without -remoting,\nin some cases using slightly different syntax such as requiring shell redirects to access local files.\nAs part of this change, some CLI commands, options, and option types in Jenkins core have been removed, other than -remoting itself:\n\nThe login and logout commands, and the --username and --password options.\n\nThe -p option to select a proxy. (The CLI in default -http mode accesses Jenkins no differently than any other HTTP client.)\n\nThe install-tool, set-build-parameter, and set-build-result commands relied on a fundamentally insecure idiom that is no longer supportable.\n\nCommand options or arguments which took either a local file or = for standard input/output (e.g., install-plugin, build -p, support) now only accept the latter.\n\nSome features of relatively little-used plugins will no longer work, such as:\n\nDistFork\n\nRemote Terminal Access\n\nBuild Env Propagator","title":"Remoting-based CLI removed from Jenkins","tags":["core","security","remoting"],"authors":[{"avatar":null,"blog":null,"github":"jglick","html":"<div class=\"paragraph\">\n<p>Jesse has been developing Jenkins core and plugins for years.\nHe is the coauthor with Kohsuke of the core infrastructure of the Pipeline system.</p>\n</div>","id":"jglick","irc":null,"linkedin":null,"name":"Jesse Glick","slug":"/blog/authors/jglick/","twitter":"tyvole"}]}},{"node":{"date":"2018-09-10T00:00:00.000Z","id":"3cfe2a34-ab7c-5a8e-baa2-c46178e12dd1","slug":"/blog/2018/09/10/scaling-network-connections/","strippedHtml":"Oleg Nenashev and I will be speaking at DevOps World | Jenkins World in San Francisco this year about\nScaling Network Connections from the Jenkins Controller.\nOver the years there have been many efforts to analyze, optimize, and fortify the “Remoting channel”\nthat allows a controller to orchestrate agent activity and receive build results.\nTechniques such as tuning the agent launcher can improve service,\nbut qualitative change can only come from fundamentally reworking what gets transmitted and how.\n\nIn March, jira:27035[] introduced a framework for inspecting the traffic on a Remoting channel at a high level.\nPreviously, developers could only use generic low-level tools such as Wireshark,\nwhich cannot identify the precise piece of Jenkins code responsible for traffic.\n\nOver the past few months, the\nCloud Native SIG\nhas been making progress in addressing root causes.\nThe\nArtifact Manager on S3 plugin\nhas been released and integrated with Jenkins Evergreen,\nallowing upload and download of large artifacts to happen entirely between the agent and Amazon servers.\nPrototype plugins allow all build log content generated by an agent (such as in sh steps)\nto be streamed directly to external storage services such as AWS CloudWatch Logs.\nWork has also begun on uploading JUnit-format test results, which can sometimes get big,\ndirectly from an agent to database storage.\nAll these efforts can reduce the load on the Jenkins controller and local network\nwithout requiring developers to touch their Pipeline scripts.\n\nOther approaches are on the horizon.\nWhile “one-shot” agents run in fresh VMs or containers greatly improve reproducibility,\nthey suffer from the need to transmit megabytes of Java code for every build,\nso Jenkins features will need to be built to precache most or all of it.\nWork is underway to use Apache Kafka to make channels more robust against network failures.\nMost dramatically, the proposed\nCloud Native Jenkins MVP\nwould eliminate the bottleneck of a single Jenkins controller service handling hundreds of builds.\n\nCome meet Jesse, Oleg, and other Cloud Native SIG members at\nJenkins World on September 16-19th,\nregister with the code JWFOSS for a 30% discount off your pass.","title":"Scaling Network Connections from the Jenkins Controller","tags":["jenkinsworld","jenkinsworld2018","cloud-native","performance","scalability","remoting"],"authors":[{"avatar":null,"blog":null,"github":"jglick","html":"<div class=\"paragraph\">\n<p>Jesse has been developing Jenkins core and plugins for years.\nHe is the coauthor with Kohsuke of the core infrastructure of the Pipeline system.</p>\n</div>","id":"jglick","irc":null,"linkedin":null,"name":"Jesse Glick","slug":"/blog/authors/jglick/","twitter":"tyvole"}]}},{"node":{"date":"2018-05-15T00:00:00.000Z","id":"85458a0f-8ad5-53fb-a455-a4b6ae133fb4","slug":"/blog/2018/05/15/incremental-deployment/","strippedHtml":"A couple of weeks ago, Tyler mentioned some\ndeveloper improvements in Essentials\nthat had been recently introduced:\nthe ability for\nci.jenkins.io\nbuilds to get deployed automatically to an “Incrementals” Maven repository,\nas described in\nJEP-305.\nFor a plugin maintainer, you just need to\nturn on this support\nand you are ready to both deploy individual Git commits from your repository\nwithout the need to run heavyweight traditional Maven releases,\nand to depend directly on similar commits of Jenkins core or other plugins.\nThis is a stepping stone toward continuous delivery, and ultimately deployment, of Jenkins itself.\n\nHere I would like to peek behind the curtain a bit at how we did this,\nsince the solution turns out to be very interesting for people thinking about security in Jenkins.\nI will gloss over the Maven arcana required to get the project version to look like 1.40-rc301.87ce0dd8909b\n(a real example from the\nCopy Artifact plugin)\nrather than the usual 1.40-SNAPSHOT, and why this format is even useful.\nSuffice it to say that if you had enough permissions, you could run\n\nmvn -Dset.changelist -DskipTests clean deploy\n\nfrom your laptop to publish your latest commit.\nIndeed as\nmentioned in the JEP,\nthe most straightforward server setup would be to run more or less that command\nfrom the buildPlugin function called from a typical Jenkinsfile,\nwith some predefined credentials adequate to upload to the Maven repository.\n\nUnfortunately, that simple solution did not look very secure.\nIf you offer deployment credentials to a Jenkins job,\nyou need to trust anyone who might configure that job (here, its Jenkinsfile)\nto use those credentials appropriately.\n(The withCredentials step will mask the password from the log file, to prevent accidental disclosures.\nIt in no way blocks deliberate misuse or theft.)\nIf your Jenkins service runs inside a protected network and works with private repositories,\nthat is probably good enough.\n\nFor this project, we wanted to permit incremental deployments from any pull request.\nJenkins will refuse to run Jenkinsfile modifications from people\nwho would not normally be able to merge the pull request or push directly,\nand those people would be more or less trustworthy Jenkins developers,\nbut that is of no help if a pull request changes pom.xml\nor other source files used by the build itself.\nIf the server administrator exposes a secret to a job,\nand it is bound to an environment variable while running some open-ended command like a Maven build,\nthere is no practical way to control what might happen.\n\nThe lesson here is that the unit of access control in Jenkins is the job.\nYou can control who can configure a job, or who can edit files it uses,\nbut you have no control over what the job does or how it might use any credentials.\nFor JEP-305, therefore, we wanted a way to perform deployments from builds considered as black boxes.\nThis means a division of responsibility:\nthe build produces some artifacts, however it sees fit;\nand another process picks up those artifacts and deploys them.\n\nThis worked was tracked in\nINFRA-1571.\nThe idea was to create a “serverless function” in Azure\nthat would retrieve artifacts from Jenkins at the end of a build,\nperform a set of validations to ensure that the artifacts follow an expected repository path pattern,\nand finally deploy them to Artifactory using a trusted token.\nI prototyped this in Java, Tyler\nrewrote it in JavaScript,\nand together we brought it into production.\n\nThe crucial bit here is what information (or misinformation!) the Jenkins build can send to the function.\nAll we actually need to know is the build URL, so the\ncall site from Jenkins\nis quite simple.\nWhen the function is called with this URL,\nit starts off by performing input validation:\nit knows what the Jenkins base URL is,\nand what a build URL from inside an organization folder is supposed to look like:\nhttps://ci.jenkins.io/job/Plugins/job/git-plugin/job/PR-582/17/ , for example.\n\nThe next step is to call back to Jenkins and ask it for some metadata about that build.\nWhile we do not trust the build, we trust the server that ran it to be properly configured.\nAn obstacle here was that the ci.jenkins.io server had been configured to disable the Jenkins REST API;\nwith Tyler’s guidance I was able to amend this policy to permit API requests from registered users\n(or, in the case of the Incrementals publisher, a bot).\n\nIf you want to try this at home, get an\nAPI token,\npick a build of an “incrementalified” plugin or Jenkins core,\nand run something like\n\ncurl -igu : 'https://ci.jenkins.io/job/Plugins/job/git-plugin/job/PR-582/17/api/json?pretty&tree=actions[revision[hash,pullHash]]'\n\nYou will see a hash or pullHash corresponding to the main commit of that build.\n(This information was added to the Jenkins REST API to support this use case in\nJENKINS-50777.)\nThe main commit is selected when the build starts\nand always corresponds to the version of Jenkinsfile in the repository for which the job is named.\nWhile a build might checkout any number of repositories,\ncheckout scm always picks “this” repository in “this” version.\nTherefore the deployment function knows for sure which commit the sources came from,\nand will refuse to deploy artifacts named for some other commit.\n\nNext it looks up information about the Git repository at the folder level (again from JENKINS-50777):\n\ncurl -igu : 'https://ci.jenkins.io/job/Plugins/job/git-plugin/api/json?pretty&tree=sources[source[repoOwner,repository]]'\n\nThe Git repository now needs to be correlated to a list of Maven artifact paths that this component is expected to produce.\nThe\nrepository-permissions-updater\n(RPU) tool already had a list of artifact paths used to perform permission checks on regular release deployments to Artifactory; in\nINFRA-1598\nI extended it to also record the GitHub repository name, as can be seen\nhere.\nNow the function knows that the CI build in this example may legitimately create artifacts in the org/jenkins-ci/plugins/git/ namespace\nincluding 38c569094828 in their versions.\nThe build is expected to have produced artifacts in the same structure as mvn install sends to the local repository,\nso the function downloads everything associated with that commit hash:\n\ncurl -sg 'https://ci.jenkins.io/job/Plugins/job/git-plugin/job/PR-582/17/artifact/**/*-rc*.38c569094828/*-rc*.38c569094828*/*zip*/archive.zip' | jar t\n\nWhen all the artifacts are indeed inside the expected path(s),\nand at least one POM file is included (here org/jenkins-ci/plugins/git/3.9.0-rc1671.38c569094828/git-3.9.0-rc1671.38c569094828.pom),\nthen the ZIP file looks good—ready to send to Artifactory.\n\nOne last check is whether the commit has already been deployed (perhaps this is a rebuild).\nIf it has not, the function uses the Artifactory REST API to atomically upload the ZIP file\nand uses the GitHub Status API to associate a message with the commit\nso that you can see right in your pull request that it got deployed:\n\nOne more bit of caution was required.\nJust because we successfully published some bits from some PR does not mean they should be used!\nWe also needed a tool which lets you select the newest published version of some artifact\nwithin a particular branch, usually master.\nThis was tracked in\nJENKINS-50953\nand is available to start with as a Maven command operating on a pom.xml :\n\nmvn incrementals:update\n\nThis will check Artifactory for updates to relevant components.\nWhen each one is found, it will use the GitHub API to check whether the commit has been merged to the selected branch.\nOnly matches are offered for update.\n\nPutting all this together, we have a system for continuously delivering components\nfrom any of the hundreds of Jenkins Git repositories\ntriggered by the simple act of filing a pull request.\nSecuring that system was a lot of work\nbut highlights how boundaries of trust interact with CI/CD.","title":"Automatic deployment of “incremental” commits to Jenkins core and plugins","tags":["evergreen","developer"],"authors":[{"avatar":null,"blog":null,"github":"jglick","html":"<div class=\"paragraph\">\n<p>Jesse has been developing Jenkins core and plugins for years.\nHe is the coauthor with Kohsuke of the core infrastructure of the Pipeline system.</p>\n</div>","id":"jglick","irc":null,"linkedin":null,"name":"Jesse Glick","slug":"/blog/authors/jglick/","twitter":"tyvole"}]}},{"node":{"date":"2018-04-30T00:00:00.000Z","id":"b56ab05f-d077-5b8d-ab93-27f00b4c8c1e","slug":"/blog/2018/04/30/using-the-beta-annotation/","strippedHtml":"This sort of slid under the radar in the middle of some bigger changes\nfor the JEP-202\nreference implementation, so I wanted to call it out now. Arguably this could\ndeserve a retroactive JEP, though I would rather fold it into a JEP for\nJENKINS-49651 (see below).\n\nAs of Jenkins 2.118, or plugin parent POM 3.7, you can mark any Java member\n( class, method, constructor, field, or I suppose also interface,\nenum, or annotation) with API visibility ( protected or public) with an\nannotation :\n\n@Restricted(Beta.class)\n\nThe idea is to announce to potential users of the member that the API\nmay still be in flux and only code prepared to keep up should be using\nit. For an example, 2.118 added a VirtualFile.toExternalURL() method\nthat is being implemented in artifact-manager-s3 and (pending some\nPR merges) called in copyartifact and workflow-basic-steps. We do\nnot necessarily want this to be called yet by unknown parties out\nthere in the Jenkins ecosystem. To enforce that, any attempt to call\nor implement toExternalURL will produce a build failure, unless you\nadd this property to your plugin POM, as these plugins have done:\n\ntrue\n\nWhy? Because there is a chance the design is wrong and it might need\nto be changed—perhaps some upcoming bug fix would demand a boolean\nparameter be added, for example.\n\nUnder the conventional notion of Jenkins API deprecation and compatibility\npolicy, once an API like this makes it into a release version, that is it—we\nmight mark it @Deprecated but we need to maintain compatibility indefinitely,\nand find some way to migrate existing implementations / call sites.\n\nWith the @Beta annotation, that promise is not being made. If it needs\na boolean parameter for some reason, that will be added and those\nthree plugins updated to match; we are not going to bother retaining\nthe original overload and somehow delegating to the new one. This\nsimplification of the developer workflow is important to the use cases\nof Essentials (JEP-3xx), and I would expect the useBeta mark to\nbecome widespread among plugins included in Essentials. Such as the situation\nwhere one team needs to feel\ncomfortable refactoring code under its aegis freely, and the refactored result\nshould be deliverable as a unit to production via the Evergreen distribution\nsystem.\n\nSo that leaves two important questions:\n\nFirst, is the annotation\npermanent, and if not, when should it be removed? I do not think there\nis any hard policy, but the intention is that it should be removed\nonce the API is in more or less widespread use and has held up. For\nthis example, if people start using S3 artifacts, and especially if\nsomeone successfully writes an implementation of artifact storage in\nAzure that uses the API, the concept will have been reasonably proven.\nAt that point we want the API to be used wherever it would make sense,\nand if there is some very belated realization that the design is not\nquite right, we accept the burden of deprecating the original and\nmigrating callers compatibly.\n\nSecond, it is fine and well to say that someone changing the signature\nof a beta toExternalURL is on the hook to update the three plugins\nusing it, but what if a Jenkins admin ( not running Essentials, for\nshame) upgrades to (say) Jenkins 2.125 with the new signature but\ndeclines to accept the updates to those plugins (say,\nworkflow-basic-steps 2.9) which adapt to the change? It is not\nenough to say that it is their fault for holding back on the updates\narbitrarily; the plugin manager offers you updates but does nothing\nto tell you when they are required, so suddenly throwing\nNoSuchMethodError is not a helpful response.\n\nThe solution needs to be ironed out, but my expectation is to use\nJENKINS-49651\nfor this. For example, workflow-basic-steps 2.8,\nusing toExternalURL(), would have declared itself compatible with\nJenkins-Version: 2.118, and thus implicitly anything newer. The\ndeveloper doing the refactoring would also amend some 2.125 (and\nnewer) core metadata to say that it conflicts with anything older than\nthe 2.9 release of the plugin. The plugin manager would therefore\nblock the 2.8 plugin from even being loaded on the 2.125 core; the\nadmin would need to update before using it. In the case of an\nincompatible change made to a plugin API, rather than a core API, the\nUX is a little smoother since the plugin manager could just refuse to\nlet you update one without the other.\n\nIf you’re a plugin or core developer who is interested in using the @Beta\nannotations, or have questions about our motiviations, please join the\ndiscussion on\nthis mailing list thread.","title":"Using new core APIs with the Beta annotation","tags":["core","developer","plugin"],"authors":[{"avatar":null,"blog":null,"github":"jglick","html":"<div class=\"paragraph\">\n<p>Jesse has been developing Jenkins core and plugins for years.\nHe is the coauthor with Kohsuke of the core infrastructure of the Pipeline system.</p>\n</div>","id":"jglick","irc":null,"linkedin":null,"name":"Jesse Glick","slug":"/blog/authors/jglick/","twitter":"tyvole"}]}},{"node":{"date":"2018-01-13T00:00:00.000Z","id":"b30f609a-7e4a-56ef-93e3-f8629da662cd","slug":"/blog/2018/01/13/jep-200/","strippedHtml":"There is a newer version of the announcement for Jenkins administrators.\nPlease see this blogpost.\n\nOverview\n\nJEP-200 has been integrated into Jenkins weekly builds\nand (if all goes well) will be a part of the next LTS line.\nIn a nutshell, this change is a security hardening measure\nto be less permissive about deserializing Java classes defined in the Java Platform or libraries bundled with Jenkins.\nFor several years now, Jenkins has specifically blacklisted certain classes and packages according to known or suspected exploits;\nnow it will reject all classes not explicitly mentioned in a whitelist, or defined in Jenkins core or plugins.\n\nFor Jenkins administrators\n\nBefore upgrade\n\nBack up your Jenkins instance prior to upgrade so you have any easy way of rolling back.\nIf you are running any of the plugins listed in\nPlugins affected by fix for JEP-200,\nupdate them after taking the backup but before upgrading Jenkins core.\n\nIf you have a way of testing the upgrade in an isolated environment before applying it to production,\ndo so now.\n\nUsing backups and a staging server is good advice before any upgrade but especially this one,\nwith a relatively high risk of regression.\n\nAfter upgrade\n\nTo the extent that advance testing of the impact of this change on popular plugins has been completed,\nmost users (and even plugin developers) should not notice any difference.\nIf you do encounter a java.lang.SecurityException: Rejected: some.pkg.and.ClassName in the Jenkins UI or logs,\nyou may have found a case where an unusual plugin, or an unusual usage mode of a common plugin,\nviolates the existing whitelist.\nThis will be visible in the Jenkins system log as a message from jenkins.security.ClassFilterImpl like the following:\n\nsome.pkg.and.ClassName in file:/var/lib/jenkins/plugins/some-plugin-name/WEB-INF/lib/some-library-1.2.jar might be dangerous, so rejecting; see https://jenkins.io/redirect/class-filter/\n\nwhere the link would direct you here.\n\nIf you find such a case, please report it in the Jenkins issue tracker, under the appropriate plugin component.\nLink it to JENKINS-47736 and add the JEP-200 label.\nIf at all possible, include complete steps to reproduce the problem from scratch.\nJenkins developers will strive to evaluate the reason for the violation and offer a fix in the form of a core and/or plugin update.\nFor more details and current status, see\nPlugins affected by fix for JEP-200.\n\nAssuming you see no particular reason to think that the class in question has dangerous deserialization semantics, which is rare,\nit is possible to work around the problem in your own installation as a temporary expedient.\nSimply make note of any class name(s) mentioned in such log messages,\nand run Jenkins with this startup option (details will depend on your installation method):\n\n-Dhudson.remoting.ClassFilter=some.pkg.and.ClassName,some.pkg.and.OtherClassName\n\nFor plugin developers\n\nTesting plugins against Jenkins 2.102 and above\n\nAs a plugin developer encountering this kind of error,\nyour first task is to ensure that it is reproducible in a functional ( JenkinsRule) test\nwhen running Jenkins 2.102 or newer to reproduce the error.\n\nmvn test -Djenkins.version=2.102 -Denforcer.skip=true\n\nThe above assumes you are using a recent 2.x or 3.x parent Plugin POM.\nFor certain cases you may need to use Plugin Compat Tester (PCT)\nto run tests against Jenkins core versions newer than your baseline.\n\nRunning PCT against the latest Jenkins core:\n\njava -jar pct-cli.jar -reportFile $(pwd)/out/pct-report.xml \\\n    -workDirectory $(pwd)/work -skipTestCache true -mvn $(which mvn) \\\n    -includePlugins ${ARTIFACT_ID} -localCheckoutDir ${YOUR_PLUGIN_REPO}\n\nYou may need to run tests using an agent or force saves of plugin settings.\n\nFor maven plugins you can also specify custom Jenkins versions in Jenkinsfile to run tests against JEP-200:\n\nbuildPlugin(jenkinsVersions: [null, '2.102'])\n\n(again picking whatever version you need to test against)\nso that the test is included during CI builds, even while your minimum core baseline predates JEP-200.\n\nIf your plugins are built with Gradle, your mileage may vary.\n\nMaking plugins compatible with Jenkins 2.102 or above\n\nIf you discover a compatibility issue in your plugin,\nyou then have several choices for fixing the problem:\n\nIdeally, simplify your code so that the mentioned class is not deserialized via Jenkins Remoting or XStream to begin with:\n\nIf the problem occurred when receiving a response from an agent, change your Callable (or FileCallable) to return a plainer type.\n\nIf the problem occurred when saving an XML file (such as a config.xml or build.xml), use a plainer type in non- transient fields in your persistable plugin classes.\n\nIf the class(es) are defined in the Java Platform or some library bundled in Jenkins core, propose a pull request adding it to core/src/main/resources/jenkins/security/whitelisted-classes.txt in jenkinsci/jenkins.\n\nIf the class(es) are defined in a third-party library bundled in your plugin, create a resource file META-INF/hudson.remoting.ClassFilter listing them. ( example)\n\nYou may also do this for Java or Jenkins core library classes, as a hotfix until your core baseline includes the whitelist entry proposed above.\n\nIf the class(es) are defined in a JAR you build and then bundle in your plugin’s *.jpi, add a Jenkins-ClassFilter-Whitelisted: true manifest entry. This whitelists every class in the JAR. ( example)","title":"JEP-200: Remoting / XStream whitelist integrated into Jenkins core","tags":["core","security","remoting","upgrade"],"authors":[{"avatar":null,"blog":null,"github":"jglick","html":"<div class=\"paragraph\">\n<p>Jesse has been developing Jenkins core and plugins for years.\nHe is the coauthor with Kohsuke of the core infrastructure of the Pipeline system.</p>\n</div>","id":"jglick","irc":null,"linkedin":null,"name":"Jesse Glick","slug":"/blog/authors/jglick/","twitter":"tyvole"}]}},{"node":{"date":"2016-05-31T00:00:00.000Z","id":"ad068db4-bb27-52c3-9916-bb8b132c6e53","slug":"/blog/2016/05/31/pipeline-snippetizer/","strippedHtml":"Those of you updating the Pipeline Groovy plugin\nto 2.3 or later will notice a change to the appearance of the configuration form.\nThe Snippet Generator tool is no longer a checkbox enabled inside the configuration page.\nRather, there is a link Pipeline Syntax which opens a separate page with several options.\n(The link appears in the project’s sidebar; Jenkins 2 users will not see the sidebar from the configuration screen,\nso as of 2.4 there is also a link beneath the Pipeline definition.)\n\nSnippet Generator continues to be available for learning the available\nPipeline steps and creating sample calls given various configuration options.\nThe new page also offers clearer links to static reference documentation, online\nPipeline documentation resources, and an IntelliJ IDEA code completion file\n(Eclipse support is unfinished).\n\nOne motivation for this change\n( JENKINS-31831) was to\ngive these resources more visual space and more prominence.  But another\nconsideration was that people using multibranch projects or organization folders\nshould be able to use Snippet Generator when setting up the project, before\nany code is committed.\n\nThose using\nPipeline\nMultibranch plugin or organization folder plugins should upgrade to 2.4 or\nlater to see these improvements as well.","title":"New display of Pipeline’s \"snippet generator\"","tags":["pipeline"],"authors":[{"avatar":null,"blog":null,"github":"jglick","html":"<div class=\"paragraph\">\n<p>Jesse has been developing Jenkins core and plugins for years.\nHe is the coauthor with Kohsuke of the core infrastructure of the Pipeline system.</p>\n</div>","id":"jglick","irc":null,"linkedin":null,"name":"Jesse Glick","slug":"/blog/authors/jglick/","twitter":"tyvole"}]}}]}},"pageContext":{"author":"jglick","limit":8,"skip":0,"numPages":2,"currentPage":1}},
    "staticQueryHashes": ["3649515864"]}