{
    "componentChunkName": "component---src-templates-post-js",
    "path": "/blog/2016/11/21/gc-tuning/",
    "result": {"data":{"blog":{"html":"<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<i class=\"fa icon-note\" title=\"Note\"></i>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>This is a\n<a href=\"https://www.cloudbees.com/blog/joining-big-leagues-tuning-jenkins-gc-responsiveness-and-stability\">cross\npost</a> by <a href=\"https://github.com/svanoort\">Sam Van Oort</a>, Software Engineer at\n<a href=\"https://cloudbees.com\">CloudBees</a> and contributor to the Jenkins project.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>Today I&#8217;m going to show you how easy it is to tune Jenkins Java settings to\nmake your controllers more responsive and stable, especially with large heap sizes.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"the-magic-settings\"><a class=\"anchor\" href=\"#the-magic-settings\"></a>The Magic Settings:</h3>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Basics:</strong> <code>-server -XX:+AlwaysPreTouch</code></p>\n</li>\n<li>\n<p><strong>GC Logging:</strong> <code>-Xloggc:$JENKINS_HOME/gc-%t.log -XX:NumberOfGCLogFiles=5 -XX:+UseGCLogFileRotation -XX:GCLogFileSize=20m -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintHeapAtGC -XX:+PrintGCCause -XX:+PrintTenuringDistribution -XX:+PrintReferenceGC -XX:+PrintAdaptiveSizePolicy</code></p>\n</li>\n<li>\n<p><strong>G1 GC settings:</strong> <code>-XX:+UseG1GC -XX:+ExplicitGCInvokesConcurrent -XX:+ParallelRefProcEnabled -XX:+UseStringDeduplication -XX:+UnlockExperimentalVMOptions -XX:G1NewSizePercent=20 -XX:+UnlockDiagnosticVMOptions -XX:G1SummarizeRSetStatsPeriod=1</code></p>\n</li>\n<li>\n<p><strong>Heap settings:</strong> set your minimum heap size (<code>-Xms</code>) to at least 1/2 of your maximum size (<code>-Xmx</code>).</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Now, let&#8217;s look at where those came from!  We&#8217;re going to focus on garbage\ncollection (GC) here and dig fast and deep to strike for gold; if you&#8217;re not\nfamiliar with GC fundamentals\n<a href=\"https://blog.takipi.com/garbage-collectors-serial-vs-parallel-vs-cms-vs-the-g1-and-whats-new-in-java-8/\">take a look at this source</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>Because performance tuning is data driven, I&#8217;m going to use real-world data\nselected three very large Jenkins instances that I help support.</p>\n</div>\n<div class=\"paragraph\">\n<p><strong>What we&#8217;re not going to do:</strong> Jenkins basics, or play with max heap.  See the\nsection \"what should I do before tuning.\"  This is for cases where we really\n<strong>do</strong> need a big heap and can&#8217;t easily split our Jenkins controllers into smaller\nones.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"the-problem-hangups\"><a class=\"anchor\" href=\"#the-problem-hangups\"></a>The Problem: Hangups</h3>\n<div class=\"paragraph\">\n<p>Symptom: Users report that the Jenkins instance periodically hangs, taking\nseveral seconds to handle normally fast requests.  We may even see lockups or\ntimeouts from systems communicating with the Jenkins controller (build agents,\netc).  In long periods of heavy load, users may report Jenkins running slowly.\nApplication monitoring shows that during lockups all or most of the CPU cores\nare fully loaded, but there&#8217;s not enough activity to justify it.  Process and\nJStack dumps will reveal that the most active Java threads are doing garbage\ncollection.</p>\n</div>\n<div class=\"paragraph\">\n<p>With Instance A, they had this problem.  Their Jenkins Java arguments are very\nclose to the default, aside from sizing the heap:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>24 GB max heap, 4 GB initial, default GC settings (ParallelGC)</p>\n</li>\n<li>\n<p>A few flags set (some coming in as defaults): <code>-XX:-BytecodeVerificationLocal -XX:-BytecodeVerificationRemote -XX:+ReduceSignalUsage -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:-UseLargePagesIndividualAllocation</code></p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>After enabling garbage collection (GC) logging we see the following rough stats:</p>\n</div>\n<div class=\"paragraph\">\n<p><span class=\"image\"><img src=\"/images/post-images/gc-tuning/s-bulkstats-company-a-red-parallelgc.png\" alt=\"HeapStats Instance A System Red CPU use-parallelGC\"></span>.</p>\n</div>\n<div class=\"paragraph\">\n<p>Diving deeper, we get this chart of GC pause durations:</p>\n</div>\n<div class=\"paragraph\">\n<p><span class=\"image\"><img src=\"/images/post-images/gc-tuning/s-duration-company-a-red-parallelgc.png\" alt=\"Instance A Jenkins Red GC duration use-parallelGC\"></span></p>\n</div>\n<div class=\"paragraph\">\n<p><strong>Key stats:</strong></p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Throughput: 99.64%  (percent of time spent executing application code, not doing garbage collection)</p>\n</li>\n<li>\n<p>Average GC time: 348 ms (ugh!)</p>\n</li>\n<li>\n<p>GC cycles over 2 seconds: 36 (2.7%)</p>\n</li>\n<li>\n<p>Minor/Full GC average time: 263 ms / 2.803 sec</p>\n</li>\n<li>\n<p>Object creation &amp; promotion rate: 42.4 MB/s &amp; 1.99 MB/s</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p><strong>Explanations:</strong></p>\n</div>\n<div class=\"paragraph\">\n<p>As you can see, young GC cycles very quickly clear away freshly-created\ngarbage, but the deeper old-gen GC cycles run very slowly: 2-4 seconds. This is\nwhere our problems happen.  The default Java garbage collection algorithm\n(ParallelGC) pauses everything when it has to collect garbage (often called a\n\"stop the world pause\"). During that period, Jenkins is fully halted: normally\n(with small heaps) these pauses are too brief to be an issue.  With heaps of 4\nGB or larger, the time required becomes long enough to be a problem: several\nseconds over short windows, and over a longer interval you occasionally see\nmuch longer pauses (tens of seconds, or minutes.)</p>\n</div>\n<div class=\"paragraph\">\n<p>This is where the user-visible hangs and lock-ups happen.  It also adds\nsignificant latency to those build/deploy tasks.  In periods of heavy load, the\nsystem was even experiencing hangs of 30+ seconds for a single full GC cycle.\nThis was long enough to trigger network timeouts (or internal Jenkins thread\ntimeouts) and cause even larger problems.</p>\n</div>\n<div class=\"paragraph\">\n<p>Fortunately there&#8217;s a solution: the concurrent low-pause garbage collection\nalgorithms, Concurrent Mark Sweep (CMS) and Garbage First (G1). These attempt\nto do much of the garbage collection concurrently with application threads,\nresulting in much shorter pauses (at a slight cost in extra CPU use).  We&#8217;re\ngoing to focus on G1, because it is slated to become the default in Java 9 and\nis the official recommendation for large heap sizes.</p>\n</div>\n<div class=\"paragraph\">\n<p><strong>Let&#8217;s see what happens when someone uses G1 on a similarly-sized Jenkins\ncontroller with Instance B (17 GB heap):</strong></p>\n</div>\n<div class=\"paragraph\">\n<p>Their settings:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>16 GB max heap, 0.5 GB initial size</p>\n</li>\n<li>\n<p>Java flags (mostly defaults, except for G1): <code>-XX:+UseG1GC -XX:+UseCompressedClassPointers -XX:+UseCompressedOops</code></p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>And the GC log analysis:</p>\n</div>\n<div class=\"paragraph\">\n<p><span class=\"image\"><img src=\"/images/post-images/gc-tuning/s-duration-company-b-g1.png\" alt=\"Instance B Jenkins G1 duration\"></span></p>\n</div>\n<div class=\"paragraph\">\n<p><strong>Key stats:</strong></p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Throughput: 98.76%  (not great, but still only slowing things down a bit)</p>\n</li>\n<li>\n<p>Average GC time: 128 ms</p>\n</li>\n<li>\n<p>GC cycles over 2 seconds: 11, 0.27%</p>\n</li>\n<li>\n<p>Minor/Full GC average time: 122 ms / 1 sec 232 ms</p>\n</li>\n<li>\n<p>Object creation &amp; promotion rate: 132.53 MB/s &amp; 522 KB/s</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Okay, <strong>much better</strong>: some improvement may be expected from a 30% smaller\nheap, but not as much as we&#8217;ve seen.  Most of the GC pauses are well\nunder 2 seconds, but we have 11 outliers - long Full GC pauses of 2-12 seconds.\nThose are troubling; we&#8217;ll take a deeper dive into their causes in a second.\nFirst, let&#8217;s look at the big picture and at how Jenkins behaves with G1 GC for\na second instance.</p>\n</div>\n<div class=\"paragraph\">\n<p><strong>G1 Garbage Collection with Instance C (24 GB heap):</strong></p>\n</div>\n<div class=\"paragraph\">\n<p>Their settings:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>24 GB max heap, 24 GB initial heap, 2 GB max metaspace</p>\n</li>\n<li>\n<p>Some custom flags: `-XX:+UseG1GC -XX:+AlwaysPreTouch -XX:+UseStringDeduplication  -XX:+UseCompressedClassPointers -XX:+UseCompressedOops `</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Clearly they&#8217;ve done some garbage collection tuning and optimization.  The\nAlwaysPreTouch pre-zeros allocated heap pages, rather than waiting until\nthey&#8217;re first used. This is suggested especially for large heap sizes, because\nit trades slightly slower startup times for improved runtime performance.  Note\nalso that they pre-allocated the whole heap.  This is a common optimization.</p>\n</div>\n<div class=\"paragraph\">\n<p>They also enabled StringDeduplication, a G1 option introduced in Java 8 Update\n20 that transparently replaces identical character arrays with pointers to the\noriginal, reducing memory use (and improving cache performance).  Think of it\nlike <code>String.intern()</code> but it silently happens during garbage collection.  This\nis a concurrent operation added on to normal GC cycles, so it doesn&#8217;t pause the\napplication.  We&#8217;ll look at its impacts later.</p>\n</div>\n<div class=\"paragraph\">\n<p>Looking at the basics:</p>\n</div>\n<div class=\"paragraph\">\n<p><span class=\"image\"><img src=\"/images/post-images/gc-tuning/s-duration-company-c-g1.png\" alt=\"Instance C G1 duration\"></span></p>\n</div>\n<div class=\"paragraph\">\n<p>Similar picture to Instance B, but it&#8217;s hidden by the sheer number of points\n(this is a longer period here, 1 month).  Those same occasional Full GC\noutliers are present!</p>\n</div>\n<div class=\"paragraph\">\n<p><strong>Key stats:</strong></p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Throughput: 99.93%</p>\n</li>\n<li>\n<p>Average GC time: 127 ms</p>\n</li>\n<li>\n<p>GC cycles over 2 seconds: 235 (1.56%)</p>\n</li>\n<li>\n<p>Minor/Full GC average time: 56 ms / 3.97 sec</p>\n</li>\n<li>\n<p>Object creation &amp; promotion rate: 34.06 MB/s &amp; 286 kb/s</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Overall fairly similar to Instance B: ~100 ms GC cycles, all the minor GC\ncycles are very fast.  Object promotion rates sound similar.</p>\n</div>\n<div class=\"paragraph\">\n<p><strong>Remember those random long pauses?</strong></p>\n</div>\n<div class=\"paragraph\">\n<p>Let&#8217;s find out what caused them and how to get rid of them.  Instance B had 11\nsuper-long pause outliers.  Let&#8217;s get some more detail, by opening GC Logs in\n<a href=\"https://github.com/chewiebug/GCViewer\">GCViewer</a>.\nThis tool gives a tremendous amount of information.  Too much, in fact&#8201;&#8212;&#8201; I\nprefer to use\n<a href=\"https://gceasy.io/\">GCEasy.io</a>\nexcept where needed.  Since GC logs do not contain compromising information\n(unlike heap dumps or some stack traces), web apps are a great tool for\nanalysis.</p>\n</div>\n<div class=\"paragraph\">\n<p><span class=\"image\"><img src=\"/images/post-images/gc-tuning/s-gccauses-company-b-g1-highlighted.png\" alt=\"Instance B Jenkins G1 causes\"></span></p>\n</div>\n<div class=\"paragraph\">\n<p>What we care about are at the Full GC times in the middle (highlighted).  See\nhow much longer they are vs. the young and concurrent GC cycles up top (2\nseconds or less)?</p>\n</div>\n<div class=\"paragraph\">\n<p>Now, I lied a bit earlier - sorry!  For concurrent garbage collectors, there\nare actually 3 modes: young GC, concurrent GC, and full GC.  Concurrent GC\nreplaces the Full GC mode in Parallel GC with a faster concurrent operation\nthat runs in parallel with the application.  But in a few cases, we are\nforced to fall back to a non-concurrent Full GC operation, which will use the\nserial  (single-threaded) garbage collector.  That means that even if we have\n30+ CPU cores, only one is working. This is what is happening here, and on a\nlarge-heap, multicore system it is S  L  O  W.  How slow?  280 MB/s vs. 12487\nMB/s for Instance B (for instance C, the difference is also about 50:1).</p>\n</div>\n<div class=\"paragraph\">\n<p>What triggers a full GC instead of concurrent:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Explicit calls to <code>System.gc()</code>  (most common culprit, often tricky to trace down)</p>\n</li>\n<li>\n<p>Metadata GC Threshold: Metaspace (used for Class data mostly) has hit the\ndefined size to force garbage collection or increase it.  Documentation is\nterrible for this,\n<a href=\"https://stackoverflow.com/questions/25251388/what-is-the-metadata-gc-threshold-and-how-do-i-tune-it\">Stack Overflow</a>\nwill be your friend.</p>\n</li>\n<li>\n<p>Concurrent mode failure: concurrent GC can&#8217;t complete fast enough to keep up\nwith objects the application is creating (there are JVM arguments to trigger\nconcurrent GC earlier)</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p><strong>How do we fix this?</strong></p>\n</div>\n<div class=\"paragraph\">\n<p>For explicit GC:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>-XX:+DisableExplicitGC</code> will turn off Full GC triggered by <code>System.gc()</code>.  Often set in production, but the below option is safer.</p>\n</li>\n<li>\n<p>We can trigger a concurrent GC in place of a full one with <code>-XX:+ExplicitGCInvokesConcurrent</code> - this will take the explicit call as a hint to do deeper cleanup, but with less performance cost.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p><strong>Gotcha for people who&#8217;ve used CMS:</strong> if you have used CMS in the past, you\nmay have used the option <code>-XX:+ExplicitGCInvokesConcurrentAndUnloadsClasses</code>&#8201;&#8212;&#8201;which does what it says.  This option will silently fail in G1, meaning you\nstill see the very long pauses from Full GC cycles as if it wasn&#8217;t set (no\nwarning is generated).  I have logged a JVM bug for this issue.</p>\n</div>\n<div class=\"paragraph\">\n<p>For the Metadata GC threshold:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Increase your initial metaspace to the final amount to avoid resizing. For example: <code>-XX:MetaspaceSize=500M</code></p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Instance C also suffered the same problem with explicit GC calls, with almost\nall our outliers accounted for (230 out of 235) by slow, nonconcurrent Full GC\ncycles (all from explicit <code>System.gc()</code> calls, since they tuned metaspace):</p>\n</div>\n<div class=\"paragraph\">\n<p><span class=\"image\"><img src=\"/images/post-images/gc-tuning/s-gccauses-company-c-g1-highlighted.png\" alt=\"Instance C Jenkins G1 GC causes\"></span></p>\n</div>\n<div class=\"paragraph\">\n<p>Here&#8217;s what GC pause durations look like if we remove the log entries for the\nexplicit <code>System.gc()</code> calls, assuming that they&#8217;ll blend in with the other\nconcurrent GC pauses (not 100% accurate, but a good approximation):</p>\n</div>\n<div class=\"paragraph\">\n<p><strong>Instance B:</strong></p>\n</div>\n<div class=\"paragraph\">\n<p><span class=\"image\"><img src=\"/images/post-images/gc-tuning/s-duration-company-b-g1-explicitremoved.png\" alt=\"Instance B Jenkins GC duration - G1 - no explicit pauses\"></span></p>\n</div>\n<div class=\"paragraph\">\n<p>The few long Full GC cycles at the start are from metaspace expansion&#8201;&#8212;&#8201;they\ncan be removed by increasing initial Metaspace size, as noted above. The\nspikes?  That&#8217;s when we&#8217;re about to resize the Java heap, and memory pressure\nis high.  <strong>You can avoid this by setting the minimum/initial heap to at least\nhalf of the maximum, to limit resizing.</strong></p>\n</div>\n<div class=\"paragraph\">\n<p><strong>Stats:</strong></p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Throughput: 98.93%</p>\n</li>\n<li>\n<p>Average GC time: 111 ms</p>\n</li>\n<li>\n<p>GC cycles over 2 seconds: 3</p>\n</li>\n<li>\n<p>Minor &amp; Full or concurrent GC average time: 122 ms / 25 ms (yes, faster than minor!)</p>\n</li>\n<li>\n<p>Object creation &amp; promotion rate: 132.07 MB/s &amp; 522 kB/s</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p><strong>Instance C:</strong></p>\n</div>\n<div class=\"paragraph\">\n<p><span class=\"image\"><img src=\"/images/post-images/gc-tuning/s-duration-company-c-g1-explicitremoved.png\" alt=\"Instance C Jenkins G1 - no explicit pauses\"></span></p>\n</div>\n<div class=\"paragraph\">\n<p><strong>Stats:</strong></p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Throughput: 99.97%</p>\n</li>\n<li>\n<p>Average GC time: 56 ms</p>\n</li>\n<li>\n<p>GC cycles over 2 seconds: 0 (!!!)</p>\n</li>\n<li>\n<p>Minor &amp; Full or concurrent GC average time: 56 ms &amp; 10 ms (yes, faster than minor!)</p>\n</li>\n<li>\n<p>Object creation &amp; promotion rate: 33.31 MB/s &amp; 286 kB/s</p>\n</li>\n<li>\n<p>Side point: GCViewer is claiming GC performance of 128 GB/s (not unreasonable, we clear ~10 GB of young generation in under 100 ms usually)</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p><strong>Okay, so we&#8217;ve tamed the long worst-case pauses!</strong></p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"but-what-about-those-long-minor-gc-pauses-we-saw\"><a class=\"anchor\" href=\"#but-what-about-those-long-minor-gc-pauses-we-saw\"></a>But What About Those Long Minor GC Pauses We Saw?</h3>\n<div class=\"paragraph\">\n<p>Okay, now we&#8217;re in the home stretch!  We&#8217;ve tamed the old-generation GC pauses\nwith concurrent collection, but what about those longer young-generation\npauses?  Lets look at stats for the different phases and causes again in\nGCViewer.</p>\n</div>\n<div class=\"paragraph\">\n<p><span class=\"image\"><img src=\"/images/post-images/gc-tuning/s-gccauses-company-b-g1-noexplicit-highlighted.png\" alt=\"Instance C Jenkins G1 causes -no explicit pauses\"></span></p>\n</div>\n<div class=\"paragraph\">\n<p>Highlighted in yellow we see the culprit: the remark phase of G1 garbage\ncollection. This stop-the-world phase ensures we&#8217;ve identified all live\nobjects, and process references (\n<a href=\"https://www.infoq.com/articles/G1-One-Garbage-Collector-To-Rule-Them-All\">more info</a>).</p>\n</div>\n<div class=\"paragraph\">\n<p>Let&#8217;s look at a sample execution to get more info:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight nowrap\"><code>2016-09-07T15:28:33.104+0000: 26230.652: [GC remark 26230.652: [GC ref-proc, 1.7204585 secs], 1.7440552 secs]\n\n [Times: user=1.78 sys=0.03, real=1.75 secs]</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This turns out to be typical for the GC log: the longest pauses are spent in\nreference processing. This is not surprising because Jenkins internally uses\nreferences heavily for caching, especially weak references, and the default\nreference processing algorithm is single-threaded.  Note that user (CPU) time\nmatches real time, and it would be higher if we were using multiple cores.</p>\n</div>\n<div class=\"paragraph\">\n<p>So, we add the GC flag <code>-XX:+ParallelRefProcEnabled</code> which enables us to use the multiple cores more effectively.</p>\n</div>\n<div class=\"paragraph\">\n<p><strong>Tuning young-generation GC further based on Instance C:</strong></p>\n</div>\n<div class=\"paragraph\">\n<p>Back to GCViewer we go, to see what&#8217;s time consuming with the GC for Instance C.</p>\n</div>\n<div class=\"paragraph\">\n<p><span class=\"image\"><img src=\"/images/post-images/gc-tuning/s-gccauses-company-c-g1-noexplicit-highlighted.png\" alt=\"Instance C Jenkins G1 causes -no explicit pauses\"></span></p>\n</div>\n<div class=\"paragraph\">\n<p>That&#8217;s good, because most of the time is just sweeping out the trash\n(evacuation pause).  But the 1.8 second pause looks odd.  Let&#8217;s look at the raw\nGC log for the longest pause:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight nowrap\"><code>2016-09-24T16:31:27.738-0700: 106414.347: [GC pause (G1 Evacuation Pause) (young), 1.8203527 secs]\n[Parallel Time: 1796.4 ms, GC Workers: 8]\n [GC Worker Start (ms): Min: 106414348.2, Avg: 106414348.3, Max: 106414348.6, Diff: 0.4]\n[Ext Root Scanning (ms): Min: 0.3, Avg: 1.7, Max: 5.7, Diff: 5.4, Sum: 14.0]\n  [Update RS (ms): Min: 0.0, Avg: 7.0, Max: 19.6, Diff: 19.6, Sum: 55.9]\n    [Processed Buffers: Min: 0, Avg: 45.1, Max: 146, Diff: 146, Sum: 361]\n [Scan RS (ms): Min: 0.2, Avg: 0.4, Max: 0.7, Diff: 0.6, Sum: 3.5]\n [Code Root Scanning (ms): Min: 0.0, Avg: 0.0, Max: 0.1, Diff: 0.1, Sum: 0.2]\n [Object Copy (ms): Min: 1767.1, Avg: 1784.4, Max: 1792.6, Diff: 25.5, Sum: 14275.2]\n [Termination (ms): Min: 0.3, Avg: 2.4, Max: 3.5, Diff: 3.2, Sum: 19.3]\n    [Termination Attempts: Min: 11, Avg: 142.5, Max: 294, Diff: 283, Sum: 1140]\n [GC Worker Other (ms): Min: 0.0, Avg: 0.1, Max: 0.4, Diff: 0.3, Sum: 0.8]\n [GC Worker Total (ms): Min: 1795.9, Avg: 1796.1, Max: 1796.2, Diff: 0.3, Sum: 14368.9]\n [GC Worker End (ms): Min: 106416144.4, Avg: 106416144.5, Max: 106416144.5, Diff: 0.1]</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>&#8230;&#8203;oh, well dang. Almost the entire time (1.792 s out of 1.820) is walking\nthrough the live objects and copying them.  And wait, what about this line,\nshowing the summary statistics:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight nowrap\"><code>Eden: 13.0G(13.0G)-&gt;0.0B(288.0M) Survivors: 1000.0M-&gt;936.0M Heap: 20.6G(24.0G)-&gt;7965.2M(24.0G)]</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Good grief, we flushed out 13 GB (!!!) of freshly-allocated garbage in one\nswoop and compacted the leftovers!  No wonder it was so slow.  I wonder how we\naccumulated so much&#8230;&#8203;</p>\n</div>\n<div class=\"paragraph\">\n<p><span class=\"image\"><img src=\"/images/post-images/gc-tuning/s-younggen-company-c-g1-explicitremoved.png\" alt=\"Instance C Jenkins G1-ExplictGC removed\"></span></p>\n</div>\n<div class=\"paragraph\">\n<p>Oh, right&#8230;&#8203; we set up for 24 GB of heap initially, and each minor GC clears\nmost of the young generation.  Okay, so we&#8217;ve set aside tons of space for trash\nto collect, which means longer but less frequent GC periods.  This also gets\nthe best performance from Jenkins memory caches which are using WeakReferences\n(survives until collected by GC) and SoftReferences (more long-lived). Those\ncaches boost performance a lot.</p>\n</div>\n<div class=\"paragraph\">\n<p>We could take actions to prevent those rare longer pauses. The best ways are to\nlimit total heap size or reduce the value of <code>-XX:MaxGCPauseMillis=200</code> from\nits default (200).  A more advanced way (if those don&#8217;t help enough) is to\nexplicitly set the maximum size of the young generation smaller (say\n<code>-XX:G1MaxNewSizePercent=45</code> instead of the default of 60).  We could also\nthrow more CPUs at the problem.</p>\n</div>\n<div class=\"paragraph\">\n<p>But if we look up, most pauses are around 100 ms (200 ms is the default value\nfor MaxGCPauseMillis).  For Jenkins on this hardware, this appears to work\n<strong>just fine</strong> and a rare longer pause is OK as long as they don&#8217;t get too\nbig.  Also remember, if this happens often, G1 GC will try to autotune for\nlower pauses and more predictable performance.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"a-few-final-settings\"><a class=\"anchor\" href=\"#a-few-final-settings\"></a>A Few Final Settings</h3>\n<div class=\"paragraph\">\n<p>We mentioned StringDeduplication was on with Instance C, what is the impact?\nThis only triggers on Strings that have survived a few generations (most of our\ngarbage does not), has limits on the CPU time it can use, and replaces\nduplicate references to their immutable backing character arrays.\n<a href=\"https://java-performance.info/java-string-deduplication/\">For more info, look here</a>.\nSo, we should be trading a little CPU time for improved memory efficiently\n(similarly to string interning).</p>\n</div>\n<div class=\"paragraph\">\n<p>At the beginning, this has a huge impact:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight nowrap\"><code>[GC concurrent-string-deduplication, 375.3K-&gt;222.5K(152.8K), avg 63.0%, 0.0     024966 secs]\n[GC concurrent-string-deduplication, 4178.8K-&gt;965.5K(3213.2K), avg 65.3%, 0     .0272168 secs]\n[GC concurrent-string-deduplication, 36.1M-&gt;9702.6K(26.6M), avg 70.3%, 0.09     65196 secs]\n[GC concurrent-string-deduplication, 4895.2K-&gt;394.9K(4500.3K), avg 71.9%, 0     .0114704 secs]</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This peaks at an average of about ~90%:</p>\n</div>\n<div class=\"paragraph\">\n<p>After running for a month, less of an impact - many of the strings that can be\ndeduplicated already are:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight nowrap\"><code>[GC concurrent-string-deduplication, 138.7K-&gt;39.3K(99.4K), avg 68.2%, 0.0007080 secs]\n[GC concurrent-string-deduplication, 27.3M-&gt;21.5M(5945.1K), avg 68.1%, 0.0554714 secs]\n[GC concurrent-string-deduplication, 304.0K-&gt;48.5K(255.5K), avg 68.1%, 0.0021169 secs]\n[GC concurrent-string-deduplication, 748.9K-&gt;407.3K(341.7K), avg 68.1%, 0.0026401 secs]\n[GC concurrent-string-deduplication, 3756.7K-&gt;663.1K(3093.6K), avg 68.1%, 0.0270676 secs]\n[GC concurrent-string-deduplication, 974.3K-&gt;17.0K(957.3K), avg 68.1%, 0.0121952 secs]</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>However it&#8217;s cheap to use: in average, each dedup cycle takes 8.8 ms and\nremoves 2.4 kB of duplicates.  The median takes 1.33 ms and removes 17.66 kB\nfrom the old generation.  A small change per cycle, but in aggregate it adds up\nquickly&#8201;&#8212;&#8201;in periods of heavy load, this can save hundreds of megabytes of\ndata. But that&#8217;s still small, relative to multi-GB heaps.</p>\n</div>\n<div class=\"paragraph\">\n<p><strong>Conclusion: turn string deduplication on</strong> string deduplication is fairly\ncheap to use, and reduces the steady-state memory needed for Jenkins.  That\nfrees up more room for the young generation, and should overall reduce GC time\nby removing duplicate objects.  I think it&#8217;s worth turning on.</p>\n</div>\n<div class=\"paragraph\">\n<p><strong>Soft reference flushing:</strong> Jenkins uses soft references for caching build\nrecords and in pipeline FlowNodes.  The only guarantee for these is that they\nwill be removed instead of causing an OutOfMemoryError&#8230;&#8203; however Java\napplications can slow to a crawl from memory pressure long before that happens.\nThere&#8217;s an option that provides a hint to the JVM based on time &amp; free memory,\ncontrolled by <code>-XX:SoftRefLRUPolicyMSPerMB</code> (default 1000).  The SoftReferences\nbecome eligible for garbage collection after this many milliseconds have\nelapsed since last touch&#8230;&#8203; per MB of unused heap (vs the maximum).  The\nreferenced objects don&#8217;t count towards that target.  So, with 10 GB of heap\nfree and the default 1000 ms setting, soft references stick around for ~2.8\nhours (!).</p>\n</div>\n<div class=\"paragraph\">\n<p>If the system is continuously allocating more soft references, it may trigger\nheavy GC activity, rather than clearing out soft references. See the open bug\n<a href=\"https://bugs.openjdk.java.net/browse/JDK-6912889\">JDK-6912889</a>\nfor more details.</p>\n</div>\n<div class=\"paragraph\">\n<p>If Jenkins consumes excessive old generation memory, it <strong>may</strong> help to make soft\nreferences easier to flush  by reducing -XX:SoftRefLRUPolicyMSPerMB from its\ndefault (1000) to something smaller (say 10-200).  The catch is that\nSoftReferences are often used for objects that are relatively expensive to\nload, such lazy-loaded build records and pipeline FlowNode data.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"caveats\"><a class=\"anchor\" href=\"#caveats\"></a>Caveats</h3>\n<div class=\"paragraph\">\n<p><strong>G1 vs. CMS:</strong></p>\n</div>\n<div class=\"paragraph\">\n<p><strong>G1 was available on later releases of JRE 7, but unstable and slow.</strong>  If you\nuse it you absolutely must be using JRE 8, and the later the release the better\n(it&#8217;s gotten a lot of patches).  Googling around will show horrible G1 vs CMS\nbenchmarks from around 2014: these are probably best ignored, since the G1\nimplementation was still immature then. There&#8217;s probably a niche for CMS use\nstill, especially on midsized heaps (1-3 GB) or where settings are already\ntuned.  With appropriate tuning it <strong>can</strong> still perform generally well for\nJenkins (which mostly generates short-lived garbage), but CMS eventually suffer\nfrom heap fragmentation and need a slow, non-concurrent Full GC to clear this.\nIt also needs considerably more tuning than G1.</p>\n</div>\n<div class=\"paragraph\">\n<p><strong>General GC tuning caveats</strong>:</p>\n</div>\n<div class=\"paragraph\">\n<p>No single setting is perfect for everybody.  We avoid tweaking settings that we\ndon&#8217;t have strong evidence for here, but there are of course many additional\nsettings to tweak.  One shouldn&#8217;t change them without evidence though, because\nit can cause unexpected side effects.  The GC logs we enabled earlier will\ncollect this evidence.  The only setting that jumps out as a likely candidate\nfor further tuning is G1 region size (too small and there are many humungous\nobject allocations, which hurt performance).  Running on smaller systems,\nI&#8217;ve seen evidence that regions shouldn&#8217;t be smaller than 4 MB because\nthere are 1-2 MB objects allocated somewhat regularly&#8201;&#8212;&#8201;but it&#8217;s not\nenough to make solid guidance without more data.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"what-should-i-do-before-tuning-jenkins-gc\"><a class=\"anchor\" href=\"#what-should-i-do-before-tuning-jenkins-gc\"></a>What Should I Do Before Tuning Jenkins GC:</h3>\n<div class=\"paragraph\">\n<p>If you&#8217;ve seen\n<a href=\"https://www.cloudbees.com/so-you-want-build-worlds-biggest-jenkins-cluster\">Stephen Connolly&#8217;s excellent Jenkins World talk</a>,\nyou know that most Jenkins instances can and should get by with 4 GB or less of\nallocated heap, even up to very large sizes.  You will want to turn on GC\nlogging (suggested above) and look at stats over a few weeks (remember\n<a href=\"https://gceasy.io/\">GCeasy.io</a>).\nIf you&#8217;re not seeing periodic longer pause times, you&#8217;re probably okay.</p>\n</div>\n<div class=\"paragraph\">\n<p>For this post we assume we&#8217;ve already done the basic performance work for Jenkins:</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Jenkins is running on fast, SSD-backed storage.</p>\n</li>\n<li>\n<p>We&#8217;ve set up build rotation for your Jobs, to delete old builds so they don&#8217;t pile up.</p>\n</li>\n<li>\n<p>The weather column is already disabled for folders.</p>\n</li>\n<li>\n<p>All builds/deploys are running on build agents not on the controller. If the controller has executors allocated, they are exclusively used for backup tasks.</p>\n</li>\n<li>\n<p>We&#8217;ve verified that Jenkins really does need the large heap size and can&#8217;t easily be split into separate controllers.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<p>If not, we need to do that FIRST before looking at GC tuning, because those will have larger impacts.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"conclusions\"><a class=\"anchor\" href=\"#conclusions\"></a>Conclusions</h3>\n<div class=\"paragraph\">\n<p>We&#8217;ve gone from:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Average 350 ms pauses (bad user experience) including less frequent 2+ second generation pauses</p>\n</li>\n<li>\n<p>To an average pause of ~50 ms, with almost all under 250 ms</p>\n</li>\n<li>\n<p>Reduced total memory footprint from String deduplication</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>How:</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Use Garbage First (G1) garbage collection, which performs generally very well for Jenkins.  Usually there&#8217;s enough spare CPU time to enable concurrent running.</p>\n</li>\n<li>\n<p>Ensure explicit <code>System.gc()</code> and metaspace resizing do not trigger a Full GC because this can trigger a very long pause</p>\n</li>\n<li>\n<p>Turn on parallel reference processing for Jenkins to use all CPU cores fully.</p>\n</li>\n<li>\n<p>Use String deduplication, which generates a tidy win for Jenkins</p>\n</li>\n<li>\n<p>Enable GC logging, which can then be used for the next level of tuning and diagnostics, if needed.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<p>There&#8217;s still a little unpredictability, but using appropriate settings gives a\n<strong>much</strong> more stable, responsive CI/CD server&#8230;&#8203; even up to 20 GB heap sizes!</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"further-reading\"><a class=\"anchor\" href=\"#further-reading\"></a>Further Reading:</h3>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://product.hubspot.com/blog/g1gc-fundamentals-lessons-from-taming-garbage-collection\">G1GC fundamentals</a></p>\n</li>\n<li>\n<p><a href=\"https://mechanical-sympathy.blogspot.com/2013/07/java-garbage-collection-distilled.html\">MechanicalSympathy: Garbage Collection Distilled</a></p>\n</li>\n<li>\n<p><a href=\"https://www.oracle.com/technetwork/articles/java/g1gc-1984535.html\">Oracle Garbage First Garbage Collector Tuning</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"one-additional-thing\"><a class=\"anchor\" href=\"#one-additional-thing\"></a>One additional thing</h3>\n<div class=\"paragraph\">\n<p>I&#8217;ve added <code>-XX:+UnlockExperimentalVMOptions -XX:G1NewSizePercent=20</code> to our\noptions above.  This is covering a complex and usually infrequent case where G1\nself-tuning can trigger bad performance for Jenkins&#8201;&#8212;&#8201;but that&#8217;s material for\nanother post&#8230;&#8203;</p>\n</div>\n</div>","id":"bf6fcc51-7017-5159-91de-3d955a72998d","title":"Tuning Jenkins GC For Responsiveness and Stability with Large Instances","date":"2016-11-21T00:00:00.000Z","slug":"/blog/2016/11/21/gc-tuning/","authors":[{"avatar":null,"blog":null,"github":"svanoort","html":"","id":"svanoort","irc":null,"linkedin":null,"name":"Sam Van Oort","slug":"/blog/authors/svanoort/","twitter":null}]}},"pageContext":{"id":"bf6fcc51-7017-5159-91de-3d955a72998d"}},
    "staticQueryHashes": ["1271460761","3649515864"]}