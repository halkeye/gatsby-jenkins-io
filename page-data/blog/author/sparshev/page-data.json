{
    "componentChunkName": "component---src-templates-author-blog-list-template-js",
    "path": "/blog/author/sparshev",
    "result": {"data":{"author":{"avatar":{"childImageSharp":null},"blog":"https://www.state-of-the-art.io/","github":"sparshev","html":"<div class=\"paragraph\">\n<p>Sergei is a DevOps engineer and using Jenkins as a main automation tool since 2011.\nWants to automate everything to make sure that there no more room for boring tasks.</p>\n</div>","id":"sparshev","irc":null,"linkedin":null,"name":"Sergei Parshev","slug":"blog/author/sparshev","twitter":null},"allBlog":{"edges":[{"node":{"date":"2019-01-08T00:00:00.000Z","id":"058588ed-0d18-5d03-bd6b-e2e62be7093b","slug":"/blog/2019/01/08/mpl-modular-pipeline-library/","strippedHtml":"Despite speeding up development with deployment automation, one of our clients\nwas experiencing slow time-to-market due to a lack of collaboration in DevOps.\nWhile they had invested in DevOps, every production pipeline was set up\nindividually, forcing teams to remake the wheel for each project. Making matters\nworse, there was no cross-team collaboration, so any bug in the platform was\npresent in each new pipeline. Many of our clients have similar issues, so we\ndecided that we should develop a common tool which would both help current\nclients, and be adaptable for use in the future. While the most obvious option\nwas standardizing the CI/CD platform with a common framework, this led to a\nmonolithic structure, which was inflexible and ultimately unworkable. Since each\nteam needed to work on their own pipelines, we developed a solution that would\nstore each reusable part of the DevOps pipeline for later use: a Jenkins-powered\nmodular pipeline library.\n\nSolution: a modular pipeline library\n\nThe modular pipeline library ( MPL) we\ncreated is a highly-flexible shared library for a Jenkins Pipeline that enables\neasy sharing of best practices across the entire company. It has a clear modular\nstructure, an advanced testing framework, multi-level nesting, a pipeline\nconfiguration system, improved error handling, and many other useful components.\n\nWe will take a look under the hood and explain how our solution works in several\nparts:\n\nExplore the technologies and tools we used to build the MPL\n\nReview the MPL, and illustrate why it’s effective\n\nFollow a step-by-step guide to operate the MPL on a sample pipeline\n\nDive into some of the more important components of the solution, such as the test framework and nested libraries\n\nSo now let’s jump right into an explanation of the crucial features we used to\nbuild our solution.\n\nBuilding the MPL with shared libraries and Jenkins pipelines\n\nJenkins, our main automation platform, recently received some updates to\nJenkins Pipeline. These updates allow us to\ncreate one Jenkinsfile that\ndescribes the entire pipeline, and the steps that need to be executed with a\nseries of self-explanatory scripts. This increases the visibility of CI/CD\nautomation processes for end users, and improves supportability by DevOps teams.\n\nHowever, there’s a large issue with Pipeline: it’s hard to support multiple\nJenkinsfiles (and therefore multiple projects) with unique pipelines. We need to\nstore the common logic somewhere, which is where\nJenkins Shared Libraries\ncome in. They are included in the Jenkinsfile, and allow the use of prepared\ninterfaces to simplify automation and store common pieces.\n\nWhile shared libraries allow you to store logic and manipulate Jenkins, they\ndon’t provide a good way to utilize all the common information. Therefore, the\nMPL optimizes the pipeline and shared libraries by allowing users to create\neasy-to-follow descriptions for processes, which are then stored for later use\nby other teams.\n\nThe MPL works to create collaborative DevOps processes across teams\n\nWith the MPL, we are now able to collaborate and share our DevOps practices\nacross teams, easily adopt existing pipelines for specific projects, and debug\nand test features before we actually integrate them into the library. Each team\ncan create a nested library, add a number of pipelines and modules inside, and\nuse it with pipeline automation to create great visibility of the processes for\nthe end user. The MPL can also work on any project to prepare a Jenkinsfile, and\nmanage it as flexibly as the project team wants.\n\nAt its core, the MPL provides a simple way to:\n\nSeparate pipelines and steps by introducing modules\n\nDescribe steps in the modules with an easy configuration interface\n\nTest the described modules and share the results with other pipelines and projects\n\nThere are a lot of other features in the MPL, but it’s essentially a platform to\nsolve general DevOps collaboration issues. To simplify development and manual\ntesting, the MPL provides modules overriding and an inheritance model, allowing\nusers to test specific fixes in the project without affecting anything else. In\nJenkins, a module is a file with scripted steps and logic to reach a simple goal\n(build an artifact, run tests, create an image, etc.). These modules are\ncombined in the pipeline stages, and are easily readable for anyone who knows\nthe Jenkins Pipeline syntax.\n\nThe MPL allows users to use the core features of the library (structure,\nmodules, pipelines) and create nested libraries for specific DevOps team needs.\nA DevOps team can prepare complete pipelines with any custom logic and use it\nfor their projects. They can also override and inherit the core MPL modules in a\nnumber of ways, or prepare custom modules which are easy to share with other\nteams. Check out the infographic below to see how modules fit in:\n\nYou can also specify certain pipeline required poststeps in a module. For\nexample, a dynamic deployment module creates the test environment, which needs\nto be destroyed when the pipeline ends. To take a closer look at the MPL calling\nprocess, check out the infographic below:\n\nThis infographic shows how calls are executed in the MPL. First, you need a job\non your Jenkins, which will call a Jenkinsfile (for example, when the source\ncode is changed), after which the Jenkinsfile will call a pipeline. The pipeline\ncould be described on the MPL side, in the pipeline script in the job, in the\nnested library, or in the project Jenkinsfile. Finally, the stages of the\npipeline will call the modules, and these modules will use features, which could\nbe groovy logic, pipeline steps, or steps in the shared libraries.\n\nNow that we’ve done an overview of the solution, let’s take a look at a simple\npipeline execution to see how the MPL works in action.\n\nAn example of a pipeline execution in the MPL\n\nFor example, let’s say you have a common Java Maven project. You are creating a\nJenkinsfile in the repo, and want to use the default pipeline prepared by your\nDevOps team. The MPL already has a simple pipeline: the core MPLPipeline. It’s\na really simple pipeline, but it’s a good start for anyone who wants to try the\nMPL. Let’s look at a simple Jenkinsfile:\n\n@Library('mpl') _\nMPLPipeline {}\n\nThis Jenkinsfile contains a single line to load the MPL, and another line to run\nthe pipeline. Most of the shared libraries implement an interface like this,\ncalling one step and providing some parameters. MPLPipeline is merely a custom\nPipeline step, as it lies in the vars directory, and its structure is very\nsimple, following these steps:\n\nInitialize the MPL\nThe MPL uses the MPLManager singleton object to control the pipeline\n\nMerge configuration with default and store it\nA default configuration needed to specify stages and predefine some useful configs\n\nDefine a declarative pipeline with 4 stages and poststeps:\n\nCheckout - Getting the project sources\n\nBuild - Compiling, validation of static, unit tests\n\nDeploy - Uploading artifacts to the dynamic environment and running the app\n\nTest - Checking integration with other components\n\nPoststeps - Cleaning dynamic environment, sending notifications, etc.\n\nRunning the defined pipeline\nThis is where the MPL starts to work its magic and actually runs\n\nStages of the main MPL usually have just one step, the MPLModule .\nThis step contains the core functionality of the MPL: executing the modules\nwhich contain the pipeline logic. You can find default modules in the MPL\nrepository, which are placed in resources/com/griddynamics/devops/mpl/modules.\nSome of the folders include: Checkout, Build, Deploy, and Test, and in each of\nthem we can find Groovy files with the actual logic for the stages. This\ninfographic is a good example of a simplified MPL repository\nstructure:\n\nWhen the Checkout stage starts, MPLModule loads the module by name (by default\na stage name), and runs the Checkout/Checkout.groovy\nlogic:\n\nif( CFG.'git.url' )\n  MPLModule('Git Checkout', CFG)\nelse\n  MPLModule('Default Checkout', CFG)\n\nIf the configuration contains the git.url option, it will load a Git Checkout\nmodule; otherwise, it will run the Default Checkout module. All the called\nmodules use the same configuration as the parent module, which is why CFG was\npassed to the MPLModule call. In this case, we have no specific configuration,\nso it will run the\nCheckout/DefaultCheckout.groovy\nlogic. The space in the name is a separator to place the module into a specific\nfolder.\n\nIn the Default Checkout module, there is just one line with checkout scm\nexecution, which clones the repository specified in the Jenkins job. That’s all\nthe Checkout stage does, as the MPL functionality is excessive for such a small\nstage, and we only need to talk about it here to show how the MPL works in\nmodules.\n\nThe same process applies to the Build stage, as the pipeline runs the\nMaven Build\nmodule:\n\nwithEnv([\"PATH+MAVEN=${tool(CFG.'maven.tool_version' ?: 'Maven 3')}/bin\"]) {\n  def settings = CFG.'maven.settings_path' ? \"-s '${CFG.'maven.settings_path'}'\" : ''\n  sh \"\"\"mvn -B ${settings} -DargLine='-Xmx1024m -XX:MaxPermSize=1024m' clean install\"\"\"\n}\n\nThis stage is a little bit more complicated, but the action is simple: we take\nthe tool with the default name Maven 3, and use it to run mvn clean install.\nThe modules are scripted pipelines, so you can do the same steps usually\navailable in the Jenkins Pipeline. The files don’t need any specific and\ncomplicated syntax, just a plain file with steps and CFG as a predefined\nvariable with a stage configuration. The MPL modules inherited the sandbox from\nthe parent, so your scripts will be safe and survive the Jenkins restart, just\nlike a plain Jenkins pipeline.\n\nIn the Deploy folder, we find the sample structure of the Openshift Deploy\nmodule. Its main purpose here is to show how to use poststep definitions in the\nmodules:\n\nMPLPostStep('always') {\n  echo \"OpenShift Deploy Decommission poststep\"\n}\necho 'Executing Openshift Deploy process'\n\nFirst, we define the always poststep. It is stored in the MPLManager, and is\ncalled when poststeps are executed. We can call MPLPostStep with always as\nmany times as we want: all the poststeps will be stored and executed in FILO\norder. Therefore, we can store poststep logic for actions that need to be done,\nand then undone, in the same module, such as the decommission of the dynamic\nenvironment. This ensures that the actions will be executed when the pipeline\nis complete.\n\nAfter the deploy stage, the pipeline executes the Test stage, but nothing too\ninteresting happens there. However, there is an aspect of testing which is very\nimportant, and that’s the testing framework of the MPL itself.\n\nTesting of the MPL\n\nThe testing framework of the MPL is based on the\nJenkinsPipelineUnit\nfrom LesFurets, with the one small difference being its ability to test the MPL\nmodules. Testing the whole pipeline doesn’t work, as pipelines can be really\ncomplicated, and writing tests for such monsters is a Sisyphean task. It is much\neasier to test a black box with a small amount of steps, ensuring that this\nparticular task is working correctly.\n\nIn the MPL, you can find Build module testing examples: all the tests are\nstored in the\ntest/groovy/com/griddynamics/devops/mpl/modules\ndirectory, and you can find the\nBuild/BuildTest.groovy\nfile with a number of test cases there. Tests are executed during the MPL build\nprocess, allowing users to see traces like this:\n\nLoading shared library mpl with version snapshot\n  MPLModule.call(Build, {maven={tool_version=Maven 2}})\n    Build.run()\n      Build.MPLModule(Maven Build, {maven.tool_version=Maven 2})\n        MavenBuild.run()\n          MavenBuild.tool(Maven 2)\n          MavenBuild.withEnv([PATH+MAVEN=Maven 2_HOME/bin], groovy.lang.Closure)\n            MavenBuild.sh(mvn -B  -DargLine='-Xmx1024m -XX:MaxPermSize=1024m' clean install)\n      Build.fileExists(openshift)\n\nThe test runs the MPLModule with custom configuration and mocked steps to\ncheck that, during execution, the tool was changed to Maven 2 according to the\nprovided configuration. We cover all test cases with such tests, ensuring that\nthe modules are working as expected, and that the pipeline will work properly.\nYou can test the whole pipeline if you want, but testing by modules is just an\nadditional way to simplify the testing process.\n\nNow that we’ve looked at how to test the MPL modules, it’s time to look at one\nof the key features of the MPL, which is nested libraries.\n\nThe benefits of nested libraries\n\nWhen working with a large company, supporting one big library makes no sense.\nEach department requires multiple configuration options and tuning for a\nsomewhat standard pipeline, which creates extra work. The MPL solves such\nproblems by introducing nested libraries. This infographic displays how a nested\nlibrary compares to just using the main library:\n\nA nested library is the same as a shared library that imports the MPL and uses\nits functionality, modules, and pipelines. Also, it allows the separation of\nsome team-related logic from the company common logic. Here is the structure of\nthe MPL with nested libraries:\n\nYou can import the MPL in the overridden pipeline, specify the path of some\nadditional modules, override module logic, and use Jenkins power moves: there\nare no limitations. When another team needs your unique module, you can just\ncreate a change request to the basic company MPL repo, and share your functional\nmodule with the others.\n\nWith nested libraries, it’s possible to debug and modify MPL-provided steps\n( MPLModule for example) and pipelines. This is because nested libraries can\noverride low-level functionalities of the MPL or the Jenkins Pipeline. There are\nno limitations to what you can or can’t change, as these overrides only affect\nyour own pipeline. This enables experimentation to be done, and then discussed\nwith other teams to see if it will work in other nested libraries as well.\n\nThere are also no limits to the number of nesting levels created, but we\nrecommend using just two (MPL and nested), because additional levels make\nconfiguration and testing of the nested libraries on lower levels very\ncomplicated.\n\nThe power of module overriding\n\nFurther into the nested libraries or project-side modules, it’s possible to\nstore a module with the same name as one in the upper-level library. This is a\ngood way to override the logic - you can just replace Build/Build.groovy with\nyour own - as the functional module will be executed instead of the upper-level\nmodule. For example, this infographic shows module overriding:\n\nEven better, one of the strengths of the MPL is that you still can use the\nupper-level module! The MPL has mechanisms to prevent loops, so the same module\ncan’t be executed in the same executing branch again. However, you can easily\ncall the original module a name from another module to use the upper-level\nlogic.\n\nThe Petclinic-Selenium example above uses the default MPLPipeline (you can\nfind it on the MPL Wiki-page), and\ncontains project-side modules in a.jenkins directory. These modules will be\ncalled before the library modules. For example, the Checkout module is not\nplaced on the project side, so it will be called from the MPL, but the Build\nmodule exists in a.jenkins directory on the project side, and it will be\ncalled:\n\nMPLPostStep('always') {\n  junit 'target/surefire-reports/*.xml'\n}\n\nMPLModule('Build', CFG)\n\nif( fileExists('Dockerfile') ) {\n  MPLModule('Docker Build', CFG)\n}\n\nAs you can see, the Build module from the project registers the poststep,\ncalls the original Build module from the MPL, and then calls the additional\nDocker Build module. The following stages of the pipeline are more\ncomplicated, but all module overriding essentially works like this. Some\nprojects can be tricky, and need some small tunings for the existing modules.\nHowever, you can easily implement those changes on the project level, and think\nabout how to move the functionality to the nested library or MPL later.\n\nConclusion: what the MPL brings to DevOps\n\nMany DevOps teams and companies work with bloated, restrictive, and buggy CI/CD\nautomation platforms. These increase the learning curve for users, cause teams\nto work slower, and raise production costs. DevOps teams frequently run into\nsimilar issues on different projects, but a lack of collaboration means that\nthey have to be individually fixed each time.\n\nHowever, with the MPL, DevOps teams have a shared, simple, and flexible CI/CD\nplatform to improve user support, collaboration, and overall project source code\nto the production process. By utilizing the MPL, your company can find an\nautomation consensus, reach cross-company collaboration goals, and reuse the\nbest practices from a large community, all with open source tools. If you’re\ninterested in building an MPL, please contact us to learn more!\n\nAdditional resources\n\nJenkins Pipeline Engine\n\nJenkins Shared Libraries\n\nMPL GitHub repository\n\nOverview & demo videos:\n\nIntroduction\n\nOverview\n\nDemo of the MPL Build\n\nDemo of the Nested Library\n\nDemo of the Petclinic Pipeline","title":"MPL - Modular Pipeline Library","tags":["jenkinsfile","pipeline","sharedlibrary"],"authors":[{"avatar":{"childImageSharp":null},"blog":"https://www.state-of-the-art.io/","github":"sparshev","html":"<div class=\"paragraph\">\n<p>Sergei is a DevOps engineer and using Jenkins as a main automation tool since 2011.\nWants to automate everything to make sure that there no more room for boring tasks.</p>\n</div>","id":"sparshev","irc":null,"linkedin":null,"name":"Sergei Parshev","slug":"blog/author/sparshev","twitter":null}]}}]}},"pageContext":{"author":"sparshev","limit":8,"skip":0,"numPages":1,"currentPage":1}},
    "staticQueryHashes": ["3649515864"]}