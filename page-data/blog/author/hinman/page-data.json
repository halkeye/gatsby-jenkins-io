{
    "componentChunkName": "component---src-templates-author-blog-list-template-js",
    "path": "/blog/author/hinman",
    "result": {"data":{"author":{"avatar":null,"blog":null,"github":null,"html":"","id":"hinman","irc":null,"linkedin":null,"name":"Hannah Inman","slug":"/blog/author/hinman","twitter":null},"allBlog":{"edges":[{"node":{"date":"2017-08-17T00:00:00.000Z","id":"9f425a12-2f6e-51cd-8c44-5ddce8f1bcb3","slug":"/blog/2017/08/17/speaker-blog-blazemeter/","strippedHtml":"This is a guest post by Guy Salton, Sr. Professional Services Engineer for\nCA BlazeMeter.\n\nJenkins\nPipeline is an important Jenkins feature for creating and managing a project\nin Jenkins. This is opposed to the traditional way of creating a Jenkins\nproject by using the Jenkins GUI. When running your open-source load test,\nJenkins Pipeline enables resilience, execution control, advanced logic and\nVersion Control management.  This blog post will explain how to run any\nopen-source load test with Jenkins Pipeline, through Taurus.\n\nTaurus is an open source test automation framework\nthat enables running and analyzing tests from 9 open source load and functional\ntesting tools: JMeter,\nSelenium, Gatling, The Grinder, Locust, Tsung, Siege, Apache Bench, and PBench.\nTest results can be analyzed in Taurus. For advanced analyses or running tests\nin the cloud, Taurus integrates with\nBlazeMeter.\n\nGuy will be\npresenting\nmore on this topic at\nJenkins World in August,\nregister with the code JWFOSS for a 30% discount off your pass.\n\nGetting started with Taurus\n\nInstall Taurus.\n\nCreate the following Taurus configuration in YAML. Learn more about YAML in Taurus from\nthis tutorial.\n\n## execution:\n- concurrency: 100\n  hold-for: 10m\n  ramp-up: 120s\n  scenario: Thread Group\nscenarios:\n  Thread Group:\n    requests:\n    - label: blazedemo\n      method: GET\n      url: http://blazedemo.com/\n\nThis script runs 100 concurrent users, holds the load for 10 minutes, the\nramp-up is 120 seconds and the thread group runs one GET request to\nblazedemo.com.\n\nYou can specify an executor by adding executor: to the\nscript. Otherwise, the default executor will be JMeter. In the background,\nTaurus will create an artifact directory with a jmx file (or a Scala file if\nyou run Gatling, a Python file if you are running Selenium, etc.).\n\nOpen a terminal and run: bzt.yml\n\nView the test results:\n\nIf you want to conduct an in-depth analysis of your test results, run your\ntests on BlazeMeter. You will be able to monitor KPIs through advanced and\ncolorful reports, evaluate system health over time, and run your tests from\nmultiple geo-locations.\n\nRun the following command from the terminal:\n\nbzt.yml -report\n\nIntegrate Taurus With Pipeline\n\nTo run Taurus through Pipeline, you can also go\nstraight to Jenkins after creating your Taurus script.\n\nOpen Jenkins → New Item → Fill in an item name → Click on ‘Pipeline’\n\nNow create a Pipeline script. You can include all parts of\nyour CI/CD process in this script: Commit, Build, Unit Test, Performance Test,\netc., by creating different stages.\n\nThis Pipeline has three stages: The first is called “build”. In this example it\nis empty, but you can add commands that will build your code. The second,\ncalled “Performance Tests”, creates a folder called “Taurus-Repo” and runs the\nTaurus script that we created. At the same time (note the “parallel” command),\nthere is a “sleep” command for 60 seconds. Obviously it makes no sense to put\nthose two commands together, this is just to show you the option of running 2\ncommands in parallel. The third stage called “Deploy” is also empty in this\nexample. This is where you could deploy your new version.\n\nnode {\n   stage('Build') {\n      // Run the Taurus build\n   }\n   stage('Performance Tests') {\n    parallel(\n        BlazeMeterTest: {\n            dir ('Taurus-Repo') {\n                sh 'bzt.yml -report'\n            }\n        },\n        Analysis: {\n            sleep 60\n        })\n   }\n\n   stage(‘Deploy’) {\n   }\n}\n\nNote that you can either add the Pipeline inline, or choose the “Pipeline\nscript from SCM” option and add the URL to the script on GitHub (in this case\nyou need to upload a Jenkinsfile to GitHub). With \"Pipeline from SCM\",\nwhenever you need to update the tests, you can just add new commits to the\nJenkinsfile.\n\nSave the Pipeline\n\nClick on ‘Build Now’ to run the Pipeline\n\nClick on the new Build that is running now (build #6 in this example).\n\nClick on ‘Console Output’ to see the test results:\n\nIn the Console Output you can see the test results and also the link to the report in BlazeMeter.\n\nThat’s it! Jenkins Pipeline is now running open-source load testing tools via Taurus.\n\nCome to\nmy\nfree hands-on workshop “Learn to Release Faster by Load Testing With Jenkins”\nat Jenkins World 2017 on Tuesday August 29th from 1-5pm.  You will learn how to\ntest continuously with Jenkins, JMeter, BlazeMeter and Taurus, including how to\nrun JMeter with Jenkins, run the BlazeMeter plugin for Jenkins and how to use\nopen-source Taurus.\n\nTo learn more about BlazeMeter,\nclick here.","title":"Running load tests in Jenkins Pipeline with Taurus","tags":["event","jenkinsworld"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"","id":"hinman","irc":null,"linkedin":null,"name":"Hannah Inman","slug":"/blog/author/hinman","twitter":null}]}},{"node":{"date":"2017-07-17T00:00:00.000Z","id":"0116e3f0-03af-5f1b-b064-5d44aaf60398","slug":"/blog/2017/07/17/speaker-blog-care/","strippedHtml":"This is a guest post by Mandy Hubbard, Software Engineer/QA Architect at\nCare.com.\n\nImagine this: It’s 4:30pm on a Friday,\nyou have a major release on Monday, and your Jenkins server goes down.\nIt doesn’t matter if it experienced a hardware failure,\nfell victim to a catastrophic\nfat-finger error,\nor just got hit by a meteor - your Jenkins server is toast.\nHow long did it take to perfect your Pipeline,\nall your Continuous Delivery jobs, plugins, and credentials?\nHopefully you at least have a recent backup of your Jenkins home directory,\nbut you’re still going have to work over the weekend with IT to procure a new server,\ninstall it, and do full regression testing to be up and running by Monday morning.\nGo ahead and take a moment, go to your car and just scream.\nIt will help …​ a little.\n\nBut what if you could have a Jenkins environment that is completely disposable,\none that could be easily rebuilt at any time?\nUsing Docker and Joyent’s\nContainerPilot, the team at\nCare.com HomePay\nhas created a production Jenkins environment that is completely software-defined.\nEverything required to set up a new Jenkins environment is stored in source control,\nversioned, and released just like any other software.\nAt Jenkins World, I’ll do a developer deep-dive into this approach during my technical session,\nIndispensable, Disposable Jenkins,\nincluding a demo of bringing up a fully configured Jenkins server in a Docker container.\nFor now, let me give you a basic outline of what we’ve done.\n\nMandy will be\npresenting\nmore on this topic at\nJenkins World in August,\nregister with the code JWFOSS for a 30% discount off your pass.\n\nFirst, we add ContainerPilot to our Jenkins image by including it in the Dockerfile.\n\nDockerfile\n\n## ContainerPilot\n\nENV CONTAINERPILOT_VERSION 2.7.0\nENV CONTAINERPILOT_SHA256 3cf91aabd3d3651613942d65359be9af0f6a25a1df9ec9bd9ea94d980724ee13\nENV CONTAINERPILOT file:///etc/containerpilot/containerpilot.json\n\nRUN curl -Lso /tmp/containerpilot.tar.gz https://github.com/joyent/containerpilot/releases/download/${CONTAINERPILOT_VERSION}/containerpilot-${CONTAINERPILOT_VERSION}.tar.gz && \\\n    echo \"${CONTAINERPILOT_SHA256}  /tmp/containerpilot.tar.gz\" | sha256sum -c && \\\n    tar zxf /tmp/containerpilot.tar.gz -C /bin && \\\nrm /tmp/containerpilot.tar.gz\n\nThen we specify containerpilot as the Docker command in the docker-compose.yml\nand pass the Jenkins startup script as an argument.\nThis allows ContainerPilot to perform our preStart business before starting the Jenkins server.\n\ndocker-compose.yml\n\njenkins:\n    image: devmandy/auto-jenkins:latest\n    restart: always\n    mem_limit: 8g\n    ports:\n      - 80\n      - 22\n    dns:\n      - 8.8.8.8\n      - 127.0.0.1\n    env_file: _env\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    environment:\n      - CONSUL=consul\n    links:\n      - consul:consul\n    ports:\n      - \"8080:80\"\n      - \"2222:22\"\n    command: >\n      containerpilot\n      /usr/local/bin/jenkins.sh\n\nConfiguration data is read from a Docker Compose _env file,\nas specified in the docker-compose.yml file,\nand stored in environment variables inside the container.\nThis is an example of our _env file:\n\n_env\n\nGITHUB_TOKEN=\nGITHUB_USERNAME=DevMandy\nGITHUB_ORGANIZATION=DevMandy\nDOCKERHUB_ORGANIZATION=DevMandy\nDOCKERHUB_USERNAME=DevMandy\nDOCKERHUB_PASSWORD=\nDOCKER_HOST=\nSLACK_TEAM_DOMAIN=DevMandy\nSLACK_CHANNEL=jenkinsbuilds\nSLACK_TOKEN=\nBASIC_AUTH=\nAD_NAME=\nAD_SERVER=\nPRIVATE_KEY=\n\nJenkins stores its credentials and plugin information in various xml files.\nThe preStart script modifies the relevant files,\nsubstituting the environment variables as appropriate,\nusing a set of command line utilities called xmlstarlet.\nHere is an example method from our preStart script that configures Github credentials:\n\ngithub_credentials_setup() {\n    ## Setting Up Github username in credentials.xml file\n    echo\n    echo -e \"Adding Github username to credentials.xml file for SSH key\"\n    xmlstarlet \\\n        ed \\\n        --inplace \\\n        -u '//com.cloudbees.jenkins.plugins.sshcredentials.impl.BasicSSHUserPrivateKey[id=\"github\"]/username' \\\n        -v ${GITHUB_USERNAME} \\\n        ${JENKINS_HOME}/credentials.xml\n\n    echo -e \"Adding Github username to credentials.xml file for Github token\"\n    xmlstarlet \\\n        ed \\\n         --inplace \\\n        -u '//com.cloudbees.plugins.credentials.impl.UsernamePasswordCredentialsImpl[id=\"github_token\"]/username' \\\n        -v ${GITHUB_USERNAME} \\\n        ${JENKINS_HOME}/credentials.xml\n\n    PASSWORD=${GITHUB_TOKEN}\n    echo -e \"Adding Github token to credentials.xml\"\n    xmlstarlet \\\n        ed \\\n        --inplace \\\n        -u '//com.cloudbees.plugins.credentials.impl.UsernamePasswordCredentialsImpl[id=\"github_token\"]/password' \\\n        -v ${PASSWORD} \\\n        ${JENKINS_HOME}/credentials.xml\n}\n\nThis approach can be used to automate all things Jenkins.\nThese are just a few of the things I’ll show you in my Jenkins World session,\nwhich you can build on to automate anything else your Jenkins environment needs.\n\nCreation of credentials sets for interacting with third party services\nlike Github, Docker Hub and Slack\n\nConfiguration of the Active Directory plugin\nand setup of matrix-based security\n\nConfiguration of the Github Organization plugin,\nwhich results in the automatic creation of all Jenkins pipeline jobs\nby scanning the organization for all repositories containing a Jenkinsfile\n\nConfiguration of the\nDocker Pipeline plugin, including creating templates for all custom build agents\n\nConfiguration of the Global Pipeline Libraries plugin\n\nConfiguration of the Slack Notifier plugin\n\nWith software-defined Jenkins, pipeline infrastructure\ngains the same flexibility and resiliency as the rest of the development pipeline.\nIf we decide to change our Jenkins configuration in any way –\nfor example installing a new plugin or upgrading an existing one,\nadding a new global library, or adding new Docker images for build agents –\nwe simply edit our preStart script to include these changes, build a new Docker image,\nand the Jenkins environment is automatically reconfigured when we start a new container.\nBecause the entire configuration specification lives in a Github repository,\nchanges are merged to the \"master\" branch using pull requests,\nand our Jenkins Docker image is tagged using\nsemantic versioning just like any other component.\nJenkins can be both indispensable and completely disposable at the same time.","title":"Indispensable, Disposable Jenkins","tags":["event","JenkinsWorld"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"","id":"hinman","irc":null,"linkedin":null,"name":"Hannah Inman","slug":"/blog/author/hinman","twitter":null}]}},{"node":{"date":"2017-07-13T00:00:00.000Z","id":"00ad337a-5c86-5032-be3e-f6f4934de80d","slug":"/blog/2017/07/13/speaker-blog-rosetta-stone/","strippedHtml":"This is a guest post by Kevin Burnett, DevOps Lead at\nRosetta Stone.\n\nHave you experienced that thing where you make a change in an app, and when you\ngo to check on the results of the build, you find an error that really doesn’t\nseem relevant to your change? And then you notice that your build is the first\nin over a year. And then you realize that you have accidentally become the\nsubject matter expert in this app.\n\nYou have no clue what change caused this failure or when that change occurred.\nDid one Jenkins agent become a\nsnowflake server,\naccruing cruft on the file system that is not cleaned up before each build?\nDid some unpinned external dependency upgrade in a backwards-incompatible fashion?\nDid the credentials the build plan was using to connect to source control get rotated?\nDid a dependent system go offline?\nOr - and I realize that this is unthinkable - did you legitimately break a test?\n\nNot only is this type of archaeological expedition often a bad time for the\nperson who happened to commit to this app (\"No good deed goes unpunished\"), but\nit’s also unnecessary. There’s a simple way to reduce the cognitive load it\ntakes to connect cause and effect: build more frequently.\n\nOne way we achieve this is by writing scripts to maintain our apps. When we\nbuild, the goal is that an equivalent artifact should be produced unless there\nwas a change to the app in source control. As such, we pin all of our\ndependencies to specific versions. But we also don’t want to languish on old\nversions of dependencies, whether internal or external. So we also have an\nauto-maintain script that bumps all of these versions and commits the result.\n\nI’ll give an example. We use docker to build and deploy our apps, and each app\ndepends on a base image that we host in a docker registry. So a Dockerfile in\none of our apps would have a line like this:\n\nFROM our.registry.example.com/rosettastone/sweet-repo:jenkins-awesome-project-sweet-repo-5\n\nWe build our base images in Jenkins and tag them with the Jenkins $BUILD_TAG,\nso this app is using build 5 of the rosettastone/sweet-repo base image.\nLet’s say we updated our sweet-repo base image to use ubuntu 16.04 instead of 14.04\nand this resulted in build 6 of the base image. Our auto-maintain script takes\ncare of upgrading an app that uses this base image to the most recent version.\nThe steps in the auto-maintain script look like this:\n\nFigure out what base image tag you’re using.\n\nFind the newest version of that base image tag by querying the docker registry.\n\nIf necessary, update the FROM line in the app’s Dockerfile to pull in the most recent version.\n\nWe do the same thing with library dependencies.\nIf our Gemfile.lock is referencing an old library, running auto-maintain will update things.\nThe same applies to the Jenkinsfile for each app. If we decide to implement a new policy where we\ndiscard old builds, we update auto-maintain so that it will bring each app into\ncompliance with the policy, by changing, for example, this Jenkinsfile :\n\nJenkinsfile (Before)\n\npipeline {\n  agent { label 'docker' }\n  stages {\n    stage('commit_stage') {\n      steps {\n        sh('./bin/ci')\n      }\n    }\n  }\n}\n\nto this:\n\nJenkinsfile (After)\n\npipeline {\n  agent { label 'docker' }\n  options {\n    buildDiscarder(logRotator(numToKeepStr: '100'))\n  }\n  stages {\n    stage('commit_stage') {\n      steps {\n        sh('./bin/ci')\n      }\n    }\n  }\n}\n\nWe try to account for these sorts of things (everything that we can) in our\nauto-maintain script rather than updating apps manually, since this reduces the\nfriction in keeping apps standardized.\n\nOnce you create an auto-maintain script (start small), you just have to run it.\nWe run ours based on both \"actions\" and \"non-actions.\" When an internal library\nchanges, we kick off app builds, so a library’s Jenkinsfile might look like\nthis:\n\nJenkinsfile\n\npipeline {\n  agent { label 'docker' }\n  stages {\n    stage('commit_stage') {\n      steps {\n        sh('./bin/ci')\n      }\n    }\n    stage('auto_maintain_things_that_might_be_using_me') {\n      steps {\n        build('hot-project/auto-maintain-all-apps/master')\n      }\n    }\n  }\n}\n\nWhen auto-maintain updates something in an app, we have it commit the change\nback to the app, which in turn triggers a build of that app, and—​if all is\nwell—​a production deployment.\n\nThe only missing link then for avoiding one-year build droughts is to get around\nthe problem where auto-maintain isn’t actually updating anything in a certain app.\nIf no dependencies are changing, or if the technology in question is not\nreceiving much attention, auto-maintain might not do anything for an\nextended period of time, even if the script is run on a schedule using\ncron . For those cases, putting\na cron trigger in the Pipeline for each app will ensure that builds still happen periodically:\n\nJenkinsfile\n\npipeline {\n  agent { label 'docker' }\n  triggers {\n    cron('@weekly')\n  }\n  stages {\n    stage('commit_stage') {\n      steps {\n        sh('./bin/ci')\n      }\n    }\n  }\n}\n\nIn most cases, these periodic builds won’t do anything different from the last\nbuild, but when something does break, this strategy will allow you to decide\nwhen you find out about it (by making your cron @weekly, @daily, etc)\ninstead of letting some poor developer find out about it when they do\nsomething silly like commit code to an infrequently-modified app.\n\nKevin will be\npresenting\nmore on this topic at\nJenkins World in August,\nregister with the code JWFOSS for a 30% discount off your pass.","title":"Automated Software Maintenance","tags":["event","jenkinsworld","jenkinsworld2017"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"","id":"hinman","irc":null,"linkedin":null,"name":"Hannah Inman","slug":"/blog/author/hinman","twitter":null}]}},{"node":{"date":"2017-06-27T00:00:00.000Z","id":"c68cdf2d-4588-5bfa-80b3-2a0c1581eedd","slug":"/blog/2017/06/27/speaker-blog-SAS-jenkins-world/","strippedHtml":"This is a guest post by Brent Laster, Senior Manager, Research and Development at\nSAS.\n\nJenkins Pipeline\nhas fundamentally changed how users can orchestrate their pipelines and workflows.\nEssentially, anything that you can do in a script or program can now be done in a Jenkinsfile or in a pipeline script created within the application.\nBut just because you can do nearly anything directly in those mechanisms doesn’t mean you necessarily should.\n\nIn some cases, it’s better to abstract the functionality out separately from your main Pipeline.\nPreviously, the main way to do this in Jenkins itself was through creating plugins.\nWith Jenkins 2 and the tight incorporation of Pipeline, we now have another approach – shared libraries.\n\nBrent will be\npresenting\nmore of this topic at Jenkins World in\nAugust, register with the code JWFOSS for a 30% discount off your pass.\n\nShared libraries\nprovide solutions for a number of situations that can be challenging or time-consuming to deal with in Pipeline.\nAmong them:\n\nProviding common routines that can be accessed across a number of pipelines or within a designated scope (more on scope later)\n\nAbstracting out complex or restricted code\n\nProviding a means to execute scripted code from calls in declarative pipelines (where scripted code is not normally allowed)\n\nSimplifying calls in a script to custom code that only differ by calling parameters\n\nTo understand how to use shared libraries in Pipeline, we first need to understand how they are constructed.\nA shared library for Jenkins consists of a source code repository with a structure like the one below:\n\nEach of the top-level directories has its own purpose.\n\nThe resources directory can have non-groovy resources that get loaded via the libraryResource step.\nThink of this as a place to store supporting data files such as json files.\n\nThe src directory uses a structure similar to the standard Java src layout.\nThis area is added to the classpath when a Pipeline that includes this shared library is executed.\n\nThe vars directory holds global variables that should be accessible from pipeline scripts.\nA corresponding.txt file can be included that defines documentation for objects here.\nIf found, this will be pulled in as part of the documentation in the Jenkins application.\n\nAlthough you might think that it would always be best to define library functions in the src structure, it actually works better in many cases to define them in the vars area.\nThe notion of a global variable may not correspond very well to a global function, but you can think of it as the function being a global value that can be pulled in and used in your pipeline.\nIn fact, to work in a declarative style pipeline, having your function in the vars area is the only option.\n\nLet’s look at a simple function that we can create for a shared library.\nIn this case, we’ll just wrap picking up the location of the Gradle installation from Jenkins and calling the corresponding executable with whatever tasks are passed in as arguments.\nThe code is below:\n\n/vars/gbuild.groovy\n\ndef call(args) {\n      sh \"${tool 'gradle3'}/bin/gradle ${args}\"\n}\n\nNotice that we are using a structured form here with the def call syntax.\nThis allows us to simply invoke the routine in our pipeline (assuming we have loaded the shared library) based on the name of the file in the vars area.\nFor example, since we named this file gbuild.groovy, then we can invoke it in our pipeline via a step like this:\n\ngbuild 'clean compileJava'\n\nSo, how do we get our shared library loaded to use in our pipeline?\nThe shared library itself is just code in the structure outlined above committed/pushed into a source code repository that Jenkins can access.\nIn our example, we’ll assume we’ve staged, committed, and pushed this code into a local Git repository on the system at /opt/git/shared-library.git.\n\nLike most other things in Jenkins, we need to first tell Jenkins where this shared library can be found and how to reference it \"globally\" so that pipelines can reference it specifically.\n\nFirst, though, we need to decide at what scope you want this shared library to be available.\nThe most common case is making it a \"global shared library\" so that all Pipelines can access it.\nHowever, we also have the option of only making shared libraries available for projects in a particular Jenkins Folder structure,\nor those in a Multibranch Pipeline, or those in a GitHub Organization pipeline project.\n\nTo keep it simple, we’ll just define ours to be globally available to all pipelines.\nDoing this is a two-step process.\nWe first tell Jenkins what we want to call the library and define some default behavior for Jenkins related to the library,\nsuch as whether we wanted it loaded implicitly for all pipelines.\nThis is done in the Global Pipeline Libraries section of the Configure System page.\n\nFor the second part, we need to tell Jenkins where the actual source repository for the shared library is located.\nSCM plugins that have been modified to understand how to work with shared libraries are called \" Modern SCM\".\nThe git plugin in one of these updated plugin, so we just supply the information in the same Configure System page.\n\nAfter configuring Jenkins so that it can find the shared library repository, we can load the shared library into our pipeline using the @Library(' ') annotation.\nSince Annotations\nare designed to annotate something that follows them,\nwe need to either include a specific import statement, or, if we want to include everything, we can use an underscore character as a placeholder.\nSo our basic step to load the library in a pipeline would be:\n\n@Library('Utilities2') _\n\nBased on this step, when Jenkins runs our Pipeline, it will first go out to the repository that holds the shared library and clone down a copy to use.\nThe log output during this part of the pipeline execution would look something like this:\n\nLoading library Utilities2@master\n > git rev-parse --is-inside-work-tree # timeout=10\nSetting origin to /opt/git/shared-libraries\n > git config remote.origin.url /opt/git/shared-libraries # timeout=10\nFetching origin...\nFetching upstream changes from origin\n > git --version # timeout=10\nusing GIT_SSH to set credentials Jenkins2 SSH\n > git fetch --tags --progress origin +refs/heads/*:refs/remotes/origin/*\n > git rev-parse master^{commit} # timeout=10\n > git rev-parse origin/master^{commit} # timeout=10\nCloning the remote Git repository\nCloning repository /opt/git/shared-libraries\n\nThen Pipeline can call our shared library gbuild function and translate it to the desired Gradle build commands.\n\nFirst time build.\nSkipping changelog.\n[Pipeline] }\n[Pipeline] // stage\n[Pipeline] stage\n[Pipeline] { (Compile)\n[Pipeline] tool\n[Pipeline] sh\n[gsummit17_lab2-4T357CUTJORMC2TIF7WW5LMRR37F7PM2QRUHXUNSRTWTTRHB3XGA]\nRunning shell script\n+ /usr/share/gradle/bin/gradle clean compileJava -x test\nStarting a Gradle Daemon (subsequent builds will be faster)\n\nThis is a very basic illustration of how using shared libraries work.\nThere is much more detail and functionality surrounding shared libraries, and extending your pipeline in general, than we can cover here.\n\nBe sure to catch my talk on\nExtending your Pipeline with Shared Libraries, Global Functions and External Code\nat Jenkins World 2017.\nAlso, watch for my new book on\nJenkins 2 Up and Running\nwhich will have a dedicated chapter on this – expected to be available later this year from O’Reilly.","title":"Extending your Pipeline with Shared Libraries, Global Functions and External Code","tags":["event","jenkinsworld","jenkinsworld2017"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"","id":"hinman","irc":null,"linkedin":null,"name":"Hannah Inman","slug":"/blog/author/hinman","twitter":null}]}},{"node":{"date":"2016-08-29T00:00:00.000Z","id":"95ef7134-7327-5121-a920-cbff69688c23","slug":"/blog/2016/08/29/jenkins-world-speaker-blog-goodgame/","strippedHtml":"This is a guest post by Jenkins World speaker David Hinske, Release\nEngineer at Goodgame Studios.\n\nHey there, my name is David Hinske and I work at Goodgame Studios (GGS), a game\ndevelopment company in Hamburg, Germany. As Release Engineer in a company with\nseveral development teams, it comes in handy using several Jenkins instances.\nWhile this approach works fine in our company and gives the developers a lot of\nfreedom, we came across some long-term problems concerning maintenance and\nstandards. These problems were mostly caused by misconfiguration or non-use of\nplugins. With “configuration as code” in mind, I took the approach to apply\nstatic code analysis with the help of SonarQube, a platform to manage code\nquality, for all of our Jenkins job configurations.\n\nAs a small centralized team, we were looking for an easy way to control the\nhealth of our growing Jenkins infrastructure. With considering “configuration\nas code“, I developed a simple extension of SonarQube, to manage the quality\nand usage of all spawned Jenkins instances. The given SonarQube features (like\ncustomized rules/metrics, quality profiles and dashboards) allow us and the\ndevelopment teams to analyze and measure the quality of all created jobs in our\ncompany. Even though Jenkins configuration analysis cannot cover all\nSonarQube’s axes of code quality, I think there is still potential for\nconventions/standards, duplications, complexity, potential bugs\n(misconfiguration) and design and architecture.\n\nThe results of this analysis can be used by all people working with Jenkins. To\nachieve this, I developed a simple extension of SonarQube, containing\neverything which is needed to hook up our SonarQube with our Jenkins\nenvironment. The implementation contains a new basic-language “Jenkins“ and an\ninitial set of rules.\n\nOf course the needs depend strongly on the way Jenkins is being used, so not\nevery rule implemented might be useful for every team, but this applies to all\ntypes of code analysis. The main inspirations for the rules were developer\nfeedback and some articles found in the web. The different ways Jenkins can be\nconfigured provides the potential for many more rules. With this new approach\nof quality analysis, we can enforce best practices like:\n\nPolling must die (Better to triggerb uilds from pushes than poll the\nrepository every x minutes).\n\nUse Log Rotator (Not using log-rotator can result in disk space problems on\nthe controller).\n\nUse agents/labels (Jobs should be defined where to run).\n\nDon’t build on the controller (In larger systems, don’t build on the controller).\n\nEnforce plugin usage (For example: Timestamp, Mask-Passwords).\n\nNaming sanity (Limit project names to a sane (e.g. alphanumeric) character\nset).\n\nAnalyze Groovy Scripts (For example: Prevent System.exit(0) in System Groovy\nScripts).\n\nBesides taking control of all configuration of any Jenkins instance we want,\nthere is also room for additional metrics, like measuring the amount and\ndifferent types of jobs (Freestyle/Maven etc…​) to get an overview about the\ngeneral load of the Jenkins instance. A more sophisticated idea is to measure\ncomplexity of jobs and even pipelines. As code, jobs configuration gets harder\nto understand the more steps are involved. On the one hand scripts, conditions\nand many parameters can negatively influence the readability, especially if you\nhave external dependencies (like scripts) in different locations. On the other\nhand, pipelines can also grow very complex when many jobs are involved and\nchained for execution. It will be very interesting for us to see where and why\ntoo complex pipelines are being created.\n\nOn visualization we rely on the data and its interpretation of SonarQube, which\noffers a big bandwidth of widgets. Everybody can use and customize the\ndashboards. Our centralized team for example has a separate dashboard where we\ncan get a quick overview over all instances.\n\nThe problem of \"growing\" Jenkins with maintenance problems is not new.\nEspecially when you have many developers involved, including with the access to\ncreate jobs and pipelines themselves, an analysis like this SonarQube plugin\nprovides can be useful for anyone who wants to keep their Jenkins in shape.\nCustomization and standards are playing a big role in this scenario. This blog\npost surely is not an advertisement for my developed plugin, it is more about\nthe crazy idea of using static code analysis for Jenkins job configuration. I\nhaven’t seen anything like it so far and I feel that there might be some\npotential behind this idea.\n\nJoin me at my Enforcing Jenkins Best Practices session at the 2016 Jenkins\nWorld to hear more!\n\nDavid will be\npresenting\nmore of this concept at\nJenkins World in September.\nRegister with the code JWFOSS for 20% off your full conference pass.","title":"Enforcing Jenkins Best Practices","tags":["event","jenkinsworld","jenkinsworld2016"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"","id":"hinman","irc":null,"linkedin":null,"name":"Hannah Inman","slug":"/blog/author/hinman","twitter":null}]}},{"node":{"date":"2016-08-17T00:00:00.000Z","id":"a1ee11d9-c611-5f2a-a517-91048345fcf6","slug":"/blog/2016/08/17/jenkins-world-speaker-blog-aquilent/","strippedHtml":"This is a guest post by Jenkins World speaker Neil Hunt, Senior DevOps Architect at Aquilent.\n\nIn smaller companies with a handful of apps and fewer silos, implementing CD\npipelines to support these apps is fairly straightforward using one of the many\ndelivery orchestration tools available today. There is likely a constrained\ntool set to support - not an abundance of flavors of applications and security\npractices - and generally fewer cooks in the kitchen. But in a larger\norganization, I have found that in the past, there were seemingly endless\nunique requirements and mountains to climb to reach this level of automation on\neach new project.\n\nNeil will be presenting more\nof this concept at Jenkins World in\nSeptember, register with the code JWFOSS for a 20% discount off your pass.\n\nEnter the Jenkins Pipeline plugin. My recently departed former company, a large\nfinancial services organization with a 600+ person IT organization and 150+\napplication portfolio, set out to implement continuous delivery\nenterprise-wide. After considering several pipeline orchestration tools, we\ndetermined the Pipeline plugin (at the time called Workflow) to be the superior\nsolution for our company. Pipeline has continued Jenkins' legacy of presenting\nan extensible platform with just the right set of features to allow\norganizations to scale its capabilities as they see fit, and do so rapidly. As\nearly adopters of Pipeline with a protracted set of requirements, we used it\nboth to accelerate the pace of onboarding new projects and to reduce the\nongoing feature delivery time of our applications.\n\nIn my presentation at Jenkins World, I will demonstrate the methods we used to\nenable this. A few examples:\n\nWe leveraged the Pipeline Remote File Loader plugin to write shared common\ncode and sought and received community enhancements to these functions.\n\nJenkinsfile, loading a shared AWS utilities function library\n\nawsUtils.groovy, snippets of some AWS functions\n\nWe migrated from EC2 agents to Docker-based agents running on Amazon’s\nElastic Container Service, allowing us to spin up new executors in seconds\nand for teams to own their own executor definitions.\n\nPipeline run #1 using standard EC2 executors, spinning up EC2 instance for each\nnode; Pipeline run #2 using shared ECS cluster with near-instant instantiation\nof a Docker agent in the cluster for each node.\n\nWe also created a Pipeline Library of common pipelines, enabling projects\nthat fit certain models to use ready-made end-to-end pipelines. Some\nexamples:\n\nMaven JAR Pipeline: Pipeline that clones git repository, builds JAR file\nfrom pom.xml, deploys to Artifactory, and runs maven release plugin to\nincrement next version\n\nAnuglar.JS Pipeline: Pipeline that executes a grunt and bower build, then\nruns S3 sync to Amazon S3 bucket in Dev, then Stage, then Prod buckets.\n\nPentaho Reports Pipeline: Pipeline that clones git repository, constructs\nzip file, and executes Pentaho Business Intelligence Platform CLI to import new\nset of reports in Dev, Stage, then Prod servers.\n\nPerhaps most critically, a shout-out to the saving grace of this quest for our\nsecurity and ops teams: the manual 'input' step! While the ambition of\ncontinuous delivery is to have as few of these as possible, this was the\nsingle-most pivotal feature in convincing others of Pipeline’s viability, since\nnow any step of the delivery process could be gate-checked by an LDAP-enabled\npermission group. Were it not for the availability of this step, we may still\nbe living in the world of: \"This seems like a great tool for development, but\nwe will have a segregated process for production deployments.\" Instead, we had\na pipeline full of many 'input' steps at first, and then used the data we\ncollected around the longest delays to bring management focus to them and unite\neveryone around the goal of strategically removing them, one by one.\n\nGoing forward, having recently joined Aquilent’s Cloud Solutions Architecture\nteam, I’ll be working with our project teams here to further mature the use of\nthese Pipeline plugin features as we move towards continuous delivery. Already,\nwe have migrated several components of our healthcare.gov project to Pipeline.\nThe team has been able to consolidate several Jenkins jobs into a single,\nvisible delivery pipeline, to maintain the lifecycle of the pipeline with our\napplication code base in our SCM, and to more easily integrate with our\nexternal tools.\n\nDue to functional shortcomings in the early adoption stages of the Pipeline\nplugin and the ever-present political challenges of shifting organizational\npolicy, this has been and continues to be far from a bruise-free journey. But\nwe plodded through many of these issues to bring this to fruition and\nultimately reduced the number of manual steps in some pipelines from 12 down to\n1 and brought our 20+ Jenkins-minute pipelines to only six minutes after months\nof iteration. I hope you’ll join this session at Jenkins World and learn about\nour challenges and successes in achieving the promise of continuous delivery at\nenterprise scale.","title":"Continuously Delivering Continuous Delivery Pipelines","tags":["event","jenkinsworld","jenkinsworld2016"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"","id":"hinman","irc":null,"linkedin":null,"name":"Hannah Inman","slug":"/blog/author/hinman","twitter":null}]}},{"node":{"date":"2016-08-11T00:00:00.000Z","id":"9a0e4574-a766-5c76-b7d9-8fc1d1fbcde3","slug":"/blog/2016/08/11/speaker-blog-edx-jenkins-world/","strippedHtml":"This is a guest post by Ben Patterson, Engineering Manager at\nedX.\n\nPicking a pear from a basket is straightforward when you can hold it in your hand, feel its weight, perhaps give a gentle squeeze, observe its color and look more closely at any bruises. If the only information we had was a photograph from one angle, we’d have to do some educated guessing.\n\nAs developers, we don’t get a photograph; we get a green checkmark or a red x. We use that to decide whether or not we need to switch gears and go back to a pull request we submitted recently. At edX, we take advantage of some Jenkins features that could give us more granularity on GitHub pull requests, and make that decision less of a guessing game.\n\nMultiple contexts reporting back when they’re available\n\nPull requests on our platform are evaluated from several angles: static code analysis including linting and security audits, javascript unit tests, python unit tests, acceptance tests and accessibility tests. Using an elixir of plugins, including the GitHub Pull Request Builder Plugin, we put more direct feedback into the hands of the contributor so s/he can quickly decide how much digging is going to be needed.\n\nFor example, if I made adjustments to my branch and know more requirements are coming, then I may not be as worried about passing the linter; however, if my unit tests have failed, I likely have a problem I need to address regardless of when the new requirements arrive. Timing is important as well. Splitting out the contexts means we can run tests in parallel and report results faster.\n\nDevelopers can re-run specific contexts\n\nOccasionally the feedback mechanism fails. It is oftentimes a flaky condition in a test or in test setup. (Solving flakiness is a different discussion I’m sidestepping. Accept the fact that the system fails for purposes of this blog entry.) Engineers are armed with the power of re-running specific contexts, also available through the PR plugin. A developer can say “jenkins run bokchoy” to re-run the acceptance tests, for example. A developer can also re-run everything with “jenkins run all”. These phrases are set through the GitHub Pull Request Builder configuration.\n\nMore granular data is easier to find for our Tools team\n\nSplitting the contexts has also given us important data points for our Tools team to help in highlighting things like flaky tests, time to feedback and other metrics that help the org prioritize what’s important. We use this with a log aggregator (in our case, Splunk) to produce valuable reports such as this one.\n\nI could go on! The short answer here is we have an intuitive way of divvying up our tests, not only for optimizing the overall amount of time it takes to get build results, but also to make the experience more user-friendly to developers.\n\nBen will be presenting more on this topic at\nJenkins World in September,\nregister with the code JWFOSS for a 20% discount off your pass.","title":"Using Jenkins for Disparate Feedback on GitHub","tags":["event","jenkinsworld","jenkinsworld2016"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"","id":"hinman","irc":null,"linkedin":null,"name":"Hannah Inman","slug":"/blog/author/hinman","twitter":null}]}},{"node":{"date":"2015-12-21T00:00:00.000Z","id":"7ab2189c-9e15-5a58-9230-a1a13f92071b","slug":"/blog/2015/12/21/december-jam-world-tour-toulouse-france/","strippedHtml":"On December 15, the Toulouse\nJAM\nwas co-hosted with the Toulouse\nJUG and Toulouse\nDevOps. Indeed it made sense since Jenkins is\nwritten in Java, makes use of Groovy code in many places (system groovy script,\njob dsl, workflow…​), and it also made sense to co-organize with the local\nDevOps community since Jenkins is also a great tool to enable Continuous\nIntegration, Continuous Delivery and automation in general. There were 103\nRSVPs, with 80 to 90 people in attendance.\n\nThere were 3 talks planned for the evening:\n\nJob DSL\nIntro [fr], by Ghislain Mahieux\n\nVideo recording\n\nWorkflow plugin [fr], by Michaël Pailloncy (co-maintainer of the Build Trigger Badge plugin)\n\nVideo recording\n\nFeedback on almost 10 years of CI and what’s upcoming [fr], demo with Jenkins build scaling with Docker Swarm, by Baptiste Mathus\n\nVideo recording\n\nPhotos can be found here","title":"December JAM World Tour: Toulouse, France","tags":["general","meetup","jenkinsci","pipeline","workflow"],"authors":[{"avatar":null,"blog":null,"github":null,"html":"","id":"hinman","irc":null,"linkedin":null,"name":"Hannah Inman","slug":"/blog/author/hinman","twitter":null}]}}]}},"pageContext":{"author":"hinman","limit":8,"skip":0,"numPages":5,"currentPage":1}},
    "staticQueryHashes": ["3649515864"]}