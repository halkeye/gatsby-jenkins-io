{
    "componentChunkName": "component---src-templates-author-blog-list-template-js",
    "path": "/blog/author/rtyler/page/3",
    "result": {"data":{"author":{"avatar":{"childImageSharp":null},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/author/rtyler","twitter":"agentdero"},"allBlog":{"edges":[{"node":{"date":"2016-08-31T00:00:00.000Z","id":"b54df086-f29e-5e1b-ba30-0dde098bb3d0","slug":"/blog/2016/08/31/scaling-jenkins-at-jenkins-world/","strippedHtml":"This is a guest post by R. Tyler Croy, who is a\nlong-time contributor to Jenkins and the primary contact for Jenkins project\ninfrastructure. He is also a Jenkins Evangelist at\nCloudBees, Inc.\n\nI find the topic of \"scaling Jenkins\" to be incredibly interesting because,\nmore often than not, scaling Jenkins isn’t just about scaling a single instance\nbut rather scaling an organization and its continuous delivery processes. In\nmany cases when people talk about \"scaling Jenkins\" they’re talking about\n\"Jenkins as a Service\" or \"Continuous Delivery as a Service\" which introduces a\nmuch broader scope, and also more organization-specific requirements, to the\nproblem.\n\nOne of my favorite parts of a big conference like\nJenkins World is getting to\nsee how other people are solving similar problems at different organizations,\nin essence:\n\" how\nthe sausage is made.\" This year’s Jenkins World will be no different, with a number\nof sessions by developers and engineers from the companies leading the way,\nscaling continuous delivery and Jenkins.\n\nRegister for Jenkins World in\nSeptember with the code JWFOSS for a 20% discount off your pass.\n\nIn the realm of \"scaling Jenkins\" the following sessions stand-out to me as\n\"must-attend\" for those interested in the space:\n\nJenkinsOps:\nAn Initiative to Streamline and Automate Jenkins\n\nSeptember 14th 4:15 PM - 5:00 PM, Exhibit Hall A-1\n\nNPR’s Digital Media team uses Jenkins to build, test and deploy code to various\nstaging and production environments. As the complexity of the software\ncomponents, environments and tests have grown - both generally and due to our\nquest to achieve continuous deployment - management of Jenkins has become a\nchallenge. In this talk, we share information about our “JenkinsOps” effort\nwhich has allowed us to automate many of the administrative tasks necessary to\nmanage feature code branches, handle deployments, run tests and configure our\nenvironments properly.\n\n— Paul Miles and Grant Dickie of NPR\n\nThinking\nInside the Container: A Continuous Delivery Story\n\nSeptember 15th 1:30 PM - 2:15 PM, Exhibit Hall C\n\nAt Riot Games, we build a lot of software. Come learn how we built an\nintegrated Docker solution using Jenkins that accepts Docker images submitted\nas build environments by engineers around the company. Our containerized farm\nnow creates over 10,000 containers per week and handles nearly 1,000 jobs at a\nrate of about 100 jobs per hour. All this is done with readily available, open\nsource Jenkins plugins. We’ll explore lessons learned, best practices and how\nto scale and build your own system, as well as why we chose to solve the\nproblem this way…and whether or not we succeeded!\n\n— Maxfield F Stewart of Riot Games\n\nHow\nto Do Continuous Delivery with Jenkins Pipeline, Docker and Kubernetes\n\nSeptember 15th 2:30 PM - 3:15 PM, Great America J\n\nIn this talk, we’ll show how to use Jenkins Pipeline together with Docker and\nKubernetes to implement a complete end-to-end continuous delivery and\ncontinuous improvement system for microservices and monolithic applications\nusing open source software. We’ll demonstrate how to easily create new\nmicroservices projects or import existing projects, have them automatically\nbuilt, system and integration tested, staged and then deployed. Once deployed,\nwe will also see how to manage and update applications using continuous\ndelivery practices along with integrated ChatOps - all completely automated!\n\n— James Strachan of Red Hat\n\nScaling\nJenkins with Docker: Swarm, Kubernetes or Mesos?\n\nSeptember 15th 2:30 PM - 3:15 PM, Exhibit Hall C\n\nThe Jenkins platform can be dynamically scaled by using several Docker cluster\nand orchestration platforms, using containers to run agents and jobs and also\nisolating job execution. But which cluster technology should be used? Docker\nSwarm? Apache Mesos? Kubernetes? How do they compare? All of them can be used\nto dynamically run jobs inside containers. This talk will cover these main\ncontainer clusters, outlining the pros and cons of each, the current state of\nthe art of the technologies and Jenkins support. I believe people will be very\ninterested in learning about the multiple options available.\n\n— Carlos Sanchez of CloudBees\n\nSo,\nYou Want to Build the World’s Biggest Jenkins Cluster?\n\nSeptember 15th 3:45 PM - 4:30 PM, Exhibit Hall C\n\nHow can we do it? We start with some real world results realized by Jenkins\nusers who have built large clusters and review how they got there. Next, we\nwill do experiments scaling some individual sub-components of Jenkins in\nisolation and see what challenges we will face when integrated. The famous\nlarge, distributed systems undoubtedly faced problems scaling - and we can\nlearn from them, too. The result will be recipes for building Jenkins\nclusters with different scaling capabilities. After all of this, you can\nbuild the biggest Jenkins cluster in the world…or maybe just make your own\nJenkins cluster more efficient.\n\n— Stephen Connolly of CloudBees\n\nJenkins at\nSplunk and Splunking Jenkins\n\nSeptember 15th 3:45 PM - 4:30 PM, Exhibit Hall A-1\n\nThis session will highlight how Splunk uses Jenkins to provide an end-to-end\nsolution in the development CI system. Attendees will see how test results are\ndelivered to a Splunk indexer, where they can be analyzed and presented in a\nvariety of ways. This session will also include a live demonstration.\n\n— Bill Houston of Splunk\n\nJenkins inside Google\n\nSeptember 15th 4:45 PM - 5:30 PM, Exhibit Hall C\n\nLast year, we presented our initial investigations and stress testing as we\nprepared to deploy a large-scale Jenkins installation at Google. Now, with a\nyear of real-world use under our belts, we’ll discuss how our expectations held\nup, what new issues we encountered and how we have addressed them.\n\n— David Hoover of Google\n\nIn addition to these, we will also be hosting a\nJenkins World\nContributor Summit where \"scaling\" relevant topics such as \"Storage\nPluggability\" will be discussed.\n\nThe Jenkins World agenda is packed\nwith even more sessions, so it should be a very informational event for\neverybody; hope to see you there!","title":"Scaling Jenkins at Jenkins World 2016","tags":["event","jenkinsworld","jenkinsworld2016"],"authors":[{"avatar":{"childImageSharp":null},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/author/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-08-30T00:00:00.000Z","id":"b2605074-9e8e-5012-bc3d-7f504b0e0b2f","slug":"/blog/2016/08/30/ask-experts-demos/","strippedHtml":"At this year’s Jenkins World,\nour events officer Alyssa has been working to\norganize various activities in the \"Open Source Hub\" on the expo floor.  Both\ndays of the conference (Sept. 14th and 15th), during the break for lunch, there\nwill be 15 minute demos by many of the\nexperts helping to staff\nthe Open Source Hub.\n\nDemo Schedule\n\nWednesday, September 14th\n\nTime\nSession\nDetails\nPresenter\n\n12:15 - 12:30\nBlue Ocean in Action\nShowcase of Blue Ocean and how it will make Jenkins a pleasure to use.\nKeith Zantow\n\n12:30 - 12:45\nNotifications with Jenkins Pipeline\nSending information to Slack, HipChat, email and more from your Pipeline\nLiam Newman\n\n12:45 - 13:00\nDocker and Pipeline\nLearn how to use Docker inside of Pipeline for clean, repeatable testing environments\nR Tyler Croy\n\n13:00 - 13:15\nGit plugin - large repos, submodule authentication and more\nTechniques for managing large Git repositories, Submodule authentication, Pipelines and Git\nMark Waite\n\n13:15 - 13:30\nFreestyle to Pipeline\nOverview of how easy it is to migrate from a confusing series of Freestyle Jobs to Jenkins Pipeline\nR Tyler Croy\n\n13:30 - 13:45\npackage.json and Jenkins\nUsing package.json to control your build; running tests, coverage and generating documentation in Jenkins\nCasey Vega\n\n13:45 - 14:00\nExtending Pipeline with Libraries\nWhen you have many jobs using similar configuration, it is natural to factor out the common parts into libraries. See some ways Pipeline lets you do this.\nJesse Glick\n\nThursday, September 15th\n\nTime\nSession\nDetails\nPresenter\n\n12:15 - 12:30\nA simpler way to define Jenkins Pipelines\nGet to know a new way to define your Pipelines in a more configuration-like way!\nAndrew Bayer\n\n12:30 - 12:45\nMultibranch Pipelines + Git symbolic-ref\nPipeline Multibranch Plugin is amazing, but is even better when used with\nGit symbolic references. The combination of the two gives users a way to create\nindividual Jenkins jobs for each of their build/test configurations, instead of\nusing a single parameterized job. I’ll show how to use these tools together to\nhome in on problematic tests, systems under test, or both.\nJon Hermansen\n\n12:45 - 13:00\nExternal Workspace Manager plugin for Jenkins Pipeline\nMeet the External Workspace Manager plugin, which supports managing workspaces across multiple Jenkins jobs running on different nodes and more!\nAlex Somai\n\n13:00 - 13:15\nOwnership plugin for Jenkins\nThe presentation will introduce the Ownership engine for Jenkins jobs, folders and nodes. The presentation will cover plugin WebUI features, Ownership-based security and integration with Jenkins Pipeline\nOleg Nenashev\n\n13:15 - 13:30\nPipelines for building and deploying Android apps\nUsing the various Android-related plugins for Jenkins, we will demonstrate pipelines to automatically build, test, and securely deploy Android apps.\nChristopher Orr\n\nAs you can see there is a lot to see in the Open Source Hub at Jenkins World.\nTo my knowledge these demos are not going to be recorded, so your only\nopportunities to see them might be at Jenkins World or your local\nJenkins Area Meetup!\n\nRegister for Jenkins World in\nSeptember with the code JWFOSS for a 20% discount off your pass.","title":"Demos at Jenkins World 2016","tags":["event","jenkinsworld","jenkinsworld2016"],"authors":[{"avatar":{"childImageSharp":null},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/author/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-08-26T00:00:00.000Z","id":"ddf1a677-43d3-5209-a6e7-d331d574df38","slug":"/blog/2016/08/26/ask-the-experts-jenkins-world/","strippedHtml":"Our events officer Alyssa has been working for\nthe past several weeks to organize the \"Open Source Hub\" at\nJenkins World 2016. The Hub\nis a location on the expo floor where contributors to the Jenkins project can hang\nout, share demos and help Jenkins users via the \"Ask the Experts\" program. Thus\nfar we have a great list of experts who have volunteered to help staff the\nbooth, which includes many frequent contributors, JAM\norganizers and board members.\n\nA few of the friendly folks you will see at Jenkins World are:\n\nPaul Allen -\nP4 Plugin\nmaintainer and Pipeline contributor.\n\nR Tyler Croy -\nJenkins infrastructure maintainer and\nboard member.\n\nJesse Glick - Pipeline\nmaintainer and long-time contributor to Jenkins\ncore.\n\nEddú Meléndez Gonzales - Organizer for\nthe Lima (Perú)\nJenkins Area Meetup and contributor to Spring.\n\nJon Hermansen - Organizer for the\nLos Angeles\nJenkins Area Meetup, developer and Pipeline user.\n\nOwen Mehegan -\nGitLab plugin\ncontributor, release engineer and copy editor for jenkins.io.\n\nOleg Nenashev -\nGoogle Summer of Code organizer, maintainer of multiple\nplugins and St.\nPetersburg Jenkins Area Meetup organizer.\n\nChristopher Orr - Maintainer of multiple\nAndroid-related plugins, including the\nAndroid\nEmulator plugin and contributor to numerous projects behind the scenes of\nJenkins.\n\nCasey Vega - Organizer for the\nLos Angeles\nJenkins Area Meetup and release engineer at Verizon Digital Media.\n\nMark Waite - Maintainer of the\nGit plugin and\ncontributor to a number of other Git-related plugins.\n\nDean Yu - Long-time contributor, board member\nand release engineer at Shutterfly.\n\nI hope that this list isn’t exhaustive! If you are an active member of the\nJenkins community and/or a contributor, consider taking part in the \"Ask the\nExperts\" program. It’s a great opportunity to bond with other contributors and\ntalk with fellow users at Jenkins World.\n\nYou will be able to find us in the expo hall under the \"Open Source Hub\" sign;\nplease stop by at Jenkins World to say hello, pick up stickers and to ask\nquestions!\n\nRegister for Jenkins World in\nSeptember with the code JWFOSS for a 20% discount off your pass.","title":"Ask the Experts at Jenkins World 2016","tags":["event","jenkinsworld","jenkinsworld2016"],"authors":[{"avatar":{"childImageSharp":null},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/author/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-08-10T00:00:00.000Z","id":"4f2eb2b7-1bb1-501e-8b2c-1b8c4ac3e6e3","slug":"/blog/2016/08/10/rails-cd-with-pipeline/","strippedHtml":"This is a guest post by R. Tyler Croy, who is a\nlong-time contributor to Jenkins and the primary contact for Jenkins project\ninfrastructure. He is also a Jenkins Evangelist at\nCloudBees, Inc.\n\nWhen the Ruby on Rails framework debuted it\nchanged the industry in two noteworthy ways: it created a trend of opinionated web\napplication frameworks ( Django,\nPlay, Grails) and it\nalso strongly encouraged thousands of developers to embrace test-driven\ndevelopment along with many other modern best practices (source control, dependency\nmanagement, etc). Because Ruby, the language underneath Rails, is interpreted\ninstead of compiled there isn’t a \"build\" per se but rather tens, if not\nhundreds, of tests, linters and scans which are run to ensure the application’s\nquality. With the rise in popularity of Rails, the popularity of application\nhosting services with easy-to-use deployment tools like Heroku or\nEngine Yard rose too.\n\nThis combination of good test coverage and easily automated deployments\nmakes Rails easy to continuously deliver with Jenkins. In this post we’ll cover\ntesting non-trivial Rails applications with Jenkins\nPipeline and, as an added bonus, we will add security scanning via\nBrakeman and the\nBrakeman\nplugin.\n\nTopics\n\nPreparing the app\n\nPreparing Jenkins\n\nWriting the Pipeline\n\nSecurity scanning\n\nDeploying the good stuff\n\nWrap up\n\nFor this demonstration, I used Ruby Central 's\ncfp-app :\n\nA Ruby on Rails application that lets you manage your conference’s call for\nproposal (CFP), program and schedule. It was written by Ruby Central to run the\nCFPs for RailsConf and RubyConf.\n\nI chose this Rails app, not only because it’s a sizable application with lots\nof tests, but it’s actually the application we used to collect talk proposals\nfor the \" Community Tracks\" at this\nyear’s Jenkins World. For the most part,\ncfp-app is a standard Rails application. It uses\nPostgreSQL for its database,\nRSpec for its tests and\nRuby 2.3.x as its runtime.\n\nIf you prefer to just to look at the code, skip straight to the\nJenkinsfile.\n\nPreparing the app\n\nFor most Rails applications there are few, if any, changes needed to enable\ncontinuous delivery with Jenkins. In the case of\ncfp-app, I added two gems to get\nthe most optimal integration into Jenkins:\n\nci_reporter, for test report\nintegration\n\nbrakeman, for security scanning.\n\nAdding these was simple, I just needed to update the Gemfile and the\nRakefile in the root of the repository to contain:\n\nGemfile\n\n# .. snip ..\ngroup :test do\n  # RSpec, etc\n  gem 'ci_reporter'\n  gem 'ci_reporter_rspec'\n  gem \"brakeman\", :require => false\nend\n\nRakefile\n\n# .. snip ..\nrequire 'ci/reporter/rake/rspec'\n# Make sure we setup ci_reporter before executing our RSpec examples\ntask :spec => 'ci:setup:rspec'\n\nPreparing Jenkins\n\nWith the cfp-app project set up, next on the list is to ensure that Jenkins itself\nis ready. Generally I suggest running the latest LTS of\nJenkins; for this demonstration I used Jenkins 2.7.1 with the following\nplugins:\n\nPipeline plugin\n\nBrakeman plugin\n\nCloudBees\nDocker Pipeline plugin\n\nI also used the\nGitHub\nOrganization Folder plugin to automatically create pipeline items in my\nJenkins instance; that isn’t required for the demo, but it’s pretty cool to see\nrepositories and branches with a Jenkinsfile automatically show up in\nJenkins, so I recommend it!\n\nIn addition to the plugins listed above, I also needed at least one\nJenkins agent with the Docker daemon installed and\nrunning on it. I label these agents with \"docker\" to make it easier to assign\nDocker-based workloads to them in the future.\n\nAny Linux-based machine with Docker installed will work, in my case I was\nprovisioning on-demand agents with the\nAzure\nplugin which, like the\nEC2 plugin,\nhelps keep my test costs down.\n\nIf you’re using Amazon Web Services, you might also be interested in\nthis blog post from\nearlier this year unveiling the\nEC2\nFleet plugin for working with EC2 Spot Fleets.\n\nWriting the Pipeline\n\nTo make sense of the various things that the Jenkinsfile needs to do, I find\nit easier to start by simply defining the stages of my pipeline. This helps me\nthink of, in broad terms, what order of operations my pipeline should have.\nFor example:\n\n/* Assign our work to an agent labelled 'docker' */\nnode('docker') {\n    stage 'Prepare Container'\n    stage 'Install Gems'\n    stage 'Prepare Database'\n    stage 'Invoke Rake'\n    stage 'Security scan'\n    stage 'Deploy'\n}\n\nAs mentioned previously, this Jenkinsfile is going to rely heavily on the\nCloudBees\nDocker Pipeline plugin. The plugin provides two very important features:\n\nAbility to execute steps inside of a running Docker container\n\nAbility to run a container in the \"background.\"\n\nLike most Rails applications, one can effectively test the application with two\ncommands: bundle install followed by bundle exec rake. I already had some\nDocker images prepared with RVM and Ruby 2.3.0 installed,\nwhich ensures a common and consistent starting point:\n\nnode('docker') {\n    // .. 'stage' steps removed\n    docker.image('rtyler/rvm:2.3.0').inside { (1)\nrvm 'bundle install' (2)\nrvm 'bundle exec rake'\n    } (3)\n}\n\n1\nRun the named container. The inside method can take optional additional flags for the docker run command.\n\n2\nExecute our shell commands using our tiny sh step wrapper\nrvm . This ensures that the shell code is executed in the correct RVM environment.\n\n3\nWhen the closure completes, the container will be destroyed.\n\nUnfortunately, with this application, the bundle exec rake command will fail\nif PostgreSQL isn’t available when the process starts. This is where the\nsecond important feature of the CloudBees Docker Pipeline plugin comes\ninto effect: the ability to run a container in the \"background.\"\n\nnode('docker') {\n    // .. 'stage' steps removed\n    /* Pull the latest `postgres` container and run it in the background */\n    docker.image('postgres').withRun { container -> (1)\necho \"PostgreSQL running in container ${container.id}\" (2)\n} (3)\n}\n\n1\nRun the container, effectively docker run postgres\n\n2\nAny number of steps can go inside the closure\n\n3\nWhen the closure completes, the container will be destroyed.\n\nRunning the tests\n\nCombining these two snippets of Jenkins Pipeline is, in my opinion, where the\npower of the DSL\nshines:\n\nnode('docker') {\n    docker.image('postgres').withRun { container ->\n        docker.image('rtyler/rvm:2.3.0').inside(\"--link=${container.id}:postgres\") { (1)\nstage 'Install Gems'\n            rvm \"bundle install\"\n\n            stage 'Invoke Rake'\n            withEnv(['DATABASE_URL=postgres://postgres@postgres:5432/']) { (2)\nrvm \"bundle exec rake\"\n            }\n            junit 'spec/reports/*.xml' (3)\n}\n    }\n}\n\n1\nBy passing the --link argument, the Docker daemon will allow the RVM container to talk to the PostgreSQL container under the host name 'postgres'.\n\n2\nUse the withEnv step to set environment variables for everything that is in the closure. In this case, the cfp-app DB scaffolding will look for the DATABASE_URL variable to override the DB host/user/dbname defaults.\n\n3\nArchive the test reports generated by ci_reporter so that Jenkins can display test reports and trend analysis.\n\nWith this done, the basics are in place to consistently run the tests for\ncfp-app in fresh Docker containers for each execution of the pipeline.\n\nSecurity scanning\n\nUsing Brakeman, the security scanner for Ruby\non Rails, is almost trivially easy inside of Jenkins Pipeline, thanks to the\nBrakeman\nplugin which implements the publishBrakeman step.\n\nBuilding off our example above, we can implement the \"Security scan\" stage:\n\nnode('docker') {\n    /* --8 (1)\npublishBrakeman 'brakeman-output.tabs' (2)\n/* --8\n\n1\nRun the Brakeman security scanner for Rails and store the output for later in brakeman-output.tabs\n\n2\nArchive the reports generated by Brakeman so that Jenkins can display detailed reports with trend analysis.\n\nAs of this writing, there is work in progress\n( JENKINS-31202) to\nrender trend graphs from plugins like Brakeman on a pipeline project’s main\npage.\n\nDeploying the good stuff\n\nOnce the tests and security scanning are all working properly, we can start to\nset up the deployment stage. Jenkins Pipeline provides the variable\ncurrentBuild which we can use to determine whether our pipeline has been\nsuccessful thus far or not. This allows us to add the logic to only deploy when\neverything is passing, as we would expect:\n\nnode('docker') {\n    /* --8 (1)\nsh './deploy.sh' (2)\n}\n    else {\n        mail subject: \"Something is wrong with ${env.JOB_NAME} ${env.BUILD_ID}\",\n                  to: 'nobody@example.com',\n                body: 'You should fix it'\n    }\n    /* --8\n\n1\ncurrentBuild has the result property which would be 'SUCCESS', 'FAILED', 'UNSTABLE', 'ABORTED'\n\n2\nOnly if currentBuild.result is successful should we bother invoking our deployment script (e.g. git push heroku master)\n\nWrap up\n\nI have gratuitously commented the full\nJenkinsfile\nwhich I hope is a useful summation of the work outlined above. Having worked\non a number of Rails applications in the past, the consistency provided by\nDocker and Jenkins Pipeline above would have definitely improved those\nprojects' delivery times. There is still room for improvement however, which\nis left as an exercise for the reader. Such as: preparing new containers with\nall their\ndependencies\nbuilt-in instead of installing them at run-time. Or utilizing the parallel\nstep for executing RSpec across multiple Jenkins agents simultaneously.\n\nThe beautiful thing about defining your continuous delivery, and continuous\nsecurity, pipeline in code is that you can continue to iterate on it!","title":"Continuous Security for Rails apps with Pipeline and Brakeman","tags":["tutorial","ruby","pipeline","rails","brakeman","continuousdelivery"],"authors":[{"avatar":{"childImageSharp":null},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/author/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-06-29T00:00:00.000Z","id":"72dc398a-0c3a-5b36-949e-7687e6ecbefc","slug":"/blog/2016/06/29/from-freestyle-to-pipeline/","strippedHtml":"This is a guest post by R. Tyler Croy, who is a\nlong-time contributor to Jenkins and the primary contact for Jenkins project\ninfrastructure. He is also a Jenkins Evangelist at\nCloudBees, Inc.\n\nFor ages I have used the \"Build After\" feature in Jenkins to cobble together\nwhat one might refer to as a \"pipeline\" of sorts. The Jenkins project itself, a\nmajor consumer of Jenkins, has used these daisy-chained Freestyle jobs to drive\na myriad of delivery pipelines in our infrastructure.\n\nOne such \"pipeline\" helped drive the complex process of generating the pretty\nblue charts on\nstats.jenkins.io.\nThis statistics generation process primarily performs two major tasks, on rather\nlarge sets of data:\n\nGenerate aggregate monthly \"census data.\"\n\nProcess the census data and create trend charts\n\nThe chained jobs allowed us to resume the independent stages of the pipeline,\nand allowed us to run different stages on different hardware (different\ncapabilities) as needed. Below is a diagram of what this looked like:\n\nThe infra_generate_monthly_json would run periodically creating the\naggregated census data, which would then be picked up by infra_census_push\nwhose sole responsibility was to take census data and publish it to the\nnecessary hosts inside the project’s infrastructure.\n\nThe second, semi-independent, \"pipeline\" would also run periodically. The\ninfra_statistics job’s responsibility was to use the census data, pushed\nearlier by infra_census_push, to generate the myriad of pretty blue charts\nbefore triggering the\ninfra_checkout_stats job which would make sure stats.jenkins.io was\nproperly updated.\n\nSuffice it to say, this \"pipeline\" had grown organically over a period time when\nmore advanced tools weren’t quite available.\n\nWhen we migrated to newer infrastructure for\nci.jenkins.io earlier this year I took the\nopportunity to do some cleaning up. Instead of migrating jobs verbatim, I pruned\nstale jobs and refactored a number of others into proper\nPipelines, statistics generation being an obvious\ntarget!\n\nOur requirements for statistics generation, in their most basic form, are:\n\nEnable a sequence of dependent tasks to be executed as a logical group (a\npipeline)\n\nEnable executing those dependent tasks on various pieces of infrastructure\nwhich support different requirements\n\nActually generate those pretty blue charts\n\nIf you wish to skip ahead, you can jump straight to the\nJenkinsfile\nwhich implements our new Pipeline.\n\nThe first iteration of the Jenkinsfile simply defined the conceptual stages we\nwould need:\n\nnode {\n    stage 'Sync raw data and census files'\n\n    stage 'Process raw logs'\n\n    stage 'Generate census data'\n\n    stage 'Generate stats'\n\n    stage 'Publish census'\n\n    stage 'Publish stats'\n}\n\nHow exciting! Although not terrifically useful. When I began actually\nimplementing the first couple stages, I noticed that the Pipeline might sync\ndozens of gigabytes of data every time it ran on a new agent in the cluster.\nWhile this problem will soon be solved by the\nExternal\nWorkspace Manager plugin, which is currently being developed. Until it’s ready,\nI chose to mitigate the issue by pinning the execution to a consistent agent.\n\n/* `census` is a node label for a single machine, ideally, which will be\n * consistently used for processing usage statistics and generating census data\n */\nnode('census && docker') {\n    /* .. */\n}\n\nRestricting a workload which previously used multiple agents to a single one\nintroduced the next challenge. As an infrastructure administrator, technically\nspeaking, I could just install all the system dependencies that I want on this\none special Jenkins agent. But what kind of example would that be setting!\n\nThe statistics generation process requires:\n\nJDK8\n\nGroovy\n\nA running MongoDB instance\n\nFortunately, with Pipeline we have a couple of useful features at our disposal:\ntool auto-installers and the\nCloudBees\nDocker Pipeline plugin.\n\nTool Auto-Installers\n\nTool Auto-Installers are exposed in Pipeline through the tool step and on\nci.jenkins.io we already had JDK8 and Groovy\navailable. This meant that the Jenkinsfile would invoke tool and Pipeline\nwould automatically install the desired tool on the agent executing the current\nPipeline steps.\n\nThe tool step does not modify the PATH environment variable, so it’s usually\nused in conjunction with the withEnv step, for example:\n\nnode('census && docker') {\n    /* .. */\n\n    def javaHome = tool(name: 'jdk8')\n    def groovyHome = tool(name: 'groovy')\n\n    /* Set up environment variables for re-using our auto-installed tools */\n    def customEnv = [\n        \"PATH+JDK=${javaHome}/bin\",\n        \"PATH+GROOVY=${groovyHome}/bin\",\n        \"JAVA_HOME=${javaHome}\",\n    ]\n\n    /* use our auto-installed tools */\n    withEnv(customEnv) {\n        sh 'java --version'\n    }\n\n    /* .. */\n}\n\nCloudBees Docker Pipeline plugin\n\nSatisfying the MongoDB dependency would still be tricky. If I caved in and installed\nMongoDB on a single unicorn agent in the cluster, what could I say the next time\nsomebody asked for a special, one-off, piece of software installed on our\nJenkins build agents?\n\nAfter doing my usual complaining and whining, I discovered that the CloudBees\nDocker Pipeline plugin provides the ability to run containers inside of a\nJenkinsfile. To make things even better, there are\nofficial MongoDB docker images readily\navailable on DockerHub!\n\nThis feature requires that the machine has a running Docker daemon which is\naccessible to the user running the Jenkins agent. After that, running a\ncontainer in the background is easy, for example:\n\nnode('census && docker') {\n    /* .. */\n\n    /* Run MongoDB in the background, mapping its port 27017 to our host's port\n     * 27017 so our script can talk to it, then execute our Groovy script with\n     * tools from our `customEnv`\n     */\n    docker.image('mongo:2').withRun('-p 27017:27017') { container ->\n        withEnv(customEnv) {\n            sh \"groovy parseUsage.groovy --logs ${usagestats_dir} --output ${census_dir} --incremental\"\n        }\n    }\n\n    /* .. */\n}\n\nThe beauty, to me, of this example is that you can pass a\nclosure to withRun which will\nexecute while the container is running. When the closure is finished executin,\njust the sh step in this case, the container is destroyed.\n\nWith that system requirement satisfied, the rest of the stages of the Pipeline\nfell into place. We now have a single source of truth, the\nJenkinsfile,\nfor the sequence of dependent tasks which need to be executed, accounting for\nvariations in systems requirements, and it actually generates\nthose pretty\nblue charts!\n\nOf course, a nice added bonus is the beautiful visualization of our\nnew Pipeline!\n\nLinks\n\nPipeline documentation\n\nCloudBees Docker Pipeline plugin documentation\n\nLive statistics Pipeline","title":"Migrating from chained Freestyle jobs to Pipelines","tags":["pipeline","infra"],"authors":[{"avatar":{"childImageSharp":null},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/author/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-05-18T00:00:00.000Z","id":"3098cab4-cd65-52ec-a4a9-2b36e2ac20d7","slug":"/blog/2016/05/18/announcing-azure-partnership/","strippedHtml":"I am pleased to announce that we have partnered with Microsoft to migrate and\npower the Jenkins project’s infrastructure with\nMicrosoft Azure. The partnership comes\nat an important time, after the recent launch of Jenkins 2.0,\nJenkins users are more readily adopting Pipeline as\nCode and many other plugins at an increasing rate, elevating the importance of\nJenkins infrastructure to the overall success of the project. That strong and\ncontinued growth has brought new demands to our infrastructure’s design and\nimplementation, requiring the next step in its evolution. This partnership helps\nus grow with the rest of the project by unifying our existing infrastructure\nunder one comprehensive, modern and scalable platform.\n\nIn March we\ndiscussed\nthe potential partnership in our regularly scheduled\nproject\nmeeting,\nhighlighting some of the infrastructure challenges that we face:\n\nCurrently we have infrastructure in four different locations, with four\ndifferent infrastructure providers, each with their own APIs and tools for\nmanaging resources, each with varying capabilities and capacities.\n\nProject infrastructure is managed by a team of volunteers, operating\nmore than 15 different services and managing a number of additional external\nservices.\n\nOur current download/mirror network, while geographically distributed, is\nrelatively primitive and its implementation prevents us from using more modern\ndistribution best practices.\n\nIn essence, five years of tremendous growth for Jenkins has outpaced our\norganically grown, unnecessarily complex, project infrastructure. Migrating to\nAzure simplifies and improves our infrastructure in a dramatic way that would\nnot be possible without a comprehensive platform consisting of: compute, CDN,\nstorage and data-store services. Our partnership covers, at minimum, the next\nthree years of the project’s infrastructure needs, giving us a great home for\nthe future.\n\nAzure also enables a couple of projects that I\nhave long been dreaming of providing to Jenkins users and contributors:\n\nEnd-to-end TLS encrypted distribution of Jenkins packages, plugins and\nmetadata via the Azure CDN.\n\nMore complete build/test/release support and capacity on\nci.jenkins.io for plugin developers using\nAzure\nContainer Service and generic VMs.\n\nThe Jenkins infrastructure is all open\nsource which means  all of our Docker containers, Puppet code and many of our\ntools are all available on GitHub. Not\nonly can you watch the migration process to Azure as it happens, but I also\ninvite you to participate in making our project’s infrastructure better (join\nus in the #jenkins-infra channel on Freenode or our\nmailing list).\n\nSuffice it to say, I’m very excited about the bright [blue] future for the\nJenkins project and the infrastructure that powers it!","title":"Partnering with Microsoft to run Jenkins infrastructure on Azure","tags":["azure","infra","infrastructure"],"authors":[{"avatar":{"childImageSharp":null},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/author/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-05-12T00:00:00.000Z","id":"16aeac16-1357-5d74-85da-84ef71ddbaa2","slug":"/blog/2016/05/12/sf-jam-jenkins-and-azure/","strippedHtml":"A few weeks ago, my colleague Brian Dawson\nand I were invited to present on\nScaling Jenkins for\nContinuous Delivery with Microsoft Azure in Microsoft’s\nReactor space. Azure is Microsoft’s\npublic cloud offering and one of the many tools available to Jenkins users for\nadding elastic compute capacity, among other things, to their build/test/deploy\ninfrastructure. While our presentations are applicable to practically\nany cloud-based Jenkins environment, Thiago Almeida and Oguz Pastirmaci from\nMicrosoft were also on-hand and presented some interesting Azure-specific\nofferings like\nAzure\nContainer Service with Jenkins.\n\nWhile we do not have video from the meetup, Brian and I did record\na\nsession with Thiago and Oguz for Channel9\nwhich covers much of the same content:\n\nTo kick-off the meetup we asked attendees a few polling questions and\nreceived very telling responses:\n\nHow big is your Development/IT organization?\n\nWhat is your role?\n\nBy show of hands do you practice CI/CD/DevOps/etc?\n\nAt what scale (tooling and practice)?\n\nThe responses indicated that the majority of attendees were from small to medium\norganizations where they practiced Continuous Delivery across multiple teams. A\nnotable 25% or greater attendees considered themselves \"fullstack\" or\nparticipating in all of the roles of Developer, QA, and Operations. Interesting\nwhen paired with the high number (~80%) of those who practice CD.  This is\nlikely because modern teams, with mature CD practices, tend to blur the\ntraditional lines of Developer, QA and Operations. However, In my experience,\nwhile this is often the case for small to medium companies in large\norganizations team members tend to fall into the traditional roles, with CD\nproviding the practice and platform to unify teams across roles.\n\n— Brian Dawson\n\nAfter gauging the audience, Thiago and Brian reviewed Continuous Delivery (CD)\nand implementing it at scale. They highlighted the fact that CD is being rapidly\nadopted across teams and organizations, providing the ability: to deliver a demonstrably\nhigher quality product, shipping more rapidly than before, and to keep team members happier.\n\nHowever, when organizations fail to properly support CD as they scale, they run\ninto issues such as: developers acting as administrators at the cost of\nproductivity, potential lack of security and/or exposure of IP and difficulty in\nsharing best practices across teams.\n\nThiago then highlighted that properly scaling CD practices in the organization\nalong with the infrastructure itself can alleviate these issues, and discussed\nthe benefits of scaling CD to on cloud platforms to provide \"CD-as-a-Service.\"\n\nOverall I found the \"theory\" discussion to be on point, continuous delivery is\nnot just a technology nor a people problem. Successful organizations scale their\nprocesses and tooling together.\n\nThe slides from our respective presentations are linked below:\n\n(Brian) Scaling Jenkins for Continuous Delivery (.pdf)\n\n(Tyler) Scaling Jenkins with Azure (.pdf)\n\nI hope you join us at future\nSan Francisco\nJAM s!","title":"SF JAM Report: Scaling Jenkins for Continuous Delivery with Azure","tags":["jam","azure","meetup"],"authors":[{"avatar":{"childImageSharp":null},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/author/rtyler","twitter":"agentdero"}]}},{"node":{"date":"2016-05-10T00:00:00.000Z","id":"e79a5ee4-9a52-50ad-875a-d0e23f888bab","slug":"/blog/2016/05/10/jenkins-20-vjam/","strippedHtml":"Last week we hosted our first ever\nOnline JAM with the debut\ntopic of: Jenkins 2.0. Alyssa, our\nEvents officer, and I pulled together a\nseries of\nsessions focusing on some of the most notable aspects of Jenkins 2 with:\n\nA Jenkins 2.0 keynote from project founder\nKohsuke Kawaguchi\n\nAn overview of \"Pipeline as Code\" from Patrick\nWolf\n\nA deep-dive into Pipeline and related plugins like Multibranch, etc from\nJesse Glick and\nKishore Bhatia\n\nAn overview of new user experience changes in 2.0 from\nKeith Zantow\n\nA quick lightning talk about documentation by yours truly\n\nWrapping up the sessions, was Kohsuke again, talking about the road beyond\nJenkins 2.0 and what big projects he sees on the horizon.\n\nThe event was really interesting for me, and I hope informative for those who\nparticipated in the live stream and Q&A session. I look forward to hosting more\nVirtual JAM events in the future, and I hope you will\njoin us!\n\nQuestions and Answers\n\nBelow are a collection of questions and answers, that were posed during the\nVirtual JAM. Many of these were answered during the course of the sessions, but\nfor posterity all are included below.\n\nPipeline\n\nWhat kind of DSL is used behind pipeline as code? Groovy or allow freely use\ndifferent languages as a user prefer?\n\nPipeline uses a Groovy-based domain specific language.\n\nHow do you test your very own pipeline DSL?\n\nReplay helps in testing/debugging while creating pipelines and at the branch\nlevel. There are some ideas which Jesse Glick\nhas proposed for testing Jenkinsfile and Pipeline libraries captured in\nJENKINS-33925.\n\nIsn’t \"Survive Jenkins restart\" exclusive to [CloudBees] Jenkins Enterprise?\n\nNo, this feature does not need\nCloudBees\nJenkins Enterprise. All features shown\nduring the virtual JAM are free and open source. CloudBees' Jenkins Enterprise\nproduct does support restarting from a specified stage however, and that is not\nopen source.\n\nHow well is jenkins 2.0 integrate with github for tracking job definitions?\n\nUsing the\nGitHub\nOrganization Folder plugin, Jenkins can automatically detect a Jenkinsfile in\nsource repositories to create Pipeline projects.\n\nPlease make the ability for re-run failed stages Open Source too :)\n\nThis has been passed on to our friends at CloudBees for consideration :)\n\nIf Jenkinsfile is in the repo, co-located with code, does this mean Jenkins can\nauto-detect new jobs for different branches?\n\nThis is possible using the\nPipeline Multibranch plugin.\n\nWhat documentation sources are there for Pipeline?\n\nOur documentation section contains a number of pagesaround Pipeline.\nThere is also additional documentation and examples in the plugin’s\ngit repository and the\njenkinsci/pipeline-examples\nrepository. (contributions welcome!)\n\nWhere we can find the DSL method documentation?\n\nThere is generated documentation on jenkins.io which\nincldues steps from all public plugins. Inside of a running Jenkins instance,\nyou can also navigate to\nJENKINS_URL/workflow-cps-snippetizer/dslReference\nto see the documentation for the plugins which are installed in that instance.\n\nIf Pipeline is not support some plugins (there is a lot actually), I needed\nSonarQube Runner but unfortunately it’s not supported yet, in Job DSL plugin i\ncan use \"Configure Block\" and cover any plugin via XML, how i can achieve the\nsame with a Pipeline?\n\nNot at this time\n\nIs there a possibility to create custom tooltips i.e. with a quick reference or\na link to internal project documentation? Might be useful i.e. for junior team\nmembers who need to refer to external docs.\n\nNot generally. Though in the case of Pipeline global libraries, you can create\ndescriptions of vars/functions like standardBuild in the demo, and these will\nappear in Snippet Generator under Global Variables.\n\nOh pipeline supports joining jobs? It’s really good, but I cannot find document\nat https://jenkins.io/doc/ could you tell me where is it?\n\nThere is a build step, but the Pipeline system is optimized for single-job\npipelines\n\nWe have multiple projects that we would like to follow the same pipeline.  How\nwould I write a common pipeline that can be shared across multiple projects.\n\nYou may want to look at implementing some additional steps using the\nPipeline Global\nLibrary feature. This would allow you to define\norganization-specific extensions to the Pipeline DSL to abstract away common\npatterns between projects.\n\nHow much flexibility is there with creating context / setting environment\nvariables or changing / modifying build tool options when calling a web hook /\napi to parameterize pipelines for example to target deployments to different env\nusing same pipeline\n\nVarious environment variables are exposed under the env variable in the Groovy\nDSL which would allow you to construct logic as simple or as complex as\nnecessary to achieve your goal.\n\nWhen you set up the job for the first time, does it build every branch in git,\nor is there a way to stop it from building old branches?\n\nNot at this time, the best way to prevent older branches from being built is to\nremove the Jenkinsfile in those branches. Alternatively, you could use the\n\"include\" or \"exclude\" patterns when setting up the SCM configuration of your\nmultibranch Pipeline. See also\nJENKINS-32396.\n\nSimilar to GitHub organizations, will BitBucket \"projects\" (ways of organizing\ncollections of repos) be supported?\n\nYes, these are supported via the\nBitbucket\nBranch Source plugin.\n\nHow do you handle build secrets with the pipeline plugin? Using unique\ncredentials stored in the credentials plugin per project and/or branch?\n\nThis can be accomplished by using the\nCredentials\nBinding plugin.\n\nSimilar to GitHub Orgs, are Gitlab projects supported in the same way?\n\nGitLab projects are not explicitly supported at this time, but the extension\npoints which the GitHub Organization Folder plugin uses could be extended in a\nsimilar manner for GitLab. See also JENKINS-34396\n\nIs Perforce scm supported by the Pipeline plugin?\n\nAs a SCM source for discovering a Jenkinsfile, not at this time. The\nP4\nplugin does provide some p4 steps which can be used in a Pipeline script\nhowever, see here for documentation.\n\nIs Mercurial supported with multibranch?\n\nYes, it is.\n\nCan Jenkinsfile detect when it’s running against a pull request vs an approved commit, so that it can perform a different type of build?\n\nYes, via the env variables provided in the DSL scope. Using an if statement,\none could guard specific behaviors with:\n\nif (env.CHANGE_ID != null) {\n    /* do things! */\n}\n\nLet’s say I’m building RPMs with Jenkins and use build number as an RPM\nversion/release number. Is there a way to maintain build numbers and leverage\nversioning of Jenkinsfile?\n\nThrough the env variable, it’s possible to utilize env.BUILD_NUMBER or the\nSCM commit ID, etc.\n\nLove the snippet generator! Any chance of separating it out from the pipeline\ninto a separate page on its own, available in the left nav?\n\nYes, this is tracked in\nJENKINS-31831\n\nAny tips on pre-creating the admin user credential and selecting plugins to\nautomate the Jenkins install?\n\nThere are various configuration\nmanagement modules which provide parts of this functionality.\n\nI’m looking at the pipeline syntax (in Jenkins 2.0) how do I detect a\nstep([…​]) has failed and create a notification inside the Jenkinsfile?\n\nThis can be done by wrapping a step invocation with a Groovy try/catch block.\nSee also JENKINS-28119\n\nUser Interface/Experience\n\nIs the user experience same as before when we replace the Jenkins.war(1.x to\n2.x) in an existing (with security in place) installation?\n\nYou will get the new UI features like redesigned configuration forms, but the\ninitial setup wizard will be skipped. In its stead, Jenkins will offer to\ninstall Pipeline-related functionality.\n\nIs it possible to use custom defined syntax highlighting ?\n\nWithin the Pipeline script editor itself, no. It is using the\nACE editor system,\nso it may be possible for a plugin to change the color scheme used.\n\nCan you elaborate on what the Blue Ocean UI is? Is there a link or more\ninformation on it?\n\nBlue Ocean is the name of user experience an design project, unfortunately at\nthis point in time there is not more information available on it.\n\nGeneral\n\nHow well this integrate with cloud environment?\n\nThe Jenkins controller and agents can run easily in any public cloud environment\nthat supports running Java applications. Through the\nEC2,\nJClouds,\nAzure, or\nany other plugins which extend the cloud\nextension\npoint, it is possible to dynamically provision new build agents on a configured\ncloud provider.\n\nAre help texts and other labels and messages updated for other localizations /\nlanguages as well?\n\nPractically every string in Jenkins core is localizable. The extent to which those\nstrings have been translated depends on contributors by speakers of those\nlanguages to the project. If you want to contribute translations, this\nwiki\npage should get you started.\n\nAny additional WinRM/Windows remoting functionality in 2.0?\n\nNo\n\nIs there a CLI to find all the jobs created by a specific user?\n\nNo, out-of-the-box Jenkins does not keep track of which user created which jobs.\nThe functionality provided by the\nOwnership\nplugin may be of interest though.\n\nPlease consider replacing terms like \"master\" and \"slave\" with \"primary\" and\n\"secondary\".\n\n\"slave\" has been replaced with \"agent\" in Jenkins 2.0.\n\nUpdated 2020-09-18 : The term \"master\" is being replaced with \"controller\".\n\nWe’ve been making tutorial videos on Jenkins for awhile (mostly geared toward\npassing the upcoming CCJPE). Because of that we’re using 1.625.2 (since that is\nwhat is listed on the exam), but should we instead base the videos on 2.0?\n\nAs of right now all of the\nJenkins Certification work done by CloudBees is\nfocused around the Jenkins LTS 1.625.x.","title":"Jenkins 2.0 Online JAM Wrap-up","tags":["jenkins2","jam","meetup"],"authors":[{"avatar":{"childImageSharp":null},"blog":"http://unethicalblogger.com","github":"rtyler","html":"<div class=\"paragraph\">\n<p>R&#46; Tyler Croy has been part of the Jenkins project for the past seven years.\nWhile avoiding contributing any Java code, Tyler is involved in many of the\nother aspects of the project which keep it running, such as this website,\ninfrastructure, governance, etc.</p>\n</div>","id":"rtyler","irc":null,"linkedin":null,"name":"R. Tyler Croy","slug":"/blog/author/rtyler","twitter":"agentdero"}]}}]}},"pageContext":{"author":"rtyler","limit":8,"skip":16,"numPages":21,"currentPage":3}},
    "staticQueryHashes": ["3649515864"]}