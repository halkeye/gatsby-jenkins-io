{
    "componentChunkName": "component---src-templates-author-blog-list-template-js",
    "path": "/blog/author/michaelhuettermann",
    "result": {"data":{"author":{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/c09ea/michaelhuettermann.jpg","srcSet":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/534e5/michaelhuettermann.jpg 32w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/99887/michaelhuettermann.jpg 64w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/c09ea/michaelhuettermann.jpg 128w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/76fd4/michaelhuettermann.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/59a6b/michaelhuettermann.webp 32w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/cbb78/michaelhuettermann.webp 64w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/96250/michaelhuettermann.webp 128w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/50511/michaelhuettermann.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":171}}},"blog":"http://huettermann.net","github":"michaelhuettermann","html":"<div class=\"paragraph\">\n<p>Michael is expert in Continuous Delivery, DevOps and SCM/ALM supporting enterprises in implementing DevOps.\nMichael is Jenkins Ambassador.</p>\n</div>","id":"michaelhuettermann","irc":null,"linkedin":null,"name":"Michael Hüttermann","slug":"/blog/author/michaelhuettermann","twitter":"huettermann"},"allBlog":{"edges":[{"node":{"date":"2018-11-12T00:00:00.000Z","id":"19ad29c6-3758-5c7b-bc08-31222e0bfca7","slug":"/blog/2018/11/12/inspecting-binaries-with-jenkins/","strippedHtml":"In a past blog post,\nDelivery Pipelines, with Jenkins 2, SonarQube, and Artifactory,\nwe talked about pipelines which result in binaries for development versions, and in\nDelivery pipelines, with Jenkins 2: how to promote Java EE and Docker binaries toward production,\nwe examined ways to consistently promote applications toward production. In this blog post, I continue on both by discussing more details on security related quality gates\nand bringing this together with the handling of Docker images.\n\nUse case: Foster security on given, containerized business application\n\nSecurity is an overloaded term with varying meaning in different contexts. For this contribution, I consider security as the sum of rules regarding vulnerabilities\n(Common Vulnerability and Exposure, CVE), in binaries. In a past blog post, we’ve identified SonarQube already, as a very helpful tool to identify flaws\nin source code, particularly concerning reliability (bugs), vulnerabilities (security, e.g. CWE, that is common weakness enumaration, and OWASP, that is the Open Web Application Security Project), and\nmaintainability (code smells). Now it is a good time to add another tool to the chain, that is Twistlock, for inspection binaries for security issues.\nFeatures of Twistlock include\n\nCompliance and vulnerability management, transitively\n\nRuntime defense\n\nCloud-native CI/CD support\n\nBroad coverage of supported artifact types and platforms\n\nAPI, dashboards, and Jenkins integration, with strong configuration options\n\nThe underlying use case can be derived from several real-world security initiatives, in enterprises, based on given containerized applications. In practice, it is not a surprise that after adding such new\nquality gates, you identify historically grown issues. However, there are many good reasons to do so. You don’t need any Word documents to check any governance criteria manually, rather\nexecution and reporting are done automatically and also part of the actions are taken automatically. And above all, of course, your application is quality assured regarding known vulnerability issues, aligned with\nthe DevOps approach: development is interested in quick feedback whether their change would introduce any vulnerabilities, and operations is interested in insights whether and\nhow running applications are affected if a new CVE is discovered.\n\nThe term DevSecOps was coined to explicitely add security concerns to DevOps.\nIn my opinion, security is already inherent part of DevOps.\nThus, there is no strong reason to introduce a new word. Surely, new words are catchy.\nBut they have limits.\nOr have you ever experienced NoDev, the variant of DevOps where features are suddenly falling from the sky and deployed to production automatically?\n\nConceptually, container inspection is now part of the delivery pipeline and Twistlock processing is now triggered once we have produced our Docker images, see below, in order to get\nfast feedback.\n\nSoftware is staged over different environments by configuration, without rebuilding. All changes go through the entire staging process, although defined\nexception routines may be in place, for details see Michael Hüttermann, Agile ALM (Manning, 2012). The staged software consists of all artifacts which\nmake up the release, consistently, including the business application, test cases, build scripts, Chef cookbooks, Dockerfiles, Jenkins files to build all\nthat in a self-contained way, for details see Michael Hüttermann, DevOps for Developers (Apress, 2012).\n\nThis blog post covers sample tools. Please note, that there are also alternative tools available, and the best target architecture is aligned with concrete requirements and given basic\nconditions. Besides that, the sample toolchain is derived from couple of real world success stories, designed and implemented in the field. However, this blog post\nsimplifies and abstracts them in order to stay focussed while discussing the primitives of delivery units. For example, aggregating multiple Docker images with ASCII files, does not change the\nunderlying primitives and their handlings. For more information on all parts of the blog post, please consult the respective documentation, good books or attend fine conferences. Or go to the extremes: talk to your colleagues.\n\nIn our sample process, we produce a web application that is packaged in a Docker image. The produced Docker images are distributed only if the dedicated quality gate passes.\nA quality gate is a stage in the overall pipeline and a sum of defined commitments, often\ncalled requirements, the unit of work must pass. In our case, the quality gate comprises inspection of produced binaries and it fails if vulnerabilities of severity 'critical' are found.\nWe can configure Twistlock according to our requirements. Have a look how we’ve integrated it into our Jenkins pipeline, with focus on detecting vulnerabilities.\n\nJenkinsfile (excerpt): Twistlock inspection triggered\n\nstage('Twistlock: Analysis') { (1)\nString version = readFile('version.properties').trim() (2)\nprintln \"Scanning for version: ${version}\"\n    twistlockScan ca: '', cert: '', compliancePolicy: 'critical', \\\n        dockerAddress: 'unix:///var/run/docker.sock', \\\n        ignoreImageBuildTime: false, key: '', logLevel: 'true', \\\n        policy: 'critical', repository: 'huttermann-docker-local.jfrog.io/michaelhuettermann/alpine-tomcat7', \\ (3)\nrequirePackageUpdate: false, tag: \"$version\", timeout: 10\n}\n\nstage('Twistlock: Publish') { (4)\nString version = readFile('version.properties ').trim()\n    println \"Publishing scan results for version: ${version}\"\n    twistlockPublish ca: '', cert: '', \\\n        dockerAddress: 'unix:///var/run/docker.sock', key: '', \\\n        logLevel: 'true', repository: 'huttermann-docker-local.jfrog.io/michaelhuettermann/alpine-tomcat7', tag: \"$version\", \\\n        timeout: 10\n}\n\n1\nTwistlock inspection as part of the sequence of stages in Jenkinsfile\n\n2\nNailing down the version of the to be inspected image, dynamically\n\n3\nConfiguring analysis including vulnerability severity level\n\n4\nPublishing the inspection results to Twistlock console, that is the dashboard\n\nNow let’s start with the first phase to bring our application in shape again, that is gaining insight about the security related flaws.\n\nPhase 1: Gain insights about security related flaws\n\nAfter we’ve introduced the new quality gate, it failed, see image above. As integration with other tools, Jenkins is the automation engine and does provide helpful context information,\nhowever, those cannot replace features and data the dedicated, triggered tool does offer. Thus, this is the moment to switch to the dedicated tool, that is Twistlock. Opening\nthe dashboard, we can navigate to the Jenkins build jobs, that is the specific run of the build, and the respective results of the Twistlock analysis. What we see now is a list\nof vulnerabilities, and we need to fix those of severity critical in order to pass the quality gate, and get our changes again toward production. The list shows entries of\ntype jar, that is a finding in a binary as part of the Docker image, in our case the WAR file we’ve deployed to a web container (Tomcat), and of type OS, those are issues of the underlying image itself, the\noperating system, either part of the base image, or as a package added/changed in our Dockerfile.\n\nWe can now easily zoom in and examine the vulnerabilities of the Docker layers. This really helps to structure work and identify root causes. Since, typically,\na Docker image extends a Docker base image, the findings in the base image are shown on the top, see next screenshot, grouped by severity.\n\nOther Docker layers were added to the base image, and those can add vulnerabilities too. In our case, the packaged WAR file obviously contains a vulnerability. The next image shows how we examine that finding, while this time\nexpanding the Twistlock wizard (that is the plus sign) to directly see the list of found vulnerabilities.\n\nFinding and visualizing the issues are a very good first step, and we’ve even made those findings actionable, so we now have to take action and address them.\n\nPhase 2: Address the findings\n\nTo address the findings, we need to split our initiative into two parts:\n\nFixing the critical vulnerabilities related to the Docker image (in our case largely the base image)\n\nFixing the critical vulnerabilities related to the embedded deployment unit (in our case the WAR)\n\nLet’s proceed bottom up, first coping with the Docker base image.\n\nThis is an easy example covering multiple scenarios particularly identifying and fixing vulnerabilities in transitive binaries, i.e. binaries contained in\nother binaries, e.g. a Docker image containing a WAR file that in turn contains libraries. To expand this vertical feasibility spike, you can easily add\nmore units of each layer, or add more abstractions, however, the idea can always be nailed down to the primitives, covered in this blog post.\n\nLet’s now have a look at the used Docker image by looking at the used Dockerfile.\n\nDockerfile: The Dockerfile based on Alpine, running OpenJDK 8\n\nFROM openjdk:8-jre-alpine (1)\nLABEL maintainer \"michael@huettermann.net\"\n\n# Domain of your Artifactory. Any other storage and URI download link works, just change the ADD command, see below.\nARG ARTI\nARG VER\n\n# Expose web port\nEXPOSE 8080\n\n# Tomcat Version\nENV TOMCAT_VERSION_MAJOR 9 (2)\nENV TOMCAT_VERSION_FULL  9.0.6\n\n# Download, install, housekeeping\nRUN apk add --update curl &&\\ (3)\napk add bash &&\\\n  #apk add -u libx11 &&\\ (4)\nmkdir /opt &&\\\n  curl -LO ${ARTI}/list/generic-local/apache/org/tomcat/tomcat-${TOMCAT_VERSION_MAJOR}/v${TOMCAT_VERSION_FULL}/bin/apache-tomcat-${TOMCAT_VERSION_FULL}.tar.gz &&\\\n  gunzip -c apache-tomcat-${TOMCAT_VERSION_FULL}.tar.gz | tar -xf - -C /opt &&\\\n  rm -f apache-tomcat-${TOMCAT_VERSION_FULL}.tar.gz &&\\\n  ln -s /opt/apache-tomcat-${TOMCAT_VERSION_FULL} /opt/tomcat &&\\\n  rm -rf /opt/tomcat/webapps/examples /opt/tomcat/webapps/docs &&\\\n  apk del curl &&\\\n  rm -rf /var/cache/apk/*\n\n# Download and deploy the Java EE WAR\nADD http://${ARTI}/list/libs-release-local/com/huettermann/web/${VER}/all-${VER}.war /opt/tomcat/webapps/all.war (5)\n\nRUN chmod 755 /opt/tomcat/webapps/*.war\n\n# Set environment\nENV CATALINA_HOME /opt/tomcat\n\n# Start Tomcat on startup\nCMD ${CATALINA_HOME}/bin/catalina.sh run\n\n1\nBase image ships OpenJDK 8, on Alpine\n\n2\nDefined version of web container\n\n3\nApplying some defined steps to configure Alpine, according to requirements\n\n4\nUpdating package itself would address one vulnerability already\n\n5\nDeploying the application\n\nBy checking available versions of the official OpenJDK Alpine image, we see that there’s a newer version 8u181 which we could use.\nWe can zoom in and study release notes and contents, or we just pragmatically switch the base image to a more recent version. Often it is a good idea\nto upgrade versions regularly, in defined intervals. This leads to the following change in the Dockerfile.\n\nDockerfile (excerpt): The Dockerfile based on Alpine, running OpenJDK 8u181\n\nFROM openjdk:8u181-jre-alpine (1)\nLABEL maintainer \"michael@huettermann.net\"\n\n1\nBase image is now OpenJDK 8u181, on Alpine\n\nThere are more options available to fix the issues, but let’s proceed to the second part, the vulnerabilities in the deployment unit.\n\nBefore we push this change to GitHub, we also address the vulnerability issue in the deployment unit, that is jetty-io. Here we are a bit unsure about\nwhy, in this specific use case, the library is used. To retrieve more information about dependencies, we run a dependency:tree command on our Maven\nbased project. We now see that jetty-io is transitively referenced by org.seleniumhq.selenium:htmlunit-driver. We can surely discuss why this is a compile\ndependency and the libraries are shipped as part of the WAR, but let’s consider this to be given according to requirements, thus we must take special attention now\nto version 2.29.0 of the specific library.\n\nAlso here we can browse release notes and content (particularly how those libs are built themselves), and come to the conclusion to\nswitch from the used version, that is 2.29.0, to a newer version of htmlunit-driver, that is 2.31.1.\n\npom.xml (excerpt): Build file\n\n(1)\n\norg.seleniumhq.selenium\nselenium-java\n3.14.0\n\norg.seleniumhq.selenium (2)\nhtmlunit-driver\n2.31.1\n\njunit\njunit\n4.7\n\n1\nPart of the underlying POM defining dependencies\n\n2\nDefinition of the dependency, causing the vulnerability finding; we use a newer version now\n\nOK, now we are done. We push the changes to GitHub, and our GitHub webhook directly triggers the workflow. This time the quality gate passes, so it\nlooks like our fixes did address the root causes and eliminated those with the configured threshold severity.\n\nFinally, after running through our entire workflow, that is made up of different pipelines, our inspected and quality assured container does successfully\nrun in our production runtime environment, that is on Oracle Cloud.\n\nCrisp, isn’t it?\n\nSummary\n\nThis closes our quick walkthrough of how to inject security related quality gates into a Jenkins based delivery pipeline.\nWe’ve discussed some concepts and how this can look like with sample tools.\nIn the center of our efforts, we used Jenkins, the swiss army knife of automation.\nWe enriched our ecosystem by integrating couple of platforms and tools, above all Twistlock.\nAfter this tasty appetizer you are ready to assess your own delivery pipelines,\nconcepts and tools, and to possibly invest even more attention to security.\n\nReferences\n\n'Agile ALM', Manning, 2011\n\n'DevOps for Developers', Apress, 2012\n\nDocker, the standard to develop and ship set of changes\n\nDocker images, shipping OpenJDK\n\nOracle Cloud Infrastructure, for containers\n\nAlpine Linux\n\nSonarQube, the language/platform agnostic Continuous Inspection tool\n\nTwistlock, the container security platform\n\nSources on GitHub\n\nASCII, commonly used standard to work on primitives, such as Docker (and their aggregations)\n\nCommon Vulnerabilities and Exposures\n\nHolistic pipelines, Live 15-minute Jenkins Demos, Part 1, on YouTube\n\nHolistic pipelines, Live 15-minute Jenkins Demos, Part 2, on YouTube\n\nDelivery Pipelines, with Jenkins 2, SonarQube, and Artifactory\n\nDelivery pipelines, with Jenkins 2: how to promote Java EE and Docker binaries toward production","title":"The Silence of the Lambs: Inspecting binaries with Jenkins","tags":["devops","devsecops","security","vulnerabilities","compliance","twistlock"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/c09ea/michaelhuettermann.jpg","srcSet":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/534e5/michaelhuettermann.jpg 32w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/99887/michaelhuettermann.jpg 64w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/c09ea/michaelhuettermann.jpg 128w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/76fd4/michaelhuettermann.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/59a6b/michaelhuettermann.webp 32w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/cbb78/michaelhuettermann.webp 64w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/96250/michaelhuettermann.webp 128w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/50511/michaelhuettermann.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":171}}},"blog":"http://huettermann.net","github":"michaelhuettermann","html":"<div class=\"paragraph\">\n<p>Michael is expert in Continuous Delivery, DevOps and SCM/ALM supporting enterprises in implementing DevOps.\nMichael is Jenkins Ambassador.</p>\n</div>","id":"michaelhuettermann","irc":null,"linkedin":null,"name":"Michael Hüttermann","slug":"/blog/author/michaelhuettermann","twitter":"huettermann"}]}},{"node":{"date":"2017-07-05T00:00:00.000Z","id":"fd2d8b89-b05f-5caf-a6b1-923d33960065","slug":"/blog/2017/07/05/continuousdelivery-devops-artifactory/","strippedHtml":"This is a guest post by Michael Hüttermann. Michael is an expert\nin Continuous Delivery, DevOps and SCM/ALM. More information about him at huettermann.net, or\nfollow him on Twitter: @huettermann.\n\nIn a past blog post Delivery Pipelines,\nwith Jenkins 2, SonarQube, and Artifactory, we talked about pipelines which result in binaries for development versions. Now, in this blog post, I zoom in to different parts of the\nholistic pipeline and cover the handling of possible downstream steps once you have the binaries of development versions, in our example a Java EE WAR and a Docker image (which contains the WAR).\nWe discuss basic concept of staging software, including further information about quality gates, and show example toolchains. This contribution particularly examines the staging from binaries from\ndev versions to release candidate versions and from release candidate versions to final releases from the perspective of the automation server Jenkins, integrating with the binary\nrepository manager JFrog Artifactory and the distribution management platform JFrog Bintray, and ecosystem.\n\nStaging software\n\nStaging (also often called promoting) software is the process of completely and consistently transferring a release with all its configuration items\nfrom one environment to another. This is even more true with DevOps, where you want to accelerate the cycle time (see Michael Hüttermann, DevOps for Developers (Apress, 2012), 38ff).\nFor accelerating the cycle time, meaning to bring software to production, fast and in good quality, it is crucial to have fine processes and integrated tools to streamline the\ndelivery of software. The process of staging releases consists of deploying software to different staging levels, especially different test environments.\nStaging also involves configuring the software for various environments without needing to recompile or rebuild the software. Staging is necessary\nto transport the software to production systems in high quality. Many Agile projects make great experience with implementing a staging ladder in\norder to optimize the cycle time between development software and the point when the end user is able to use the software in production.\n\nCommonly, the staging ladder is illustrated on its side, with the higher rungs being the boxes further to the right. It’s good practice not to skip any rungs during staging.\nThe central development environment packages and integrates all respective configuration items and is the base for releasing. Software is staged over different environments by\nconfiguration, without rebuilding. All changes go through the entire staging process, although defined exception routines may be in place,\nfor details see Michael Hüttermann, Agile ALM (Manning, 2012).\n\nTo make concepts clearer, this blog post covers sample tools. Please note, that there are also alternative tools available. As one example: Sonatype Nexus is also able to host the covered binaries and also offers scripting functionality.\n\nWe nowadays often talk about delivery pipelines. A pipeline is just a set of stages and transition rules between those stages. From a DevOps perspective, a pipeline bridges multiple\nfunctions in organizations, above all development and operations. A pipeline is a staging ladder. A change enters the pipeline at the beginning and leaves it at the end. The processing\ncan be triggered automatically (typical for delivery pipelines) or by a human actor (typical for special steps at overall pipelines, e.g. pulling and thus cherry-picking specific\nversions to promote them to be release candidates are final releases).\n\nPipelines often look different, because they strongly depend on requirements and basic conditions, and can contain further sub pipelines. In our scenario, we have two sub pipelines to\nmanage the promotion of continuous dev versions to release candidates and the promotion of release candidates to final release. A change typically waits at a stage for further processing\naccording to the transition rules, aligned with defined requirements to meet, which are the Quality Gates, explored next.\n\nQuality Gates\n\nQuality gates allow the software to pass through stages only if it meets their defined requirements. The next illustration shows a staging ladder with quality gates injected. You and\nother engaged developers commit code to the version control system (please, use VCS as an abbreviation, not SCM, because the latter is much more) in order to update the central test\nenvironment only if the code satisfies the defined quality requirements; for instance, the local build may need to run successfully and have all tests pass locally. Build, test, and\nmetrics should pass out of the central development environment, and then automated and manual acceptance tests are needed to pass the system test. In our case, the last quality gate\nto pass is the one from the  production mirror to production. Here, for example, specific production tests are done or relevant documents must be filled in and signed.\n\nIt’s mandatory to define the quality requirements in advance and to resist customizing them after the fact, when the software has failed. Quality gates are different at lower and\nhigher stages; the latter normally consist of a more severe or broader set of quality requirements, and they often include the requirements of the lower gates. The binary repository\nmanager must underpin corresponding quality gates, while managing the binaries, what we cover next.\n\nThis blog post illustrates typical concepts and sample toolchains. For more information, please consult the respective documentation, good books or attend top notch conferences, e.g.\nJenkins World, powered by CloudBees.\n\nBinary repository manager\n\nA central backbone of the staging ladder is the binary repository manager, e.g. JFrog Artifactory. The binary repository manager manages all binaries including the self-produced\nones (producing view) and the 3rd party ones (consuming view), across all artifact types, in our case a Java EE WAR file and a Docker image. Basic idea here is that the repo manager serves\nas a proxy, thus all developers access the repo manager, and not remote binary pools directly, e.g. Maven Central. The binary repository manager offers cross-cutting services,\ne.g. role-based access control on specific logical repositories, which may correspond to specific stages of the staging ladder.\n\nLogical repositories can be generic ones (meaning they are agnostic regarding any tools and platforms, thus you can also just upload the menu of your local canteen) or repos\nspecific to tools and platforms. In our case, we need a repository for managing the Java EE WAR files and for the Docker images. This can be achieved by\n\na generic repository (preferred for higher stages) or a repo which is aligned with the layout of the Maven build tool, and\n\na repository for managing Docker images, which serves as a Docker registry.\n\nIn our scenario, preparing the staging of artifacts includes the following ramp-up activities\n\nCreating two sets of logical repositories, inside JFrog Artifactory, where each set has a repo for the WAR file and a repo for the Docker image, and one set is for managing dev\nversions and one set is for release candidate versions.\n\nDefining and implementing processes to promote the binaries from the one set of repositories (which is for dev versions) to the other set of repositories (which is for RC versions).\nPart of the process is defining roles, and JFrog Artifactory helps you to implement role-based access control.\n\nSetting up procedures or scripts to bring binaries from one set of repositories to the other set of repositories, reproducibly. Adding meta data to binaries is important if the degree of maturity\nof the binary cannot be easily derived from the context.\n\nThe following illustration shows a JFrog Artifactory instance with the involved logical repos in place. In our simplified example, the repo promotions are supposed to go from\ndocker-local to docker-prod-local, and from libs-release-local to libs-releases-staging-local. In our use case, we promote the software in version 1.0.0.\n\nAnother type of binary repository manager is JFrog Bintray, which serves as a universal distribution platform for many technologies. JFrog Bintray can be an interesting choice\nif you have strong requirements for scalability and worldwide coverage including IP restrictions and handy features around statistics. Most of the concepts and ramp up activities\n are similar compared to JFrog Artifactory, thus I do not want to repeat them here. Bintray is used by lot of projects e.g. by Groovy, to host their deliverables in the public.\n But keep in mind that you can of course also host your release binaries in JFrog Artifactory.\n In this blog post, I’d like to introduce different options, thus we promote our release candidates to JFrog Artifactory and our releases to JFrog Bintray.\n Bintray has the concept of products, packages and versions. A product can have multiple packages and has different versions. In our example, the product has two packages, namely the Java EE WAR and\n the Docker image, and the concrete version that will be processed is 1.0.0.\n\nSome tool features covered in this blog post are available as part of commercial offerings of tool vendors. Examples include the Docker support of JFrog Artifactory or the Firehose Event API of JFrog Bintray.\nPlease consult the respective documentation for more information.\n\nNow it is time to have a deeper look at the pipelines.\n\nImplementing Pipelines\n\nOur example pipelines are implemented with Jenkins, including its Blue Ocean and declarative pipelines facilities, JFrog Artifactory and JFrog Bintray. To derive your personal\npipelines, please check your individual requirements and basic conditions to come up with the best solution for your target architecture, and consult the respective documentation for\n more information, e.g. about scripting the tools.\n\nIn case your development versions are built with Maven, and have SNAPSHOT character, you need to either rebuild the software after setting the release version, as part of\nyour pipeline, or you solely use Maven releases from the very beginning. Many projects make great experience with morphing Maven snapshot versions into\nrelease versions, as part of the pipeline, by using a dedicated Maven plugin, and externalizing it into a Jenkins shared library. This can look like the following:\n\nsl.groovy (excerpt): A Jenkins shared library, to include in Jenkins pipelines.\n\n#!/usr/bin/groovy\n    def call(args) { (1)\necho \"Calling shared library, with ${args}.\"\n       sh \"mvn com.huettermann:versionfetcher:1.0.0:release versions:set -DgenerateBackupPoms=false -f ${args}\" (2)\n}\n\n1\nWe provide a global variable/function to include it in our pipelines.\n\n2\nThe library calls a Maven plugin, which dynamically morphs the snapshot version of a Maven project to a release version.\n\nAnd including it into the pipeline is then also very straight forward:\n\npipeline.groovy (excerpt): A stage calling a Jenkins shared library.\n\nstage('Produce RC') { (1)\nreleaseVersion 'all/pom.xml' (2)\n}\n\n1\nThis stage is part of a scripted pipeline and is dedicated to morphing a Maven snapshot version into a release version, dynamically.\n\n2\nWe call the Jenkins shared library, with a parameter pointing to the Maven POM file, which can be a parent POM.\n\nYou can find the code of the underlying Maven plugin here.\n\nLet’s now discuss how to proceed for the release candidates.\n\nRelease Candidate (RC)\n\nThe pipeline to promote a dev version to a RC version does contain a couple of different stages, including stages to certify the binaries (meaning labeling it or adding context information) and stages to process the concrete promotion.\nThe following illustration shows the successful run of the promotion, for software version 1.0.0.\n\nWe utilize Jenkins Blue Ocean that is a new user experience for Jenkins based on a personalizable, modern design that allows users to graphically create, visualize and diagnose\ndelivery pipelines. Besides the new approach in general, single Blue Ocean features help to boost productivity dramatically, e.g. to provide log information at your fingertips\nand the ability to search pipelines. The stages to perform the promote are as follows starting with the  Jenkins pipeline stage for promoting the WAR file. Keep in mind that all\nscripts are parameterized, including variables for versions and Artifactory domain names, which are either injected to the pipeline run by user input or set system wide in the Jenkins admin panel,\nand the underlying call is using the JFrog command line interface, CLI in short. JFrog Artifactory\nas well as JFrog Bintray can be used and managed by scripts, based on a REST API. The JFrog CLI\nis an abstraction on top of the JFrog REST API, and we show sample usages of both.\n\npipeline.groovy (excerpt): Staging WAR file to different logical repository\n\nstage('Promote WAR') { (1)\nsteps { (2)\nsh 'jfrog rt cp --url=https://$ARTI3 --apikey=$artifactory_key --flat=true libs-release-local/com/huettermann/web/$version/ ' + (3)\n'libs-releases-staging-local/com/huettermann/web/$version/'\n       }\n    }\n\n1\nThe dedicated stage for running the promotion of the WAR file.\n\n2\nHere we have the steps which make up the stage, based on Jenkins declarative pipeline syntax.\n\n3\nCopying the WAR file, with JFrog CLI, using variables, e.g. the domain name of the Artifactory installation. Many options available, check the docs.\n\nThe second stage to explore more is the promotion of the Docker image. Here, I want to show you a different way how to achieve the goal, thus in this use case we utilize the JFrog REST API.\n\npipeline.grovvy (excerpt): Promote Docker image\n\nstage('Promote Docker Image') {\n          sh '''curl -H \"X-JFrog-Art-Api:$artifactory_key\" -X POST https://$ARTI3/api/docker/docker-local/v2/promote ''' + (1)\n'''-H \"Content-Type:application/json\" ''' + (2)\n'''-d \\'{\"targetRepo\" : \"docker-prod-local\", \"dockerRepository\" : \"michaelhuettermann/tomcat7\", \"tag\": \"\\'$version\\'\", \"copy\": true }\\' (3)\n'''\n    }\n\n1\nThe shell script to perform the staging of Docker image is based on JFrog REST API.\n\n2\nPart of parameters are sent in JSON format.\n\n3\nThe payload tells the REST API endpoint what to to, i.e. gives information about target repo and tag.\n\nOnce the binaries are promoted (and hopefully deployed and tested on respective environments before), we can promote them to become final releases, which I like to call GA.\n\nGeneral Availability (GA)\n\nIn our scenario, JFrog Bintray serves as the distribution platform to manage and provide binaries for further usage. Bintray can also serve as a Docker registry, or can just\nprovide binaries for scripted or manual download. There are again different ways how to promote binaries, in this case from the RC repos inside JFrog Artifactory to the GA storage in JFrog Bintray, and I summarize one of those possible ways. First, let’s look at the Jenkins pipeline, showed in the next illustration. The processing is on its way, currently, and we again have a list of linked stages.\n\nZooming in now to the key stages, we see that promoting the WAR file is a set of steps that utilize JFrog REST API. We download the binary from JFrog Artifactory, parameterized,\nand upload it to JFrog Bintray.\n\npipeline.groovy (excerpt): Promote WAR to Bintray\n\nstage('Promote WAR to Bintray') {\n       steps {\n          sh '''\n             curl -u michaelhuettermann:${bintray_key} -X DELETE https://api.bintray.com/packages/huettermann/meow/cat/versions/$version (1)\ncurl -u michaelhuettermann:${bintray_key} -H \"Content-Type: application/json\" -X POST https://api.bintray.com/packages/huettermann/meow/cat/$version --data \"\"\"{ \"name\": \"$version\", \"desc\": \"desc\" }\"\"\" (2)\ncurl -T \"$WORKSPACE/all-$version-GA.war\" -u michaelhuettermann:${bintray_key} -H \"X-Bintray-Package:cat\" -H \"X-Bintray-Version:$version\" https://api.bintray.com/content/huettermann/meow/ (3)\ncurl -u michaelhuettermann:${bintray_key} -H \"Content-Type: application/json\" -X POST https://api.bintray.com/content/huettermann/meow/cat/$version/publish --data '{ \"discard\": \"false\" }' (4)\n'''\n       }\n    }\n\n1\nFor testing and demo purposes, we remove the existing release version.\n\n2\nNext we create the version in Bintray, in our case the created version is 1.0.0. The value was insert by user while triggering the pipeline.\n\n3\nThe upload of the WAR file.\n\n4\nBintray needs a dedicated publish step to make the binary publicly available.\n\nProcessing the Docker image is as easy as processing the WAR. In this case, we just push the Docker image to the Docker registry, which is served by JFrog Bintray.\n\npipeline.groovy (excerpt): Promote Docker image to Bintray\n\nstage('Promote Docker Image to Bintray') { (1)\nsteps {\n          sh 'docker push $BINTRAYREGISTRY/michaelhuettermann/tomcat7:$version' (2)\n}\n    }\n\n1\nThe stage for promoting the Docker image. Please note, depending on your setup, you may add further stages, e.g. to login to your Docker registry.\n\n2\nThe Docker push of the specific version. Note, that also here all variables are parameterized.\n\nWe now have promoted the binaries and uploaded them to JFrog Bintray. The overview page of our product lists two packages: the WAR file and the Docker image. Both can be downloaded\nnow and used, the Docker image can be pulled from the JFrog Bintray Docker registry with native Docker commands.\n\nAs part of its graphical visualization capabilitites, Bintray is able to show the single layers of the uploaded Docker images.\n\nBintray can also display usage statistics, e.g. download details. Now guess where I’m sitting right now while downloading the binary?\n\nBesides providing own statistics, Bintray provides the JFrog Firehose Event API. This API streams live usage data, which in turn can be integrated or aggregated with your ecosystem.\nIn our case, we visualize the data, particularly download, upload, and delete statistics, with the ELK stack, as part of a functional monitoring initiative.\n\nCrisp, isn’t it?\n\nSummary\n\nThis closes are quick ride through the world of staging binaries, based on Jenkins. We’ve discussed concepts and example DevOps enabler tools, which can help to implement\n the concepts. Along the way, we discussed some more options how to integrate with ecosystem, e.g. releasing Maven snapshots and functional monitoring with dedicated tools.\n After this appetizer you may want to now consider to double-check your staging processes and toolchains, and maybe you find some room for further adjustments.\n\nReferences\n\n'Agile ALM', Manning, 2011\n\nBinary Repository Manager Feature Matrix\n\n'DevOps for Developers', Apress, 2012\n\nDocker\n\nELK\n\nJFrog Artifactory\n\nJFrog Bintray\n\nJFrog CLI\n\nJFrog REST API\n\nSonatype Nexus","title":"Delivery pipelines, with Jenkins 2: how to promote Java EE and Docker binaries toward production.","tags":["devops","jenkins","artifactory","bintray"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/c09ea/michaelhuettermann.jpg","srcSet":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/534e5/michaelhuettermann.jpg 32w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/99887/michaelhuettermann.jpg 64w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/c09ea/michaelhuettermann.jpg 128w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/76fd4/michaelhuettermann.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/59a6b/michaelhuettermann.webp 32w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/cbb78/michaelhuettermann.webp 64w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/96250/michaelhuettermann.webp 128w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/50511/michaelhuettermann.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":171}}},"blog":"http://huettermann.net","github":"michaelhuettermann","html":"<div class=\"paragraph\">\n<p>Michael is expert in Continuous Delivery, DevOps and SCM/ALM supporting enterprises in implementing DevOps.\nMichael is Jenkins Ambassador.</p>\n</div>","id":"michaelhuettermann","irc":null,"linkedin":null,"name":"Michael Hüttermann","slug":"/blog/author/michaelhuettermann","twitter":"huettermann"}]}},{"node":{"date":"2017-04-18T00:00:00.000Z","id":"714ef576-2af6-5d69-a880-a3280836f4e4","slug":"/blog/2017/04/18/continuousdelivery-devops-sonarqube/","strippedHtml":"This is a guest post by Michael Hüttermann. Michael is an expert\nin Continuous Delivery, DevOps and SCM/ALM. More information about him at huettermann.net, or\nfollow him on Twitter: @huettermann.\n\nContinuous Delivery and DevOps are well known and widely spread practices nowadays. It is commonly accepted that it\nis crucial to form great teams and define shared goals first and then choose and integrate the tools fitting best to\ngiven tasks. Often it is a mashup of lightweight tools, which are integrated to build up Continuous Delivery pipelines\nand underpin DevOps initiatives. In this blog post, we zoom in to an important part of the overall pipeline, that is the discipline\noften called Continuous Inspection, which comprises inspecting code and injecting a quality gate on that, and show how artifacts can\nbe uploaded after the quality gate was met. DevOps enabler tools covered are Jenkins, SonarQube, and Artifactory.\n\nThe Use Case\n\nYou already know that quality cannot be injected after the fact, rather it should be part of the process and product from the very beginning.\nAs a commonly used good practice, it is strongly recommended to inspect the code and make findings visible, as soon as possible.\nFor that SonarQube is a great choice. But SonarQube is not just running on any isolated\nisland, it is integrated in a Delivery Pipeline. As part of the pipeline, the code is inspected, and only if the code is fine according to defined\nrequirements, in other words: it meets the quality gates, the built artifacts are uploaded to the binary repository manager.\n\nLet’s consider the following scenario. One of the busy developers has to fix code, and checks in changes to the central\nversion control system. The day was long and the night short, and against all team commitments the developer\ndid not check the quality of the code in the local sandbox. Luckily, there is the build engine Jenkins\nwhich serves as a single point of truth, implementing the Delivery Pipeline with its native pipeline features, and as a handy coincidence\nSonarQube has support for Jenkins pipeline.\n\nThe change triggers a new run of the pipeline. Oh no! The build pipeline broke, and the change is not further processed.\nIn the following image you see that a defined quality gate was missed. The visualizing is done with Jenkins Blue Ocean.\n\nSonarQube inspection\n\nWhat is the underlying issue? We can open the SonarQube web application and drill down to the finding. In the Java code, obviously a string literal is not placed on the right side.\n\nDuring a team meeting it was decided to define this to be a Blocker, and SonarQube was configured accordingly. Furthermore, a SonarQube quality gate was created to break any build, if a blocker was identified. Let’s now quickly look into the code.\nYes, SonarQube is right, there is the issue with the following code snippet.\n\nWe do not want to discuss in detail all used tools, and also covering the complete Jenkins build job would be out of scope.\nBut the interesting extract here in regard of the inspection is the following stage defined in Jenkins pipeline DSL:\n\nconfig.xml: SonarQube inspection\n\nstage('SonarQube analysis') { (1)\nwithSonarQubeEnv('Sonar') { (2)\nsh 'mvn org.sonarsource.scanner.maven:sonar-maven-plugin:3.3.0.603:sonar ' + (3)\n'-f all/pom.xml ' +\n          '-Dsonar.projectKey=com.huettermann:all:master ' +\n          '-Dsonar.login=$SONAR_UN ' +\n          '-Dsonar.password=$SONAR_PW ' +\n          '-Dsonar.language=java ' +\n          '-Dsonar.sources=. ' +\n          '-Dsonar.tests=. ' +\n          '-Dsonar.test.inclusions=**/*Test*/** ' +\n          '-Dsonar.exclusions=**/*Test*/**'\n        }\n    }\n\n1\nThe dedicated stage for running the SonarQube analysis.\n\n2\nAllow to select the SonarQube server you want to interact with.\n\n3\nRunning and configuring the scanner, many options available, check the docs.\n\nMany options are available to integrate and configure SonarQube. Please consult the documentation for alternatives. Same applies to the other covered tools.\n\nSonarQube Quality Gate\n\nAs part of a Jenkins pipeline stage, SonarQube is configured to run and inspect the code. But this is just the first part,\nbecause we now also want to add the quality gate in order to break the build. The next stage is covering exactly that, see\nnext snippet. The pipeline is paused until the quality gate is computed, specifically the waitForQualityGate step will pause the\npipeline until SonarQube analysis is completed and returns the quality gate status. In case a quality gate was missed, the build breaks.\n\nconfig.xml: SonarQube Quality Gate\n\nstage(\"SonarQube Quality Gate\") { (1)\ntimeout(time: 1, unit: 'HOURS') { (2)\ndef qg = waitForQualityGate() (3)\nif (qg.status != 'OK') {\n             error \"Pipeline aborted due to quality gate failure: ${qg.status}\"\n           }\n        }\n    }\n\n1\nThe defined quality gate stage.\n\n2\nA timeout to define when to proceed without waiting for any results for ever.\n\n3\nHere we wait for the OK. Underlying implementation is done with SonarQube’s webhooks feature.\n\nThis blog post is an appetizer, and scripts are excerpts. For more information, please consult the respective documentation, or a good book, or the great community, or ask your local expert.\n\nSince they all work in a wonderful Agile team, the next available colleague just promptly fixes the issue. After checking in\nthe fixed code, the build pipeline runs again.\n\nThe pipeline was processed successfully, including the SonarQube quality gate, and as the final step, the packaged and tested artifact was\ndeployed to Artifactory. There are a couple of different flexible ways how to upload the artifacts,\nthe one we use here is using an upload spec to actually collect and upload the artifact which was built at the very beginning of the pipeline.\nAlso meta information are published to Artifactory, since it is the context which matters and thus we can add valuable labels to the artifact for further processing.\n\nconfig.xml: Upload to Artifactory\n\nstage ('Distribute binaries') { (1)\ndef SERVER_ID = '4711' (2)\ndef server = Artifactory.server SERVER_ID\n    def uploadSpec = (3)\"\"\"\n    {\n    \"files\": [\n        {\n            \"pattern\": \"all/target/all-(*).war\",\n            \"target\": \"libs-snapshots-local/com/huettermann/web/{1}/\"\n        }\n      ]\n    }\n    \"\"\"\n    def buildInfo = Artifactory.newBuildInfo() (4)\nbuildInfo.env.capture = true (5)\nbuildInfo=server.upload(uploadSpec) (6)\nserver.publishBuildInfo(buildInfo) (7)\n}\n\n1\nThe stage responsible for uploading the binary.\n\n2\nThe server can be defined Jenkins wide, or as part of the build step, as done here.\n\n3\nIn the upload spec, in JSON format, we define what to deploy to which target, in a fine-grained way.\n\n4\nThe build info contains meta information attached to the artifact.\n\n5\nWe want to capture environmental data.\n\n6\nUpload of artifact, according to upload spec.\n\n7\nBuild info are published as well.\n\nNow let’s see check that the binary was deployed to Artifactory, successfully. As part of the context information, also a reference to the\nproducing Jenkins build job is available for better traceability.\n\nSummary\n\nIn this blog post, we’ve discovered tips and tricks to integrate Jenkins with SonarQube, how to define\nJenkins stages with the Jenkins pipeline DSL, how those stages are visualized with Jenkins Blue Ocean, and how the artifact\nwas deployed to our binary repository manager Artifactory.\nNow I wish you a lot of further fun with your great tools of choice to implement your Continuous Delivery pipelines.\n\nReferences\n\nJenkins 2\n\nSonarqube\n\nSonarqube Jenkins plugin\n\nArtifactory\n\nJenkins Artifactory plugin\n\n'DevOps for Developers', Apress, 2012\n\n'Agile ALM', Manning, 2011","title":"Delivery Pipelines, with Jenkins 2, SonarQube, and Artifactory","tags":["quality","sonarqube","jenkins","artifactory"],"authors":[{"avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/c09ea/michaelhuettermann.jpg","srcSet":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/534e5/michaelhuettermann.jpg 32w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/99887/michaelhuettermann.jpg 64w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/c09ea/michaelhuettermann.jpg 128w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/76fd4/michaelhuettermann.jpg 256w","sizes":"(min-width: 128px) 128px, 100vw"},"sources":[{"srcSet":"/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/59a6b/michaelhuettermann.webp 32w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/cbb78/michaelhuettermann.webp 64w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/96250/michaelhuettermann.webp 128w,\n/gatsby-jenkins-io/static/f049e8f2aa656a7f976c387e735fa4a3/50511/michaelhuettermann.webp 256w","type":"image/webp","sizes":"(min-width: 128px) 128px, 100vw"}]},"width":128,"height":171}}},"blog":"http://huettermann.net","github":"michaelhuettermann","html":"<div class=\"paragraph\">\n<p>Michael is expert in Continuous Delivery, DevOps and SCM/ALM supporting enterprises in implementing DevOps.\nMichael is Jenkins Ambassador.</p>\n</div>","id":"michaelhuettermann","irc":null,"linkedin":null,"name":"Michael Hüttermann","slug":"/blog/author/michaelhuettermann","twitter":"huettermann"}]}}]}},"pageContext":{"author":"michaelhuettermann","limit":8,"skip":0,"numPages":1,"currentPage":1}},
    "staticQueryHashes": ["3649515864"]}